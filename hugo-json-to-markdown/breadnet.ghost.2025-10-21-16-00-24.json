{"db":[{"meta":{"exported_on":1761062424531,"version":"5.40.2"},"data":{"benefits":[],"custom_theme_settings":[{"id":"6425f07d192c0c150413d52c","theme":"casper","key":"navigation_layout","type":"select","value":"Logo on cover"},{"id":"6425f07d192c0c150413d52d","theme":"casper","key":"title_font","type":"select","value":"Modern sans-serif"},{"id":"6425f07d192c0c150413d52e","theme":"casper","key":"body_font","type":"select","value":"Elegant serif"},{"id":"6425f07d192c0c150413d52f","theme":"casper","key":"show_publication_cover","type":"boolean","value":"true"},{"id":"6425f07d192c0c150413d530","theme":"casper","key":"header_style","type":"select","value":"Center aligned"},{"id":"6425f07d192c0c150413d531","theme":"casper","key":"feed_layout","type":"select","value":"Classic"},{"id":"6425f07d192c0c150413d532","theme":"casper","key":"color_scheme","type":"select","value":"Light"},{"id":"6425f07d192c0c150413d533","theme":"casper","key":"post_image_style","type":"select","value":"Wide"},{"id":"6425f07d192c0c150413d534","theme":"casper","key":"email_signup_text","type":"text","value":"Sign up for more like this."},{"id":"6425f07d192c0c150413d535","theme":"casper","key":"show_recent_posts_footer","type":"boolean","value":"true"}],"newsletters":[{"id":"6425f076192c0c150413d32f","uuid":"fabcb6a0-7a8a-4d88-aa56-bb83254ddd6a","name":"breadNET","description":null,"feedback_enabled":0,"slug":"default-newsletter","sender_name":"Bradley","sender_email":null,"sender_reply_to":"newsletter","status":"active","visibility":"members","subscribe_on_signup":1,"sort_order":0,"header_image":null,"show_header_icon":1,"show_header_title":1,"title_font_category":"sans_serif","title_alignment":"center","show_feature_image":1,"body_font_category":"sans_serif","footer_content":null,"show_badge":0,"show_header_name":0,"show_post_title_section":1,"show_comment_cta":1,"show_subscription_details":0,"show_latest_posts":0,"created_at":"2023-03-30T20:26:30.000Z","updated_at":"2023-03-30T20:59:50.000Z"}],"offer_redemptions":[],"offers":[],"posts":[{"id":"6425f3bc192c0c150413d58b","uuid":"fd5da009-075a-4a59-b526-838e4f2edea9","title":"Stuff I find cool","slug":"stuff-i-find-cool","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"hr\",{}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/06/xoa.png\",\"caption\":\"\",\"alt\":\"xcp-ng logo\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/06/mino-1.jpg\",\"caption\":\"\",\"alt\":\"Minio logo\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/06/do.png\",\"alt\":\"Digital ocean Logo\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/06/51KORkIMXqL.png\",\"alt\":\"Jellyfin\"}],[\"hr\",{}]],\"markups\":[[\"a\",[\"href\",\"https://www.upwork.com/freelancers/~01c61ee9802b94133e\"]],[\"a\",[\"href\",\"mailto:work@breadnet.co.uk\"]]],\"sections\":[[10,0],[10,1],[1,\"p\",[[0,[],0,\"I'm not sure if this would come under software but this is what runs my servers. XCP-NG is the hypervisor and xen orchestra is the orchestration service I use. It's got an API called 'xo-cli' and is terraform (woo!) compatible. You only need one xoa server to manage a whole data centre worth of servers (physical hosts)\"]]],[10,2],[1,\"p\",[[0,[],0,\"Minio is a self hosted s3 compatible object storage with a web interface and a client called mc which makes admin work a breeze. Some times it can be a real pain to get started with if you've never used s3 storage before.\"]]],[1,\"p\",[]],[10,3],[1,\"p\",[[0,[],0,\"Well, not really software but they are who I host all my external services with.\"]]],[10,4],[1,\"p\",[[0,[],0,\"An Emby fork designed to be user friendly and not steal your data. Which is important to me as I don't like being sold as a product. It can run on docker or bare metal. I have mine somewhere in London as a docker with an s3 as a back end. (post incoming)\"]]],[10,5],[1,\"p\",[[0,[],0,\"You can hire me via \"],[0,[0],1,\"Upwork\"],[0,[],0,\" or \"],[0,[1],1,\"emailing me\"],[0,[],0,\" for weekend projects!\"]]]],\"ghostVersion\":\"3.0\"}","lexical":null,"html":"<hr><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/06/xoa.png\" class=\"kg-image\" alt=\"xcp-ng logo\" loading=\"lazy\"></figure><p>I'm not sure if this would come under software but this is what runs my servers. XCP-NG is the hypervisor and xen orchestra is the orchestration service I use. It's got an API called 'xo-cli' and is terraform (woo!) compatible. You only need one xoa server to manage a whole data centre worth of servers (physical hosts)</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/06/mino-1.jpg\" class=\"kg-image\" alt=\"Minio logo\" loading=\"lazy\"></figure><p>Minio is a self hosted s3 compatible object storage with a web interface and a client called mc which makes admin work a breeze. Some times it can be a real pain to get started with if you've never used s3 storage before.</p><p></p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/06/do.png\" class=\"kg-image\" alt=\"Digital ocean Logo\" loading=\"lazy\"></figure><p>Well, not really software but they are who I host all my external services with.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/06/51KORkIMXqL.png\" class=\"kg-image\" alt=\"Jellyfin\" loading=\"lazy\"></figure><p>An Emby fork designed to be user friendly and not steal your data. Which is important to me as I don't like being sold as a product. It can run on docker or bare metal. I have mine somewhere in London as a docker with an s3 as a back end. (post incoming)</p><hr><p>You can hire me via <a href=\"https://www.upwork.com/freelancers/~01c61ee9802b94133e\">Upwork</a> or <a href=\"mailto:work@breadnet.co.uk\">emailing me</a> for weekend projects!</p>","comment_id":"5eda7bfb235f3b3c70a4eb7f","plaintext":"I'm not sure if this would come under software but this is what runs my servers. XCP-NG is the hypervisor and xen orchestra is the orchestration service I use. It's got an API called 'xo-cli' and is terraform (woo!) compatible. You only need one xoa server to manage a whole data centre worth of servers (physical hosts)\n\nMinio is a self hosted s3 compatible object storage with a web interface and a client called mc which makes admin work a breeze. Some times it can be a real pain to get started with if you've never used s3 storage before.\n\n\n\nWell, not really software but they are who I host all my external services with.\n\nAn Emby fork designed to be user friendly and not steal your data. Which is important to me as I don't like being sold as a product. It can run on docker or bare metal. I have mine somewhere in London as a docker with an s3 as a back end. (post incoming)\n\nYou can hire me via Upwork or emailing me for weekend projects!","feature_image":"https://images.unsplash.com/photo-1494599948593-3dafe8338d71?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=2000&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2020-06-05T17:08:11.000Z","updated_at":"2021-05-02T01:48:21.000Z","published_at":"2020-06-05T18:10:00.000Z","custom_excerpt":"Cool sheeeeeeet","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"6425f3bc192c0c150413d58c","uuid":"d6dcee22-7f0e-4ff9-b403-d8dc32ca387a","title":"What I'm running","slug":"what-im-running","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"image\",{\"src\":\"https://lh5.googleusercontent.com/XAPS6nULuTmhGaHC7RuES25PCLfOIBRjd_7HtRANUJrkLR9LL6Xce8dJNxqVw1mNCsvzENRDl5tAUUxpTfCqZcAjlp1pNBJlZQsHr0_bkbEN0W4LjI0=w1280\",\"alt\":\"\",\"title\":\"\"}],[\"image\",{\"src\":\"https://lh3.googleusercontent.com/Il4pC2DJSA9x560nvPcXFdmzXnkcjR4lnlY6i4BQqxZwjoI_8L4OBh3EkWXvvbozRaaMi3ec6FEBmb_7CWT0346fqzpAZnfkIVqJCVd_QkwchOBqSg=w1280\",\"alt\":\"\",\"title\":\"\"}],[\"image\",{\"src\":\"https://lh4.googleusercontent.com/YnxjBvCN5qrTW4zEESgGvZPeknWCqR8BPj0vfSqVOrbI24z_gDqM6FSR2qazD1I3M7_JfZnV6SU48GS0JAwetY_yYA72znBqQK8gzJKsm5znOknbzQ=w1280\",\"alt\":\"\",\"title\":\"\"}],[\"image\",{\"src\":\"https://lh6.googleusercontent.com/ABp5kC-WPMbIEHlOTpKoZDm-rggMis3UsRxsWoF-xxSQYEIl7EFIcxSAjl2voS0KBHYiaPeP1MDo_yss5Ow0A4XLp0Q3kVXeIMzo2mcqOIWuVjWCoxw=w1280\",\"alt\":\"\",\"title\":\"\"}],[\"image\",{\"src\":\"https://lh4.googleusercontent.com/ZnJS5SPIUaDYldCQ46tLZfgFCtNQKdTsI_0UABLKQupUYKFF7eipix4MK10vUYuCcRSmmBMGQ1p7ImIuzS8hriG3lWq_1Dhr5y2-5mTUH7sL4CsUgg=w1280\",\"alt\":\"\",\"title\":\"\"}],[\"image\",{\"src\":\"https://lh5.googleusercontent.com/u3ViM-3nkpKF0aLW6S-NgvMs3UoByTS88I8AYlb_MCRi0dxLYvX8oybZLYg2mL-osOdHsOx6EMJ7UipqSP8YaE4x2cYQDJjdCyBpJ-bfHuGWjj8qMg=w1280\",\"alt\":\"\",\"title\":\"\"}],[\"image\",{\"src\":\"https://lh6.googleusercontent.com/ghCKGzhrPTG_CmB9GrudtEaGVgmEYTOiMXJaQdu2DIZVnqFQMe90aRJqXgNoLaT12esEgVJLuqhX01IM4WnLThTneiGxvHBgPbAFCHSQrH8ccXPIWGA=w1280\",\"alt\":\"\",\"title\":\"\"}],[\"image\",{\"src\":\"https://lh5.googleusercontent.com/zD9KmDxsB4yxD7tSAvWfGgXnYyQrYL65r1Ip3xEUvun0Gpyh6A0x31bD4ymqvgW-YJLi6yQ44qxbMV38qNX11_03KpmSFhLmjVXC6I324hiradkaunc=w1280\",\"alt\":\"\",\"title\":\"\"}],[\"image\",{\"src\":\"https://lh3.googleusercontent.com/V9imIVZxj2wcSTac6d2nrDxEkUP6bNE4JH0OJllYCE1rRrHZRGe_SNZvQEKJE1G3nLg9fSykSi_CcOJECRmP3kTz0unUqysZsm-G7DNgM-P53yDWyQg=w1280\",\"alt\":\"\",\"title\":\"\"}],[\"image\",{\"src\":\"https://lh4.googleusercontent.com/2pGT3PFBVXCzogsS68HJgHQo4DbFLgBVN36mISdDJ1JC6BzrkxrRZA8Z6COz_0M-_WkvH5TZhMDLMJSVAhXpycJNwo5670Hlo9WeFG7UGErnkVOx3w=w1280\",\"alt\":\"\",\"title\":\"\"}],[\"image\",{\"src\":\"https://lh5.googleusercontent.com/bEHuIzfp3uNsUmwMJPYrIqEW1wWUmldZcyOu0DriywL07-L3Vahif7QJARQIgyarFzhTxYw_wpEdtgYTpCq2yb2rsB-ZJvs7BaiCl9d0dmIpukCZwCYV=w1280\",\"alt\":\"\",\"title\":\"\"}]],\"markups\":[[\"a\",[\"href\",\"https://www.google.com/url?q=https%3A%2F%2Fxcp-ng.org&sa=D&sntz=1&usg=AFQjCNHpemVlzMiyZtgC6GhF_G6v1rq7rQ\"]],[\"a\",[\"href\",\"https://status.breadnet.co.uk/?pk_campaign=BreadnetMain&pk_kwd=https%3A%2F%2Fbreadnet.co.uk%2Fwhat-im-running%2F\"]],[\"a\",[\"href\",\"https://documentation.breadnet.co.uk/?pk_campaign=BreadnetMain&pk_kwd=https%3A%2F%2Fbreadnet.co.uk%2Fwhat-im-running%2F\"]],[\"a\",[\"href\",\"https://www.youtube.com/watch?v=dQw4w9WgXcQ&pk_campaign=BreadnetMain&pk_kwd=mailServer\"]],[\"a\",[\"href\",\"http://www.google.com/url?q=http%3A%2F%2Freleases.ubuntu.com%2F18.04%2F&sa=D&sntz=1&usg=AFQjCNH3s1iSgARCKCp3uTjMvN2c83Vb2w\"]]],\"sections\":[[1,\"p\",[]],[1,\"p\",[[0,[],0,\"All my servers are currently (and always will be)  running Linux. My choice is mainly Ubuntu LTS how ever there are 2 Debian VM's running.\"]]],[1,\"p\",[[0,[],0,\"I have got my backups of VM's done over XOA which copies the full image of the VM to an external hard drive.\"]]],[1,\"p\",[[0,[],0,\"For file based backups as well as database backups, I have writted my own scripts to back up servers to a Wasabi bucket. This is done for, you guessed it, Databases as well a media servers as it doest not make sense to backup 300gb vm easch week when nothing changes\"]]],[1,\"p\",[[0,[],0,\"Server: Dell R710\"]]],[1,\"p\",[[0,[],0,\"Use: Hypervisor\"]]],[1,\"p\",[[0,[],0,\"OS: \"],[0,[0],1,\"XCP-NG\"],[0,[],0,\" (centos basically)\"]]],[1,\"p\",[[0,[],0,\"VM's:\"]]],[1,\"p\",[[0,[],0,\"1: Firefly-iii\"],[1,[],0,0],[0,[],0,\"2: Grafana\"],[1,[],0,1],[0,[],0,\"3: phpipam\"],[1,[],0,2],[0,[],0,\"4: invoiceninja\"],[1,[],0,3],[0,[],0,\"5: JellyFin\"],[1,[],0,4],[0,[],0,\"6: Librenms\"],[1,[],0,5],[0,[],0,\"7: Nextcloud\"],[1,[],0,6],[0,[],0,\"8: Database server\"],[1,[],0,7],[0,[],0,\"9: Unifi\"],[1,[],0,8],[0,[],0,\"10: Xen Orchestra\"],[1,[],0,9],[0,[],0,\"11: Zabbix\"],[1,[],0,10],[0,[],0,\"12: Bookstack\"],[1,[],0,11],[0,[],0,\"13: postfix and dovecot with dkim\"],[1,[],0,12],[0,[],0,\"14: Leantime Project managment\"],[1,[],0,13],[0,[],0,\"15: Status page\"],[1,[],0,14],[0,[],0,\"16: Bind dns\"],[1,[],0,15],[0,[],0,\"17: Nginx Reverse proxy\"],[1,[],0,16],[0,[],0,\"18: Kanboard\"]]],[1,\"p\",[[1,[],0,17]]],[1,\"p\",[[0,[],0,\"I have a few sites that are publicly accessible:\"]]],[3,\"ul\",[[[0,[1],1,\"Status page\"],[0,[],0,\" \"]],[[0,[2],1,\"Bookstack\"],[0,[],0,\" (update 2023: Migrated to mkdocs)\"]],[[0,[3],1,\"email server\"],[0,[],0,\" \"]]]],[1,\"p\",[[0,[],0,\"All the names link to their respective project. They are all FOSS and run on linux\"]]],[1,\"p\",[[1,[],0,18]]],[1,\"p\",[[0,[],0,\"Second server (Bottom one)\"]]],[1,\"p\",[[0,[],0,\"Server: R710\"]]],[1,\"p\",[[0,[],0,\"OS: \"],[0,[4],1,\"Ubuntu 18.04 LTS\"]]],[1,\"p\",[[0,[],0,\"Use: In house Min.io server\"]]],[1,\"p\",[[0,[],0,\"Screenshot of XOA\"]]],[1,\"blockquote\",[[0,[],0,\"I just want to add that I know the photos are broken - it's in my to do list\"]]],[10,0],[1,\"p\",[]],[10,1],[1,\"p\",[[0,[],0,\"Xcp-ng host\"]]],[10,2],[1,\"p\",[[0,[],0,\"Firefly login screen\"]]],[10,3],[1,\"p\",[[0,[],0,\"Jellyfin Home screen\"]]],[10,4],[1,\"p\",[[0,[],0,\"Unifi dashboard\"]]],[10,5],[1,\"p\",[[0,[],0,\"Nextcloud - Login page\"]]],[1,\"p\",[]],[10,6],[1,\"p\",[[0,[],0,\"Database server - Phpmyadmin\"]]],[10,7],[1,\"p\",[]],[10,8],[1,\"p\",[[0,[],0,\"Leantime\"],[1,[],0,19],[0,[],0,\"This is the server I use to manage my projects and a built in kanboard.\"]]],[10,9],[1,\"p\",[[0,[],0,\"Digital ocean, this is where I have my public facing servers\"]]],[10,10],[1,\"h1\",[]]],\"ghostVersion\":\"3.0\"}","lexical":null,"html":"<p></p><p>All my servers are currently (and always will be)  running Linux. My choice is mainly Ubuntu LTS how ever there are 2 Debian VM's running.</p><p>I have got my backups of VM's done over XOA which copies the full image of the VM to an external hard drive.</p><p>For file based backups as well as database backups, I have writted my own scripts to back up servers to a Wasabi bucket. This is done for, you guessed it, Databases as well a media servers as it doest not make sense to backup 300gb vm easch week when nothing changes</p><p>Server: Dell R710</p><p>Use: Hypervisor</p><p>OS: <a href=\"https://www.google.com/url?q=https%3A%2F%2Fxcp-ng.org&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNHpemVlzMiyZtgC6GhF_G6v1rq7rQ\">XCP-NG</a> (centos basically)</p><p>VM's:</p><p>1: Firefly-iii<br>2: Grafana<br>3: phpipam<br>4: invoiceninja<br>5: JellyFin<br>6: Librenms<br>7: Nextcloud<br>8: Database server<br>9: Unifi<br>10: Xen Orchestra<br>11: Zabbix<br>12: Bookstack<br>13: postfix and dovecot with dkim<br>14: Leantime Project managment<br>15: Status page<br>16: Bind dns<br>17: Nginx Reverse proxy<br>18: Kanboard</p><p><br></p><p>I have a few sites that are publicly accessible:</p><ul><li><a href=\"https://status.breadnet.co.uk/?pk_campaign=BreadnetMain&amp;pk_kwd=https%3A%2F%2Fbreadnet.co.uk%2Fwhat-im-running%2F\">Status page</a> </li><li><a href=\"https://documentation.breadnet.co.uk/?pk_campaign=BreadnetMain&amp;pk_kwd=https%3A%2F%2Fbreadnet.co.uk%2Fwhat-im-running%2F\">Bookstack</a> (update 2023: Migrated to mkdocs)</li><li><a href=\"https://www.youtube.com/watch?v=dQw4w9WgXcQ&amp;pk_campaign=BreadnetMain&amp;pk_kwd=mailServer\">email server</a> </li></ul><p>All the names link to their respective project. They are all FOSS and run on linux</p><p><br></p><p>Second server (Bottom one)</p><p>Server: R710</p><p>OS: <a href=\"http://www.google.com/url?q=http%3A%2F%2Freleases.ubuntu.com%2F18.04%2F&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNH3s1iSgARCKCp3uTjMvN2c83Vb2w\">Ubuntu 18.04 LTS</a></p><p>Use: In house Min.io server</p><p>Screenshot of XOA</p><blockquote>I just want to add that I know the photos are broken - it's in my to do list</blockquote><figure class=\"kg-card kg-image-card\"><img src=\"https://lh5.googleusercontent.com/XAPS6nULuTmhGaHC7RuES25PCLfOIBRjd_7HtRANUJrkLR9LL6Xce8dJNxqVw1mNCsvzENRDl5tAUUxpTfCqZcAjlp1pNBJlZQsHr0_bkbEN0W4LjI0=w1280\" class=\"kg-image\" alt loading=\"lazy\"></figure><p></p><figure class=\"kg-card kg-image-card\"><img src=\"https://lh3.googleusercontent.com/Il4pC2DJSA9x560nvPcXFdmzXnkcjR4lnlY6i4BQqxZwjoI_8L4OBh3EkWXvvbozRaaMi3ec6FEBmb_7CWT0346fqzpAZnfkIVqJCVd_QkwchOBqSg=w1280\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>Xcp-ng host</p><figure class=\"kg-card kg-image-card\"><img src=\"https://lh4.googleusercontent.com/YnxjBvCN5qrTW4zEESgGvZPeknWCqR8BPj0vfSqVOrbI24z_gDqM6FSR2qazD1I3M7_JfZnV6SU48GS0JAwetY_yYA72znBqQK8gzJKsm5znOknbzQ=w1280\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>Firefly login screen</p><figure class=\"kg-card kg-image-card\"><img src=\"https://lh6.googleusercontent.com/ABp5kC-WPMbIEHlOTpKoZDm-rggMis3UsRxsWoF-xxSQYEIl7EFIcxSAjl2voS0KBHYiaPeP1MDo_yss5Ow0A4XLp0Q3kVXeIMzo2mcqOIWuVjWCoxw=w1280\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>Jellyfin Home screen</p><figure class=\"kg-card kg-image-card\"><img src=\"https://lh4.googleusercontent.com/ZnJS5SPIUaDYldCQ46tLZfgFCtNQKdTsI_0UABLKQupUYKFF7eipix4MK10vUYuCcRSmmBMGQ1p7ImIuzS8hriG3lWq_1Dhr5y2-5mTUH7sL4CsUgg=w1280\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>Unifi dashboard</p><figure class=\"kg-card kg-image-card\"><img src=\"https://lh5.googleusercontent.com/u3ViM-3nkpKF0aLW6S-NgvMs3UoByTS88I8AYlb_MCRi0dxLYvX8oybZLYg2mL-osOdHsOx6EMJ7UipqSP8YaE4x2cYQDJjdCyBpJ-bfHuGWjj8qMg=w1280\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>Nextcloud - Login page</p><p></p><figure class=\"kg-card kg-image-card\"><img src=\"https://lh6.googleusercontent.com/ghCKGzhrPTG_CmB9GrudtEaGVgmEYTOiMXJaQdu2DIZVnqFQMe90aRJqXgNoLaT12esEgVJLuqhX01IM4WnLThTneiGxvHBgPbAFCHSQrH8ccXPIWGA=w1280\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>Database server - Phpmyadmin</p><figure class=\"kg-card kg-image-card\"><img src=\"https://lh5.googleusercontent.com/zD9KmDxsB4yxD7tSAvWfGgXnYyQrYL65r1Ip3xEUvun0Gpyh6A0x31bD4ymqvgW-YJLi6yQ44qxbMV38qNX11_03KpmSFhLmjVXC6I324hiradkaunc=w1280\" class=\"kg-image\" alt loading=\"lazy\"></figure><p></p><figure class=\"kg-card kg-image-card\"><img src=\"https://lh3.googleusercontent.com/V9imIVZxj2wcSTac6d2nrDxEkUP6bNE4JH0OJllYCE1rRrHZRGe_SNZvQEKJE1G3nLg9fSykSi_CcOJECRmP3kTz0unUqysZsm-G7DNgM-P53yDWyQg=w1280\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>Leantime<br>This is the server I use to manage my projects and a built in kanboard.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://lh4.googleusercontent.com/2pGT3PFBVXCzogsS68HJgHQo4DbFLgBVN36mISdDJ1JC6BzrkxrRZA8Z6COz_0M-_WkvH5TZhMDLMJSVAhXpycJNwo5670Hlo9WeFG7UGErnkVOx3w=w1280\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>Digital ocean, this is where I have my public facing servers</p><figure class=\"kg-card kg-image-card\"><img src=\"https://lh5.googleusercontent.com/bEHuIzfp3uNsUmwMJPYrIqEW1wWUmldZcyOu0DriywL07-L3Vahif7QJARQIgyarFzhTxYw_wpEdtgYTpCq2yb2rsB-ZJvs7BaiCl9d0dmIpukCZwCYV=w1280\" class=\"kg-image\" alt loading=\"lazy\"></figure><h1></h1>","comment_id":"5eda80d9235f3b3c70a4ebb9","plaintext":"All my servers are currently (and always will be)  running Linux. My choice is mainly Ubuntu LTS how ever there are 2 Debian VM's running.\n\nI have got my backups of VM's done over XOA which copies the full image of the VM to an external hard drive.\n\nFor file based backups as well as database backups, I have writted my own scripts to back up servers to a Wasabi bucket. This is done for, you guessed it, Databases as well a media servers as it doest not make sense to backup 300gb vm easch week when nothing changes\n\nServer: Dell R710\n\nUse: Hypervisor\n\nOS: XCP-NG (centos basically)\n\nVM's:\n\n1: Firefly-iii\n2: Grafana\n3: phpipam\n4: invoiceninja\n5: JellyFin\n6: Librenms\n7: Nextcloud\n8: Database server\n9: Unifi\n10: Xen Orchestra\n11: Zabbix\n12: Bookstack\n13: postfix and dovecot with dkim\n14: Leantime Project managment\n15: Status page\n16: Bind dns\n17: Nginx Reverse proxy\n18: Kanboard\n\n\n\n\nI have a few sites that are publicly accessible:\n\n * Status page\n * Bookstack (update 2023: Migrated to mkdocs)\n * email server\n\nAll the names link to their respective project. They are all FOSS and run on linux\n\n\n\n\nSecond server (Bottom one)\n\nServer: R710\n\nOS: Ubuntu 18.04 LTS\n\nUse: In house Min.io server\n\nScreenshot of XOA\n\nI just want to add that I know the photos are broken - it's in my to do list\n\n\n\nXcp-ng host\n\nFirefly login screen\n\nJellyfin Home screen\n\nUnifi dashboard\n\nNextcloud - Login page\n\n\n\nDatabase server - Phpmyadmin\n\n\n\nLeantime\nThis is the server I use to manage my projects and a built in kanboard.\n\nDigital ocean, this is where I have my public facing servers\n\n\n","feature_image":"https://images.unsplash.com/photo-1551703599-6b3e8379aa8c?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=2000&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2020-06-05T17:28:57.000Z","updated_at":"2023-11-24T14:20:12.000Z","published_at":"2020-06-05T17:35:25.000Z","custom_excerpt":"Finally, the question you never had about my stuff has been answered","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"6425f3bc192c0c150413d58d","uuid":"84b5e670-e489-437e-b45f-aa9eee99eabd","title":"Installing your_spotify","slug":"your-spotify-2020","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"bookmark\",{\"url\":\"__GHOST_URL__/your-spotify-2020\",\"metadata\":{\"url\":\"__GHOST_URL__/your-spotify-2020/\",\"title\":\"Installing your_spotify\",\"description\":\"Your_spotify is a web application running on docker showing you stats about your account in the past 24 hours as well as current.\",\"author\":\"Bradley Stannard\",\"publisher\":\"breadNET\",\"thumbnail\":\"https://images.unsplash.com/photo-1532354058425-ba7ccc7e4a24?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=2000&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ\",\"icon\":\"https://breadnet.co.uk/favicon.png\"}}],[\"hr\",{}],[\"bookmark\",{\"url\":\"https://github.com/Yooooomi/your_spotify\",\"metadata\":{\"url\":\"https://github.com/Yooooomi/your_spotify\",\"title\":\"Yooooomi/your_spotify\",\"description\":\"Self hosted Spotify tracking dashboard. Contribute to Yooooomi/your_spotify development by creating an account on GitHub.\",\"author\":\"Yooooomi\",\"publisher\":\"GitHub\",\"thumbnail\":\"https://avatars1.githubusercontent.com/u/17204739?s=400&v=4\",\"icon\":\"https://github.githubassets.com/favicons/favicon.svg\"}}],[\"bookmark\",{\"url\":\"https://www.reddit.com/r/selfhosted/comments/fjjw0j/yourspotify/\",\"metadata\":{\"url\":\"https://www.reddit.com/r/selfhosted/comments/fjjw0j/yourspotify/\",\"title\":\"r/selfhosted - YourSpotify\",\"description\":\"84 votes and 36 comments so far on Reddit\",\"author\":null,\"publisher\":\"reddit\",\"thumbnail\":\"https://external-preview.redd.it/pWsf1-4nzsxAO9oN7whpPEyHEF3xY0NI_ggA3sakpdo.jpg?auto=webp&s=c5c9eb469e5369a2096718fbd20b8b2d9dd0b27c\",\"icon\":\"https://www.redditstatic.com/desktop2x/img/favicon/android-icon-192x192.png\"}}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/06/68747470733a2f2f692e696d6775722e636f6d2f77624f687030462e706e67.png\",\"alt\":\"Pages\"}],[\"code\",{\"code\":\"stannardb@bread-d1:~/github$ git clone git@github.com:Yooooomi/your_spotify.git\",\"language\":\"bash\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/06/image-2.png\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/06/image-3.png\"}],[\"code\",{\"code\":\"version: \\\"3\\\"\\n\\nservices:\\n  app:\\n    image: yooooomi/your_spotify_server\\n    container_name: express-mongo\\n    restart: always\\n    ports:\\n      - \\\"8080:8080\\\"\\n    links:\\n      - mongo\\n    depends_on:\\n      - mongo\\n    environment:\\n      - API_ENDPOINT=http://localhost:8080 # This MUST be included as a valid URL in the spotify dashboard\\n      - CLIENT_ENDPOINT=http://localhost:3000\\n      - SPOTIFY_PUBLIC=__your_spotify_client_id__\\n      - SPOTIFY_SECRET=__your_spotify_secret__\\n      - CORS=http://localhost:3000,http://localhost:3001\\n      #- CORS=all\\n      #- MONGO_ENDPOINT=mongodb://mongo:27017/your_spotify\\n  mongo:\\n    container_name: mongo_spotify\\n    image: mongo\\n    volumes:\\n      - ./your_spotify_db:/data/db\\n\\n  web:\\n    image: yooooomi/your_spotify_client\\n    container_name: web\\n    restart: always\\n    ports:\\n      - \\\"3000:3000\\\"\\n    environment:\\n      - API_ENDPOINT=http://localhost:8080\",\"language\":\"yaml\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/06/image-4.png\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/06/image-5.png\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/06/image-6.png\"}],[\"hr\",{}]],\"markups\":[[\"a\",[\"href\",\"https://github.com/Yooooomi/your_spotify\"]],[\"a\",[\"href\",\"https://bookstack.breadnet.co.uk/books/kb-articles/page/docker-intro-and-notes\"]],[\"a\",[\"href\",\"https://developer.spotify.com/dashboard/applications\"]],[\"strong\"],[\"code\"],[\"em\"],[\"a\",[\"href\",\"https://github.com/Yooooomi/your_spotify/issues?q=is%3Aissue+\"]],[\"a\",[\"href\",\"https://www.upwork.com/freelancers/~01c61ee9802b94133e\"]],[\"a\",[\"href\",\"mailto:work@breadnet.co.uk\"]]],\"sections\":[[1,\"p\",[]],[1,\"p\",[[0,[],0,\"This page is now outdated.\"]]],[10,0],[10,1],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"I came across this beautiful project by someone called Timothee Boussus or better known as \"],[0,[0],1,\"Yooooomi\"],[0,[],0,\" on Github. This project displays stats about your spotify account like recently played, most listened to artists in near real time. \"]]],[10,2],[10,3],[10,4],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"You will need to have some knowledge of docker. I have a quick write up \"],[0,[1],1,\"here\"],[0,[],0,\" on how to install it on Linux. \"]]],[1,\"p\",[[0,[],0,\"Once your docker is up and running, we will need to gitclone this to our computer. I prefer to keep all my github projects in one folder. Where you put it is up to you.\"]]],[10,5],[1,\"p\",[[0,[],0,\"Once that's downloaded, we can go to \"],[0,[2],1,\"developer.spotify.com\"],[0,[],0,\" and create a new application. \"]]],[1,\"p\",[[0,[],0,\"Login and click 'Create an app'\"]]],[10,6],[1,\"p\",[[0,[],0,\"Name your app as you like. I called mine 'Docker' because #originality Answer the questions and make up something on what it does. I just wrote docker again. Sign away your first born and family name.\"]]],[10,7],[1,\"p\",[[0,[],0,\"Keep this page open as we will need it again.\"]]],[1,\"p\",[[0,[],0,\"I had some issues with the config file so you can use mine below. It's based off the original except I have had to change the name of the mongo container. \"]]],[1,\"p\",[[0,[],0,\"Save this file as docker-compose.yml\"]]],[10,8],[1,\"p\",[[0,[],0,\"On the spotify webpage, replace the \"],[0,[3,4],1,\"your_spotify_client_id\"],[0,[],1,\" \"],[0,[],0,\"with the public key and then the same thing for the secret. Obviously using the secret. \"]]],[1,\"p\",[[0,[],0,\"Next go back to spotify and click edit. \"]]],[10,9],[1,\"p\",[[0,[],0,\"In here scroll down till you find redirect links:\"]]],[10,10],[1,\"p\",[[0,[],0,\"I had some issues the first time so I slapped both these in there. If you're hosting this on a publically accessable server, you will put the address such as:\"]]],[1,\"p\",[[0,[4],1,\"https://spotify.breadnet.co.uk/oauth/spotify/callback\"]]],[1,\"p\",[[0,[],0,\"just append \"],[0,[4],1,\"/oauth/spotify/callback\"],[0,[],0,\" to the end of the address to access this.\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"Now we can start actually using the application. In the same folder as your \"],[0,[4],1,\"docker-compose.yml\"],[0,[],0,\" file run \"],[0,[4],1,\"docker-compose up\"],[0,[],0,\" and it should whirr away. it took mine around 5 minutes the first time.\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"Depending on where this is being hosted, you can go to \"],[0,[4],1,\"http://localhost:3000\"],[0,[],0,\" or where ever you've got it. If you're going to have this publicly hosted, use a reverse proxy like nginx or traefik\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"Once running, click register and make a username and password up:\"]]],[10,11],[1,\"p\",[[0,[],0,\"then login. Once loggedin you'll be presented asking to connect to spotify. Click and login. \"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"Give it around 20 minutes for good measure and you \"],[0,[5],1,\"should\"],[0,[],0,\" start to see stats being generated.\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"If you're having issues, check the \"],[0,[6,6],1,\"g\"],[0,[],1,\"ithub\"],[0,[],0,\" issues or the reddit post from the top! \"]]],[1,\"blockquote\",[[0,[],0,\"A quick update. if you see that you cant login to the app, check all the docker containers are running. I found that after rebooting one of the containers had not started up and thus I was unable to login\"]]],[10,12],[1,\"p\",[[0,[],0,\"You can hire me via \"],[0,[7],1,\"Upwork\"],[0,[],0,\" or \"],[0,[8],1,\"emailing me\"],[0,[],0,\" for weekend projects!\"]]]],\"ghostVersion\":\"3.0\"}","lexical":null,"html":"<p></p><p>This page is now outdated.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"__GHOST_URL__/your-spotify-2020\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Installing your_spotify</div><div class=\"kg-bookmark-description\">Your_spotify is a web application running on docker showing you stats about your account in the past 24 hours as well as current.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://breadnet.co.uk/favicon.png\" alt=\"\"><span class=\"kg-bookmark-author\">breadNET</span><span class=\"kg-bookmark-publisher\">Bradley Stannard</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://images.unsplash.com/photo-1532354058425-ba7ccc7e4a24?ixlib&#x3D;rb-1.2.1&amp;q&#x3D;80&amp;fm&#x3D;jpg&amp;crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;w&#x3D;2000&amp;fit&#x3D;max&amp;ixid&#x3D;eyJhcHBfaWQiOjExNzczfQ\" alt=\"\"></div></a></figure><hr><p></p><p>I came across this beautiful project by someone called Timothee Boussus or better known as <a href=\"https://github.com/Yooooomi/your_spotify\">Yooooomi</a> on Github. This project displays stats about your spotify account like recently played, most listened to artists in near real time. </p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://github.com/Yooooomi/your_spotify\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Yooooomi/your_spotify</div><div class=\"kg-bookmark-description\">Self hosted Spotify tracking dashboard. Contribute to Yooooomi/your_spotify development by creating an account on GitHub.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://github.githubassets.com/favicons/favicon.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">Yooooomi</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://avatars1.githubusercontent.com/u/17204739?s&#x3D;400&amp;v&#x3D;4\" alt=\"\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://www.reddit.com/r/selfhosted/comments/fjjw0j/yourspotify/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">r/selfhosted - YourSpotify</div><div class=\"kg-bookmark-description\">84 votes and 36 comments so far on Reddit</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://www.redditstatic.com/desktop2x/img/favicon/android-icon-192x192.png\" alt=\"\"><span class=\"kg-bookmark-author\">reddit</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://external-preview.redd.it/pWsf1-4nzsxAO9oN7whpPEyHEF3xY0NI_ggA3sakpdo.jpg?auto&#x3D;webp&amp;s&#x3D;c5c9eb469e5369a2096718fbd20b8b2d9dd0b27c\" alt=\"\"></div></a></figure><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/06/68747470733a2f2f692e696d6775722e636f6d2f77624f687030462e706e67.png\" class=\"kg-image\" alt=\"Pages\" loading=\"lazy\"></figure><p></p><p>You will need to have some knowledge of docker. I have a quick write up <a href=\"https://bookstack.breadnet.co.uk/books/kb-articles/page/docker-intro-and-notes\">here</a> on how to install it on Linux. </p><p>Once your docker is up and running, we will need to gitclone this to our computer. I prefer to keep all my github projects in one folder. Where you put it is up to you.</p><pre><code class=\"language-bash\">stannardb@bread-d1:~/github$ git clone git@github.com:Yooooomi/your_spotify.git</code></pre><p>Once that's downloaded, we can go to <a href=\"https://developer.spotify.com/dashboard/applications\">developer.spotify.com</a> and create a new application. </p><p>Login and click 'Create an app'</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/06/image-2.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>Name your app as you like. I called mine 'Docker' because #originality Answer the questions and make up something on what it does. I just wrote docker again. Sign away your first born and family name.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/06/image-3.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>Keep this page open as we will need it again.</p><p>I had some issues with the config file so you can use mine below. It's based off the original except I have had to change the name of the mongo container. </p><p>Save this file as docker-compose.yml</p><pre><code class=\"language-yaml\">version: \"3\"\n\nservices:\n  app:\n    image: yooooomi/your_spotify_server\n    container_name: express-mongo\n    restart: always\n    ports:\n      - \"8080:8080\"\n    links:\n      - mongo\n    depends_on:\n      - mongo\n    environment:\n      - API_ENDPOINT=http://localhost:8080 # This MUST be included as a valid URL in the spotify dashboard\n      - CLIENT_ENDPOINT=http://localhost:3000\n      - SPOTIFY_PUBLIC=__your_spotify_client_id__\n      - SPOTIFY_SECRET=__your_spotify_secret__\n      - CORS=http://localhost:3000,http://localhost:3001\n      #- CORS=all\n      #- MONGO_ENDPOINT=mongodb://mongo:27017/your_spotify\n  mongo:\n    container_name: mongo_spotify\n    image: mongo\n    volumes:\n      - ./your_spotify_db:/data/db\n\n  web:\n    image: yooooomi/your_spotify_client\n    container_name: web\n    restart: always\n    ports:\n      - \"3000:3000\"\n    environment:\n      - API_ENDPOINT=http://localhost:8080</code></pre><p>On the spotify webpage, replace the <strong><code>your_spotify_client_id</code> </strong>with the public key and then the same thing for the secret. Obviously using the secret. </p><p>Next go back to spotify and click edit. </p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/06/image-4.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>In here scroll down till you find redirect links:</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/06/image-5.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>I had some issues the first time so I slapped both these in there. If you're hosting this on a publically accessable server, you will put the address such as:</p><p><code>https://spotify.breadnet.co.uk/oauth/spotify/callback</code></p><p>just append <code>/oauth/spotify/callback</code> to the end of the address to access this.</p><p></p><p>Now we can start actually using the application. In the same folder as your <code>docker-compose.yml</code> file run <code>docker-compose up</code> and it should whirr away. it took mine around 5 minutes the first time.</p><p></p><p>Depending on where this is being hosted, you can go to <code>http://localhost:3000</code> or where ever you've got it. If you're going to have this publicly hosted, use a reverse proxy like nginx or traefik</p><p></p><p>Once running, click register and make a username and password up:</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/06/image-6.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>then login. Once loggedin you'll be presented asking to connect to spotify. Click and login. </p><p></p><p>Give it around 20 minutes for good measure and you <em>should</em> start to see stats being generated.</p><p></p><p>If you're having issues, check the <a href=\"https://github.com/Yooooomi/your_spotify/issues?q=is%3Aissue+\"><a href=\"https://github.com/Yooooomi/your_spotify/issues?q=is%3Aissue+\">g</a>ithub</a> issues or the reddit post from the top! </p><blockquote>A quick update. if you see that you cant login to the app, check all the docker containers are running. I found that after rebooting one of the containers had not started up and thus I was unable to login</blockquote><hr><p>You can hire me via <a href=\"https://www.upwork.com/freelancers/~01c61ee9802b94133e\">Upwork</a> or <a href=\"mailto:work@breadnet.co.uk\">emailing me</a> for weekend projects!</p>","comment_id":"5eda8388235f3b3c70a4ec05","plaintext":"This page is now outdated.\n\nInstalling your_spotifyYour_spotify is a web application running on docker showing you stats about your account in the past 24 hours as well as current.breadNETBradley Stannard\n\n\n\nI came across this beautiful project by someone called Timothee Boussus or better known as Yooooomi on Github. This project displays stats about your spotify account like recently played, most listened to artists in near real time.\n\nYooooomi/your_spotifySelf hosted Spotify tracking dashboard. Contribute to Yooooomi/your_spotify development by creating an account on GitHub.GitHubYooooomir/selfhosted - YourSpotify84 votes and 36 comments so far on Redditreddit\n\n\n\nYou will need to have some knowledge of docker. I have a quick write up here on how to install it on Linux.\n\nOnce your docker is up and running, we will need to gitclone this to our computer. I prefer to keep all my github projects in one folder. Where you put it is up to you.\n\nstannardb@bread-d1:~/github$ git clone git@github.com:Yooooomi/your_spotify.git\n\nOnce that's downloaded, we can go to developer.spotify.com and create a new application.\n\nLogin and click 'Create an app'\n\nName your app as you like. I called mine 'Docker' because #originality Answer the questions and make up something on what it does. I just wrote docker again. Sign away your first born and family name.\n\nKeep this page open as we will need it again.\n\nI had some issues with the config file so you can use mine below. It's based off the original except I have had to change the name of the mongo container.\n\nSave this file as docker-compose.yml\n\nversion: \"3\"\n\nservices:\n  app:\n    image: yooooomi/your_spotify_server\n    container_name: express-mongo\n    restart: always\n    ports:\n      - \"8080:8080\"\n    links:\n      - mongo\n    depends_on:\n      - mongo\n    environment:\n      - API_ENDPOINT=http://localhost:8080 # This MUST be included as a valid URL in the spotify dashboard\n      - CLIENT_ENDPOINT=http://localhost:3000\n      - SPOTIFY_PUBLIC=__your_spotify_client_id__\n      - SPOTIFY_SECRET=__your_spotify_secret__\n      - CORS=http://localhost:3000,http://localhost:3001\n      #- CORS=all\n      #- MONGO_ENDPOINT=mongodb://mongo:27017/your_spotify\n  mongo:\n    container_name: mongo_spotify\n    image: mongo\n    volumes:\n      - ./your_spotify_db:/data/db\n\n  web:\n    image: yooooomi/your_spotify_client\n    container_name: web\n    restart: always\n    ports:\n      - \"3000:3000\"\n    environment:\n      - API_ENDPOINT=http://localhost:8080\n\nOn the spotify webpage, replace the your_spotify_client_id with the public key and then the same thing for the secret. Obviously using the secret.\n\nNext go back to spotify and click edit.\n\nIn here scroll down till you find redirect links:\n\nI had some issues the first time so I slapped both these in there. If you're hosting this on a publically accessable server, you will put the address such as:\n\nhttps://spotify.breadnet.co.uk/oauth/spotify/callback\n\njust append /oauth/spotify/callback to the end of the address to access this.\n\n\n\nNow we can start actually using the application. In the same folder as your docker-compose.yml file run docker-compose up and it should whirr away. it took mine around 5 minutes the first time.\n\n\n\nDepending on where this is being hosted, you can go to http://localhost:3000 or where ever you've got it. If you're going to have this publicly hosted, use a reverse proxy like nginx or traefik\n\n\n\nOnce running, click register and make a username and password up:\n\nthen login. Once loggedin you'll be presented asking to connect to spotify. Click and login.\n\n\n\nGive it around 20 minutes for good measure and you should start to see stats being generated.\n\n\n\nIf you're having issues, check the github issues or the reddit post from the top!\n\nA quick update. if you see that you cant login to the app, check all the docker containers are running. I found that after rebooting one of the containers had not started up and thus I was unable to login\n\nYou can hire me via Upwork or emailing me for weekend projects!","feature_image":"https://images.unsplash.com/photo-1532354058425-ba7ccc7e4a24?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=2000&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2020-06-05T17:40:24.000Z","updated_at":"2022-10-23T16:18:56.000Z","published_at":"2020-06-05T17:40:39.000Z","custom_excerpt":"Your_spotify is a web application running on docker showing you stats about your account in the past 24 hours as well as current. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"6425f3bc192c0c150413d58e","uuid":"ac173375-efca-4347-a0ea-9bcc566d258c","title":"The story","slug":"the-story","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/06/EmbeddedImage.jpg\",\"cardWidth\":\"\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/06/EmbeddedImage--1-.jpg\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/06/EmbeddedImage--2-.jpg\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/06/EmbeddedImage--3-.jpg\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/06/EmbeddedImage--4-.jpg\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/06/EmbeddedImage--5-.jpg\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/06/EmbeddedImage--6-.jpg\"}],[\"image\",{\"src\":\"https://lh3.googleusercontent.com/KDrUyJpPBtV7FQe120334ckIMrW9fnmC5Zex8ZrO0Apw4P8hs-26kQefQa4jSA7WER8C9FU=w1280\",\"alt\":\"\",\"title\":\"\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/06/EmbeddedImage--7-.jpg\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/06/EmbeddedImage--8-.jpg\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/06/EmbeddedImage--9-.jpg\"}]],\"markups\":[[\"strong\"],[\"em\"]],\"sections\":[[1,\"p\",[]],[1,\"h1\",[[0,[0,0],2,\"What is breadNET?\"]]],[1,\"p\",[[0,[],0,\"BreadNET is the name I have given my home lab. The name originated form my original nickname of breadbin from primary school. From there, branched breadNET. I had originally planned to purchase the domain of bread.net but this was taken, hence breadnet.co.uk\"]]],[1,\"h1\",[[0,[0,0],2,\"What qualifies me to write about this stuff?\"]]],[1,\"p\",[[0,[],0,\"Nothing at all, I am a Cloud engineer by Trade so that contributes something?\"]]],[1,\"h1\",[[0,[0,0],2,\"How did I get in to computers?\"]]],[1,\"p\",[[0,[],0,\"So It really started back in 2012/13 when I had moved abroad with an old XP computer and from there it branched out. I started out wanting to work at Openrach as a technician, but decided not to. I used to work on the welpdesk for an \"],[0,[1],1,\"msp \"],[0,[],0,\"in Potters Bar. I had moved on and accepted a job as a Junior System administrator for a cloud based company! woo\"]]],[1,\"p\",[[0,[],0,\"It all really started falling in place at my school that had a BYOD policy and I got bored in class, as you do. I started to look at how the internet works, web pages, all those kind of things. Eventually I got IIS to work on my third laptop (see below) and it just took off from there. Whilst in India, I managed to get my second laptop to accept ubuntu as it's OS. From there we moved to installing apache2 and building a basic website. Since then (around 2013) I have moved back to the UK (2015) and finally had the space and the money to get better equipment.\"]]],[1,\"p\",[[0,[],0,\"The laptop that started it all. This is the one where I learnt HTML. (I'm hopeless now)\"]]],[10,0],[1,\"p\",[[0,[],0,\"This is the second laptop I owned. This was the laptop I was able to get running Ubuntu 13.04 (EOL) running with apache2 as a website. Was quite cool!\"]]],[10,1],[1,\"p\",[[0,[],0,\"My third laptop was my everyday driver. This is the laptop that I spent most time working from, running VM's and everything else you could imagine!\"]]],[10,2],[1,\"p\",[[0,[],0,\"Finally, my Thinkpad! A trusty Lenovo T450 rocking Linux Mint!\"]]],[10,3],[1,\"p\",[[0,[],0,\"I had (bare in mind this was back in 2017 I think) planned to purchase some servers and needed a rack for them. So naturally I built one...\"]]],[1,\"p\",[[0,[],0,\"The below photo is from whiteboardCAD showing the specifics of the rack design. \"]]],[10,4],[1,\"p\",[[0,[],0,\"Now we're making progress. The \"],[0,[1],1,\"server rack \"],[0,[],0,\"is starting take place. It's quite solid once the rest of the cross supports were added. This was wood left over from building my desk.\"]]],[10,5],[1,\"p\",[[0,[],0,\"Excuse the 2003 flip phone camera quality, but this is what the finished rack looked \"],[1,[],0,0],[0,[],0,\"Photos don't do it justice how much of a fire hazard this was. \"]]],[10,6],[1,\"h1\",[[0,[0],1,\"The final straw\"]]],[1,\"p\",[[0,[],0,\"So after running like this for around a year I decided that it needed to go. Stuff was all over the place. I really needed to upgrade. Plus my \"],[0,[1],1,\"\\\"server\\\"\"],[0,[],0,\" was an old computer with 8gb of ram and 2 cores.\"]]],[1,\"p\",[[0,[],0,\"I saved up from my job, which at the time was a warehouse operative, and purchased some servers and a server rack to populate some space in the garage. \"]]],[1,\"p\",[[0,[],0,\"I purchased:\"]]],[3,\"ul\",[[[0,[],0,\"1 Dell R210 to run the router platform\"]],[[0,[],0,\"2 Dell R710's with E5520 processors, 16gb of ram each, which would run my hypervisor platform (xcp-ng)\"]],[[0,[],0,\"1 Dell R710 with 1 E5520 to run my NAS for storage of backups and Block level storage to be presented to the VM servers (lol, never happened. electricity isn't free)\"]]]],[1,\"p\",[[1,[],0,1]]],[1,\"h1\",[[0,[0],1,\"Pictures:\"]]],[10,7],[1,\"p\",[[0,[],0,\"This is the 48U rack I purchased off Ebay and had delivered. \"]]],[1,\"p\",[[0,[],0,\"I didn't quite realize how big 48U is but \"],[0,[1],1,\"yeah\"],[0,[],0,\". \"]]],[10,8],[1,\"p\",[[0,[],0,\"The front of the rack (yes, there's no hard drive caddies. (\"],[0,[1],1,\"Were on a budget here\"],[0,[],0,\"))\"]]],[10,9],[1,\"p\",[[0,[],0,\"Rear of the rack. Sphagetttttttiiiii junction. \"]]],[10,10],[1,\"h1\",[[0,[0,0],2,\"Plans for the future\"]]],[3,\"ol\",[[[0,[],0,\"Get more storage\"]],[[0,[],0,\"Faster HDD's (or ssd?)\"]],[[0,[],0,\"10gig (eh, half way there as of August 2019)\"]],[[0,[],0,\"bring the 3rd server online (Just need more ram... anyone?)\"]]]],[1,\"p\",[]],[1,\"p\",[]],[1,\"p\",[]]],\"ghostVersion\":\"3.0\"}","lexical":null,"html":"<p></p><h1 id=\"what-is-breadnet\"><strong><strong>What is breadNET?</strong></strong></h1><p>BreadNET is the name I have given my home lab. The name originated form my original nickname of breadbin from primary school. From there, branched breadNET. I had originally planned to purchase the domain of bread.net but this was taken, hence breadnet.co.uk</p><h1 id=\"what-qualifies-me-to-write-about-this-stuff\"><strong><strong>What qualifies me to write about this stuff?</strong></strong></h1><p>Nothing at all, I am a Cloud engineer by Trade so that contributes something?</p><h1 id=\"how-did-i-get-in-to-computers\"><strong><strong>How did I get in to computers?</strong></strong></h1><p>So It really started back in 2012/13 when I had moved abroad with an old XP computer and from there it branched out. I started out wanting to work at Openrach as a technician, but decided not to. I used to work on the welpdesk for an <em>msp </em>in Potters Bar. I had moved on and accepted a job as a Junior System administrator for a cloud based company! woo</p><p>It all really started falling in place at my school that had a BYOD policy and I got bored in class, as you do. I started to look at how the internet works, web pages, all those kind of things. Eventually I got IIS to work on my third laptop (see below) and it just took off from there. Whilst in India, I managed to get my second laptop to accept ubuntu as it's OS. From there we moved to installing apache2 and building a basic website. Since then (around 2013) I have moved back to the UK (2015) and finally had the space and the money to get better equipment.</p><p>The laptop that started it all. This is the one where I learnt HTML. (I'm hopeless now)</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/06/EmbeddedImage.jpg\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>This is the second laptop I owned. This was the laptop I was able to get running Ubuntu 13.04 (EOL) running with apache2 as a website. Was quite cool!</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/06/EmbeddedImage--1-.jpg\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>My third laptop was my everyday driver. This is the laptop that I spent most time working from, running VM's and everything else you could imagine!</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/06/EmbeddedImage--2-.jpg\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>Finally, my Thinkpad! A trusty Lenovo T450 rocking Linux Mint!</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/06/EmbeddedImage--3-.jpg\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>I had (bare in mind this was back in 2017 I think) planned to purchase some servers and needed a rack for them. So naturally I built one...</p><p>The below photo is from whiteboardCAD showing the specifics of the rack design. </p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/06/EmbeddedImage--4-.jpg\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>Now we're making progress. The <em>server rack </em>is starting take place. It's quite solid once the rest of the cross supports were added. This was wood left over from building my desk.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/06/EmbeddedImage--5-.jpg\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>Excuse the 2003 flip phone camera quality, but this is what the finished rack looked <br>Photos don't do it justice how much of a fire hazard this was. </p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/06/EmbeddedImage--6-.jpg\" class=\"kg-image\" alt loading=\"lazy\"></figure><h1 id=\"the-final-straw\"><strong>The final straw</strong></h1><p>So after running like this for around a year I decided that it needed to go. Stuff was all over the place. I really needed to upgrade. Plus my <em>\"server\"</em> was an old computer with 8gb of ram and 2 cores.</p><p>I saved up from my job, which at the time was a warehouse operative, and purchased some servers and a server rack to populate some space in the garage. </p><p>I purchased:</p><ul><li>1 Dell R210 to run the router platform</li><li>2 Dell R710's with E5520 processors, 16gb of ram each, which would run my hypervisor platform (xcp-ng)</li><li>1 Dell R710 with 1 E5520 to run my NAS for storage of backups and Block level storage to be presented to the VM servers (lol, never happened. electricity isn't free)</li></ul><p><br></p><h1 id=\"pictures-\"><strong>Pictures:</strong></h1><figure class=\"kg-card kg-image-card\"><img src=\"https://lh3.googleusercontent.com/KDrUyJpPBtV7FQe120334ckIMrW9fnmC5Zex8ZrO0Apw4P8hs-26kQefQa4jSA7WER8C9FU=w1280\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>This is the 48U rack I purchased off Ebay and had delivered. </p><p>I didn't quite realize how big 48U is but <em>yeah</em>. </p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/06/EmbeddedImage--7-.jpg\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>The front of the rack (yes, there's no hard drive caddies. (<em>Were on a budget here</em>))</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/06/EmbeddedImage--8-.jpg\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>Rear of the rack. Sphagetttttttiiiii junction. </p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/06/EmbeddedImage--9-.jpg\" class=\"kg-image\" alt loading=\"lazy\"></figure><h1 id=\"plans-for-the-future\"><strong><strong>Plans for the future</strong></strong></h1><ol><li>Get more storage</li><li>Faster HDD's (or ssd?)</li><li>10gig (eh, half way there as of August 2019)</li><li>bring the 3rd server online (Just need more ram... anyone?)</li></ol><p></p><p></p>","comment_id":"5eda83b3235f3b3c70a4ec0b","plaintext":"What is breadNET?\n\nBreadNET is the name I have given my home lab. The name originated form my original nickname of breadbin from primary school. From there, branched breadNET. I had originally planned to purchase the domain of bread.net but this was taken, hence breadnet.co.uk\n\n\nWhat qualifies me to write about this stuff?\n\nNothing at all, I am a Cloud engineer by Trade so that contributes something?\n\n\nHow did I get in to computers?\n\nSo It really started back in 2012/13 when I had moved abroad with an old XP computer and from there it branched out. I started out wanting to work at Openrach as a technician, but decided not to. I used to work on the welpdesk for an msp in Potters Bar. I had moved on and accepted a job as a Junior System administrator for a cloud based company! woo\n\nIt all really started falling in place at my school that had a BYOD policy and I got bored in class, as you do. I started to look at how the internet works, web pages, all those kind of things. Eventually I got IIS to work on my third laptop (see below) and it just took off from there. Whilst in India, I managed to get my second laptop to accept ubuntu as it's OS. From there we moved to installing apache2 and building a basic website. Since then (around 2013) I have moved back to the UK (2015) and finally had the space and the money to get better equipment.\n\nThe laptop that started it all. This is the one where I learnt HTML. (I'm hopeless now)\n\nThis is the second laptop I owned. This was the laptop I was able to get running Ubuntu 13.04 (EOL) running with apache2 as a website. Was quite cool!\n\nMy third laptop was my everyday driver. This is the laptop that I spent most time working from, running VM's and everything else you could imagine!\n\nFinally, my Thinkpad! A trusty Lenovo T450 rocking Linux Mint!\n\nI had (bare in mind this was back in 2017 I think) planned to purchase some servers and needed a rack for them. So naturally I built one...\n\nThe below photo is from whiteboardCAD showing the specifics of the rack design.\n\nNow we're making progress. The server rack is starting take place. It's quite solid once the rest of the cross supports were added. This was wood left over from building my desk.\n\nExcuse the 2003 flip phone camera quality, but this is what the finished rack looked\nPhotos don't do it justice how much of a fire hazard this was.\n\n\nThe final straw\n\nSo after running like this for around a year I decided that it needed to go. Stuff was all over the place. I really needed to upgrade. Plus my \"server\" was an old computer with 8gb of ram and 2 cores.\n\nI saved up from my job, which at the time was a warehouse operative, and purchased some servers and a server rack to populate some space in the garage.\n\nI purchased:\n\n * 1 Dell R210 to run the router platform\n * 2 Dell R710's with E5520 processors, 16gb of ram each, which would run my hypervisor platform (xcp-ng)\n * 1 Dell R710 with 1 E5520 to run my NAS for storage of backups and Block level storage to be presented to the VM servers (lol, never happened. electricity isn't free)\n\n\n\n\n\nPictures:\n\nThis is the 48U rack I purchased off Ebay and had delivered.\n\nI didn't quite realize how big 48U is but yeah.\n\nThe front of the rack (yes, there's no hard drive caddies. (Were on a budget here))\n\nRear of the rack. Sphagetttttttiiiii junction.\n\n\nPlans for the future\n\n 1. Get more storage\n 2. Faster HDD's (or ssd?)\n 3. 10gig (eh, half way there as of August 2019)\n 4. bring the 3rd server online (Just need more ram... anyone?)\n\n\n\n","feature_image":"https://images.unsplash.com/photo-1558494949-ef010cbdcc31?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=2000&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ","featured":0,"type":"page","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2020-06-05T17:41:07.000Z","updated_at":"2025-04-02T12:32:14.000Z","published_at":"2020-06-05T17:57:52.000Z","custom_excerpt":"Page full of my servers and the journey we've been on","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"6425f3bc192c0c150413d58f","uuid":"4cd89cca-e642-44af-bd9b-658745931272","title":"HTML 5 Speed test server","slug":"html-5-speed-test-server","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"code\",{\"code\":\"sudo apt-get install apache2 php -y\"}],[\"code\",{\"code\":\"sudo systemctl enable apache2\\n\"}],[\"code\",{\"code\":\"git clone https://github.com/adolfintel/speedtest.git\"}],[\"code\",{\"code\":\"cp speedtest/* /var/www/html\"}],[\"code\",{\"code\":\"touch index.html\\ncp example-pretty.html index.html\"}],[\"code\",{\"code\":\"sudo systemctl restart apache2\"}],[\"hr\",{}]],\"markups\":[[\"a\",[\"href\",\"https://www.old.breadnet.co.uk\"]],[\"code\"],[\"a\",[\"href\",\"https://www.upwork.com/freelancers/~01c61ee9802b94133e\"]],[\"a\",[\"href\",\"mailto:work@breadnet.co.uk\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Originally this was posted on my \"],[0,[0],1,\"old\"],[0,[],0,\" website but seeing as several people still come to my site for this, I will add it here\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"Firstly you will need to SSH to the server you are running. If you are unsure how to do this, there are lots of good videos on youtube. If you're still struggling, hit the Contact button at the top and get in touch! \"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"Install apache2 and php \"]]],[10,0],[1,\"p\",[[0,[],0,\"Once that is done, enable apache2 to start on boot. This may vary but on most modern systems you can use \"]]],[10,1],[1,\"p\",[[0,[],0,\"In your current directory, git clone the repository to your server\"]]],[10,2],[1,\"p\",[[0,[],0,\"Copy all the files from here in to \"],[0,[1],1,\"/var/www/html\"],[0,[],0,\" with \"]]],[10,3],[1,\"p\",[[0,[],0,\"Once that is done, go to the \"],[0,[1],1,\"/var/www/html\"],[0,[],0,\" folder with \"],[0,[1],1,\"cd /var/www/html\"],[0,[],0,\" \"]]],[1,\"p\",[[0,[],0,\"In here, delete the index.html\"]]],[1,\"p\",[[0,[1],1,\"sudo rm index.html\"]]],[1,\"p\",[[0,[],0,\"Now depending on what page you like, I chose the one called 'Example-pretty' as it sounded nice, you just need to change it's name to index.html/\"]]],[1,\"p\",[[0,[],0,\"I suggest copying it\"]]],[10,4],[1,\"p\",[[0,[],0,\"Once done, restart apache2 for good measures\"]]],[10,5],[1,\"p\",[[0,[],0,\"Go to the IP address or host name of your server, and you should be presented with the page!\"]]],[10,6],[1,\"p\",[[0,[],0,\"You can hire me via \"],[0,[2],1,\"Upwork\"],[0,[],0,\" or \"],[0,[3],1,\"emailing me\"],[0,[],0,\" for weekend projects!\"]]]],\"ghostVersion\":\"3.0\"}","lexical":null,"html":"<p>Originally this was posted on my <a href=\"https://www.old.breadnet.co.uk\">old</a> website but seeing as several people still come to my site for this, I will add it here</p><p></p><p>Firstly you will need to SSH to the server you are running. If you are unsure how to do this, there are lots of good videos on youtube. If you're still struggling, hit the Contact button at the top and get in touch! </p><p></p><p>Install apache2 and php </p><pre><code>sudo apt-get install apache2 php -y</code></pre><p>Once that is done, enable apache2 to start on boot. This may vary but on most modern systems you can use </p><pre><code>sudo systemctl enable apache2\n</code></pre><p>In your current directory, git clone the repository to your server</p><pre><code>git clone https://github.com/adolfintel/speedtest.git</code></pre><p>Copy all the files from here in to <code>/var/www/html</code> with </p><pre><code>cp speedtest/* /var/www/html</code></pre><p>Once that is done, go to the <code>/var/www/html</code> folder with <code>cd /var/www/html</code> </p><p>In here, delete the index.html</p><p><code>sudo rm index.html</code></p><p>Now depending on what page you like, I chose the one called 'Example-pretty' as it sounded nice, you just need to change it's name to index.html/</p><p>I suggest copying it</p><pre><code>touch index.html\ncp example-pretty.html index.html</code></pre><p>Once done, restart apache2 for good measures</p><pre><code>sudo systemctl restart apache2</code></pre><p>Go to the IP address or host name of your server, and you should be presented with the page!</p><hr><p>You can hire me via <a href=\"https://www.upwork.com/freelancers/~01c61ee9802b94133e\">Upwork</a> or <a href=\"mailto:work@breadnet.co.uk\">emailing me</a> for weekend projects!</p>","comment_id":"5edbcaf9544a0d4341b69fd5","plaintext":"Originally this was posted on my old website but seeing as several people still come to my site for this, I will add it here\n\n\n\nFirstly you will need to SSH to the server you are running. If you are unsure how to do this, there are lots of good videos on youtube. If you're still struggling, hit the Contact button at the top and get in touch!\n\n\n\nInstall apache2 and php\n\nsudo apt-get install apache2 php -y\n\nOnce that is done, enable apache2 to start on boot. This may vary but on most modern systems you can use\n\nsudo systemctl enable apache2\n\n\nIn your current directory, git clone the repository to your server\n\ngit clone https://github.com/adolfintel/speedtest.git\n\nCopy all the files from here in to /var/www/html with\n\ncp speedtest/* /var/www/html\n\nOnce that is done, go to the /var/www/html folder with cd /var/www/html\n\nIn here, delete the index.html\n\nsudo rm index.html\n\nNow depending on what page you like, I chose the one called 'Example-pretty' as it sounded nice, you just need to change it's name to index.html/\n\nI suggest copying it\n\ntouch index.html\ncp example-pretty.html index.html\n\nOnce done, restart apache2 for good measures\n\nsudo systemctl restart apache2\n\nGo to the IP address or host name of your server, and you should be presented with the page!\n\nYou can hire me via Upwork or emailing me for weekend projects!","feature_image":"https://images.unsplash.com/photo-1561474119-1b76f3a79816?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=2000&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2020-06-06T16:57:29.000Z","updated_at":"2021-05-02T01:48:11.000Z","published_at":"2020-06-09T17:56:43.000Z","custom_excerpt":"How to install a HTML5 based speedtest server for your network, family and friends!","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"6425f3bc192c0c150413d590","uuid":"02e1f2fd-ecbf-4c38-9d0e-58669a0dcb9c","title":"Jellyfin with s3 backend","slug":"jellyfin-with-s3-backend","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"bookmark\",{\"url\":\"https://github.com/jellyfin/jellyfin\",\"metadata\":{\"url\":\"https://github.com/jellyfin/jellyfin\",\"title\":\"jellyfin/jellyfin\",\"description\":\"The Free Software Media System. Contribute to jellyfin/jellyfin development by creating an account on GitHub.\",\"author\":\"jellyfin\",\"publisher\":\"GitHub\",\"thumbnail\":\"https://avatars1.githubusercontent.com/u/45698031?s=400&v=4\",\"icon\":\"https://github.githubassets.com/favicons/favicon.svg\"}}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/06/image-8.png\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/06/image-7.png\"}],[\"hr\",{}],[\"hr\",{}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/06/image-9.png\",\"cardWidth\":\"\"}],[\"code\",{\"code\":\"{\\n  \\\"Version\\\": \\\"2012-10-17\\\",\\n  \\\"Statement\\\": [\\n    {\\n      \\\"Effect\\\": \\\"Allow\\\",\\n      \\\"Principal\\\": {\\n        \\\"AWS\\\": \\\"*\\\"\\n      },\\n      \\\"Action\\\": [\\n        \\\"s3:GetBucketLocation\\\",\\n        \\\"s3:ListBucket\\\",\\n        \\\"s3:ListBucketMultipartUploads\\\"\\n      ],\\n      \\\"Resource\\\": \\\"arn:aws:s3:::<bucket name>\\\"\\n    },\\n    {\\n      \\\"Effect\\\": \\\"Allow\\\",\\n      \\\"Principal\\\": {\\n        \\\"AWS\\\": \\\"*\\\"\\n      },\\n      \\\"Action\\\": \\\"s3:GetObject\\\",\\n      \\\"Resource\\\": \\\"arn:aws:s3:::<bucket name>/*\\\"\\n    }\\n  ]\\n}\",\"language\":\"yaml\",\"caption\":\"jellyfin-media-ro\"}],[\"code\",{\"code\":\"{\\n  \\\"Version\\\": \\\"2012-10-17\\\",\\n  \\\"Statement\\\": [\\n    {\\n      \\\"Effect\\\": \\\"Allow\\\",\\n      \\\"Action\\\": \\\"s3:ListAllMyBuckets\\\",\\n      \\\"Resource\\\": \\\"arn:aws:s3:::*\\\"\\n    },\\n    {\\n      \\\"Effect\\\": \\\"Allow\\\",\\n      \\\"Action\\\": \\\"s3:*\\\",\\n      \\\"Resource\\\": [\\n        \\\"arn:aws:s3:::<your bucket>\\\",\\n        \\\"arn:aws:s3:::<your bucket>/*\\\"\\n      ]\\n    }\\n  ]\\n}\",\"caption\":\"jellyfin-media-rw\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/06/image-10.png\"}],[\"hr\",{}],[\"code\",{\"code\":\"mkdir docker\\nmkdir docker/jellyfin\\nmkdir docker/jellyfin/files\\nmkdir docker/jellyfin/files/{cache,config,media}\\ntouch docker/jellyfin/docker-compose.yml\"}],[\"code\",{\"code\":\".\\n└── docker\\n    └── jellyfin\\n        ├── docker-compose.yml\\n        └── files\\n            ├── cache\\n            ├── config\\n            └── media\\n\\n6 directories, 1 file\",\"language\":\"bash\"}],[\"code\",{\"code\":\"version: \\\"3\\\"\\nservices:\\n jellyfin:\\n  image: jellyfin/jellyfin\\n  user: 1000:1000\\n  network_mode: \\\"host\\\"\\n  volumes:\\n    - </path/to/config>:/config\\n    - </path/to/cache>:/cache\\n    - </path/to/media>:/media\",\"language\":\"yml\",\"caption\":\"docker-compose.yml\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/06/image-11.png\"}],[\"code\",{\"code\":\"[media]\\ntype = s3\\nenv_auth = \\naccess_key_id = <first key in .csv file from user creation>\\nsecret_access_key = <second key from user creation>\\nregion = \\nendpoint = https://s3.eu-central-1.wasabisys.com #if using a europe bucket, else; See below for specifics\\nlocation_constraint = \\nacl = \\nserver_side_encryption = \\nstorage_class = \\n\\n####You dont need to copy this shit in to the file, it's in reference to line 7###\\n#Wasabi US East 1 (N. Virginia): s3.wasabisys.com or s3.us-east-1.wasabisys.com\\n#Wasabi US East 2 (N. Virginia): s3.us-east-2.wasabisys.com \\n#Wasabi US West 1 (Oregon): s3.us-west-1.wasabisys.com\\n#Wasabi EU Central 1 (Amsterdam): s3.eu-central-1.wasabisys.com\"}],[\"hr\",{}]],\"markups\":[[\"em\"],[\"a\",[\"href\",\"https://www.urbandictionary.com/define.php?term=aliven't\"]],[\"a\",[\"href\",\"https://documentation.breadnet.co.uk/kb/ssh/ssh-client-setup-using-keys/\"]],[\"code\"],[\"a\",[\"href\",\"https://console.wasabisys.com/#/login\"]],[\"a\",[\"href\",\"https://documentation.breadnet.co.uk/kb/public-web-facing/jellyfin-using-s3-and-docker/\"]],[\"a\",[\"href\",\"https://www.upwork.com/freelancers/~01c61ee9802b94133e\"]],[\"a\",[\"href\",\"mailto:work@breadnet.co.uk\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Jellyfin is a FOSS media server forked from Emby before they went sketchy. \"]]],[10,0],[1,\"p\",[]],[1,\"blockquote\",[[0,[],0,\"So in a bid to speed up my jellyfin server, and cut costs I am now fully running my Jellyfin off of a vm on OVH with an s3 based backend, so this does work\"]]],[1,\"p\",[[0,[],0,\"I guess you could call this one of my shower thoughts as I wanted to be able to use jellyfin on a Digital ocean droplet or something external just so I don't have to mess about with using a reverse proxy from my home servers, to Digital ocean then out to the world. Whilst I know I could have just opened the ports on my home network, I have a Dynamic IP address which changes every so often. \"]]],[1,\"p\",[[0,[],0,\"Yes, I could write a simple script to monitor the IP address and see when it changes, then update the domain, but that's not complex enough for me, and I'm terrible at coding.\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"We will first start with the droplet. I suggest going with either of the below for Digital ocean\"]]],[10,1],[1,\"p\",[[0,[],0,\"Or if Digital ocean isn't your cuppa' tea, then vultr have a reasonable product. \"]]],[10,2],[1,\"p\",[[0,[],0,\"Or even GCP's N2, N2D, N1 servers seem to bee half decent for Jellyfin.\"]]],[1,\"p\",[[0,[],0,\"There's nothing stopping you from deploying this at home, just realise that you will need 50mbps+ download and \"],[0,[0],1,\"ideally \"],[0,[],0,\"25mbps+ upload to be able to support multiple users streams externaly if you're going though either an nginx reverse proxy like I am, or straight from you connection\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"What you want is cores over anything, especially if you're going to be trans-coding.  Yes I did try and use the $5 tier from digital ocean and I wanted to \"],[0,[1],1,\"commit alive'nt\"]]],[10,3],[1,\"h3\",[[0,[],0,\"Enough fuzz, lets get to it!\"]]],[10,4],[1,\"h4\",[[0,[],0,\"Prereqs:\"]]],[1,\"p\",[[1,[],0,0],[0,[],0,\"I am not a security expert, I am a sysadmin. The security of your server is up to you. Do basic things like enable \"],[0,[2],1,\"ssh keys\"],[0,[],0,\" and lock down ingress ports with ufw or iptables. What ever angles your dangle.\"],[1,[],0,1],[0,[],0,\"I am not a docker expert. I am a sysadmin which means I can use google (I think? bleh) so if you see anything whack in this, feel free to email me \"]]],[1,\"p\",[]],[1,\"h5\",[[0,[],0,\"You will need:\"]]],[3,\"ol\",[[[0,[],0,\"S3 compatible bucket - I suggest wasabi but you can also use Min.io if you're in to FOSS (wasabi has a 30 day free 1tb trial)\"]],[[0,[],0,\"Ability to read and copy and paste.\"]]]],[1,\"h5\",[[0,[],0,\"The first step\"]]],[1,\"p\",[[0,[],0,\"First we will spin up a digital ocean droplet but you can use what ever VPS provider you want.\"]]],[1,\"p\",[[0,[],0,\"Once the VPs is up and you're logged in run the below to update the sources and upgrade any packages that need it. Should not take longer than 5-10 minutes at worst case scenario\"]]],[1,\"p\",[[0,[3],1,\"sudo apt-get update && sudo apt-get upgrade\"]]],[1,\"p\",[[0,[],0,\"Once that's done, install rclone\"]]],[1,\"p\",[[0,[3],1,\"sudo apt-get install rclone\"]]],[1,\"p\",[[0,[],0,\"Next we need to setup the bucket and fuzz to actually have some media to present to Jellyfin once everything gets up and running. \"]]],[1,\"p\",[[0,[],0,\"Login to wasabi console at \"],[0,[4],1,\"https://console.wasabisys.com/\"]]],[1,\"blockquote\",[[0,[],0,\"A quick Gotcha here. Wasabi really pissed me off as they charge you for deleted storage for 90 days. I'm not sure if I goofed this up somewhere but I have contacted support to find out\"]]],[1,\"p\",[[0,[],0,\"Once logged in, go to buckets and create bucket. Name it something like jellyfin-media and select the region that is closest to you. If you're in Europe, pick the Europe one. It's located in the Netherlands. \"]]],[1,\"p\",[[0,[],0,\"Once the bucket is created, we need to create a policy that works with the bucket. This will be applied to a user later which grants access rights to the bucket and fun stuff.\"]]],[1,\"p\",[[0,[],0,\"Click policies on the left hand side navbar \"]]],[10,5],[1,\"p\",[[0,[],0,\"In here, name it something like \"],[0,[3],1,\"jellyfin-media-ro\"],[0,[],0,\" or what ever you like. As long as you remember it for later!\"]]],[1,\"p\",[[0,[],0,\"Once naming it, paste the below in, but change where it says \"],[0,[3],1,\"<bucket name>\"],[0,[],0,\" to the name of your bucket. This will be applied to the \"],[0,[3],1,\"docker\"],[0,[],0,\" user we will create next. It's only given read permissions to the media, just in case something weird happens and jellyfin (or you, no judgment) tries to nuke your files. \"]]],[10,6],[1,\"p\",[[0,[],0,\"For a user that can upload to the bucket create a policy called \"],[0,[3],1,\"jellyfin-media-rw\"],[0,[],0,\" where \"],[0,[3],1,\"rw\"],[0,[],0,\" is read write.  As usual substitute the \"],[0,[3],1,\"<your bucket>\"],[0,[],0,\" with the name of your actual bucket.\"]]],[10,7],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"Now we can create a user. Click users on the left hand side nav bar.\"]]],[1,\"p\",[[0,[],0,\"Click 'Create user' \"]]],[1,\"p\",[[0,[],0,\"I suggest calling it something like \"],[0,[3],1,\"docker\"],[0,[],0,\" or \"],[0,[3],1,\"jellyfin\"],[0,[],0,\" or \"],[0,[3],1,\"jeff\"],[0,[],0,\" if you really like.\"]]],[10,8],[1,\"p\",[[0,[],0,\"The important part is that you click 'Programmatic (create API key)\"]]],[1,\"p\",[[0,[],0,\"Click next till page 3 where you will assign the policy we created. Here we need the  \"],[0,[3],1,\"jellyfin-media-ro\"],[0,[],0,\" policy\"]]],[1,\"p\",[[0,[],0,\"Once done, wait a few seconds and it will ask you to download the keys. Save these and name them respective of the user. \"]]],[1,\"p\",[[0,[],0,\"You will need to do the same thing for the \"],[0,[3],1,\"rw\"],[0,[],0,\" user, so create the user and give them the rw policy, naming them something else.\"]]],[10,9],[1,\"h3\",[[0,[],0,\"Part 2: The VPS shenanigans\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"Depending on if you ave docker already installed, you can skip down a bit. If not please read the below:\"],[1,[],0,2],[1,[],0,3],[0,[],0,\"Run the below commands after each other\"]]],[1,\"p\",[[0,[3],1,\"curl -fsSL https://get.docker.com -o get-docker.sh\"]]],[1,\"p\",[[0,[3],1,\"sh get-docker.sh\"]]],[1,\"p\",[[0,[3],1,\"sudo apt-get install docker-compose\"]]],[1,\"p\",[[0,[],0,\"if all went well, docker should be installed\"]]],[1,\"p\",[[0,[],0,\"Now we need to get the docker image to actually use. This can be downloaded using the built in docker 'package manager' if you will. \"]]],[1,\"p\",[[0,[3],1,\"docker pull jellyfin/jellyfin\"]]],[1,\"p\",[[0,[],0,\"Next, we will create the folder structure. \"]]],[10,10],[1,\"p\",[[0,[],0,\"If you care, this is what it will look like.\"]]],[10,11],[1,\"p\",[[0,[],0,\"Once that is done, move to \"],[0,[3],1,\"docker/jellyfin\"],[0,[],0,\" and edit the \"],[0,[3],1,\"docker-compose.yml\"],[0,[],0,\" file\"]]],[1,\"p\",[[0,[3],1,\"sudo nano docker-compose.yml\"],[0,[],0,\" \"]]],[1,\"p\",[[0,[],0,\"In the file paste the below and change out \"],[0,[3],1,\"<path/to/config>\"],[0,[],0,\" with the full path to the folders. If you're not sure of them, go to that directory and type \"],[0,[3],1,\"pwd\"]]],[10,12],[1,\"p\",[[0,[],0,\"Next we need to sort out how to present the bucket to jellyfin. This is where rclone comes in. We installed it at the start!\"]]],[1,\"p\",[[0,[],0,\"But before we can start, we need to figure out a way to run the command, and disconnect from that session without stopping the command from running. We will use screen as I am yet to figure out how to use systemd for this\"]]],[1,\"p\",[[0,[3],1,\"screen\"]]],[1,\"p\",[[0,[],0,\"Should show you something like this. if not, \"],[0,[3],1,\"sudo apt-get install screen\"]]],[10,13],[1,\"p\",[[0,[],0,\"Now that that works, we just press enter and it shows us a command line. Press \"],[0,[3],1,\"Ctrl + a \"],[0,[],0,\"and then press \"],[0,[3],1,\"d\"],[0,[],0,\" (it's a weird combo, dont ask)\"]]],[1,\"p\",[[0,[],0,\"Now we can configure rclone. \"]]],[1,\"p\",[[0,[],0,\"Change directory to \"],[0,[3],1,\".config\"],[0,[],0,\" and if there isn't an rclone folder there \"],[0,[3],1,\"mdkir rclone\"],[0,[],0,\" \"]]],[1,\"p\",[[0,[3],1,\"nano rclone.conf\"]]],[1,\"p\",[[0,[],0,\"paste the below in there.  \"]]],[10,14],[1,\"p\",[[0,[],0,\"once done, exit that bad boy. ( \"],[0,[3],1,\"ctrl + x\"],[0,[],0,\" \"],[0,[3],1,\"y\"],[0,[],0,\" \"],[0,[3],1,\"enter\"],[0,[],0,\")\"]]],[1,\"p\",[[0,[],0,\"Go to the wasabi interface and click that bucket. Upload your favourite meme or something and then back in your terminal run:\"]]],[1,\"p\",[[0,[3],1,\"rclone ls media:<bucket name>\"],[0,[],0,\" and you should see the file name there. \"]]],[1,\"blockquote\",[[0,[],0,\"If this fails, then I'm not sure what went wrong. Email me and we can sort it out. \"]]],[1,\"p\",[[0,[],0,\"Next we need to map the bucket to a folder.\"]]],[1,\"p\",[[0,[],0,\"Open screen by typing \"],[0,[3],1,\"screen\"]]],[1,\"p\",[[0,[3],1,\"cd docker/jellyfin/files/\"]]],[1,\"p\",[[0,[3],1,\"rclone mount media:<bucket name> media --allow-others --daemon\"]]],[1,\"p\",[[0,[],0,\"It may throw back an error about using --allow-others, but just do what the command says and edit the file, remove the # in from of the allow_others or what's closest in the file.\"]]],[1,\"p\",[[0,[],0,\"It should return nothing if it worked. \"]]],[1,\"p\",[[0,[],0,\"Exit screen with \"],[0,[3],1,\"ctrl + a\"],[0,[],0,\" \"],[0,[3],1,\"d\"]]],[1,\"p\",[[0,[],0,\"Change to the media folder \"],[0,[3],1,\"docker/jellyfin/files/media\"]]],[1,\"p\",[[0,[],0,\"list the stuff in there with \"],[0,[3],1,\"ls\"],[0,[],0,\" and it should (hopefully) return the file from earlier\"]]],[1,\"h3\",[[0,[],0,\"Part 3: Making progress\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"In the folder where the docker-compose.yml file exists, run:\"]]],[1,\"p\",[[0,[3],1,\"docker-compose up\"]]],[1,\"p\",[[0,[],0,\"If all went well, you should be able to visit the IP address of the host and be greeted with the jellyfin create user account\"]]],[1,\"p\",[[0,[3],1,\"http://<ip/hostname>:8096\"]]],[1,\"p\",[[0,[],0,\"I suggest you create the tv, movies, podcasts, skin flic and what ever else folders though the webUI for wasabi then upload your stuff there. \"]]],[1,\"p\",[[0,[],0,\"Once everything is uploaded on Jellyfin, add the media as you usually would. \"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"Once done, go back to your command line and kill the docker container with \"],[0,[3],1,\"ctrl + c\"],[1,[],0,4],[0,[],0,\"Now type \"],[0,[3],1,\"docker ps -a\"],[0,[],0,\" to see all containers on your host. There should be one called Jellyfin. Make a note of the first 3 characters of the weird looking string before it's name\"]]],[1,\"p\",[[0,[],0,\"The hit it with a \"],[0,[3],1,\"docker start <characters>\"],[0,[],0,\"  Hopefully it starts and is off to the races. Go back to the url or IP address and continue as you would.\"]]],[1,\"p\",[[0,[],0,\"If you want to put it behind a nginx reverse proxy, stay tuned for my next post where I talk about how I have everything behind nginx because I'm too stubborn to learn traefik :)\"]]],[1,\"p\",[]],[1,\"blockquote\",[[0,[],0,\"this was previously posted \"],[0,[5],1,\"here\"],[0,[],0,\" but I did a terrible job\"]]],[10,15],[1,\"p\",[[0,[],0,\"You can hire me via \"],[0,[6],1,\"Upwork\"],[0,[],0,\" or \"],[0,[7],1,\"emailing me\"],[0,[],0,\" for weekend projects!\"]]]],\"ghostVersion\":\"3.0\"}","lexical":null,"html":"<p>Jellyfin is a FOSS media server forked from Emby before they went sketchy. </p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://github.com/jellyfin/jellyfin\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">jellyfin/jellyfin</div><div class=\"kg-bookmark-description\">The Free Software Media System. Contribute to jellyfin/jellyfin development by creating an account on GitHub.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://github.githubassets.com/favicons/favicon.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">jellyfin</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://avatars1.githubusercontent.com/u/45698031?s&#x3D;400&amp;v&#x3D;4\" alt=\"\"></div></a></figure><p></p><blockquote>So in a bid to speed up my jellyfin server, and cut costs I am now fully running my Jellyfin off of a vm on OVH with an s3 based backend, so this does work</blockquote><p>I guess you could call this one of my shower thoughts as I wanted to be able to use jellyfin on a Digital ocean droplet or something external just so I don't have to mess about with using a reverse proxy from my home servers, to Digital ocean then out to the world. Whilst I know I could have just opened the ports on my home network, I have a Dynamic IP address which changes every so often. </p><p>Yes, I could write a simple script to monitor the IP address and see when it changes, then update the domain, but that's not complex enough for me, and I'm terrible at coding.</p><p></p><p>We will first start with the droplet. I suggest going with either of the below for Digital ocean</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/06/image-8.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>Or if Digital ocean isn't your cuppa' tea, then vultr have a reasonable product. </p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/06/image-7.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>Or even GCP's N2, N2D, N1 servers seem to bee half decent for Jellyfin.</p><p>There's nothing stopping you from deploying this at home, just realise that you will need 50mbps+ download and <em>ideally </em>25mbps+ upload to be able to support multiple users streams externaly if you're going though either an nginx reverse proxy like I am, or straight from you connection</p><p></p><p>What you want is cores over anything, especially if you're going to be trans-coding.  Yes I did try and use the $5 tier from digital ocean and I wanted to <a href=\"https://www.urbandictionary.com/define.php?term=aliven't\">commit alive'nt</a></p><hr><h3 id=\"enough-fuzz-lets-get-to-it-\">Enough fuzz, lets get to it!</h3><hr><h4 id=\"prereqs-\">Prereqs:</h4><p><br>I am not a security expert, I am a sysadmin. The security of your server is up to you. Do basic things like enable <a href=\"https://documentation.breadnet.co.uk/kb/ssh/ssh-client-setup-using-keys/\">ssh keys</a> and lock down ingress ports with ufw or iptables. What ever angles your dangle.<br>I am not a docker expert. I am a sysadmin which means I can use google (I think? bleh) so if you see anything whack in this, feel free to email me </p><p></p><h5 id=\"you-will-need-\">You will need:</h5><ol><li>S3 compatible bucket - I suggest wasabi but you can also use Min.io if you're in to FOSS (wasabi has a 30 day free 1tb trial)</li><li>Ability to read and copy and paste.</li></ol><h5 id=\"the-first-step\">The first step</h5><p>First we will spin up a digital ocean droplet but you can use what ever VPS provider you want.</p><p>Once the VPs is up and you're logged in run the below to update the sources and upgrade any packages that need it. Should not take longer than 5-10 minutes at worst case scenario</p><p><code>sudo apt-get update &amp;&amp; sudo apt-get upgrade</code></p><p>Once that's done, install rclone</p><p><code>sudo apt-get install rclone</code></p><p>Next we need to setup the bucket and fuzz to actually have some media to present to Jellyfin once everything gets up and running. </p><p>Login to wasabi console at <a href=\"https://console.wasabisys.com/#/login\">https://console.wasabisys.com/</a></p><blockquote>A quick Gotcha here. Wasabi really pissed me off as they charge you for deleted storage for 90 days. I'm not sure if I goofed this up somewhere but I have contacted support to find out</blockquote><p>Once logged in, go to buckets and create bucket. Name it something like jellyfin-media and select the region that is closest to you. If you're in Europe, pick the Europe one. It's located in the Netherlands. </p><p>Once the bucket is created, we need to create a policy that works with the bucket. This will be applied to a user later which grants access rights to the bucket and fun stuff.</p><p>Click policies on the left hand side navbar </p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/06/image-9.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>In here, name it something like <code>jellyfin-media-ro</code> or what ever you like. As long as you remember it for later!</p><p>Once naming it, paste the below in, but change where it says <code>&lt;bucket name&gt;</code> to the name of your bucket. This will be applied to the <code>docker</code> user we will create next. It's only given read permissions to the media, just in case something weird happens and jellyfin (or you, no judgment) tries to nuke your files. </p><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-yaml\">{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"*\"\n      },\n      \"Action\": [\n        \"s3:GetBucketLocation\",\n        \"s3:ListBucket\",\n        \"s3:ListBucketMultipartUploads\"\n      ],\n      \"Resource\": \"arn:aws:s3:::&lt;bucket name&gt;\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"*\"\n      },\n      \"Action\": \"s3:GetObject\",\n      \"Resource\": \"arn:aws:s3:::&lt;bucket name&gt;/*\"\n    }\n  ]\n}</code></pre><figcaption>jellyfin-media-ro</figcaption></figure><p>For a user that can upload to the bucket create a policy called <code>jellyfin-media-rw</code> where <code>rw</code> is read write.  As usual substitute the <code>&lt;your bucket&gt;</code> with the name of your actual bucket.</p><figure class=\"kg-card kg-code-card\"><pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": \"s3:ListAllMyBuckets\",\n      \"Resource\": \"arn:aws:s3:::*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": \"s3:*\",\n      \"Resource\": [\n        \"arn:aws:s3:::&lt;your bucket&gt;\",\n        \"arn:aws:s3:::&lt;your bucket&gt;/*\"\n      ]\n    }\n  ]\n}</code></pre><figcaption>jellyfin-media-rw</figcaption></figure><p></p><p>Now we can create a user. Click users on the left hand side nav bar.</p><p>Click 'Create user' </p><p>I suggest calling it something like <code>docker</code> or <code>jellyfin</code> or <code>jeff</code> if you really like.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/06/image-10.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>The important part is that you click 'Programmatic (create API key)</p><p>Click next till page 3 where you will assign the policy we created. Here we need the  <code>jellyfin-media-ro</code> policy</p><p>Once done, wait a few seconds and it will ask you to download the keys. Save these and name them respective of the user. </p><p>You will need to do the same thing for the <code>rw</code> user, so create the user and give them the rw policy, naming them something else.</p><hr><h3 id=\"part-2-the-vps-shenanigans\">Part 2: The VPS shenanigans</h3><p></p><p>Depending on if you ave docker already installed, you can skip down a bit. If not please read the below:<br><br>Run the below commands after each other</p><p><code>curl -fsSL https://get.docker.com -o get-docker.sh</code></p><p><code>sh get-docker.sh</code></p><p><code>sudo apt-get install docker-compose</code></p><p>if all went well, docker should be installed</p><p>Now we need to get the docker image to actually use. This can be downloaded using the built in docker 'package manager' if you will. </p><p><code>docker pull jellyfin/jellyfin</code></p><p>Next, we will create the folder structure. </p><pre><code>mkdir docker\nmkdir docker/jellyfin\nmkdir docker/jellyfin/files\nmkdir docker/jellyfin/files/{cache,config,media}\ntouch docker/jellyfin/docker-compose.yml</code></pre><p>If you care, this is what it will look like.</p><pre><code class=\"language-bash\">.\n└── docker\n    └── jellyfin\n        ├── docker-compose.yml\n        └── files\n            ├── cache\n            ├── config\n            └── media\n\n6 directories, 1 file</code></pre><p>Once that is done, move to <code>docker/jellyfin</code> and edit the <code>docker-compose.yml</code> file</p><p><code>sudo nano docker-compose.yml</code> </p><p>In the file paste the below and change out <code>&lt;path/to/config&gt;</code> with the full path to the folders. If you're not sure of them, go to that directory and type <code>pwd</code></p><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-yml\">version: \"3\"\nservices:\n jellyfin:\n  image: jellyfin/jellyfin\n  user: 1000:1000\n  network_mode: \"host\"\n  volumes:\n    - &lt;/path/to/config&gt;:/config\n    - &lt;/path/to/cache&gt;:/cache\n    - &lt;/path/to/media&gt;:/media</code></pre><figcaption>docker-compose.yml</figcaption></figure><p>Next we need to sort out how to present the bucket to jellyfin. This is where rclone comes in. We installed it at the start!</p><p>But before we can start, we need to figure out a way to run the command, and disconnect from that session without stopping the command from running. We will use screen as I am yet to figure out how to use systemd for this</p><p><code>screen</code></p><p>Should show you something like this. if not, <code>sudo apt-get install screen</code></p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/06/image-11.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>Now that that works, we just press enter and it shows us a command line. Press <code>Ctrl + a </code>and then press <code>d</code> (it's a weird combo, dont ask)</p><p>Now we can configure rclone. </p><p>Change directory to <code>.config</code> and if there isn't an rclone folder there <code>mdkir rclone</code> </p><p><code>nano rclone.conf</code></p><p>paste the below in there.  </p><pre><code>[media]\ntype = s3\nenv_auth = \naccess_key_id = &lt;first key in .csv file from user creation&gt;\nsecret_access_key = &lt;second key from user creation&gt;\nregion = \nendpoint = https://s3.eu-central-1.wasabisys.com #if using a europe bucket, else; See below for specifics\nlocation_constraint = \nacl = \nserver_side_encryption = \nstorage_class = \n\n####You dont need to copy this shit in to the file, it's in reference to line 7###\n#Wasabi US East 1 (N. Virginia): s3.wasabisys.com or s3.us-east-1.wasabisys.com\n#Wasabi US East 2 (N. Virginia): s3.us-east-2.wasabisys.com \n#Wasabi US West 1 (Oregon): s3.us-west-1.wasabisys.com\n#Wasabi EU Central 1 (Amsterdam): s3.eu-central-1.wasabisys.com</code></pre><p>once done, exit that bad boy. ( <code>ctrl + x</code> <code>y</code> <code>enter</code>)</p><p>Go to the wasabi interface and click that bucket. Upload your favourite meme or something and then back in your terminal run:</p><p><code>rclone ls media:&lt;bucket name&gt;</code> and you should see the file name there. </p><blockquote>If this fails, then I'm not sure what went wrong. Email me and we can sort it out. </blockquote><p>Next we need to map the bucket to a folder.</p><p>Open screen by typing <code>screen</code></p><p><code>cd docker/jellyfin/files/</code></p><p><code>rclone mount media:&lt;bucket name&gt; media --allow-others --daemon</code></p><p>It may throw back an error about using --allow-others, but just do what the command says and edit the file, remove the # in from of the allow_others or what's closest in the file.</p><p>It should return nothing if it worked. </p><p>Exit screen with <code>ctrl + a</code> <code>d</code></p><p>Change to the media folder <code>docker/jellyfin/files/media</code></p><p>list the stuff in there with <code>ls</code> and it should (hopefully) return the file from earlier</p><h3 id=\"part-3-making-progress\">Part 3: Making progress</h3><p></p><p>In the folder where the docker-compose.yml file exists, run:</p><p><code>docker-compose up</code></p><p>If all went well, you should be able to visit the IP address of the host and be greeted with the jellyfin create user account</p><p><code>http://&lt;ip/hostname&gt;:8096</code></p><p>I suggest you create the tv, movies, podcasts, skin flic and what ever else folders though the webUI for wasabi then upload your stuff there. </p><p>Once everything is uploaded on Jellyfin, add the media as you usually would. </p><p></p><p>Once done, go back to your command line and kill the docker container with <code>ctrl + c</code><br>Now type <code>docker ps -a</code> to see all containers on your host. There should be one called Jellyfin. Make a note of the first 3 characters of the weird looking string before it's name</p><p>The hit it with a <code>docker start &lt;characters&gt;</code>  Hopefully it starts and is off to the races. Go back to the url or IP address and continue as you would.</p><p>If you want to put it behind a nginx reverse proxy, stay tuned for my next post where I talk about how I have everything behind nginx because I'm too stubborn to learn traefik :)</p><p></p><blockquote>this was previously posted <a href=\"https://documentation.breadnet.co.uk/kb/public-web-facing/jellyfin-using-s3-and-docker/\">here</a> but I did a terrible job</blockquote><hr><p>You can hire me via <a href=\"https://www.upwork.com/freelancers/~01c61ee9802b94133e\">Upwork</a> or <a href=\"mailto:work@breadnet.co.uk\">emailing me</a> for weekend projects!</p>","comment_id":"5edc03be544a0d4341b69fd9","plaintext":"Jellyfin is a FOSS media server forked from Emby before they went sketchy.\n\njellyfin/jellyfinThe Free Software Media System. Contribute to jellyfin/jellyfin development by creating an account on GitHub.GitHubjellyfin\n\n\n\nSo in a bid to speed up my jellyfin server, and cut costs I am now fully running my Jellyfin off of a vm on OVH with an s3 based backend, so this does work\n\nI guess you could call this one of my shower thoughts as I wanted to be able to use jellyfin on a Digital ocean droplet or something external just so I don't have to mess about with using a reverse proxy from my home servers, to Digital ocean then out to the world. Whilst I know I could have just opened the ports on my home network, I have a Dynamic IP address which changes every so often.\n\nYes, I could write a simple script to monitor the IP address and see when it changes, then update the domain, but that's not complex enough for me, and I'm terrible at coding.\n\n\n\nWe will first start with the droplet. I suggest going with either of the below for Digital ocean\n\nOr if Digital ocean isn't your cuppa' tea, then vultr have a reasonable product.\n\nOr even GCP's N2, N2D, N1 servers seem to bee half decent for Jellyfin.\n\nThere's nothing stopping you from deploying this at home, just realise that you will need 50mbps+ download and ideally 25mbps+ upload to be able to support multiple users streams externaly if you're going though either an nginx reverse proxy like I am, or straight from you connection\n\n\n\nWhat you want is cores over anything, especially if you're going to be trans-coding.  Yes I did try and use the $5 tier from digital ocean and I wanted to commit alive'nt\n\n\nEnough fuzz, lets get to it!\n\nPrereqs:\n\n\nI am not a security expert, I am a sysadmin. The security of your server is up to you. Do basic things like enable ssh keys and lock down ingress ports with ufw or iptables. What ever angles your dangle.\nI am not a docker expert. I am a sysadmin which means I can use google (I think? bleh) so if you see anything whack in this, feel free to email me\n\n\n\nYou will need:\n\n 1. S3 compatible bucket - I suggest wasabi but you can also use Min.io if you're in to FOSS (wasabi has a 30 day free 1tb trial)\n 2. Ability to read and copy and paste.\n\nThe first step\n\nFirst we will spin up a digital ocean droplet but you can use what ever VPS provider you want.\n\nOnce the VPs is up and you're logged in run the below to update the sources and upgrade any packages that need it. Should not take longer than 5-10 minutes at worst case scenario\n\nsudo apt-get update && sudo apt-get upgrade\n\nOnce that's done, install rclone\n\nsudo apt-get install rclone\n\nNext we need to setup the bucket and fuzz to actually have some media to present to Jellyfin once everything gets up and running.\n\nLogin to wasabi console at https://console.wasabisys.com/\n\nA quick Gotcha here. Wasabi really pissed me off as they charge you for deleted storage for 90 days. I'm not sure if I goofed this up somewhere but I have contacted support to find out\n\nOnce logged in, go to buckets and create bucket. Name it something like jellyfin-media and select the region that is closest to you. If you're in Europe, pick the Europe one. It's located in the Netherlands.\n\nOnce the bucket is created, we need to create a policy that works with the bucket. This will be applied to a user later which grants access rights to the bucket and fun stuff.\n\nClick policies on the left hand side navbar\n\nIn here, name it something like jellyfin-media-ro or what ever you like. As long as you remember it for later!\n\nOnce naming it, paste the below in, but change where it says <bucket name> to the name of your bucket. This will be applied to the docker user we will create next. It's only given read permissions to the media, just in case something weird happens and jellyfin (or you, no judgment) tries to nuke your files.\n\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"*\"\n      },\n      \"Action\": [\n        \"s3:GetBucketLocation\",\n        \"s3:ListBucket\",\n        \"s3:ListBucketMultipartUploads\"\n      ],\n      \"Resource\": \"arn:aws:s3:::<bucket name>\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"*\"\n      },\n      \"Action\": \"s3:GetObject\",\n      \"Resource\": \"arn:aws:s3:::<bucket name>/*\"\n    }\n  ]\n}\n\nFor a user that can upload to the bucket create a policy called jellyfin-media-rw where rw is read write.  As usual substitute the <your bucket> with the name of your actual bucket.\n\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": \"s3:ListAllMyBuckets\",\n      \"Resource\": \"arn:aws:s3:::*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": \"s3:*\",\n      \"Resource\": [\n        \"arn:aws:s3:::<your bucket>\",\n        \"arn:aws:s3:::<your bucket>/*\"\n      ]\n    }\n  ]\n}\n\n\n\nNow we can create a user. Click users on the left hand side nav bar.\n\nClick 'Create user'\n\nI suggest calling it something like docker or jellyfin or jeff if you really like.\n\nThe important part is that you click 'Programmatic (create API key)\n\nClick next till page 3 where you will assign the policy we created. Here we need the  jellyfin-media-ro policy\n\nOnce done, wait a few seconds and it will ask you to download the keys. Save these and name them respective of the user.\n\nYou will need to do the same thing for the rw user, so create the user and give them the rw policy, naming them something else.\n\n\nPart 2: The VPS shenanigans\n\n\n\nDepending on if you ave docker already installed, you can skip down a bit. If not please read the below:\n\nRun the below commands after each other\n\ncurl -fsSL https://get.docker.com -o get-docker.sh\n\nsh get-docker.sh\n\nsudo apt-get install docker-compose\n\nif all went well, docker should be installed\n\nNow we need to get the docker image to actually use. This can be downloaded using the built in docker 'package manager' if you will.\n\ndocker pull jellyfin/jellyfin\n\nNext, we will create the folder structure.\n\nmkdir docker\nmkdir docker/jellyfin\nmkdir docker/jellyfin/files\nmkdir docker/jellyfin/files/{cache,config,media}\ntouch docker/jellyfin/docker-compose.yml\n\nIf you care, this is what it will look like.\n\n.\n└── docker\n    └── jellyfin\n        ├── docker-compose.yml\n        └── files\n            ├── cache\n            ├── config\n            └── media\n\n6 directories, 1 file\n\nOnce that is done, move to docker/jellyfin and edit the docker-compose.yml file\n\nsudo nano docker-compose.yml\n\nIn the file paste the below and change out <path/to/config> with the full path to the folders. If you're not sure of them, go to that directory and type pwd\n\nversion: \"3\"\nservices:\n jellyfin:\n  image: jellyfin/jellyfin\n  user: 1000:1000\n  network_mode: \"host\"\n  volumes:\n    - </path/to/config>:/config\n    - </path/to/cache>:/cache\n    - </path/to/media>:/media\n\nNext we need to sort out how to present the bucket to jellyfin. This is where rclone comes in. We installed it at the start!\n\nBut before we can start, we need to figure out a way to run the command, and disconnect from that session without stopping the command from running. We will use screen as I am yet to figure out how to use systemd for this\n\nscreen\n\nShould show you something like this. if not, sudo apt-get install screen\n\nNow that that works, we just press enter and it shows us a command line. Press Ctrl + a and then press d (it's a weird combo, dont ask)\n\nNow we can configure rclone.\n\nChange directory to .config and if there isn't an rclone folder there mdkir rclone\n\nnano rclone.conf\n\npaste the below in there.  \n\n[media]\ntype = s3\nenv_auth = \naccess_key_id = <first key in .csv file from user creation>\nsecret_access_key = <second key from user creation>\nregion = \nendpoint = https://s3.eu-central-1.wasabisys.com #if using a europe bucket, else; See below for specifics\nlocation_constraint = \nacl = \nserver_side_encryption = \nstorage_class = \n\n####You dont need to copy this shit in to the file, it's in reference to line 7###\n#Wasabi US East 1 (N. Virginia): s3.wasabisys.com or s3.us-east-1.wasabisys.com\n#Wasabi US East 2 (N. Virginia): s3.us-east-2.wasabisys.com \n#Wasabi US West 1 (Oregon): s3.us-west-1.wasabisys.com\n#Wasabi EU Central 1 (Amsterdam): s3.eu-central-1.wasabisys.com\n\nonce done, exit that bad boy. ( ctrl + x y enter)\n\nGo to the wasabi interface and click that bucket. Upload your favourite meme or something and then back in your terminal run:\n\nrclone ls media:<bucket name> and you should see the file name there.\n\nIf this fails, then I'm not sure what went wrong. Email me and we can sort it out.\n\nNext we need to map the bucket to a folder.\n\nOpen screen by typing screen\n\ncd docker/jellyfin/files/\n\nrclone mount media:<bucket name> media --allow-others --daemon\n\nIt may throw back an error about using --allow-others, but just do what the command says and edit the file, remove the # in from of the allow_others or what's closest in the file.\n\nIt should return nothing if it worked.\n\nExit screen with ctrl + a d\n\nChange to the media folder docker/jellyfin/files/media\n\nlist the stuff in there with ls and it should (hopefully) return the file from earlier\n\n\nPart 3: Making progress\n\n\n\nIn the folder where the docker-compose.yml file exists, run:\n\ndocker-compose up\n\nIf all went well, you should be able to visit the IP address of the host and be greeted with the jellyfin create user account\n\nhttp://<ip/hostname>:8096\n\nI suggest you create the tv, movies, podcasts, skin flic and what ever else folders though the webUI for wasabi then upload your stuff there.\n\nOnce everything is uploaded on Jellyfin, add the media as you usually would.\n\n\n\nOnce done, go back to your command line and kill the docker container with ctrl + c\nNow type docker ps -a to see all containers on your host. There should be one called Jellyfin. Make a note of the first 3 characters of the weird looking string before it's name\n\nThe hit it with a docker start <characters>  Hopefully it starts and is off to the races. Go back to the url or IP address and continue as you would.\n\nIf you want to put it behind a nginx reverse proxy, stay tuned for my next post where I talk about how I have everything behind nginx because I'm too stubborn to learn traefik :)\n\n\n\nthis was previously posted here but I did a terrible job\n\nYou can hire me via Upwork or emailing me for weekend projects!","feature_image":"https://images.unsplash.com/photo-1508682157367-7057fcaa4410?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=2000&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2020-06-06T20:59:42.000Z","updated_at":"2023-11-24T14:18:16.000Z","published_at":"2020-06-06T22:10:19.000Z","custom_excerpt":"Wanted to run a media server on a server but don't have the storage? Here's the solution\n","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"6425f3bc192c0c150413d591","uuid":"5f2c9eb5-632f-4b4e-9c8e-b0cad5073aa9","title":"Nginx and Lets encrypt, a story of reverse proxying","slug":"nginx-reverse","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"code\",{\"code\":\"sudo apt-get install nginx -y\"}],[\"code\",{\"code\":\"sudo systemctl enable nginx\"}],[\"code\",{\"code\":\"netstat -plnt | grep 80\\nnetstat -plnt | grep 443\"}],[\"code\",{\"code\":\"sudo nano /etc/nginx/sites-available/app.domain.tld\\n\"}],[\"code\",{\"code\":\"server {     \\n      listen 80;   \\n      listen [::]:80;     \\n      server_name <domain>.breadnet.co.uk;\\n     add_header Strict-Transport-Security \\\"max-age=15552000; includeSubDomains\\\" always;      \\n\\taccess_log /var/log/nginx/<domain>/access.log;\\n\\terror_log /var/log/nginx/<domain>/error.log;\\n       location / {         \\n       proxy_pass <ip/hostname>;         \\n       proxy_next_upstream error  timeout invalid_header http_500 http_502 http_503;         \\n       proxy_set_header Host $host;        \\n       proxy_set_header X-Real-IP $remote_addr;    \\n       proxy_set_header X-Forward-For $proxy_add_x_forwarded_for;\\n       proxy_set_header X-Forwarded-Proto https;\\n       proxy_redirect off;         \\n       proxy_read_timeout 5m;     \\n   }  \\nclient_max_body_size 10M; \\n}\\n\"}],[\"code\",{\"code\":\"sudo ln -s /etc/nginx/sites-available/app.breadnet.co.uk /etc/nginx/sites-enabled/app.breadnet.co.uk\\n\"}],[\"code\",{\"code\":\"sudo apt-get update\\nsudo apt-get install software-properties-common\\nsudo add-apt-repository universe\\nsudo add-apt-repository ppa:certbot/certbot\\nsudo apt-get update\\nsudo apt-get install certbot python3-certbot-nginx\"}],[\"code\",{\"code\":\"certbot --nginx -d app.domain.com\"}],[\"hr\",{}],[\"code\",{\"code\":\"sudo nano /etc/hosts\\n\"}],[\"code\",{\"code\":\"root@reversinator:~$ cat /etc/hosts\\n127.0.0.1\\tlocalhost\\n127.0.1.1\\treversinator\\n151.101.192.144 home.connection\\n\\n\\n# The following lines are desirable for IPv6 capable hosts\\n::1     ip6-localhost ip6-loopback\\nfe00::0 ip6-localnet\\nff00::0 ip6-mcastprefix\\nff02::1 ip6-allnodes\\nff02::2 ip6-allrouters\"}],[\"code\",{\"code\":\"/etc/nginx/sites-available/<page.domain.tld>\"}],[\"code\",{\"code\":\"server {     \\n      listen 80;   \\n      listen [::]:80;     \\n      server_name <sub>.<domain>.<tld>;\\n     add_header Strict-Transport-Security \\\"max-age=15552000; includeSubDomains\\\" always;      \\n\\taccess_log /var/log/nginx/<domain>/access.log;\\n\\terror_log /var/log/nginx/<domain>/error.log;\\n       location / {         \\n       proxy_pass <ip/hostname>;         \\n       proxy_next_upstream error  timeout invalid_header http_500 http_502 http_503;         \\n       proxy_set_header Host $host;        \\n       proxy_set_header X-Real-IP $remote_addr;    \\n       proxy_set_header X-Forward-For $proxy_add_x_forwarded_for;\\n       proxy_set_header X-Forwarded-Proto https;\\n       proxy_redirect off;         \\n       proxy_read_timeout 5m;     \\n   }  \\nclient_max_body_size 10M; \\n}\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/06/image-14.png\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/06/image-15.png\"}],[\"code\",{\"code\":\"curl -s https://install.zerotier.com | sudo bash\\n\"}],[\"code\",{\"code\":\"sudo zerotier-cli join <network ID>\"}],[\"code\",{\"code\":\"server {     \\n      listen 80;   \\n      listen [::]:80;     \\n      server_name <sub>.<domain>.<tld>;\\n     add_header Strict-Transport-Security \\\"max-age=15552000; includeSubDomains\\\" always;      \\n\\taccess_log /var/log/nginx/<domain>/access.log;\\n\\terror_log /var/log/nginx/<domain>/error.log;\\n       location / {         \\n       proxy_pass <zerotier address of content server>;         \\n       proxy_next_upstream error  timeout invalid_header http_500 http_502 http_503;         \\n       proxy_set_header Host $host;        \\n       proxy_set_header X-Real-IP $remote_addr;    \\n       proxy_set_header X-Forward-For $proxy_add_x_forwarded_for;\\n       proxy_set_header X-Forwarded-Proto https;\\n       proxy_redirect off;         \\n       proxy_read_timeout 5m;     \\n   }  \\nclient_max_body_size 10M; \\n}\"}],[\"code\",{\"code\":\"sudo ln -s /etc/nginx/sites-available/config_name /etc/nginx/sites-enabled/config_name\\n\"}],[\"code\",{\"code\":\"systemctl restart nginx\"}],[\"hr\",{}]],\"markups\":[[\"a\",[\"href\",\"https://m.do.co/c/77be3c3aa96c\"]],[\"a\",[\"href\",\"https://wiki.ubuntu.com/Releases\"]],[\"code\"],[\"strong\"],[\"a\",[\"href\",\"https://my.zerotier.com/\"]],[\"a\",[\"href\",\"https://www.zerotier.com/download/\"]],[\"a\",[\"href\",\"https://www.upwork.com/freelancers/~01c61ee9802b94133e\"]],[\"a\",[\"href\",\"mailto:work@breadnet.co.uk\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"This is something that took some time to get my head around, but once I managed to (somewhat) figure it out, it's made exposing my services to the internet a lot easier\"]]],[1,\"p\",[[0,[],0,\"After a few years of building up the courage and knowledge to host stuff that my friends and I can use, it's become apparent that updating a DNS record every time your public IP at home changes, isn't a great way to host things. This is where Nginx reverse proxy comes in.\"]]],[1,\"blockquote\",[[0,[],0,\"I was alerted by a user on reddit that cloudflare have an API you can use if your DNS is managed from them that makes updating dynamic ip's to domains simle if you decide to host a reverse proxy server at home\"]]],[1,\"p\",[[0,[],0,\"Firstly we will need a server somewhere on the internet. I suggest someone like \"],[0,[0],1,\"Digital Ocean\"],[0,[],0,\" or Vultr with their cheapest option. Nginx does not require a large amount of resources to run which is nice. \"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"For this scenario, lets image I am hosting this website on my servers at home, and don't want the whole world to know my home connections IP address. Slap it behind a reverse proxy and problem solved.\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"In this example I will be using Ubuntu. Any version works, as long as it's not \"],[0,[1],1,\"EOL\"]]],[1,\"p\",[[0,[],0,\"We can start by installing Nginx\"]]],[10,0],[1,\"p\",[[0,[],0,\"Now once installed, I like to enable it so it starts on a system reboot. \"]]],[10,1],[1,\"p\",[[0,[],0,\"Sweet, now if you go to the IP address of your server you should be welcomed with a generic welcome/ hello world. (It's been a while. I cant remember)\"]]],[1,\"p\",[[0,[],0,\"If there are issues with Nginx starting, there may be another application running on port 80 or 443. You can check with the below\"]]],[10,2],[1,\"h2\",[[0,[],0,\"The reverse'y part\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"Now that we have nginx up and running on our droplet/ vps/ what ever we can start reversing stuff.\"]]],[1,\"p\",[[0,[],0,\"Based on where your server is running/ what you intend to use it for, your mileage may vary. I am going to explain 3 types of scenarios I use and why.\"]]],[1,\"p\",[[0,[],0,\"The first is a very generic way that you can build on, and the following 3 are how it can be fine tuned\"]]],[1,\"p\",[[0,[],0,\"This is the most basic configuration I could think of. You have a server on your network, we will call it \"],[0,[2],1,\"edge\"],[0,[],0,\" and on that network you also have a server called \"],[0,[2],1,\"app\"],[0,[],0,\" but you don't want to expose the \"],[0,[2],1,\"app\"],[0,[],0,\" server to the tinternet. \"]]],[1,\"p\",[[0,[],0,\"You will want to make a new nginx config based off what you want the end URL to look like. For simplicity we will make the domain \"],[0,[2],1,\"app.breadnet.co.uk\"],[0,[],0,\" but you will need to make this your domain. \"]]],[10,3],[1,\"p\",[[0,[],0,\"In there, paste the following:\"]]],[10,4],[1,\"p\",[[0,[],0,\"You will need to edit servername to your domain, and then under \"],[0,[2],1,\"proxy_pass\"],[0,[],0,\" you need to ensure this points to the domain or IP address of \"],[0,[2],1,\"app\"],[0,[],0,\" \"]]],[1,\"p\",[[0,[],0,\"I find with some services like Cachet and sensative, the web server config on the serving node needs to have it's web address set as the end result of passing it via nginx. \"]]],[1,\"p\",[[0,[],0,\"Now we can enable the page:\"]]],[10,5],[1,\"p\",[[0,[],0,\"Once done, restart nginx with \"],[0,[2],1,\"sudo systemctl restart nginx\"],[0,[],0,\" \"]]],[1,\"p\",[[0,[],0,\"You will need to login to you DNS providor and add an A record pointing to the IP address of \"],[0,[2],1,\"edge\"],[0,[],0,\" be it on your home network (in that case you will need to open a port) or a public server on digital ocean or vultr. \"]]],[1,\"p\",[[0,[],0,\"Enable SSL with Certbot. You should have this installed by now but if not please install it\"]]],[10,6],[10,7],[10,8],[1,\"p\",[[0,[],0,\"Below are more case specific examples you can build off of\"]]],[1,\"h2\",[[0,[],0,\"1: Opening a port\"]]],[1,\"p\",[[0,[],0,\"I run a media server called jellyfin on my local servers at home, and would like to be able to access it out and about with out having to connect to a VPN, as well as allow my friends and family to use it. Due to it's nature I have opened the firewall at my home to only allow port 8069 from my Digital ocean droplet to connect. \"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"To enable this to work, you will need 3 things.\"]]],[3,\"ol\",[[[0,[],0,\"Ability to copy, paste and edit\"]],[[0,[],0,\"IP address of your home internet connection\"]],[[0,[],0,\"Domain name like \"],[0,[2],1,\"media.example.com\"],[0,[],0,\" or any subdomain you like\"]]]],[1,\"p\",[[0,[],0,\"Firstly open the firewall at home to forward connections on the port of needing to the server. Once that is done, I personally add an entry in to \"],[0,[2],1,\"/etc/hosts\"],[0,[],0,\" so as my IP address changes, I don't need to update multiple nginx config files.\"]]],[10,9],[1,\"p\",[[0,[],0,\"In this file you will want to give the IP a name, I use \"],[0,[2],1,\"home.connection\"],[0,[],0,\" so my file looks like:\"]]],[10,10],[1,\"p\",[[0,[],0,\"Obviously change out your IP address and name.\"]]],[1,\"p\",[[0,[],0,\"Now we need to edit the nginx config. For this I personally prefer to name the file the page it will point to. Just makes auditing quicker\"]]],[10,11],[1,\"p\",[[0,[],0,\"So here it would be \"],[0,[2],1,\"media.example.com\"],[0,[],0,\" but you're welcome to name your config file \"],[0,[2],1,\"jeff\"]]],[1,\"p\",[[0,[],0,\"In that file, paste and edit\"]]],[10,12],[1,\"p\",[[0,[],0,\"Here you will need to change \"],[0,[2],1,\"<sub>.<domain>.<tld>\"],[0,[],0,\" to the domain that this will refer to. You can also add \"],[0,[2],0,\"www.\"],[0,[2],2,\"<sub>.<domain>.<tld>\"],[0,[],0,\" after the first one if you want \"],[0,[2],1,\"www.\"],[0,[],0,\" to go to that page.\"]]],[1,\"p\",[[0,[],0,\"Once that's done, you will also need to change \"],[0,[2],1,\"<ip/hostname>\"],[0,[],0,\" to reflect where the content is hosted, or what you are reverse proxying. Once that is done, you may also want to edit the line which reads \"],[0,[2],1,\"access_log\"],[0,[],0,\" to match \"],[0,[2],1,\"<domain>\"],[0,[],0,\" to the domain it is on. This just helps to separate logs for easy viewing. \"]]],[1,\"p\",[[0,[],0,\"Once done, you will need to symlink the config file to \"],[0,[2],1,\"sites-enabled\"],[0,[],0,\" and then restart Nginx. \"]]],[1,\"p\",[]],[1,\"h2\",[[0,[3],1,\"2: Using Zerotier\"]]],[1,\"p\",[[0,[],0,\"I'm not sure how best to describe Zerotier other than as a Virtual lan that spans the internet. You connect to it though an application either on windows or linux.\"]]],[1,\"p\",[[0,[],0,\"You will need to create an account and install it on both machines. \"]]],[1,\"p\",[[0,[],0,\"Create an account at \"],[0,[4],1,\"my.zerotier.com/\"],[0,[],0,\" \"]]],[1,\"p\",[[0,[],0,\"Once your account is creates, click \"],[0,[2],1,\"Networks\"],[0,[],0,\" at the top and then \"],[0,[2],1,\"+ Create a Network\"]]],[1,\"p\",[[0,[],0,\"You will see one appear under the \"],[0,[2],1,\"Your Networks\"],[0,[],0,\" section  \"]]],[1,\"p\",[]],[10,13],[1,\"p\",[[0,[],0,\"Click the ID of the network and pick the network range you want. You NEED it to not be used anywhere else!\"]]],[10,14],[1,\"p\",[[0,[],0,\"Once that is done, you will need to install Zerotier on your content hosting server, as well as the reverse proxy server.\"]]],[1,\"p\",[[0,[],0,\"If you trust SSL on linux:\"]]],[10,15],[1,\"p\",[[0,[],0,\"Else, it can be installed \"],[0,[5],1,\"here\"]]],[1,\"p\",[[0,[],0,\"Now we need to add the server's to the network!\"]]],[10,16],[1,\"p\",[[0,[],0,\"If it returns with \"],[0,[2],1,\"200\"],[0,[],0,\" it's happy days. Wait a few minutes and you should see the first server show up in the management page. Add a name to it so you know what's what. \"]]],[1,\"p\",[[0,[],0,\"Repeat for the other server. \"]]],[1,\"p\",[[0,[],0,\"make sure you can ping the servers via zerotier. If you cant, wait a few minutes and run pings from both servers to each other. It's weird but works eventually.\"]]],[1,\"p\",[[0,[],0,\"Next we will need to configure NGINX. This is exactly the same as all the other configurations except we will use the zerotier IP address of the content server opposed to another address.\"]]],[1,\"p\",[[0,[],0,\"As usual, make this file in the  \"],[0,[2],1,\"/etc/nginx/sites-available/\"],[0,[],0,\" folder with the name being the end domain. (see other configs above)\"]]],[10,17],[1,\"p\",[[0,[],0,\"Now symlink it\"]]],[10,18],[1,\"p\",[[0,[],0,\"Restart nginx and you should be off to the races\"]]],[10,19],[1,\"p\",[[0,[],0,\"As with all of these, I suggest SSL!\"]]],[1,\"p\",[[0,[],0,\"This is at the top of the page and I suggest you do this! \"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"As will anything on this site, if you have issues or nothing makes sense, drop me an email!\"]]],[10,20],[1,\"p\",[[0,[],0,\"You can hire me via \"],[0,[6],1,\"Upwork\"],[0,[],0,\" or \"],[0,[7],1,\"emailing me\"],[0,[],0,\" for weekend projects!\"]]]],\"ghostVersion\":\"3.0\"}","lexical":null,"html":"<p>This is something that took some time to get my head around, but once I managed to (somewhat) figure it out, it's made exposing my services to the internet a lot easier</p><p>After a few years of building up the courage and knowledge to host stuff that my friends and I can use, it's become apparent that updating a DNS record every time your public IP at home changes, isn't a great way to host things. This is where Nginx reverse proxy comes in.</p><blockquote>I was alerted by a user on reddit that cloudflare have an API you can use if your DNS is managed from them that makes updating dynamic ip's to domains simle if you decide to host a reverse proxy server at home</blockquote><p>Firstly we will need a server somewhere on the internet. I suggest someone like <a href=\"https://m.do.co/c/77be3c3aa96c\">Digital Ocean</a> or Vultr with their cheapest option. Nginx does not require a large amount of resources to run which is nice. </p><p></p><p>For this scenario, lets image I am hosting this website on my servers at home, and don't want the whole world to know my home connections IP address. Slap it behind a reverse proxy and problem solved.</p><p></p><p>In this example I will be using Ubuntu. Any version works, as long as it's not <a href=\"https://wiki.ubuntu.com/Releases\">EOL</a></p><p>We can start by installing Nginx</p><pre><code>sudo apt-get install nginx -y</code></pre><p>Now once installed, I like to enable it so it starts on a system reboot. </p><pre><code>sudo systemctl enable nginx</code></pre><p>Sweet, now if you go to the IP address of your server you should be welcomed with a generic welcome/ hello world. (It's been a while. I cant remember)</p><p>If there are issues with Nginx starting, there may be another application running on port 80 or 443. You can check with the below</p><pre><code>netstat -plnt | grep 80\nnetstat -plnt | grep 443</code></pre><h2 id=\"the-reverse-y-part\">The reverse'y part</h2><p></p><p>Now that we have nginx up and running on our droplet/ vps/ what ever we can start reversing stuff.</p><p>Based on where your server is running/ what you intend to use it for, your mileage may vary. I am going to explain 3 types of scenarios I use and why.</p><p>The first is a very generic way that you can build on, and the following 3 are how it can be fine tuned</p><p>This is the most basic configuration I could think of. You have a server on your network, we will call it <code>edge</code> and on that network you also have a server called <code>app</code> but you don't want to expose the <code>app</code> server to the tinternet. </p><p>You will want to make a new nginx config based off what you want the end URL to look like. For simplicity we will make the domain <code>app.breadnet.co.uk</code> but you will need to make this your domain. </p><pre><code>sudo nano /etc/nginx/sites-available/app.domain.tld\n</code></pre><p>In there, paste the following:</p><pre><code>server {     \n      listen 80;   \n      listen [::]:80;     \n      server_name &lt;domain&gt;.breadnet.co.uk;\n     add_header Strict-Transport-Security \"max-age=15552000; includeSubDomains\" always;      \n\taccess_log /var/log/nginx/&lt;domain&gt;/access.log;\n\terror_log /var/log/nginx/&lt;domain&gt;/error.log;\n       location / {         \n       proxy_pass &lt;ip/hostname&gt;;         \n       proxy_next_upstream error  timeout invalid_header http_500 http_502 http_503;         \n       proxy_set_header Host $host;        \n       proxy_set_header X-Real-IP $remote_addr;    \n       proxy_set_header X-Forward-For $proxy_add_x_forwarded_for;\n       proxy_set_header X-Forwarded-Proto https;\n       proxy_redirect off;         \n       proxy_read_timeout 5m;     \n   }  \nclient_max_body_size 10M; \n}\n</code></pre><p>You will need to edit servername to your domain, and then under <code>proxy_pass</code> you need to ensure this points to the domain or IP address of <code>app</code> </p><p>I find with some services like Cachet and sensative, the web server config on the serving node needs to have it's web address set as the end result of passing it via nginx. </p><p>Now we can enable the page:</p><pre><code>sudo ln -s /etc/nginx/sites-available/app.breadnet.co.uk /etc/nginx/sites-enabled/app.breadnet.co.uk\n</code></pre><p>Once done, restart nginx with <code>sudo systemctl restart nginx</code> </p><p>You will need to login to you DNS providor and add an A record pointing to the IP address of <code>edge</code> be it on your home network (in that case you will need to open a port) or a public server on digital ocean or vultr. </p><p>Enable SSL with Certbot. You should have this installed by now but if not please install it</p><pre><code>sudo apt-get update\nsudo apt-get install software-properties-common\nsudo add-apt-repository universe\nsudo add-apt-repository ppa:certbot/certbot\nsudo apt-get update\nsudo apt-get install certbot python3-certbot-nginx</code></pre><pre><code>certbot --nginx -d app.domain.com</code></pre><hr><p>Below are more case specific examples you can build off of</p><h2 id=\"1-opening-a-port\">1: Opening a port</h2><p>I run a media server called jellyfin on my local servers at home, and would like to be able to access it out and about with out having to connect to a VPN, as well as allow my friends and family to use it. Due to it's nature I have opened the firewall at my home to only allow port 8069 from my Digital ocean droplet to connect. </p><p></p><p>To enable this to work, you will need 3 things.</p><ol><li>Ability to copy, paste and edit</li><li>IP address of your home internet connection</li><li>Domain name like <code>media.example.com</code> or any subdomain you like</li></ol><p>Firstly open the firewall at home to forward connections on the port of needing to the server. Once that is done, I personally add an entry in to <code>/etc/hosts</code> so as my IP address changes, I don't need to update multiple nginx config files.</p><pre><code>sudo nano /etc/hosts\n</code></pre><p>In this file you will want to give the IP a name, I use <code>home.connection</code> so my file looks like:</p><pre><code>root@reversinator:~$ cat /etc/hosts\n127.0.0.1\tlocalhost\n127.0.1.1\treversinator\n151.101.192.144 home.connection\n\n\n# The following lines are desirable for IPv6 capable hosts\n::1     ip6-localhost ip6-loopback\nfe00::0 ip6-localnet\nff00::0 ip6-mcastprefix\nff02::1 ip6-allnodes\nff02::2 ip6-allrouters</code></pre><p>Obviously change out your IP address and name.</p><p>Now we need to edit the nginx config. For this I personally prefer to name the file the page it will point to. Just makes auditing quicker</p><pre><code>/etc/nginx/sites-available/&lt;page.domain.tld&gt;</code></pre><p>So here it would be <code>media.example.com</code> but you're welcome to name your config file <code>jeff</code></p><p>In that file, paste and edit</p><pre><code>server {     \n      listen 80;   \n      listen [::]:80;     \n      server_name &lt;sub&gt;.&lt;domain&gt;.&lt;tld&gt;;\n     add_header Strict-Transport-Security \"max-age=15552000; includeSubDomains\" always;      \n\taccess_log /var/log/nginx/&lt;domain&gt;/access.log;\n\terror_log /var/log/nginx/&lt;domain&gt;/error.log;\n       location / {         \n       proxy_pass &lt;ip/hostname&gt;;         \n       proxy_next_upstream error  timeout invalid_header http_500 http_502 http_503;         \n       proxy_set_header Host $host;        \n       proxy_set_header X-Real-IP $remote_addr;    \n       proxy_set_header X-Forward-For $proxy_add_x_forwarded_for;\n       proxy_set_header X-Forwarded-Proto https;\n       proxy_redirect off;         \n       proxy_read_timeout 5m;     \n   }  \nclient_max_body_size 10M; \n}</code></pre><p>Here you will need to change <code>&lt;sub&gt;.&lt;domain&gt;.&lt;tld&gt;</code> to the domain that this will refer to. You can also add <code>www.<code>&lt;sub&gt;.&lt;domain&gt;.&lt;tld&gt;</code></code> after the first one if you want <code>www.</code> to go to that page.</p><p>Once that's done, you will also need to change <code>&lt;ip/hostname&gt;</code> to reflect where the content is hosted, or what you are reverse proxying. Once that is done, you may also want to edit the line which reads <code>access_log</code> to match <code>&lt;domain&gt;</code> to the domain it is on. This just helps to separate logs for easy viewing. </p><p>Once done, you will need to symlink the config file to <code>sites-enabled</code> and then restart Nginx. </p><p></p><h2 id=\"2-using-zerotier\"><strong>2: Using Zerotier</strong></h2><p>I'm not sure how best to describe Zerotier other than as a Virtual lan that spans the internet. You connect to it though an application either on windows or linux.</p><p>You will need to create an account and install it on both machines. </p><p>Create an account at <a href=\"https://my.zerotier.com/\">my.zerotier.com/</a> </p><p>Once your account is creates, click <code>Networks</code> at the top and then <code>+ Create a Network</code></p><p>You will see one appear under the <code>Your Networks</code> section  </p><p></p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/06/image-14.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>Click the ID of the network and pick the network range you want. You NEED it to not be used anywhere else!</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/06/image-15.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>Once that is done, you will need to install Zerotier on your content hosting server, as well as the reverse proxy server.</p><p>If you trust SSL on linux:</p><pre><code>curl -s https://install.zerotier.com | sudo bash\n</code></pre><p>Else, it can be installed <a href=\"https://www.zerotier.com/download/\">here</a></p><p>Now we need to add the server's to the network!</p><pre><code>sudo zerotier-cli join &lt;network ID&gt;</code></pre><p>If it returns with <code>200</code> it's happy days. Wait a few minutes and you should see the first server show up in the management page. Add a name to it so you know what's what. </p><p>Repeat for the other server. </p><p>make sure you can ping the servers via zerotier. If you cant, wait a few minutes and run pings from both servers to each other. It's weird but works eventually.</p><p>Next we will need to configure NGINX. This is exactly the same as all the other configurations except we will use the zerotier IP address of the content server opposed to another address.</p><p>As usual, make this file in the  <code>/etc/nginx/sites-available/</code> folder with the name being the end domain. (see other configs above)</p><pre><code>server {     \n      listen 80;   \n      listen [::]:80;     \n      server_name &lt;sub&gt;.&lt;domain&gt;.&lt;tld&gt;;\n     add_header Strict-Transport-Security \"max-age=15552000; includeSubDomains\" always;      \n\taccess_log /var/log/nginx/&lt;domain&gt;/access.log;\n\terror_log /var/log/nginx/&lt;domain&gt;/error.log;\n       location / {         \n       proxy_pass &lt;zerotier address of content server&gt;;         \n       proxy_next_upstream error  timeout invalid_header http_500 http_502 http_503;         \n       proxy_set_header Host $host;        \n       proxy_set_header X-Real-IP $remote_addr;    \n       proxy_set_header X-Forward-For $proxy_add_x_forwarded_for;\n       proxy_set_header X-Forwarded-Proto https;\n       proxy_redirect off;         \n       proxy_read_timeout 5m;     \n   }  \nclient_max_body_size 10M; \n}</code></pre><p>Now symlink it</p><pre><code>sudo ln -s /etc/nginx/sites-available/config_name /etc/nginx/sites-enabled/config_name\n</code></pre><p>Restart nginx and you should be off to the races</p><pre><code>systemctl restart nginx</code></pre><p>As with all of these, I suggest SSL!</p><p>This is at the top of the page and I suggest you do this! </p><p></p><p>As will anything on this site, if you have issues or nothing makes sense, drop me an email!</p><hr><p>You can hire me via <a href=\"https://www.upwork.com/freelancers/~01c61ee9802b94133e\">Upwork</a> or <a href=\"mailto:work@breadnet.co.uk\">emailing me</a> for weekend projects!</p>","comment_id":"5ee01931544a0d4341b6a15c","plaintext":"This is something that took some time to get my head around, but once I managed to (somewhat) figure it out, it's made exposing my services to the internet a lot easier\n\nAfter a few years of building up the courage and knowledge to host stuff that my friends and I can use, it's become apparent that updating a DNS record every time your public IP at home changes, isn't a great way to host things. This is where Nginx reverse proxy comes in.\n\nI was alerted by a user on reddit that cloudflare have an API you can use if your DNS is managed from them that makes updating dynamic ip's to domains simle if you decide to host a reverse proxy server at home\n\nFirstly we will need a server somewhere on the internet. I suggest someone like Digital Ocean or Vultr with their cheapest option. Nginx does not require a large amount of resources to run which is nice.\n\n\n\nFor this scenario, lets image I am hosting this website on my servers at home, and don't want the whole world to know my home connections IP address. Slap it behind a reverse proxy and problem solved.\n\n\n\nIn this example I will be using Ubuntu. Any version works, as long as it's not EOL\n\nWe can start by installing Nginx\n\nsudo apt-get install nginx -y\n\nNow once installed, I like to enable it so it starts on a system reboot.\n\nsudo systemctl enable nginx\n\nSweet, now if you go to the IP address of your server you should be welcomed with a generic welcome/ hello world. (It's been a while. I cant remember)\n\nIf there are issues with Nginx starting, there may be another application running on port 80 or 443. You can check with the below\n\nnetstat -plnt | grep 80\nnetstat -plnt | grep 443\n\n\nThe reverse'y part\n\n\n\nNow that we have nginx up and running on our droplet/ vps/ what ever we can start reversing stuff.\n\nBased on where your server is running/ what you intend to use it for, your mileage may vary. I am going to explain 3 types of scenarios I use and why.\n\nThe first is a very generic way that you can build on, and the following 3 are how it can be fine tuned\n\nThis is the most basic configuration I could think of. You have a server on your network, we will call it edge and on that network you also have a server called app but you don't want to expose the app server to the tinternet.\n\nYou will want to make a new nginx config based off what you want the end URL to look like. For simplicity we will make the domain app.breadnet.co.uk but you will need to make this your domain.\n\nsudo nano /etc/nginx/sites-available/app.domain.tld\n\n\nIn there, paste the following:\n\nserver {     \n      listen 80;   \n      listen [::]:80;     \n      server_name <domain>.breadnet.co.uk;\n     add_header Strict-Transport-Security \"max-age=15552000; includeSubDomains\" always;      \n\taccess_log /var/log/nginx/<domain>/access.log;\n\terror_log /var/log/nginx/<domain>/error.log;\n       location / {         \n       proxy_pass <ip/hostname>;         \n       proxy_next_upstream error  timeout invalid_header http_500 http_502 http_503;         \n       proxy_set_header Host $host;        \n       proxy_set_header X-Real-IP $remote_addr;    \n       proxy_set_header X-Forward-For $proxy_add_x_forwarded_for;\n       proxy_set_header X-Forwarded-Proto https;\n       proxy_redirect off;         \n       proxy_read_timeout 5m;     \n   }  \nclient_max_body_size 10M; \n}\n\n\nYou will need to edit servername to your domain, and then under proxy_pass you need to ensure this points to the domain or IP address of app\n\nI find with some services like Cachet and sensative, the web server config on the serving node needs to have it's web address set as the end result of passing it via nginx.\n\nNow we can enable the page:\n\nsudo ln -s /etc/nginx/sites-available/app.breadnet.co.uk /etc/nginx/sites-enabled/app.breadnet.co.uk\n\n\nOnce done, restart nginx with sudo systemctl restart nginx\n\nYou will need to login to you DNS providor and add an A record pointing to the IP address of edge be it on your home network (in that case you will need to open a port) or a public server on digital ocean or vultr.\n\nEnable SSL with Certbot. You should have this installed by now but if not please install it\n\nsudo apt-get update\nsudo apt-get install software-properties-common\nsudo add-apt-repository universe\nsudo add-apt-repository ppa:certbot/certbot\nsudo apt-get update\nsudo apt-get install certbot python3-certbot-nginx\n\ncertbot --nginx -d app.domain.com\n\nBelow are more case specific examples you can build off of\n\n\n1: Opening a port\n\nI run a media server called jellyfin on my local servers at home, and would like to be able to access it out and about with out having to connect to a VPN, as well as allow my friends and family to use it. Due to it's nature I have opened the firewall at my home to only allow port 8069 from my Digital ocean droplet to connect.\n\n\n\nTo enable this to work, you will need 3 things.\n\n 1. Ability to copy, paste and edit\n 2. IP address of your home internet connection\n 3. Domain name like media.example.com or any subdomain you like\n\nFirstly open the firewall at home to forward connections on the port of needing to the server. Once that is done, I personally add an entry in to /etc/hosts so as my IP address changes, I don't need to update multiple nginx config files.\n\nsudo nano /etc/hosts\n\n\nIn this file you will want to give the IP a name, I use home.connection so my file looks like:\n\nroot@reversinator:~$ cat /etc/hosts\n127.0.0.1\tlocalhost\n127.0.1.1\treversinator\n151.101.192.144 home.connection\n\n\n# The following lines are desirable for IPv6 capable hosts\n::1     ip6-localhost ip6-loopback\nfe00::0 ip6-localnet\nff00::0 ip6-mcastprefix\nff02::1 ip6-allnodes\nff02::2 ip6-allrouters\n\nObviously change out your IP address and name.\n\nNow we need to edit the nginx config. For this I personally prefer to name the file the page it will point to. Just makes auditing quicker\n\n/etc/nginx/sites-available/<page.domain.tld>\n\nSo here it would be media.example.com but you're welcome to name your config file jeff\n\nIn that file, paste and edit\n\nserver {     \n      listen 80;   \n      listen [::]:80;     \n      server_name <sub>.<domain>.<tld>;\n     add_header Strict-Transport-Security \"max-age=15552000; includeSubDomains\" always;      \n\taccess_log /var/log/nginx/<domain>/access.log;\n\terror_log /var/log/nginx/<domain>/error.log;\n       location / {         \n       proxy_pass <ip/hostname>;         \n       proxy_next_upstream error  timeout invalid_header http_500 http_502 http_503;         \n       proxy_set_header Host $host;        \n       proxy_set_header X-Real-IP $remote_addr;    \n       proxy_set_header X-Forward-For $proxy_add_x_forwarded_for;\n       proxy_set_header X-Forwarded-Proto https;\n       proxy_redirect off;         \n       proxy_read_timeout 5m;     \n   }  \nclient_max_body_size 10M; \n}\n\nHere you will need to change <sub>.<domain>.<tld> to the domain that this will refer to. You can also add www.<sub>.<domain>.<tld> after the first one if you want www. to go to that page.\n\nOnce that's done, you will also need to change <ip/hostname> to reflect where the content is hosted, or what you are reverse proxying. Once that is done, you may also want to edit the line which reads access_log to match <domain> to the domain it is on. This just helps to separate logs for easy viewing.\n\nOnce done, you will need to symlink the config file to sites-enabled and then restart Nginx.\n\n\n\n\n2: Using Zerotier\n\nI'm not sure how best to describe Zerotier other than as a Virtual lan that spans the internet. You connect to it though an application either on windows or linux.\n\nYou will need to create an account and install it on both machines.\n\nCreate an account at my.zerotier.com/\n\nOnce your account is creates, click Networks at the top and then + Create a Network\n\nYou will see one appear under the Your Networks section  \n\n\n\nClick the ID of the network and pick the network range you want. You NEED it to not be used anywhere else!\n\nOnce that is done, you will need to install Zerotier on your content hosting server, as well as the reverse proxy server.\n\nIf you trust SSL on linux:\n\ncurl -s https://install.zerotier.com | sudo bash\n\n\nElse, it can be installed here\n\nNow we need to add the server's to the network!\n\nsudo zerotier-cli join <network ID>\n\nIf it returns with 200 it's happy days. Wait a few minutes and you should see the first server show up in the management page. Add a name to it so you know what's what.\n\nRepeat for the other server.\n\nmake sure you can ping the servers via zerotier. If you cant, wait a few minutes and run pings from both servers to each other. It's weird but works eventually.\n\nNext we will need to configure NGINX. This is exactly the same as all the other configurations except we will use the zerotier IP address of the content server opposed to another address.\n\nAs usual, make this file in the  /etc/nginx/sites-available/ folder with the name being the end domain. (see other configs above)\n\nserver {     \n      listen 80;   \n      listen [::]:80;     \n      server_name <sub>.<domain>.<tld>;\n     add_header Strict-Transport-Security \"max-age=15552000; includeSubDomains\" always;      \n\taccess_log /var/log/nginx/<domain>/access.log;\n\terror_log /var/log/nginx/<domain>/error.log;\n       location / {         \n       proxy_pass <zerotier address of content server>;         \n       proxy_next_upstream error  timeout invalid_header http_500 http_502 http_503;         \n       proxy_set_header Host $host;        \n       proxy_set_header X-Real-IP $remote_addr;    \n       proxy_set_header X-Forward-For $proxy_add_x_forwarded_for;\n       proxy_set_header X-Forwarded-Proto https;\n       proxy_redirect off;         \n       proxy_read_timeout 5m;     \n   }  \nclient_max_body_size 10M; \n}\n\nNow symlink it\n\nsudo ln -s /etc/nginx/sites-available/config_name /etc/nginx/sites-enabled/config_name\n\n\nRestart nginx and you should be off to the races\n\nsystemctl restart nginx\n\nAs with all of these, I suggest SSL!\n\nThis is at the top of the page and I suggest you do this!\n\n\n\nAs will anything on this site, if you have issues or nothing makes sense, drop me an email!\n\nYou can hire me via Upwork or emailing me for weekend projects!","feature_image":"https://images.unsplash.com/photo-1559555297-8ff54b6862e0?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=2000&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2020-06-09T23:20:17.000Z","updated_at":"2021-05-02T01:49:16.000Z","published_at":"2020-06-11T12:38:24.000Z","custom_excerpt":"How to configure NGINX as a reverse proxy server","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"6425f3bc192c0c150413d592","uuid":"d67336fe-6adf-4e06-a916-4c8c2cee3d65","title":"How to install kanboard","slug":"how-to-install-kanboard","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}]],\"cards\":[[\"code\",{\"code\":\"sudo apt-get update\\nsudo apt-get upgrade -y\\nsudo apt-get install nginx mariadb-server mariadb-client\\nsudo apt install -y php7.2 php7.2-mysql php7.2-gd php7.2-mbstring php7.2-common php7.2-opcache php7.2-cli php7.2-xml\"}],[\"code\",{\"code\":\"sudo apt-get install software-properties-common\\nsudo add-apt-repository universe\\nsudo add-apt-repository ppa:certbot/certbot\\nsudo apt-get update\\nsudo apt-get install certbot python3-certbot-nginx\"}],[\"code\",{\"code\":\"sudo systemctl enable nginx\\nsudo systemctl enable mariadb-server\"}],[\"code\",{\"code\":\"sudo -s\\ncd /var/www/\\nsudo git clone https://github.com/kanboard/kanboard.git\\nsudo chown -R www-data:www-data kanboard/data\"}],[\"code\",{\"code\":\"sudo mysql_secure_installation\\n\"}],[\"code\",{\"code\":\"Enter current password for root (enter for none): Enter\\nSet root password? [Y/n]: Y\\nNew password: <secure password>\\nRe-enter new password: <secure password>\\nRemove anonymous users? [Y/n]: Y\\nDisallow root login remotely? [Y/n]: Y\\nRemove test database and access to it? [Y/n]: Y\\nReload privilege tables now? [Y/n]: Y\\n\"}],[\"code\",{\"code\":\"mysql -u root -p -e \\\"CREATE DATABASE kanboard;\\\"\\nmysql -u root -p kanboard < /var/www/kanboard/app/Schema/Sql/mysql.sql\\nmysql -u root -p -e \\\"CREATE USER 'kanboarduser'@'localhost' IDENTIFIED BY 'superdupersecretpassword';\\\"\\n\\nmysql -u root -p -e \\\"GRANT ALL PRIVILEGES ON kanboard.* TO 'kanboarduser'@'localhost' IDENTIFIED BY 'superdupersecretpassword' WITH GRANT OPTION;\\\"\\n\\nmysql -u root -p -e \\\"FLUSH PRIVILEGES;\\\"\"}],[\"code\",{\"code\":\"server {\\n        listen       80;       \\n        server_name  kan.example.com;\\n        index        index.php;\\n        root         /var/www/kanboard;\\n        client_max_body_size 32M;\\n\\n        location / {\\n            try_files $uri $uri/ /index.php$is_args$args;\\n        }\\n\\n        location ~ \\\\.php$ {\\n            try_files $uri =404;\\n            fastcgi_split_path_info ^(.+\\\\.php)(/.+)$;\\n            fastcgi_pass unix:/var/run/php/php7.2-fpm.sock;\\n            fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;\\n            fastcgi_index index.php;\\n            include fastcgi_params;\\n        }\\n\\n        location ~* ^.+\\\\.(log|sqlite)$ {\\n            return 404;\\n        }\\n\\n        location ~ /\\\\.ht {\\n            return 404;\\n        }\\n\\n        location ~* ^.+\\\\.(ico|jpg|gif|png|css|js|svg|eot|ttf|woff|woff2|otf)$ {\\n            log_not_found off;\\n            expires 7d;\\n            etag on;\\n        }\\n\\n        gzip on;\\n        gzip_comp_level 3;\\n        gzip_disable \\\"msie6\\\";\\n        gzip_vary on;\\n        gzip_types\\n            text/javascript\\n            application/javascript\\n            application/json\\n            text/xml\\n            application/xml\\n            application/rss+xml\\n            text/css\\n            text/plain;\\n    }\"}],[\"code\",{\"code\":\"sudo ln -s /etc/nginx/sites-available/kan.exmaple.com /etc/nginx/sites-enabled/kan.exmaple.com\\n\"}],[\"code\",{\"code\":\"sudo cp /var/www/kanboard/config.default.php /var/www/kanboard/config.php\"}],[\"code\",{\"code\":\"nano /var/www/kanboard/config.php\"}],[\"code\",{\"code\":\"// Database driver: sqlite, mysql or postgres (sqlite by default)\\ndefine('DB_DRIVER', 'mysql');\\n\\n// Mysql/Postgres username\\ndefine('DB_USERNAME', 'kanboard');\\n\\n// Mysql/Postgres password\\ndefine('DB_PASSWORD', 'StrongPassword'); \\n\\n// Mysql/Postgres hostname\\ndefine('DB_HOSTNAME', 'localhost');\\n\\n// Mysql/Postgres database name\\ndefine('DB_NAME', 'kanboard');\"}],[\"code\",{\"code\":\"sudo systemctl restart nginx\",\"caption\":\"\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/06/image-12.png\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/06/image-13.png\"}],[\"code\",{\"code\":\"certbot --nginx -d kan.example.com\"}],[\"hr\",{}]],\"markups\":[[\"code\"],[\"a\",[\"href\",\"https://www.upwork.com/freelancers/~01c61ee9802b94133e\"]],[\"a\",[\"href\",\"mailto:work@breadnet.co.uk\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I have been using Kanboard for around 4 months to manage my internal tasks as well as organise my self. Think of it like a ticket at a restaurant that gets moved to different cooks so they know what needs to be done. \"]]],[1,\"p\",[[0,[],0,\"I will be using Ubuntu 18.04 for this, but it's the same for most version of Ubuntu.\"]]],[1,\"p\",[[0,[],0,\"Start with installing all the services we need. If you already have these installed on your server, you can skip this step\"]]],[1,\"blockquote\",[[0,[],0,\"It is worth noting that you can cause issues if you install different version of PHP on your server if it's already installed\"]]],[10,0],[1,\"p\",[[0,[],0,\"We also need to install certbot so we can get free SSL \"]]],[10,1],[1,\"p\",[[0,[],0,\"Now that's done, we need to enable the services to start on boot\"]]],[10,2],[1,\"p\",[[0,[],0,\"Download and install Kanboard from Github\"]]],[10,3],[1,\"p\",[[0,[],0,\"Now this is where it becomes a little bit interesting as you need to do stuff on SQL.\"]]],[10,4],[1,\"p\",[[0,[],0,\"Answer the following for the questions\"]]],[10,5],[1,\"p\",[[0,[],0,\"Now we can create the database, user and import the schema\"]]],[10,6],[1,\"p\",[[0,[],0,\"If you know how, you can login and run these commands your self opposed to doing it from the command line. \"]]],[1,\"p\",[[0,[],0,\"Next, edit \"],[0,[0],1,\"/etc/nginx-sites-available/kan.example.com\"],[0,[],0,\" where \"],[0,[0],1,\"example.com\"],[0,[],0,\" is your domain.  Same goes for the below which should be pasted in to the file \"],[0,[0],1,\"kan.example.com\"],[0,[],0,\" , you will need to change the domain. \"]]],[10,7],[1,\"p\",[[0,[],0,\"Once this has been done, we need to symlink it to the sites-enabled folder. \"]]],[10,8],[1,\"p\",[[0,[],0,\"Almost done!\"]]],[1,\"p\",[[0,[],0,\"You will need to edit the \"],[0,[0],1,\"config.php\"],[0,[],0,\" file under \"],[0,[0],1,\"/var/www/kanboard\"],[0,[],0,\" to tell Kanboard where the database is as we're using Mysql. \"]]],[10,9],[1,\"p\",[[0,[],0,\"Edit it\"]]],[10,10],[1,\"p\",[[0,[],0,\"In there you should find the below and edit it to reflect what we configured in the database from before.\"]]],[1,\"blockquote\",[[0,[],0,\"You will need to change \"],[0,[0],1,\"DB_DRIVER\"],[0,[],0,\" and \"],[0,[0],1,\"DB_PASSWORD\"]]],[10,11],[1,\"p\",[[0,[],0,\"Now we can restart nginx.\"]]],[10,12],[1,\"p\",[[0,[],0,\"Make sure you have got a DNS record pointing to your server. You will want an A record with the value being \"],[0,[0],1,\"kan\"],[0,[],0,\" and the value being the IP address of your server\"]]],[1,\"p\",[[0,[],0,\"Once that is done, wait a few minutes depending on your DNS provider it may take up to 48 hours but in 2020, that shouldn't happen! \"]]],[1,\"p\",[[0,[],0,\"Go to http://kan.example.com and login with\"]]],[1,\"p\",[[0,[],0,\"Username: \"],[0,[0],1,\"admin\"],[1,[],0,0],[0,[],0,\"Password: \"],[0,[0],1,\"admin\"]]],[1,\"p\",[[0,[],0,\"To reset the password, go to \"],[0,[0],1,\"Admin\"],[0,[],0,\" > \"],[0,[0],1,\"Users Management\"],[0,[],0,\" > \"],[0,[0],1,\"Admin\"],[0,[],0,\" > \"],[0,[0],1,\"Change password\"]]],[10,13],[10,14],[1,\"p\",[[0,[],0,\"Now, it's all good and well having this on the internet, but we want to be able to have some security. For this we will be using Lets Encrypt. \"]]],[10,15],[1,\"p\",[[0,[],0,\"Press 2 when asked. You shouldn't need to edit anything or restart anything but you're welcome to if wanted.\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"If you have any issues or something looks wrong, please contact me!\"]]],[10,16],[1,\"p\",[[0,[],0,\"You can hire me via \"],[0,[1],1,\"Upwork\"],[0,[],0,\" or \"],[0,[2],1,\"emailing me\"],[0,[],0,\" for weekend projects!\"]]]],\"ghostVersion\":\"3.0\"}","lexical":null,"html":"<p>I have been using Kanboard for around 4 months to manage my internal tasks as well as organise my self. Think of it like a ticket at a restaurant that gets moved to different cooks so they know what needs to be done. </p><p>I will be using Ubuntu 18.04 for this, but it's the same for most version of Ubuntu.</p><p>Start with installing all the services we need. If you already have these installed on your server, you can skip this step</p><blockquote>It is worth noting that you can cause issues if you install different version of PHP on your server if it's already installed</blockquote><pre><code>sudo apt-get update\nsudo apt-get upgrade -y\nsudo apt-get install nginx mariadb-server mariadb-client\nsudo apt install -y php7.2 php7.2-mysql php7.2-gd php7.2-mbstring php7.2-common php7.2-opcache php7.2-cli php7.2-xml</code></pre><p>We also need to install certbot so we can get free SSL </p><pre><code>sudo apt-get install software-properties-common\nsudo add-apt-repository universe\nsudo add-apt-repository ppa:certbot/certbot\nsudo apt-get update\nsudo apt-get install certbot python3-certbot-nginx</code></pre><p>Now that's done, we need to enable the services to start on boot</p><pre><code>sudo systemctl enable nginx\nsudo systemctl enable mariadb-server</code></pre><p>Download and install Kanboard from Github</p><pre><code>sudo -s\ncd /var/www/\nsudo git clone https://github.com/kanboard/kanboard.git\nsudo chown -R www-data:www-data kanboard/data</code></pre><p>Now this is where it becomes a little bit interesting as you need to do stuff on SQL.</p><pre><code>sudo mysql_secure_installation\n</code></pre><p>Answer the following for the questions</p><pre><code>Enter current password for root (enter for none): Enter\nSet root password? [Y/n]: Y\nNew password: &lt;secure password&gt;\nRe-enter new password: &lt;secure password&gt;\nRemove anonymous users? [Y/n]: Y\nDisallow root login remotely? [Y/n]: Y\nRemove test database and access to it? [Y/n]: Y\nReload privilege tables now? [Y/n]: Y\n</code></pre><p>Now we can create the database, user and import the schema</p><pre><code>mysql -u root -p -e \"CREATE DATABASE kanboard;\"\nmysql -u root -p kanboard &lt; /var/www/kanboard/app/Schema/Sql/mysql.sql\nmysql -u root -p -e \"CREATE USER 'kanboarduser'@'localhost' IDENTIFIED BY 'superdupersecretpassword';\"\n\nmysql -u root -p -e \"GRANT ALL PRIVILEGES ON kanboard.* TO 'kanboarduser'@'localhost' IDENTIFIED BY 'superdupersecretpassword' WITH GRANT OPTION;\"\n\nmysql -u root -p -e \"FLUSH PRIVILEGES;\"</code></pre><p>If you know how, you can login and run these commands your self opposed to doing it from the command line. </p><p>Next, edit <code>/etc/nginx-sites-available/kan.example.com</code> where <code>example.com</code> is your domain.  Same goes for the below which should be pasted in to the file <code>kan.example.com</code> , you will need to change the domain. </p><pre><code>server {\n        listen       80;       \n        server_name  kan.example.com;\n        index        index.php;\n        root         /var/www/kanboard;\n        client_max_body_size 32M;\n\n        location / {\n            try_files $uri $uri/ /index.php$is_args$args;\n        }\n\n        location ~ \\.php$ {\n            try_files $uri =404;\n            fastcgi_split_path_info ^(.+\\.php)(/.+)$;\n            fastcgi_pass unix:/var/run/php/php7.2-fpm.sock;\n            fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;\n            fastcgi_index index.php;\n            include fastcgi_params;\n        }\n\n        location ~* ^.+\\.(log|sqlite)$ {\n            return 404;\n        }\n\n        location ~ /\\.ht {\n            return 404;\n        }\n\n        location ~* ^.+\\.(ico|jpg|gif|png|css|js|svg|eot|ttf|woff|woff2|otf)$ {\n            log_not_found off;\n            expires 7d;\n            etag on;\n        }\n\n        gzip on;\n        gzip_comp_level 3;\n        gzip_disable \"msie6\";\n        gzip_vary on;\n        gzip_types\n            text/javascript\n            application/javascript\n            application/json\n            text/xml\n            application/xml\n            application/rss+xml\n            text/css\n            text/plain;\n    }</code></pre><p>Once this has been done, we need to symlink it to the sites-enabled folder. </p><pre><code>sudo ln -s /etc/nginx/sites-available/kan.exmaple.com /etc/nginx/sites-enabled/kan.exmaple.com\n</code></pre><p>Almost done!</p><p>You will need to edit the <code>config.php</code> file under <code>/var/www/kanboard</code> to tell Kanboard where the database is as we're using Mysql. </p><pre><code>sudo cp /var/www/kanboard/config.default.php /var/www/kanboard/config.php</code></pre><p>Edit it</p><pre><code>nano /var/www/kanboard/config.php</code></pre><p>In there you should find the below and edit it to reflect what we configured in the database from before.</p><blockquote>You will need to change <code>DB_DRIVER</code> and <code>DB_PASSWORD</code></blockquote><pre><code>// Database driver: sqlite, mysql or postgres (sqlite by default)\ndefine('DB_DRIVER', 'mysql');\n\n// Mysql/Postgres username\ndefine('DB_USERNAME', 'kanboard');\n\n// Mysql/Postgres password\ndefine('DB_PASSWORD', 'StrongPassword'); \n\n// Mysql/Postgres hostname\ndefine('DB_HOSTNAME', 'localhost');\n\n// Mysql/Postgres database name\ndefine('DB_NAME', 'kanboard');</code></pre><p>Now we can restart nginx.</p><pre><code>sudo systemctl restart nginx</code></pre><p>Make sure you have got a DNS record pointing to your server. You will want an A record with the value being <code>kan</code> and the value being the IP address of your server</p><p>Once that is done, wait a few minutes depending on your DNS provider it may take up to 48 hours but in 2020, that shouldn't happen! </p><p>Go to http://kan.example.com and login with</p><p>Username: <code>admin</code><br>Password: <code>admin</code></p><p>To reset the password, go to <code>Admin</code> &gt; <code>Users Management</code> &gt; <code>Admin</code> &gt; <code>Change password</code></p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/06/image-12.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/06/image-13.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>Now, it's all good and well having this on the internet, but we want to be able to have some security. For this we will be using Lets Encrypt. </p><pre><code>certbot --nginx -d kan.example.com</code></pre><p>Press 2 when asked. You shouldn't need to edit anything or restart anything but you're welcome to if wanted.</p><p></p><p>If you have any issues or something looks wrong, please contact me!</p><hr><p>You can hire me via <a href=\"https://www.upwork.com/freelancers/~01c61ee9802b94133e\">Upwork</a> or <a href=\"mailto:work@breadnet.co.uk\">emailing me</a> for weekend projects!</p>","comment_id":"5ee0d5ac544a0d4341b6a1c0","plaintext":"I have been using Kanboard for around 4 months to manage my internal tasks as well as organise my self. Think of it like a ticket at a restaurant that gets moved to different cooks so they know what needs to be done.\n\nI will be using Ubuntu 18.04 for this, but it's the same for most version of Ubuntu.\n\nStart with installing all the services we need. If you already have these installed on your server, you can skip this step\n\nIt is worth noting that you can cause issues if you install different version of PHP on your server if it's already installed\n\nsudo apt-get update\nsudo apt-get upgrade -y\nsudo apt-get install nginx mariadb-server mariadb-client\nsudo apt install -y php7.2 php7.2-mysql php7.2-gd php7.2-mbstring php7.2-common php7.2-opcache php7.2-cli php7.2-xml\n\nWe also need to install certbot so we can get free SSL\n\nsudo apt-get install software-properties-common\nsudo add-apt-repository universe\nsudo add-apt-repository ppa:certbot/certbot\nsudo apt-get update\nsudo apt-get install certbot python3-certbot-nginx\n\nNow that's done, we need to enable the services to start on boot\n\nsudo systemctl enable nginx\nsudo systemctl enable mariadb-server\n\nDownload and install Kanboard from Github\n\nsudo -s\ncd /var/www/\nsudo git clone https://github.com/kanboard/kanboard.git\nsudo chown -R www-data:www-data kanboard/data\n\nNow this is where it becomes a little bit interesting as you need to do stuff on SQL.\n\nsudo mysql_secure_installation\n\n\nAnswer the following for the questions\n\nEnter current password for root (enter for none): Enter\nSet root password? [Y/n]: Y\nNew password: <secure password>\nRe-enter new password: <secure password>\nRemove anonymous users? [Y/n]: Y\nDisallow root login remotely? [Y/n]: Y\nRemove test database and access to it? [Y/n]: Y\nReload privilege tables now? [Y/n]: Y\n\n\nNow we can create the database, user and import the schema\n\nmysql -u root -p -e \"CREATE DATABASE kanboard;\"\nmysql -u root -p kanboard < /var/www/kanboard/app/Schema/Sql/mysql.sql\nmysql -u root -p -e \"CREATE USER 'kanboarduser'@'localhost' IDENTIFIED BY 'superdupersecretpassword';\"\n\nmysql -u root -p -e \"GRANT ALL PRIVILEGES ON kanboard.* TO 'kanboarduser'@'localhost' IDENTIFIED BY 'superdupersecretpassword' WITH GRANT OPTION;\"\n\nmysql -u root -p -e \"FLUSH PRIVILEGES;\"\n\nIf you know how, you can login and run these commands your self opposed to doing it from the command line.\n\nNext, edit /etc/nginx-sites-available/kan.example.com where example.com is your domain.  Same goes for the below which should be pasted in to the file kan.example.com , you will need to change the domain.\n\nserver {\n        listen       80;       \n        server_name  kan.example.com;\n        index        index.php;\n        root         /var/www/kanboard;\n        client_max_body_size 32M;\n\n        location / {\n            try_files $uri $uri/ /index.php$is_args$args;\n        }\n\n        location ~ \\.php$ {\n            try_files $uri =404;\n            fastcgi_split_path_info ^(.+\\.php)(/.+)$;\n            fastcgi_pass unix:/var/run/php/php7.2-fpm.sock;\n            fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;\n            fastcgi_index index.php;\n            include fastcgi_params;\n        }\n\n        location ~* ^.+\\.(log|sqlite)$ {\n            return 404;\n        }\n\n        location ~ /\\.ht {\n            return 404;\n        }\n\n        location ~* ^.+\\.(ico|jpg|gif|png|css|js|svg|eot|ttf|woff|woff2|otf)$ {\n            log_not_found off;\n            expires 7d;\n            etag on;\n        }\n\n        gzip on;\n        gzip_comp_level 3;\n        gzip_disable \"msie6\";\n        gzip_vary on;\n        gzip_types\n            text/javascript\n            application/javascript\n            application/json\n            text/xml\n            application/xml\n            application/rss+xml\n            text/css\n            text/plain;\n    }\n\nOnce this has been done, we need to symlink it to the sites-enabled folder.\n\nsudo ln -s /etc/nginx/sites-available/kan.exmaple.com /etc/nginx/sites-enabled/kan.exmaple.com\n\n\nAlmost done!\n\nYou will need to edit the config.php file under /var/www/kanboard to tell Kanboard where the database is as we're using Mysql.\n\nsudo cp /var/www/kanboard/config.default.php /var/www/kanboard/config.php\n\nEdit it\n\nnano /var/www/kanboard/config.php\n\nIn there you should find the below and edit it to reflect what we configured in the database from before.\n\nYou will need to change DB_DRIVER and DB_PASSWORD\n\n// Database driver: sqlite, mysql or postgres (sqlite by default)\ndefine('DB_DRIVER', 'mysql');\n\n// Mysql/Postgres username\ndefine('DB_USERNAME', 'kanboard');\n\n// Mysql/Postgres password\ndefine('DB_PASSWORD', 'StrongPassword'); \n\n// Mysql/Postgres hostname\ndefine('DB_HOSTNAME', 'localhost');\n\n// Mysql/Postgres database name\ndefine('DB_NAME', 'kanboard');\n\nNow we can restart nginx.\n\nsudo systemctl restart nginx\n\nMake sure you have got a DNS record pointing to your server. You will want an A record with the value being kan and the value being the IP address of your server\n\nOnce that is done, wait a few minutes depending on your DNS provider it may take up to 48 hours but in 2020, that shouldn't happen!\n\nGo to http://kan.example.com and login with\n\nUsername: admin\nPassword: admin\n\nTo reset the password, go to Admin > Users Management > Admin > Change password\n\nNow, it's all good and well having this on the internet, but we want to be able to have some security. For this we will be using Lets Encrypt.\n\ncertbot --nginx -d kan.example.com\n\nPress 2 when asked. You shouldn't need to edit anything or restart anything but you're welcome to if wanted.\n\n\n\nIf you have any issues or something looks wrong, please contact me!\n\nYou can hire me via Upwork or emailing me for weekend projects!","feature_image":"https://images.unsplash.com/photo-1504309092620-4d0ec726efa4?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=2000&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2020-06-10T12:44:28.000Z","updated_at":"2021-05-02T01:47:43.000Z","published_at":"2020-06-10T13:21:43.000Z","custom_excerpt":"How to install kanboard on Ubuntu with Nginx","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"6425f3bc192c0c150413d593","uuid":"1ecbaa2e-2eb9-4b59-81e6-d8af2cb4baf5","title":"Introduction to home-labbing","slug":"homelab","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"hr\",{}],[\"hr\",{}],[\"hr\",{}],[\"bookmark\",{\"type\":\"bookmark\",\"url\":\"https://docs.microsoft.com/en-us/virtualization/hyper-v-on-windows/about/\",\"metadata\":{\"url\":\"https://docs.microsoft.com/en-us/virtualization/hyper-v-on-windows/about/\",\"title\":\"Introduction to Hyper-V on Windows 10\",\"description\":\"Introduction to Hyper-V, virtualization, and related technologies.\",\"author\":\"scooley\",\"publisher\":\"Microsoft Docs\",\"thumbnail\":\"https://docs.microsoft.com/en-us/media/logos/logo-ms-social.png\",\"icon\":null}}],[\"hr\",{}],[\"bookmark\",{\"url\":\"https://www.youtube.com/watch?v=bG5enpij0e8\",\"metadata\":{\"url\":\"https://www.youtube.com/watch?v=bG5enpij0e8\",\"title\":\"XCP NG Xenserver 7.4 Install Tutorial. From bare metal to loaded VM using XenCenter\",\"description\":\"Amazon Affiliate Store ➡️ https://www.amazon.com/shop/lawrencesystemspcpickup Gear we used on Kit (affiliate Links) ➡️ https://kit.co/lawrencesystems Try ITP...\",\"author\":null,\"publisher\":\"YouTube\",\"thumbnail\":\"https://i.ytimg.com/vi/bG5enpij0e8/maxresdefault.jpg\",\"icon\":\"https://www.youtube.com/yts/img/favicon_144-vfliLAfaB.png\"}}],[\"bookmark\",{\"url\":\"https://www.youtube.com/watch?v=azORbxrItOo\",\"metadata\":{\"url\":\"https://www.youtube.com/watch?v=azORbxrItOo\",\"title\":\"Virtualize Everything! - Proxmox Install Tutorial\",\"description\":\"Want to know how to setup Proxmox so you can start virtualizing servers? This tutorial is for you. But first... What am I drinking??? Tutorials mean cocktail...\",\"author\":null,\"publisher\":\"YouTube\",\"thumbnail\":\"https://i.ytimg.com/vi/azORbxrItOo/maxresdefault.jpg\",\"icon\":\"https://www.youtube.com/yts/img/favicon_144-vfliLAfaB.png\"}}],[\"bookmark\",{\"url\":\"https://www.youtube.com/watch?v=wGZrhKhj0Fk\",\"metadata\":{\"url\":\"https://www.youtube.com/watch?v=wGZrhKhj0Fk\",\"title\":\"Windows Server 2016 - Install Hyper-V Server, Virtual Switch, VMs (How to Step by Step Tutorial)\",\"description\":\"Installing HyperV from nothing to Virtual Switch to VMs step by step tutorial on Windows Server 2016 or Windows 10\",\"author\":null,\"publisher\":\"YouTube\",\"thumbnail\":\"https://i.ytimg.com/vi/wGZrhKhj0Fk/hqdefault.jpg\",\"icon\":\"https://s.ytimg.com/yts/img/favicon_144-vfliLAfaB.png\"}}],[\"hr\",{}],[\"bookmark\",{\"url\":\"https://cockpit-project.org\",\"metadata\":{\"url\":\"https://cockpit-project.org/FIXME\",\"title\":\"Cockpit Project — Cockpit Project\",\"description\":\"Cockpit makes it easy to administer your GNU/Linux servers via a web browser.\",\"author\":null,\"publisher\":null,\"thumbnail\":\"https://cockpit-project.org/FIXME\",\"icon\":\"https://cockpit-project.org/images/favicon.png\"},\"caption\":\"(Their website doesn't like it here, hence the missing image)\"}],[\"bookmark\",{\"url\":\"https://www.veeam.com/virtual-machine-backup-solution-free.html\",\"metadata\":{\"url\":\"https://www.veeam.com/virtual-machine-backup-solution-free.html\",\"title\":\"Free Backup Solution - Veeam Backup & Replication Community Edition\",\"description\":\"Veeam® ONE™, part of Veeam Availability Suite™, provides comprehensive monitoring and analytics for your backup, virtual and physical environments. With support for Veeam Backup & Replication™ and Veeam Agents, as well as VMware vSphere, Microsoft Hyper-V and Nutanix AHV, Veeam ONE delivers deep, in…\",\"author\":\"Bersayder Mejia Networking Technician\\nITSC\",\"publisher\":\"Veeam Software\",\"thumbnail\":\"https://www.veeam.com/content/dam/veeam/global/og-images/1600x800_vas_v10.png?ck=1581061105410\",\"icon\":\"https://www.veeam.com/content/dam/veeam/global/favicon_228x228px.png\"},\"caption\":\"Veeam community is free\"}],[\"hr\",{}]],\"markups\":[[\"a\",[\"href\",\"https://labgopher.com/\"]],[\"a\",[\"href\",\"https://ark.intel.com/content/www/us/en/ark/products/40200/intel-xeon-processor-e5520-8m-cache-2-26-ghz-5-86-gt-s-intel-qpi.html\"]],[\"a\",[\"href\",\"https://xcp-ng.org\"]],[\"a\",[\"href\",\"https://proxmox.com/en/\"]],[\"code\"],[\"a\",[\"href\",\"https://www.zabbix.com\"]],[\"s\"],[\"a\",[\"href\",\"http://backuppc.sourceforge.net\"]],[\"a\",[\"href\",\"https://www.upwork.com/freelancers/~01c61ee9802b94133e\"]],[\"a\",[\"href\",\"mailto:work@breadnet.co.uk\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Hello, welcome. Settle down as we're about to take a dive in to the topic of Homelabs, selfhosting and how to break in to Linux administration.\"]]],[1,\"p\",[[0,[],0,\"A little about me: I am by no means a professional writer or programmer. I do however use linux as part of my day to day job and as my daily driver. \"]]],[10,0],[1,\"p\",[[0,[],0,\"The objective here is to look at:\"]]],[3,\"ul\",[[[0,[],0,\"The server\"]],[[0,[],0,\"Building the server\"]],[[0,[],0,\"Building infrastructure \"]],[[0,[],0,\"Building a VM\"]],[[0,[],0,\"Hosting something\"]],[[0,[],0,\"Monitoring \"]],[[0,[],0,\"Backups (Most important part) \"]],[]]],[1,\"p\",[[0,[],0,\"Now you know what to expect from this, let's start.\"]]],[1,\"p\",[[0,[],0,\"Basic hardware I suggest:\"]]],[1,\"p\",[[0,[],0,\"An 8 port gigabit switch\"],[1,[],0,0],[0,[],0,\"Computer that can run all the time\"],[1,[],0,1],[0,[],0,\"15/5 connection (stable internet is nice to have)\"]]],[1,\"p\",[[0,[],0,\"As long as you have a computer that you can run a virtual machine in, it's a lab. Don't worry about what others think. It's your lab, for you.\"]]],[10,1],[1,\"h2\",[[0,[],0,\"The server\"]]],[1,\"p\",[[0,[],0,\"Depending on what purpose your lab servers, this will be the most important part of your lab. I suggest a budget of around £1500 ($1878.34) for everything.\"]]],[1,\"h3\",[[0,[],0,\"Where to get parts from\"]]],[1,\"p\",[[0,[],0,\"Depending on what you're looking for, you can build a server from off the shelf parts, the same parts you would use to build a computer. Just ensure that those parts are rated for 24/7/365 operation. \"]]],[1,\"p\",[[0,[],0,\"I chose to purchase pre-built rackmount servers. Depending on your country you can use \"],[0,[0],1,\"Labgopher\"],[0,[],0,\" if not, I found that Ebay was the best place to locate servers. Labgopher just makes it easier for you to compare server. Once again, this is dependant on your location but there can be government auctions near by where they can be selling hardware. \"]]],[1,\"p\",[[0,[],0,\"Don't expect to find a second hand server that has been released in the last 3 years unless you live next to a financial datacentre as they regularly swap servers out.\"]]],[1,\"h3\",[[0,[],0,\"What do I suggest?\"]]],[1,\"p\",[[0,[],0,\"This is a difficult question to answer but you want to buy something that will last you around 5-6 years. I personally like the Dell r710/ r720 line for servers as alot of people run these in their homelabs. My self included. Another solid choice are Supermicro but personally I find their modeling confusing, but I do plan on my next server being a supermicro. \"]]],[1,\"p\",[[0,[],0,\"Storage: Starting out, I suggest 2TB. It sounds alot, but VM's take up space, all the stuff you put on there will start to add up.\"]]],[1,\"p\",[[0,[],0,\"memory: Depending on your use case, you will want at minimum 16gb of ram, but if you're just running docker (containers) you can get away with less\"]]],[1,\"p\",[[0,[],0,\"CPU: This is a per use case, I just used the CPU's that came with my server. (\"],[0,[1],1,\"e5520\"],[0,[],0,\")\"]]],[10,2],[1,\"h2\",[[0,[],0,\"Building a server\"]]],[1,\"p\",[[0,[],0,\"Now that we have all the parts (or the whole pre-built server) we need to look at what to install.\"]]],[1,\"h2\",[[0,[],0,\"Operating system\"]]],[1,\"p\",[[0,[],0,\"This is really back to what you plan on using your lab for. I highly suggest you use a form of virtualisation as it allows for the most servers to be installed on one piece of hardware. You want to get the most dense deployment as you're the poor sod who needs to pay for power. I will admit I am very biased, but I will do my best to be diplomatic and explain the best options as of 2020.\"]]],[1,\"h3\",[[0,[],0,\"XCP-NG: Turnkey Open Source Hypervisor\"]]],[1,\"p\",[[0,[],0,\"Based on XenServer, XCP-ng is the result of massive cooperation between individuals and companies, to deliver a product without limits. No restrictions on features and every bit available on GitHub!\"]]],[1,\"h3\",[[0,[],0,\"Proxmox:\"]]],[1,\"p\",[[0,[],0,\"Proxmox VE is a complete open-source platform for enterprise virtualization. With the built-in web interface you can easily manage VMs and containers, software-defined storage and networking, high-availability clustering, and multiple out-of-the-box tools on a single solution.\"]]],[1,\"h3\",[[0,[],0,\"Hyper-v\"]]],[1,\"p\",[[0,[],0,\"This is the virtulization program from Microsoft. I've used it at work before but didn't enjoy the way it's managed and I don't like the fact that everything is closed source. \"]]],[1,\"p\",[[0,[2],1,\"https://xcp-ng.org\"]]],[1,\"p\",[[0,[3],1,\"proxmox.com/en/\"]]],[10,3],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"My personal favourite is XPC-NG as it scales so well across both home labs, data centres and cluster based computing. The control interface runs as a vm and can be used to manage thousands of servers, from one place. This is what I run in my lab across 2 servers.\"]]],[1,\"p\",[]],[10,4],[1,\"p\",[[0,[],0,\"Now that we have everything in boxes, we can start to build out. \"]]],[1,\"p\",[[0,[],0,\"Depending on what your lab will do for you, I suggest having a separate router for everything. You can virtulise it, but I prefer to run mine as hardware. I chose to go with a dell r210ii which runs Pfsense. This allows for fine grain control on what the servers can see, who can get in and any VPN's I want to connect, can be done through the firewall/ router opposed to having to connect each vm.\"]]],[1,\"p\",[[0,[],0,\"Putting everything together is quite simple, you'll need a couple of ethernet cables, a few kettle lead powerr cables, a monitor, keyboard (no mouse, we don't do that in the linux world) and a USB stick to boot the os of your choice off of.\"]]],[1,\"p\",[[0,[],0,\"Instead of filling up this page with how to install the 3 OS' I mentioned, here are 3 good videos:\"]]],[10,5],[10,6],[10,7],[1,\"p\",[[0,[],0,\"Now that you've got the OS installed, you'll probably want to configure a router of choice.\"]]],[1,\"p\",[[0,[],0,\" I highly suggest using the \"],[0,[4],1,\"172.16.0.0/16\"],[0,[],0,\" range of IP address' as it gives you quite alot of flexibility as well as easily being able to distinguish ranges. You're able to put say DNS servers on \"],[0,[4],1,\"172.16.53.0/24\"],[0,[],0,\" and then web servers on \"],[0,[4],1,\"172.16.80.0/24\"],[0,[],0,\" and then your bastion server can be on \"],[0,[4],1,\"172.16.22.0/24\"],[0,[],0,\"... you get the idea.\"]]],[1,\"p\",[[0,[],0,\"Most people will say the crux of a homelab is being able to spin up servers at will, and not have to build a server each time with hardware, get a disk, you know the pain. Seeing as we installed a virtualisation operating system, it allows us to chop the host server up in to small blocks if you will where we're able to present it to a virtual machine to use. You can give a server 1 cpu that is actually 20 cores combined or 20 cores on 1 cpu. Virtualisation allows you to have fine grain control over everything. \"]]],[1,\"p\",[]],[1,\"h3\",[[0,[],0,\"Gues OS\"]]],[1,\"p\",[[0,[],0,\"This is really dependant on what your lab is for. If you are learning windows server, you will run windows server. If you are trying to get in to the IT world, I suggest start with Linux and work back to windows as once you figure out the syntax of command line, you become a lot more powerful at managing windows. \"]]],[1,\"p\",[]],[10,8],[1,\"h2\",[[0,[],0,\"Monitoring\"]]],[1,\"p\",[[0,[],0,\"This plays a big role in working out what is wrong. having good monitoring software can really speed up locating performance issues, building dank graphs and give you a high level overview on what's going on.\"]]],[1,\"p\",[[0,[],0,\"2 tools that I use are Zabbix and Cockpit projec\"]]],[1,\"p\",[[0,[5],1,\"Zabbix\"],[0,[],0,\": (Post coming soon) \"]]],[1,\"p\",[[0,[],0,\"Cockpit:\"]]],[10,9],[1,\"h2\",[[0,[],0,\"Backups\"]]],[1,\"p\",[[0,[],0,\"These are probably the most important part of a lab as 90% of the time, your time \"],[0,[6],1,\"fucking things up\"],[0,[],0,\" \"],[0,[6],1,\"breaking things\"],[0,[],0,\" learning through breaking. It's crucial to have backups at least once a week at a minimum.\"]]],[1,\"p\",[[0,[],0,\"You can custom roll your own script backing everything up to a wasabi bucket, a flash drive or if you want to be fancy and have some extra cash, replicate it off site. \"]]],[1,\"p\",[[0,[],0,\"You can also use something like Veeam (which is awesome) or BackupPC\"]]],[1,\"p\",[[0,[7],1,\"http://backuppc.sourceforge.net\"]]],[10,10],[10,11],[1,\"p\",[[0,[],0,\"You can hire me via \"],[0,[8],1,\"Upwork\"],[0,[],0,\" or \"],[0,[9],1,\"emailing me\"],[0,[],0,\" for weekend projects!\"]]]],\"ghostVersion\":\"3.0\"}","lexical":null,"html":"<p>Hello, welcome. Settle down as we're about to take a dive in to the topic of Homelabs, selfhosting and how to break in to Linux administration.</p><p>A little about me: I am by no means a professional writer or programmer. I do however use linux as part of my day to day job and as my daily driver. </p><hr><p>The objective here is to look at:</p><ul><li>The server</li><li>Building the server</li><li>Building infrastructure </li><li>Building a VM</li><li>Hosting something</li><li>Monitoring </li><li>Backups (Most important part) </li><li></li></ul><p>Now you know what to expect from this, let's start.</p><p>Basic hardware I suggest:</p><p>An 8 port gigabit switch<br>Computer that can run all the time<br>15/5 connection (stable internet is nice to have)</p><p>As long as you have a computer that you can run a virtual machine in, it's a lab. Don't worry about what others think. It's your lab, for you.</p><hr><h2 id=\"the-server\">The server</h2><p>Depending on what purpose your lab servers, this will be the most important part of your lab. I suggest a budget of around £1500 ($1878.34) for everything.</p><h3 id=\"where-to-get-parts-from\">Where to get parts from</h3><p>Depending on what you're looking for, you can build a server from off the shelf parts, the same parts you would use to build a computer. Just ensure that those parts are rated for 24/7/365 operation. </p><p>I chose to purchase pre-built rackmount servers. Depending on your country you can use <a href=\"https://labgopher.com/\">Labgopher</a> if not, I found that Ebay was the best place to locate servers. Labgopher just makes it easier for you to compare server. Once again, this is dependant on your location but there can be government auctions near by where they can be selling hardware. </p><p>Don't expect to find a second hand server that has been released in the last 3 years unless you live next to a financial datacentre as they regularly swap servers out.</p><h3 id=\"what-do-i-suggest\">What do I suggest?</h3><p>This is a difficult question to answer but you want to buy something that will last you around 5-6 years. I personally like the Dell r710/ r720 line for servers as alot of people run these in their homelabs. My self included. Another solid choice are Supermicro but personally I find their modeling confusing, but I do plan on my next server being a supermicro. </p><p>Storage: Starting out, I suggest 2TB. It sounds alot, but VM's take up space, all the stuff you put on there will start to add up.</p><p>memory: Depending on your use case, you will want at minimum 16gb of ram, but if you're just running docker (containers) you can get away with less</p><p>CPU: This is a per use case, I just used the CPU's that came with my server. (<a href=\"https://ark.intel.com/content/www/us/en/ark/products/40200/intel-xeon-processor-e5520-8m-cache-2-26-ghz-5-86-gt-s-intel-qpi.html\">e5520</a>)</p><hr><h2 id=\"building-a-server\">Building a server</h2><p>Now that we have all the parts (or the whole pre-built server) we need to look at what to install.</p><h2 id=\"operating-system\">Operating system</h2><p>This is really back to what you plan on using your lab for. I highly suggest you use a form of virtualisation as it allows for the most servers to be installed on one piece of hardware. You want to get the most dense deployment as you're the poor sod who needs to pay for power. I will admit I am very biased, but I will do my best to be diplomatic and explain the best options as of 2020.</p><h3 id=\"xcp-ng-turnkey-open-source-hypervisor\">XCP-NG: Turnkey Open Source Hypervisor</h3><p>Based on XenServer, XCP-ng is the result of massive cooperation between individuals and companies, to deliver a product without limits. No restrictions on features and every bit available on GitHub!</p><h3 id=\"proxmox-\">Proxmox:</h3><p>Proxmox VE is a complete open-source platform for enterprise virtualization. With the built-in web interface you can easily manage VMs and containers, software-defined storage and networking, high-availability clustering, and multiple out-of-the-box tools on a single solution.</p><h3 id=\"hyper-v\">Hyper-v</h3><p>This is the virtulization program from Microsoft. I've used it at work before but didn't enjoy the way it's managed and I don't like the fact that everything is closed source. </p><p><a href=\"https://xcp-ng.org\">https://xcp-ng.org</a></p><p><a href=\"https://proxmox.com/en/\">proxmox.com/en/</a></p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://docs.microsoft.com/en-us/virtualization/hyper-v-on-windows/about/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Introduction to Hyper-V on Windows 10</div><div class=\"kg-bookmark-description\">Introduction to Hyper-V, virtualization, and related technologies.</div><div class=\"kg-bookmark-metadata\"><span class=\"kg-bookmark-author\">Microsoft Docs</span><span class=\"kg-bookmark-publisher\">scooley</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://docs.microsoft.com/en-us/media/logos/logo-ms-social.png\" alt=\"\"></div></a></figure><p></p><p>My personal favourite is XPC-NG as it scales so well across both home labs, data centres and cluster based computing. The control interface runs as a vm and can be used to manage thousands of servers, from one place. This is what I run in my lab across 2 servers.</p><p></p><hr><p>Now that we have everything in boxes, we can start to build out. </p><p>Depending on what your lab will do for you, I suggest having a separate router for everything. You can virtulise it, but I prefer to run mine as hardware. I chose to go with a dell r210ii which runs Pfsense. This allows for fine grain control on what the servers can see, who can get in and any VPN's I want to connect, can be done through the firewall/ router opposed to having to connect each vm.</p><p>Putting everything together is quite simple, you'll need a couple of ethernet cables, a few kettle lead powerr cables, a monitor, keyboard (no mouse, we don't do that in the linux world) and a USB stick to boot the os of your choice off of.</p><p>Instead of filling up this page with how to install the 3 OS' I mentioned, here are 3 good videos:</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://www.youtube.com/watch?v&#x3D;bG5enpij0e8\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">XCP NG Xenserver 7.4 Install Tutorial. From bare metal to loaded VM using XenCenter</div><div class=\"kg-bookmark-description\">Amazon Affiliate Store ➡️ https://www.amazon.com/shop/lawrencesystemspcpickup Gear we used on Kit (affiliate Links) ➡️ https://kit.co/lawrencesystems Try ITP...</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://www.youtube.com/yts/img/favicon_144-vfliLAfaB.png\" alt=\"\"><span class=\"kg-bookmark-author\">YouTube</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://i.ytimg.com/vi/bG5enpij0e8/maxresdefault.jpg\" alt=\"\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://www.youtube.com/watch?v&#x3D;azORbxrItOo\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Virtualize Everything! - Proxmox Install Tutorial</div><div class=\"kg-bookmark-description\">Want to know how to setup Proxmox so you can start virtualizing servers? This tutorial is for you. But first... What am I drinking??? Tutorials mean cocktail...</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://www.youtube.com/yts/img/favicon_144-vfliLAfaB.png\" alt=\"\"><span class=\"kg-bookmark-author\">YouTube</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://i.ytimg.com/vi/azORbxrItOo/maxresdefault.jpg\" alt=\"\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://www.youtube.com/watch?v&#x3D;wGZrhKhj0Fk\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Windows Server 2016 - Install Hyper-V Server, Virtual Switch, VMs (How to Step by Step Tutorial)</div><div class=\"kg-bookmark-description\">Installing HyperV from nothing to Virtual Switch to VMs step by step tutorial on Windows Server 2016 or Windows 10</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://s.ytimg.com/yts/img/favicon_144-vfliLAfaB.png\" alt=\"\"><span class=\"kg-bookmark-author\">YouTube</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://i.ytimg.com/vi/wGZrhKhj0Fk/hqdefault.jpg\" alt=\"\"></div></a></figure><p>Now that you've got the OS installed, you'll probably want to configure a router of choice.</p><p> I highly suggest using the <code>172.16.0.0/16</code> range of IP address' as it gives you quite alot of flexibility as well as easily being able to distinguish ranges. You're able to put say DNS servers on <code>172.16.53.0/24</code> and then web servers on <code>172.16.80.0/24</code> and then your bastion server can be on <code>172.16.22.0/24</code>... you get the idea.</p><p>Most people will say the crux of a homelab is being able to spin up servers at will, and not have to build a server each time with hardware, get a disk, you know the pain. Seeing as we installed a virtualisation operating system, it allows us to chop the host server up in to small blocks if you will where we're able to present it to a virtual machine to use. You can give a server 1 cpu that is actually 20 cores combined or 20 cores on 1 cpu. Virtualisation allows you to have fine grain control over everything. </p><p></p><h3 id=\"gues-os\">Gues OS</h3><p>This is really dependant on what your lab is for. If you are learning windows server, you will run windows server. If you are trying to get in to the IT world, I suggest start with Linux and work back to windows as once you figure out the syntax of command line, you become a lot more powerful at managing windows. </p><p></p><hr><h2 id=\"monitoring\">Monitoring</h2><p>This plays a big role in working out what is wrong. having good monitoring software can really speed up locating performance issues, building dank graphs and give you a high level overview on what's going on.</p><p>2 tools that I use are Zabbix and Cockpit projec</p><p><a href=\"https://www.zabbix.com\">Zabbix</a>: (Post coming soon) </p><p>Cockpit:</p><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://cockpit-project.org\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Cockpit Project — Cockpit Project</div><div class=\"kg-bookmark-description\">Cockpit makes it easy to administer your GNU/Linux servers via a web browser.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://cockpit-project.org/images/favicon.png\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://cockpit-project.org/FIXME\" alt=\"\"></div></a><figcaption>(Their website doesn't like it here, hence the missing image)</figcaption></figure><h2 id=\"backups\">Backups</h2><p>These are probably the most important part of a lab as 90% of the time, your time <s>fucking things up</s> <s>breaking things</s> learning through breaking. It's crucial to have backups at least once a week at a minimum.</p><p>You can custom roll your own script backing everything up to a wasabi bucket, a flash drive or if you want to be fancy and have some extra cash, replicate it off site. </p><p>You can also use something like Veeam (which is awesome) or BackupPC</p><p><a href=\"http://backuppc.sourceforge.net\">http://backuppc.sourceforge.net</a></p><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://www.veeam.com/virtual-machine-backup-solution-free.html\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Free Backup Solution - Veeam Backup &amp; Replication Community Edition</div><div class=\"kg-bookmark-description\">Veeam® ONE™, part of Veeam Availability Suite™, provides comprehensive monitoring and analytics for your backup, virtual and physical environments. With support for Veeam Backup &amp; Replication™ and Veeam Agents, as well as VMware vSphere, Microsoft Hyper-V and Nutanix AHV, Veeam ONE delivers deep, in…</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://www.veeam.com/content/dam/veeam/global/favicon_228x228px.png\" alt=\"\"><span class=\"kg-bookmark-author\">Veeam Software</span><span class=\"kg-bookmark-publisher\">Bersayder Mejia Networking TechnicianITSC</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://www.veeam.com/content/dam/veeam/global/og-images/1600x800_vas_v10.png?ck&#x3D;1581061105410\" alt=\"\"></div></a><figcaption>Veeam community is free</figcaption></figure><hr><p>You can hire me via <a href=\"https://www.upwork.com/freelancers/~01c61ee9802b94133e\">Upwork</a> or <a href=\"mailto:work@breadnet.co.uk\">emailing me</a> for weekend projects!</p>","comment_id":"5ee232df544a0d4341b6a2c8","plaintext":"Hello, welcome. Settle down as we're about to take a dive in to the topic of Homelabs, selfhosting and how to break in to Linux administration.\n\nA little about me: I am by no means a professional writer or programmer. I do however use linux as part of my day to day job and as my daily driver.\n\nThe objective here is to look at:\n\n * The server\n * Building the server\n * Building infrastructure\n * Building a VM\n * Hosting something\n * Monitoring\n * Backups (Most important part)\n * \n\nNow you know what to expect from this, let's start.\n\nBasic hardware I suggest:\n\nAn 8 port gigabit switch\nComputer that can run all the time\n15/5 connection (stable internet is nice to have)\n\nAs long as you have a computer that you can run a virtual machine in, it's a lab. Don't worry about what others think. It's your lab, for you.\n\n\nThe server\n\nDepending on what purpose your lab servers, this will be the most important part of your lab. I suggest a budget of around £1500 ($1878.34) for everything.\n\n\nWhere to get parts from\n\nDepending on what you're looking for, you can build a server from off the shelf parts, the same parts you would use to build a computer. Just ensure that those parts are rated for 24/7/365 operation.\n\nI chose to purchase pre-built rackmount servers. Depending on your country you can use Labgopher if not, I found that Ebay was the best place to locate servers. Labgopher just makes it easier for you to compare server. Once again, this is dependant on your location but there can be government auctions near by where they can be selling hardware.\n\nDon't expect to find a second hand server that has been released in the last 3 years unless you live next to a financial datacentre as they regularly swap servers out.\n\n\nWhat do I suggest?\n\nThis is a difficult question to answer but you want to buy something that will last you around 5-6 years. I personally like the Dell r710/ r720 line for servers as alot of people run these in their homelabs. My self included. Another solid choice are Supermicro but personally I find their modeling confusing, but I do plan on my next server being a supermicro.\n\nStorage: Starting out, I suggest 2TB. It sounds alot, but VM's take up space, all the stuff you put on there will start to add up.\n\nmemory: Depending on your use case, you will want at minimum 16gb of ram, but if you're just running docker (containers) you can get away with less\n\nCPU: This is a per use case, I just used the CPU's that came with my server. (e5520)\n\n\nBuilding a server\n\nNow that we have all the parts (or the whole pre-built server) we need to look at what to install.\n\n\nOperating system\n\nThis is really back to what you plan on using your lab for. I highly suggest you use a form of virtualisation as it allows for the most servers to be installed on one piece of hardware. You want to get the most dense deployment as you're the poor sod who needs to pay for power. I will admit I am very biased, but I will do my best to be diplomatic and explain the best options as of 2020.\n\n\nXCP-NG: Turnkey Open Source Hypervisor\n\nBased on XenServer, XCP-ng is the result of massive cooperation between individuals and companies, to deliver a product without limits. No restrictions on features and every bit available on GitHub!\n\n\nProxmox:\n\nProxmox VE is a complete open-source platform for enterprise virtualization. With the built-in web interface you can easily manage VMs and containers, software-defined storage and networking, high-availability clustering, and multiple out-of-the-box tools on a single solution.\n\n\nHyper-v\n\nThis is the virtulization program from Microsoft. I've used it at work before but didn't enjoy the way it's managed and I don't like the fact that everything is closed source.\n\nhttps://xcp-ng.org\n\nproxmox.com/en/\n\nIntroduction to Hyper-V on Windows 10Introduction to Hyper-V, virtualization, and related technologies.Microsoft Docsscooley\n\n\n\nMy personal favourite is XPC-NG as it scales so well across both home labs, data centres and cluster based computing. The control interface runs as a vm and can be used to manage thousands of servers, from one place. This is what I run in my lab across 2 servers.\n\n\n\nNow that we have everything in boxes, we can start to build out.\n\nDepending on what your lab will do for you, I suggest having a separate router for everything. You can virtulise it, but I prefer to run mine as hardware. I chose to go with a dell r210ii which runs Pfsense. This allows for fine grain control on what the servers can see, who can get in and any VPN's I want to connect, can be done through the firewall/ router opposed to having to connect each vm.\n\nPutting everything together is quite simple, you'll need a couple of ethernet cables, a few kettle lead powerr cables, a monitor, keyboard (no mouse, we don't do that in the linux world) and a USB stick to boot the os of your choice off of.\n\nInstead of filling up this page with how to install the 3 OS' I mentioned, here are 3 good videos:\n\nXCP NG Xenserver 7.4 Install Tutorial. From bare metal to loaded VM using XenCenterAmazon Affiliate Store ➡️ https://www.amazon.com/shop/lawrencesystemspcpickup Gear we used on Kit (affiliate Links) ➡️ https://kit.co/lawrencesystems Try ITP...YouTubeVirtualize Everything! - Proxmox Install TutorialWant to know how to setup Proxmox so you can start virtualizing servers? This tutorial is for you. But first... What am I drinking??? Tutorials mean cocktail...YouTubeWindows Server 2016 - Install Hyper-V Server, Virtual Switch, VMs (How to Step by Step Tutorial)Installing HyperV from nothing to Virtual Switch to VMs step by step tutorial on Windows Server 2016 or Windows 10YouTube\n\nNow that you've got the OS installed, you'll probably want to configure a router of choice.\n\nI highly suggest using the 172.16.0.0/16 range of IP address' as it gives you quite alot of flexibility as well as easily being able to distinguish ranges. You're able to put say DNS servers on 172.16.53.0/24 and then web servers on 172.16.80.0/24 and then your bastion server can be on 172.16.22.0/24... you get the idea.\n\nMost people will say the crux of a homelab is being able to spin up servers at will, and not have to build a server each time with hardware, get a disk, you know the pain. Seeing as we installed a virtualisation operating system, it allows us to chop the host server up in to small blocks if you will where we're able to present it to a virtual machine to use. You can give a server 1 cpu that is actually 20 cores combined or 20 cores on 1 cpu. Virtualisation allows you to have fine grain control over everything.\n\n\n\n\nGues OS\n\nThis is really dependant on what your lab is for. If you are learning windows server, you will run windows server. If you are trying to get in to the IT world, I suggest start with Linux and work back to windows as once you figure out the syntax of command line, you become a lot more powerful at managing windows.\n\n\n\n\nMonitoring\n\nThis plays a big role in working out what is wrong. having good monitoring software can really speed up locating performance issues, building dank graphs and give you a high level overview on what's going on.\n\n2 tools that I use are Zabbix and Cockpit projec\n\nZabbix: (Post coming soon)\n\nCockpit:\n\nCockpit Project — Cockpit ProjectCockpit makes it easy to administer your GNU/Linux servers via a web browser.\n\n\nBackups\n\nThese are probably the most important part of a lab as 90% of the time, your time fucking things up breaking things learning through breaking. It's crucial to have backups at least once a week at a minimum.\n\nYou can custom roll your own script backing everything up to a wasabi bucket, a flash drive or if you want to be fancy and have some extra cash, replicate it off site.\n\nYou can also use something like Veeam (which is awesome) or BackupPC\n\nhttp://backuppc.sourceforge.net\n\nFree Backup Solution - Veeam Backup & Replication Community EditionVeeam® ONE™, part of Veeam Availability Suite™, provides comprehensive monitoring and analytics for your backup, virtual and physical environments. With support for Veeam Backup & Replication™ and Veeam Agents, as well as VMware vSphere, Microsoft Hyper-V and Nutanix AHV, Veeam ONE delivers deep, in…Veeam SoftwareBersayder Mejia Networking TechnicianITSC\n\nYou can hire me via Upwork or emailing me for weekend projects!","feature_image":"https://images.unsplash.com/flagged/photo-1579274216947-86eaa4b00475?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=2000&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ","featured":1,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2020-06-11T13:34:23.000Z","updated_at":"2021-05-02T01:47:26.000Z","published_at":"2020-06-12T17:27:33.000Z","custom_excerpt":"A quick intro on a home lab and the minimum to set one up","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"6425f3bc192c0c150413d594","uuid":"212587c7-bfd1-4284-94d0-e6b07c86671f","title":"How to manage remote servers using Zerotier","slug":"zerotier-cloud-managment","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"bookmark\",{\"url\":\"__GHOST_URL__/getting-started-with-zerotier/\",\"metadata\":{\"url\":\"__GHOST_URL__/getting-started-with-zerotier/\",\"title\":\"Getting started with Zerotier\",\"description\":\"Let’s look at how we can use zerotier to bridge the gaps between us and a remote server to connect using private IP’s and not mess about with firewalls\",\"author\":\"Bradley Stannard\",\"publisher\":\"breadNET\",\"thumbnail\":\"https://images.unsplash.com/photo-1512699126689-b59fb4e97c92?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=2000&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ\",\"icon\":\"https://breadnet.co.uk/favicon.png\"}}],[\"bookmark\",{\"url\":\"__GHOST_URL__/a-beginners-guide-to-ssh/\",\"metadata\":{\"url\":\"__GHOST_URL__/a-beginners-guide-to-ssh/\",\"title\":\"A beginners guide to SSH\",\"description\":\"Sweet, you just got a linux server running on <insert cloud provider > but now\\nyou need to actually do something on it. You tried to use the web console but now you need to paste something in... It\\ndoesn’t work? Shit. Luckilly SSH was designed to allow for Secure SHell access\\nto a server. (Hence S…\",\"author\":\"Bradley Stannard\",\"publisher\":\"breadNET\",\"thumbnail\":\"https://images.unsplash.com/photo-1586772002345-339f8042a777?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=2000&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ\",\"icon\":\"https://breadnet.co.uk/favicon.png\"}}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/06/image-14.png\",\"alt\":\"\",\"title\":\"\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/06/image-15.png\",\"alt\":\"\",\"title\":\"\"}],[\"code\",{\"code\":\"curl -s https://install.zerotier.com | sudo bash\\n\"}],[\"code\",{\"code\":\"sudo zerotier-cli join <network ID>\"}],[\"code\",{\"code\":\"sudo ufw enable\\n\"}],[\"code\",{\"code\":\"sudo ufw allow from <zerotier>/<subnet> to any port 22\"}],[\"code\",{\"code\":\"sudo ufw allow from <ip> to any port 22\"}],[\"code\",{\"code\":\"sudo ufw allow from <zerotier>/<range>\"}],[\"code\",{\"code\":\"sudo ufw allow http\\nsudo ufw allow https\"}],[\"markdown\",{\"markdown\":\"==By defaul UFW is set to deny all incoming connections till you explicitly allow ports to be open==\"}],[\"code\",{\"code\":\"sudo ufw enable\\nsudo ufw reload\"}],[\"code\",{\"code\":\"sudo ufw status numbered\"}],[\"code\",{\"code\":\"sudo ufw delete <numner>\"}],[\"hr\",{}]],\"markups\":[[\"s\"],[\"a\",[\"href\",\"https://my.zerotier.com/\"]],[\"code\"],[\"a\",[\"href\",\"https://www.zerotier.com/download/\"]],[\"a\",[\"href\",\"https://www.upwork.com/freelancers/~01c61ee9802b94133e\"]],[\"a\",[\"href\",\"mailto:work@breadnet.co.uk\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"(it's hard to find photos for an article like this)\"]]],[1,\"p\",[[0,[],0,\"Currently, I use zerotier to manage cloud servers in Amsterdam, the UK as well as in the USA. Despite the huge distance there is surprisingly mininal latency. In this article, we will look in to how to use it to manage remote servers via ssh, what firewall rules you should setup to prevent people from trying to login. \"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"prereqs: Zerotier and ssh knowledge\"]]],[10,0],[10,1],[1,\"p\",[[0,[],0,\"Once you've got your head around connecting a machine to zerotier, as well as how to setup seure ssh, we can look more towards the firewall rules. How ever, \"],[0,[0],1,\"instead of telling you to go read something\"],[0,[],0,\" I will explain some basics here just to make life easier.\"]]],[1,\"p\",[[0,[],0,\"Firstly start with getting zerotier setup if you haven't already:\"]]],[1,\"p\",[[0,[],0,\"You will need to create an account and install it on both machines.\"]]],[1,\"p\",[[0,[],0,\"Create an account at \"],[0,[1],1,\"my.zerotier.com/\"]]],[1,\"p\",[[0,[],0,\"Once your account is creates, click \"],[0,[2],1,\"Networks\"],[0,[],0,\" at the top and then \"],[0,[2],1,\"+ Create a Network\"]]],[1,\"p\",[[0,[],0,\"You will see one appear under the \"],[0,[2],1,\"Your Networks\"],[0,[],0,\" section\"]]],[10,2],[1,\"p\",[[0,[],0,\"Click the ID of the network and pick the network range you want. You NEED it to not be used anywhere else!\"]]],[1,\"blockquote\",[[0,[],0,\"A note to my homies who hage CGnat, make sure that you check their range before you pick one, as well as your own internal range\"]]],[10,3],[1,\"p\",[[0,[],0,\"Once that is done, you will need to install Zerotier on the machines you want to connect to each other.\"]]],[1,\"p\",[[0,[],0,\"If you trust SSL on linux:\"]]],[10,4],[1,\"p\",[[0,[],0,\"Else, Zerotier have good instructions \"],[0,[3],1,\"here\"]]],[1,\"p\",[[0,[],0,\"Now we need to add the server's to the network!\"]]],[10,5],[1,\"p\",[[0,[],0,\"If it returns with \"],[0,[2],1,\"200\"],[0,[],0,\" it's happy days. Wait a few minutes and you should see the first server show up in the management page. Add a name to it so you know what's what.\"]]],[1,\"p\",[[0,[],0,\"Rinse and repeat for all the other devices.\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"Now that Zerotier is configured, you can optionally enable SSH Keybased authentication. For cloud servers and servers that will be managed by many people, you're going to want to set this up. I wont write how to do this, as I'm hoping you took the time to read the SSH article at the top.\"]]],[1,\"p\",[[0,[],0,\"Seeing as this machine is hopefully a new one, and you're just about to get started managing it, you're going to want to enable ufw.\"]]],[10,6],[1,\"p\",[[0,[],0,\"Now that it's enabled, we will need to add in our IP address range of either zerotier, or just one machines zerotier IP address to the ufw rule for allowing ssh\"]]],[10,7],[1,\"p\",[[0,[],0,\"or if you want, just an IP address\"]]],[10,8],[1,\"p\",[[0,[],0,\"The rule I go with which allows me broad access is to allow all ports from the zerotier ip range. This allows me to manage databases', websites and ssh.\"]]],[10,9],[1,\"p\",[[0,[],0,\"The open the ports for services you may be running. HTTP(S) are below for example\"]]],[10,10],[10,11],[1,\"p\",[[0,[],0,\"Now we can apply the rules\"]]],[10,12],[1,\"p\",[[0,[],0,\"If you want to see the rules, just type\"]]],[10,13],[1,\"p\",[[0,[],0,\"and then to delete a rule it's just pick the number from the previous command and\"]]],[10,14],[10,15],[1,\"p\",[[0,[],0,\"You can hire me via \"],[0,[4],1,\"Upwork\"],[0,[],0,\" or \"],[0,[5],1,\"emailing me\"],[0,[],0,\" for weekend projects!\"]]]],\"ghostVersion\":\"3.0\"}","lexical":null,"html":"<p>(it's hard to find photos for an article like this)</p><p>Currently, I use zerotier to manage cloud servers in Amsterdam, the UK as well as in the USA. Despite the huge distance there is surprisingly mininal latency. In this article, we will look in to how to use it to manage remote servers via ssh, what firewall rules you should setup to prevent people from trying to login. </p><p></p><p>prereqs: Zerotier and ssh knowledge</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"__GHOST_URL__/getting-started-with-zerotier/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Getting started with Zerotier</div><div class=\"kg-bookmark-description\">Let’s look at how we can use zerotier to bridge the gaps between us and a remote server to connect using private IP’s and not mess about with firewalls</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://breadnet.co.uk/favicon.png\" alt=\"\"><span class=\"kg-bookmark-author\">breadNET</span><span class=\"kg-bookmark-publisher\">Bradley Stannard</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://images.unsplash.com/photo-1512699126689-b59fb4e97c92?ixlib&#x3D;rb-1.2.1&amp;q&#x3D;80&amp;fm&#x3D;jpg&amp;crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;w&#x3D;2000&amp;fit&#x3D;max&amp;ixid&#x3D;eyJhcHBfaWQiOjExNzczfQ\" alt=\"\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"__GHOST_URL__/a-beginners-guide-to-ssh/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">A beginners guide to SSH</div><div class=\"kg-bookmark-description\">Sweet, you just got a linux server running on &lt;insert cloud provider &gt; but nowyou need to actually do something on it. You tried to use the web console but now you need to paste something in... Itdoesn’t work? Shit. Luckilly SSH was designed to allow for Secure SHell accessto a server. (Hence S…</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://breadnet.co.uk/favicon.png\" alt=\"\"><span class=\"kg-bookmark-author\">breadNET</span><span class=\"kg-bookmark-publisher\">Bradley Stannard</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://images.unsplash.com/photo-1586772002345-339f8042a777?ixlib&#x3D;rb-1.2.1&amp;q&#x3D;80&amp;fm&#x3D;jpg&amp;crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;w&#x3D;2000&amp;fit&#x3D;max&amp;ixid&#x3D;eyJhcHBfaWQiOjExNzczfQ\" alt=\"\"></div></a></figure><p>Once you've got your head around connecting a machine to zerotier, as well as how to setup seure ssh, we can look more towards the firewall rules. How ever, <s>instead of telling you to go read something</s> I will explain some basics here just to make life easier.</p><p>Firstly start with getting zerotier setup if you haven't already:</p><p>You will need to create an account and install it on both machines.</p><p>Create an account at <a href=\"https://my.zerotier.com/\">my.zerotier.com/</a></p><p>Once your account is creates, click <code>Networks</code> at the top and then <code>+ Create a Network</code></p><p>You will see one appear under the <code>Your Networks</code> section</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/06/image-14.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>Click the ID of the network and pick the network range you want. You NEED it to not be used anywhere else!</p><blockquote>A note to my homies who hage CGnat, make sure that you check their range before you pick one, as well as your own internal range</blockquote><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/06/image-15.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>Once that is done, you will need to install Zerotier on the machines you want to connect to each other.</p><p>If you trust SSL on linux:</p><pre><code>curl -s https://install.zerotier.com | sudo bash\n</code></pre><p>Else, Zerotier have good instructions <a href=\"https://www.zerotier.com/download/\">here</a></p><p>Now we need to add the server's to the network!</p><pre><code>sudo zerotier-cli join &lt;network ID&gt;</code></pre><p>If it returns with <code>200</code> it's happy days. Wait a few minutes and you should see the first server show up in the management page. Add a name to it so you know what's what.</p><p>Rinse and repeat for all the other devices.</p><p></p><p>Now that Zerotier is configured, you can optionally enable SSH Keybased authentication. For cloud servers and servers that will be managed by many people, you're going to want to set this up. I wont write how to do this, as I'm hoping you took the time to read the SSH article at the top.</p><p>Seeing as this machine is hopefully a new one, and you're just about to get started managing it, you're going to want to enable ufw.</p><pre><code>sudo ufw enable\n</code></pre><p>Now that it's enabled, we will need to add in our IP address range of either zerotier, or just one machines zerotier IP address to the ufw rule for allowing ssh</p><pre><code>sudo ufw allow from &lt;zerotier&gt;/&lt;subnet&gt; to any port 22</code></pre><p>or if you want, just an IP address</p><pre><code>sudo ufw allow from &lt;ip&gt; to any port 22</code></pre><p>The rule I go with which allows me broad access is to allow all ports from the zerotier ip range. This allows me to manage databases', websites and ssh.</p><pre><code>sudo ufw allow from &lt;zerotier&gt;/&lt;range&gt;</code></pre><p>The open the ports for services you may be running. HTTP(S) are below for example</p><pre><code>sudo ufw allow http\nsudo ufw allow https</code></pre><!--kg-card-begin: markdown--><p><mark>By defaul UFW is set to deny all incoming connections till you explicitly allow ports to be open</mark></p>\n<!--kg-card-end: markdown--><p>Now we can apply the rules</p><pre><code>sudo ufw enable\nsudo ufw reload</code></pre><p>If you want to see the rules, just type</p><pre><code>sudo ufw status numbered</code></pre><p>and then to delete a rule it's just pick the number from the previous command and</p><pre><code>sudo ufw delete &lt;numner&gt;</code></pre><hr><p>You can hire me via <a href=\"https://www.upwork.com/freelancers/~01c61ee9802b94133e\">Upwork</a> or <a href=\"mailto:work@breadnet.co.uk\">emailing me</a> for weekend projects!</p>","comment_id":"5ee288a1544a0d4341b6a2d3","plaintext":"(it's hard to find photos for an article like this)\n\nCurrently, I use zerotier to manage cloud servers in Amsterdam, the UK as well as in the USA. Despite the huge distance there is surprisingly mininal latency. In this article, we will look in to how to use it to manage remote servers via ssh, what firewall rules you should setup to prevent people from trying to login.\n\n\n\nprereqs: Zerotier and ssh knowledge\n\nGetting started with ZerotierLet’s look at how we can use zerotier to bridge the gaps between us and a remote server to connect using private IP’s and not mess about with firewallsbreadNETBradley StannardA beginners guide to SSHSweet, you just got a linux server running on <insert cloud provider > but nowyou need to actually do something on it. You tried to use the web console but now you need to paste something in... Itdoesn’t work? Shit. Luckilly SSH was designed to allow for Secure SHell accessto a server. (Hence S…breadNETBradley Stannard\n\nOnce you've got your head around connecting a machine to zerotier, as well as how to setup seure ssh, we can look more towards the firewall rules. How ever, instead of telling you to go read something I will explain some basics here just to make life easier.\n\nFirstly start with getting zerotier setup if you haven't already:\n\nYou will need to create an account and install it on both machines.\n\nCreate an account at my.zerotier.com/\n\nOnce your account is creates, click Networks at the top and then + Create a Network\n\nYou will see one appear under the Your Networks section\n\nClick the ID of the network and pick the network range you want. You NEED it to not be used anywhere else!\n\nA note to my homies who hage CGnat, make sure that you check their range before you pick one, as well as your own internal range\n\nOnce that is done, you will need to install Zerotier on the machines you want to connect to each other.\n\nIf you trust SSL on linux:\n\ncurl -s https://install.zerotier.com | sudo bash\n\n\nElse, Zerotier have good instructions here\n\nNow we need to add the server's to the network!\n\nsudo zerotier-cli join <network ID>\n\nIf it returns with 200 it's happy days. Wait a few minutes and you should see the first server show up in the management page. Add a name to it so you know what's what.\n\nRinse and repeat for all the other devices.\n\n\n\nNow that Zerotier is configured, you can optionally enable SSH Keybased authentication. For cloud servers and servers that will be managed by many people, you're going to want to set this up. I wont write how to do this, as I'm hoping you took the time to read the SSH article at the top.\n\nSeeing as this machine is hopefully a new one, and you're just about to get started managing it, you're going to want to enable ufw.\n\nsudo ufw enable\n\n\nNow that it's enabled, we will need to add in our IP address range of either zerotier, or just one machines zerotier IP address to the ufw rule for allowing ssh\n\nsudo ufw allow from <zerotier>/<subnet> to any port 22\n\nor if you want, just an IP address\n\nsudo ufw allow from <ip> to any port 22\n\nThe rule I go with which allows me broad access is to allow all ports from the zerotier ip range. This allows me to manage databases', websites and ssh.\n\nsudo ufw allow from <zerotier>/<range>\n\nThe open the ports for services you may be running. HTTP(S) are below for example\n\nsudo ufw allow http\nsudo ufw allow https\n\nBy defaul UFW is set to deny all incoming connections till you explicitly allow ports to be open\n\n\nNow we can apply the rules\n\nsudo ufw enable\nsudo ufw reload\n\nIf you want to see the rules, just type\n\nsudo ufw status numbered\n\nand then to delete a rule it's just pick the number from the previous command and\n\nsudo ufw delete <numner>\n\nYou can hire me via Upwork or emailing me for weekend projects!","feature_image":"https://images.unsplash.com/photo-1569428034239-f9565e32e224?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=2000&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2020-06-11T19:40:17.000Z","updated_at":"2021-05-02T01:47:06.000Z","published_at":"2020-06-12T18:07:38.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"6425f3bc192c0c150413d595","uuid":"e2684507-4e60-4d10-9423-cdf63434bf82","title":"Getting started with Zerotier","slug":"getting-started-with-zerotier","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"bookmark\",{\"url\":\"https://github.com/zerotier\",\"metadata\":{\"url\":\"https://github.com/zerotier\",\"title\":\"ZeroTier, Inc.\",\"description\":\"Directly Connecting the World’s Devices with Universal Software Defined Networking - ZeroTier, Inc.\",\"author\":null,\"publisher\":\"GitHub\",\"thumbnail\":\"https://avatars2.githubusercontent.com/u/4173285?s=280&v=4\",\"icon\":\"https://github.githubassets.com/favicons/favicon.svg\"},\"caption\":\"Zerotier Github\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/06/image-14.png\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/06/image-15.png\"}],[\"code\",{\"code\":\"curl -s https://install.zerotier.com | sudo bash\\n\"}],[\"code\",{\"code\":\"sudo zerotier-cli join <network ID>\"}],[\"hr\",{}]],\"markups\":[[\"a\",[\"href\",\"https://my.zerotier.com/\"]],[\"code\"],[\"a\",[\"href\",\"https://www.zerotier.com/download/\"]],[\"a\",[\"href\",\"__GHOST_URL__/nginx-reverse/\"]],[\"a\",[\"href\",\"https://www.upwork.com/freelancers/~01c61ee9802b94133e\"]],[\"a\",[\"href\",\"mailto:work@breadnet.co.uk\"]]],\"sections\":[[1,\"p\",[]],[1,\"p\",[[0,[],0,\"Zerotier is something I struggle to sumarise. The best way I can explain it is a mesh VPN that requires no setup on routers or port forwarding. It overlays a 'lan' if you will that you can connect servers to from anywhere. If possible it will pass traffic from host to host, but if that is not doable, it will bounce it past their servers.\"]]],[1,\"p\",[[0,[],0,\"Zerotier promise encryption, so I guess there's quite a bit of trust. They are Opensource so you can look in to their code if you like\"]]],[1,\"blockquote\",[[0,[],0,\"All traffic is automatically end-to-end encrypted using keys only you control. Access to virtual networks is controlled by certificates.\"]]],[10,0],[1,\"p\",[[0,[],0,\"You will need to create an account and install it on both machines. \"]]],[1,\"p\",[[0,[],0,\"Create an account at \"],[0,[0],1,\"my.zerotier.com/\"],[0,[],0,\" \"]]],[1,\"p\",[[0,[],0,\"Once your account is creates, click \"],[0,[1],1,\"Networks\"],[0,[],0,\" at the top and then \"],[0,[1],1,\"+ Create a Network\"]]],[1,\"p\",[[0,[],0,\"You will see one appear under the \"],[0,[1],1,\"Your Networks\"],[0,[],0,\" section  \"]]],[1,\"p\",[]],[10,1],[1,\"p\",[[0,[],0,\"Click the ID of the network and pick the network range you want. You NEED it to not be used anywhere else!\"]]],[1,\"blockquote\",[[0,[],0,\"A note to my homies who hage CGnat, make sure that you check their range before you pick one, as well as your own internal range\"]]],[10,2],[1,\"p\",[[0,[],0,\"Once that is done, you will need to install Zerotier on the machines you want to connect to each other. \"]]],[1,\"p\",[[0,[],0,\"If you trust SSL on linux:\"]]],[10,3],[1,\"p\",[[0,[],0,\"Else, Zerotier have good instructions \"],[0,[2],1,\"here\"]]],[1,\"p\",[[0,[],0,\"Now we need to add the server's to the network!\"]]],[10,4],[1,\"p\",[[0,[],0,\"If it returns with \"],[0,[1],1,\"200\"],[0,[],0,\" it's happy days. Wait a few minutes and you should see the first server show up in the management page. Add a name to it so you know what's what. \"]]],[1,\"p\",[[0,[],0,\"Rinse and repeat for all the other devices.\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"If you want your friends to be able to join your locally hosted game server, or view an internal webpage and you don't \"],[0,[3],1,\"fancy setting up a reverse proxy\"],[0,[],0,\" then you can get them to join your zerotier network. \"]]],[1,\"p\",[[0,[],0,\"It works the same way a lan does, except it spans over great distances and can be used (I don't suggest it) as a VPN\"]]],[10,5],[1,\"p\",[[0,[],0,\"You can hire me via \"],[0,[4],1,\"Upwork\"],[0,[],0,\" or \"],[0,[5],1,\"emailing me\"],[0,[],0,\" for weekend projects!\"]]]],\"ghostVersion\":\"3.0\"}","lexical":null,"html":"<p></p><p>Zerotier is something I struggle to sumarise. The best way I can explain it is a mesh VPN that requires no setup on routers or port forwarding. It overlays a 'lan' if you will that you can connect servers to from anywhere. If possible it will pass traffic from host to host, but if that is not doable, it will bounce it past their servers.</p><p>Zerotier promise encryption, so I guess there's quite a bit of trust. They are Opensource so you can look in to their code if you like</p><blockquote>All traffic is automatically end-to-end encrypted using keys only you control. Access to virtual networks is controlled by certificates.</blockquote><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://github.com/zerotier\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">ZeroTier, Inc.</div><div class=\"kg-bookmark-description\">Directly Connecting the World’s Devices with Universal Software Defined Networking - ZeroTier, Inc.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://github.githubassets.com/favicons/favicon.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://avatars2.githubusercontent.com/u/4173285?s&#x3D;280&amp;v&#x3D;4\" alt=\"\"></div></a><figcaption>Zerotier Github</figcaption></figure><p>You will need to create an account and install it on both machines. </p><p>Create an account at <a href=\"https://my.zerotier.com/\">my.zerotier.com/</a> </p><p>Once your account is creates, click <code>Networks</code> at the top and then <code>+ Create a Network</code></p><p>You will see one appear under the <code>Your Networks</code> section  </p><p></p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/06/image-14.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>Click the ID of the network and pick the network range you want. You NEED it to not be used anywhere else!</p><blockquote>A note to my homies who hage CGnat, make sure that you check their range before you pick one, as well as your own internal range</blockquote><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/06/image-15.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>Once that is done, you will need to install Zerotier on the machines you want to connect to each other. </p><p>If you trust SSL on linux:</p><pre><code>curl -s https://install.zerotier.com | sudo bash\n</code></pre><p>Else, Zerotier have good instructions <a href=\"https://www.zerotier.com/download/\">here</a></p><p>Now we need to add the server's to the network!</p><pre><code>sudo zerotier-cli join &lt;network ID&gt;</code></pre><p>If it returns with <code>200</code> it's happy days. Wait a few minutes and you should see the first server show up in the management page. Add a name to it so you know what's what. </p><p>Rinse and repeat for all the other devices.</p><p></p><p>If you want your friends to be able to join your locally hosted game server, or view an internal webpage and you don't <a href=\"__GHOST_URL__/nginx-reverse/\">fancy setting up a reverse proxy</a> then you can get them to join your zerotier network. </p><p>It works the same way a lan does, except it spans over great distances and can be used (I don't suggest it) as a VPN</p><hr><p>You can hire me via <a href=\"https://www.upwork.com/freelancers/~01c61ee9802b94133e\">Upwork</a> or <a href=\"mailto:work@breadnet.co.uk\">emailing me</a> for weekend projects!</p>","comment_id":"5ee2963c544a0d4341b6a2dd","plaintext":"Zerotier is something I struggle to sumarise. The best way I can explain it is a mesh VPN that requires no setup on routers or port forwarding. It overlays a 'lan' if you will that you can connect servers to from anywhere. If possible it will pass traffic from host to host, but if that is not doable, it will bounce it past their servers.\n\nZerotier promise encryption, so I guess there's quite a bit of trust. They are Opensource so you can look in to their code if you like\n\nAll traffic is automatically end-to-end encrypted using keys only you control. Access to virtual networks is controlled by certificates.\n\nZeroTier, Inc.Directly Connecting the World’s Devices with Universal Software Defined Networking - ZeroTier, Inc.GitHub\n\nYou will need to create an account and install it on both machines.\n\nCreate an account at my.zerotier.com/\n\nOnce your account is creates, click Networks at the top and then + Create a Network\n\nYou will see one appear under the Your Networks section  \n\n\n\nClick the ID of the network and pick the network range you want. You NEED it to not be used anywhere else!\n\nA note to my homies who hage CGnat, make sure that you check their range before you pick one, as well as your own internal range\n\nOnce that is done, you will need to install Zerotier on the machines you want to connect to each other.\n\nIf you trust SSL on linux:\n\ncurl -s https://install.zerotier.com | sudo bash\n\n\nElse, Zerotier have good instructions here\n\nNow we need to add the server's to the network!\n\nsudo zerotier-cli join <network ID>\n\nIf it returns with 200 it's happy days. Wait a few minutes and you should see the first server show up in the management page. Add a name to it so you know what's what.\n\nRinse and repeat for all the other devices.\n\n\n\nIf you want your friends to be able to join your locally hosted game server, or view an internal webpage and you don't fancy setting up a reverse proxy then you can get them to join your zerotier network.\n\nIt works the same way a lan does, except it spans over great distances and can be used (I don't suggest it) as a VPN\n\nYou can hire me via Upwork or emailing me for weekend projects!","feature_image":"https://images.unsplash.com/photo-1512699126689-b59fb4e97c92?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=2000&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2020-06-11T20:38:20.000Z","updated_at":"2021-05-02T01:47:35.000Z","published_at":"2020-06-11T20:54:45.000Z","custom_excerpt":"Let's look at how we can use zerotier to bridge the gaps between us and a remote server to connect using private IP's and not mess about with firewalls","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"6425f3bc192c0c150413d596","uuid":"62cb4307-0030-48c2-a114-7f6eed867d42","title":"A beginners guide to SSH","slug":"a-beginners-guide-to-ssh","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"hr\",{}],[\"code\",{\"code\":\"ssh <username>@<host/ip>\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/06/image-16.png\"}],[\"code\",{\"code\":\"The authenticity of host 'bread-exit-node-1.breadnet.co.uk (203.192.192.24)' can't be established.\\nECDSA key fingerprint is fd:fd:d4:f9:77:fe:73:84:e1:55:00:ad:d6:6d:22:fe.\\nAre you sure you want to continue connecting (yes/no)?\"}],[\"hr\",{}],[\"bookmark\",{\"url\":\"https://ubuntu.com/tutorials/tutorial-ssh-keygen-on-windows#4-key-generation-with-openssh\",\"metadata\":{\"url\":\"https://ubuntu.com/tutorials/tutorial-ssh-keygen-on-windows\",\"title\":\"Generate SSH Keys on Windows 10 | Ubuntu\",\"description\":\"Ubuntu is an open source software operating system that runs from the desktop, to the cloud, to all your internet connected things.\",\"author\":null,\"publisher\":\"Ubuntu\",\"thumbnail\":\"https://assets.ubuntu.com/v1/ebdfffbf-Aubergine_suru_background.png\",\"icon\":\"https://assets.ubuntu.com/v1/17b68252-apple-touch-icon-180x180-precomposed-ubuntu.png\"},\"caption\":\"Using OpenSSH\"}],[\"bookmark\",{\"url\":\"https://ubuntu.com/tutorials/tutorial-ssh-keygen-on-windows#5-key-generation-with-putty\",\"metadata\":{\"url\":\"https://ubuntu.com/tutorials/tutorial-ssh-keygen-on-windows\",\"title\":\"Generate SSH Keys on Windows 10 | Ubuntu\",\"description\":\"Ubuntu is an open source software operating system that runs from the desktop, to the cloud, to all your internet connected things.\",\"author\":null,\"publisher\":\"Ubuntu\",\"thumbnail\":\"https://assets.ubuntu.com/v1/ebdfffbf-Aubergine_suru_background.png\",\"icon\":\"https://assets.ubuntu.com/v1/17b68252-apple-touch-icon-180x180-precomposed-ubuntu.png\"},\"caption\":\"Putty\"}],[\"code\",{\"code\":\"ssh-keygen\"}],[\"code\",{\"code\":\"stannardb@bread-d1:~$ ssh-keygen \\nGenerating public/private rsa key pair.\\nEnter file in which to save the key (/home/<username>/.ssh/id_rsa): \"}],[\"code\",{\"code\":\"/home/<username>/.ssh/id_rsa already exists.\\nOverwrite (y/n)?\"}],[\"code\",{\"code\":\"cd .ssh\\nls\"}],[\"code\",{\"code\":\"ssh-copy-id <username>@<remote_server>\"}],[\"code\",{\"code\":\"ssh-copy-id jeff@bread-exit-node-1.breadnet.co.uk\"}],[\"hr\",{}],[\"code\",{\"code\":\"ssh jeffserver\"}],[\"code\",{\"code\":\"nano ~/.ssh/config\"}],[\"code\",{\"code\":\"### domain name example\\nHost india-exit\\n  HostName bread-exit-node-1.breadnet.co.uk\\n  User root\\n  \\n  ###IP address example\\n  Host india-exit-ip\\n  \\tHostname 203.192.192.24\\n  \\tUser root\\n  \"}],[\"bookmark\",{\"url\":\"https://security.stackexchange.com/questions/167952/copy-ssh-public-key-from-windows-to-ubuntu\",\"metadata\":{\"url\":\"https://security.stackexchange.com/questions/167952/copy-ssh-public-key-from-windows-to-ubuntu\",\"title\":\"Copy SSH Public Key from Windows to Ubuntu\",\"description\":\"I am trying to get my ssh public key from my windows client to ubuntu host, but I have no idea how to. I tried to find the authorized_keys file, but had zero success. ssh-copy-id command did not work\",\"author\":\"user15791\",\"publisher\":\"Information Security Stack Exchange\",\"thumbnail\":\"https://cdn.sstatic.net/Sites/security/Img/apple-touch-icon@2.png?v=497726d850f9\",\"icon\":\"https://cdn.sstatic.net/Sites/security/Img/apple-touch-icon.png?v=14deb51d6304\"}}],[\"hr\",{}],[\"code\",{\"code\":\"sudo nano /etc/ssh/sshd_config\"}],[\"code\",{\"code\":\"PasswordAuthentication no\"}],[\"code\",{\"code\":\"sudo service ssh restart\"}],[\"bookmark\",{\"url\":\"__GHOST_URL__/ssh-client-setup-using-keys/\",\"metadata\":{\"url\":\"__GHOST_URL__/ssh-client-setup-using-keys/\",\"title\":\"SSH Client setup using keys\",\"description\":\"The following steps will setup your ssh keypairs on your local machine, copy the\\npublic key to your server, and configure your ssh client to use a specific\\nprivate key with a server alias. -------------------------------------------------------------------------------- First, generate your keypair…\",\"author\":\"thatonesysadmin\",\"publisher\":\"breadNET\",\"thumbnail\":\"https://breadnet.co.uk/content/images/2020/08/matrix-2883623_1920.jpg\",\"icon\":\"https://breadnet.co.uk/favicon.png\"}}],[\"hr\",{}]],\"markups\":[[\"em\"],[\"a\",[\"href\",\"https://lolnein.com/2018/06/14/windowsupdate/\"]],[\"a\",[\"href\",\"https://www.chiark.greenend.org.uk/~sgtatham/putty/latest.html\"]],[\"a\",[\"href\",\"https://www.putty.org\"]],[\"code\"],[\"strong\"],[\"a\",[\"href\",\"https://www.upwork.com/freelancers/~01c61ee9802b94133e\"]],[\"a\",[\"href\",\"mailto:work@breadnet.co.uk\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Sweet, you just got a linux server running on <\"],[0,[0],1,\"insert cloud provider\"],[0,[],0,\" > but now you need to actually do something on it. \"]]],[1,\"p\",[[0,[],0,\"You tried to use the web console but now you need to paste something in... It doesn't work? Shit. Luckilly SSH was designed to allow for Secure SHell access to a server. (Hence SSH)\"]]],[1,\"p\",[[0,[],0,\"We will look in to the basics of how to connect, using keys, config files and going passwordless!  \"]]],[1,\"p\",[[0,[],0,\"Basically stops you doing what the goons in the photo are doing, sitting next to the servers to manage them\"]]],[10,0],[1,\"h2\",[[0,[],0,\"Phase 1: The connection\"]]],[1,\"p\",[[0,[],0,\"Now that your fresh server is waiting for you to come in and unleash 12 levels of hell on it, we need to connect.\"]]],[1,\"p\",[[0,[],0,\"Depending on what Operating system you are using, your milage may vary. \"]]],[1,\"p\",[[0,[],0,\"If you are on \"],[0,[1],1,\"windows\"],[0,[],0,\" you can download a tool called Putty from \"],[0,[2],1,\"here\"],[0,[],0,\" (their \"],[0,[3],1,\"website\"],[0,[],0,\" if you don't trust my links)\"],[1,[],0,0],[0,[],0,\"But if memory servers correctly, windows 10 has SSH baked in to the \"],[0,[4],1,\"cmd\"],[0,[],0,\" app. \"]]],[1,\"p\",[[0,[],0,\"Where as if you are on Linux, SSH is built in!\"]]],[1,\"p\",[[0,[],0,\"Start by opening your terminal program. On ubuntu and ubuntu based OS' press \"],[0,[4],1,\"ctrl + alt + t\"],[0,[],0,\" or on windows \"],[0,[4],1,\"win + r\"],[0,[],0,\" then type \"],[0,[4],1,\"cmd\"],[0,[],0,\" \"]]],[1,\"p\",[[0,[],0,\"Next we will have to tell the computer what to do, what user we want to connect as and where the server is located.\"]]],[1,\"p\",[[0,[],0,\"In your terminal window type the below. If it does not make sense how I have presented it,  have included a picture:\"]]],[10,1],[10,2],[1,\"p\",[[0,[],0,\"Depending on your config, you may either get presented with 1 of many results.\"],[1,[],0,1],[0,[],0,\"Below are common ones I see day to day and you may encounter the first time you SSH:\"]]],[1,\"p\",[[0,[],0,\"Password prompt: Do I really need to tell you?\"],[1,[],0,2],[0,[4],1,\"Could not resolve hostname\"],[0,[],0,\": You may have typed it wrong\"],[1,[],0,3]]],[10,3],[1,\"p\",[[0,[],0,\"Type \"],[0,[4],1,\"yes\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"Once it's gone though you will be presented with a linux shell. Go wild!\"]]],[1,\"p\",[[0,[],0,\"To get back to your local machine, type \"],[0,[4],1,\"exit\"],[0,[],0,\" \"]]],[10,4],[1,\"h2\",[[0,[],0,\"Phase 2: Keys! \"]]],[1,\"p\",[[0,[],0,\"Whilst SSH is secure if done correctly, you really want to avoid using a password to login as if someone gets your password they have the ability to login from anywhere regardless of what machine they are on. To combat this, we use SSH keys. \"]]],[1,\"p\",[[0,[],0,\"Sadly I only know Linux for this bit, but luckily the good folks at Ubuntu pulled though and saved me!\"]]],[1,\"p\",[[0,[],0,\"(guess that's another reason to move to linux)\"]]],[10,5],[10,6],[1,\"p\",[[0,[],0,\"On linux, type\"]]],[10,7],[1,\"p\",[[0,[],0,\"this will return something similar to\"]]],[10,8],[1,\"p\",[[0,[],0,\"Just press enter\"]]],[1,\"blockquote\",[[0,[],0,\"Important to read the next step\"]]],[1,\"p\",[[0,[],0,\"If you are presented with the below. Press \"],[0,[4],1,\"n\"]]],[10,9],[1,\"p\",[[0,[],0,\"If not, then it will continue to asking you for a password. I suggest pick a strong one, it will encrypt the private keyflie on the disk. \"]]],[1,\"p\",[[0,[],0,\"Now you have your keys, lets take a second to explain them.\"],[1,[],0,4],[0,[],0,\"cd to the .ssh folder and list the contense\"]]],[10,10],[1,\"p\",[[0,[],0,\"You will see 2 files at mimimum, one called \"],[0,[4],1,\"id_rsa\"],[0,[],0,\" and \"],[0,[4],1,\"id_rsa.pub\"]]],[1,\"blockquote\",[[0,[],0,\"DO NOT EVER GIVE ANYONE \"],[0,[4],1,\"ID_RSA\"],[0,[],0,\" \"]]],[1,\"p\",[[0,[],0,\"The file named \"],[0,[4],1,\"id_rsa\"],[0,[],0,\" is your private key and \"],[0,[4],1,\"id_rsa.pub\"],[0,[],0,\" is your public key. Weird huh. Let me explain it quickly.\"]]],[1,\"p\",[[0,[],0,\"Imagine you have a post box at the front of your house, it has 2 doors. One on the top where people can put post, and one on the front for retrieving post. This postbox has 2 locks. One on the top flap and one on the big door to get post.\"]]],[1,\"p\",[[0,[],0,\"The public key can be compared to giving all your friends and the Postal workers a key each, to access the top flap so they can put post in to your postbox. You want as many people to have this so it makes recieving post simple.\"]]],[1,\"p\",[[0,[],0,\"Where as the private key is the key that opens the main flap to get out the post. You \"],[0,[5],1,\"really\"],[0,[],0,\" don't want anyone to get this key as they will have full roam to your mailbox. \"]]],[1,\"p\",[[0,[],0,\"You would give your public key to your friends who have servers so you can login, and they do the same. \"]]],[1,\"p\",[[0,[],0,\"To drive the point home, imagine the private key is your underwear that your mom purchased, it has minnie the mouse on it. I don't know about you, but I don't want anyone to see them. \"]]],[1,\"p\",[[0,[],0,\"Now we have that out the way, we can talk about how it makes our lives easier.\"]]],[1,\"p\",[[0,[],0,\"We need to copy our keys to the remote server that we plan to login to. Luckilly (as usual) linux has a built in tool for this. For some reason, they named it \"],[0,[4],1,\"ssh-copy-id\"],[0,[],0,\" , odd.\"]]],[1,\"p\",[[0,[],0,\"The syntax for this is:\"]]],[10,11],[1,\"p\",[[0,[],0,\"So if I wanted to copy my desktops keys to my exit node:\"]]],[10,12],[1,\"p\",[[0,[],0,\"It will ask for a password, then once that's in you're good to go.\"]]],[1,\"p\",[[0,[],0,\"Try SSH to the host again, and you \"],[0,[0],1,\"hopefully\"],[0,[],0,\" wont be asked for a password. \"]]],[10,13],[1,\"h2\",[[0,[],0,\"Phase 3: Config file\"]]],[1,\"p\",[[0,[],0,\"So now that we have been using SSH for a while, it gets annoying having to type out the username, server name and sometimes port mapping (appending \"],[0,[4],1,\"-p <port\"],[0,[],0,\" if you wanted to know) \"]]],[1,\"p\",[[0,[],0,\"SSH can make use of a config file with a specific syntax to speed up connecting to a server. It allows something like the below where we can give our servers human names if you're in to that\"]]],[10,14],[1,\"p\",[[0,[],0,\"To achive this level of godly-ness we need to create the config file. \"]]],[1,\"p\",[[0,[],0,\"Run the below to create the file, or edit it if you already have one. \"]]],[10,15],[1,\"p\",[[0,[],0,\"This will either open up a file with  a config or a blank file. \"]]],[1,\"p\",[[0,[],0,\"In the file we need to specify at minimum 3 things. What we will call it, the address and what user\"]]],[10,16],[1,\"p\",[[0,[],0,\"Feel free to copy the above for formatting. \"]]],[1,\"blockquote\",[[0,[],0,\"Just know you cant use spaces for names\"]]],[1,\"p\",[[0,[],0,\"Now save the file with \"],[0,[4],1,\"ctrl + x\"],[0,[],0,\" \"],[0,[4],1,\"y\"],[0,[],0,\" \"],[0,[4],1,\"enter\"]]],[1,\"p\",[[0,[],0,\"Try ssh to the name you gave it. (This is the word that follows after \"],[0,[4],1,\"Host\"],[0,[],0,\")\"]]],[1,\"p\",[[0,[],0,\"It \"],[0,[0],1,\"should\"],[0,[],0,\" work if all went well.\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"Now I know my windows friends are complaining 'but what about us' - Don't worry fam. \"]]],[10,17],[1,\"p\",[[0,[],0,\"Move your public key (id_rsa.pub) and your private key (id_rsa) to \"],[0,[4],1,\"C:\\\\Users\\\\yourUserName\\\\.ssh\\\\\"],[0,[],0,\" create the .ssh folder if needed.\"]]],[1,\"p\",[[0,[5,5],2,\"2.\"],[0,[],0,\" On your windows host via cmder:\"]]],[1,\"p\",[[0,[4],1,\"cp C:\\\\Users\\\\yourUserName\\\\.ssh\\\\id_rsa.pub C:\\\\Users\\\\yourUserName\\\\authorized_keys\"]]],[1,\"p\",[[0,[5,5],2,\"3.\"],[0,[],0,\" On your ubuntu host:\"]]],[1,\"p\",[[0,[4],1,\"service ssh status\"],[0,[],0,\" Start sshd if necessary\"]]],[1,\"p\",[[0,[4],1,\"mkdir ~/.ssh/\"],[0,[],0,\" (if it doesn't already exist)\"]]],[1,\"p\",[[0,[5,5],2,\"4.\"],[0,[],0,\" On your Windows host via cmder:\"]]],[1,\"p\",[[0,[4],1,\"cd C:\\\\Users\\\\yourUserName\\\\\"]]],[1,\"p\",[[0,[4],1,\"scp authorized_keys login-id@ubuntu-Host-Ip:~/.ssh\"]]],[1,\"p\",[[0,[5,5],2,\"5.\"],[0,[],0,\" On your Ubuntu host:\"]]],[1,\"p\",[[0,[4],1,\"chmod 700 ~/.ssh\"]]],[1,\"p\",[[0,[4],1,\"chmod 600 ~/.ssh/authorized_keys\"]]],[1,\"p\",[[0,[5,5],2,\"6.\"],[0,[],0,\" On your Windows host via cmder:\"]]],[1,\"p\",[[0,[],0,\"Test if you can ssh into your ubuntu host without login/pw auth. If yes:\"]]],[1,\"p\",[[0,[4],1,\"rm C:\\\\Users\\\\yourUserName\\\\authorized_keys\"]]],[10,18],[1,\"h2\",[[0,[],0,\"Phase 4: Look mom, no password!\"]]],[1,\"p\",[[0,[],0,\"Now this has the potential to really ruin your day if it goes wrong, so I suggest open 2 ssh sessions to the server. One you will leave in the background logged in and at a sudo terminal ( \"],[0,[4],1,\"sudo -s\"],[0,[],0,\") and one where you will do the work in. This is just in case something goes wrong! \"]]],[1,\"blockquote\",[[0,[],0,\"Before completing the below steps, ensure you can ssh to the server using keys only.\"]]],[1,\"p\",[[0,[],0,\"We will start by disabling Password based authentication\"]]],[1,\"p\",[[0,[],0,\"On one of the sessions, type\"]]],[10,19],[1,\"p\",[[0,[],0,\"Int he file, locate the line \"],[0,[4],1,\"PasswordAuthentication\"],[0,[],0,\" \"]]],[1,\"p\",[[0,[],0,\"Set it to \"],[0,[4],1,\"no\"],[0,[],0,\" so it looks like the below\"]]],[10,20],[1,\"p\",[[0,[],0,\"Here comes the part that will ruin your day if goofed up. Just putting this out there but I accept to responsibility if you goof up. \"]]],[10,21],[1,\"p\",[[0,[],0,\"Now logout (\"],[0,[4],1,\"exit\"],[0,[],0,\") and ssh to the server. If all is good, you should have no problem logging in. \"]]],[1,\"p\",[[0,[5],1,\"However, \"],[0,[],0,\"should you not be able to, just open the other window we had and revert the changes, restart the service and work out the problems you may have had!\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"To wrap it up:\"]]],[1,\"p\",[[0,[],0,\"You should now have SSH knowledge and a place to start in the world of managing linux based server's over SSH, how to setup and use keys and how to stop using passwords. \"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"If you're ready to move on to something a little more complex, check out the below!\"]]],[10,22],[10,23],[1,\"p\",[[0,[],0,\"You can hire me via \"],[0,[6],1,\"Upwork\"],[0,[],0,\" or \"],[0,[7],1,\"emailing me\"],[0,[],0,\" for weekend projects!\"]]]],\"ghostVersion\":\"3.0\"}","lexical":null,"html":"<p>Sweet, you just got a linux server running on &lt;<em>insert cloud provider</em> &gt; but now you need to actually do something on it. </p><p>You tried to use the web console but now you need to paste something in... It doesn't work? Shit. Luckilly SSH was designed to allow for Secure SHell access to a server. (Hence SSH)</p><p>We will look in to the basics of how to connect, using keys, config files and going passwordless!  </p><p>Basically stops you doing what the goons in the photo are doing, sitting next to the servers to manage them</p><hr><h2 id=\"phase-1-the-connection\">Phase 1: The connection</h2><p>Now that your fresh server is waiting for you to come in and unleash 12 levels of hell on it, we need to connect.</p><p>Depending on what Operating system you are using, your milage may vary. </p><p>If you are on <a href=\"https://lolnein.com/2018/06/14/windowsupdate/\">windows</a> you can download a tool called Putty from <a href=\"https://www.chiark.greenend.org.uk/~sgtatham/putty/latest.html\">here</a> (their <a href=\"https://www.putty.org\">website</a> if you don't trust my links)<br>But if memory servers correctly, windows 10 has SSH baked in to the <code>cmd</code> app. </p><p>Where as if you are on Linux, SSH is built in!</p><p>Start by opening your terminal program. On ubuntu and ubuntu based OS' press <code>ctrl + alt + t</code> or on windows <code>win + r</code> then type <code>cmd</code> </p><p>Next we will have to tell the computer what to do, what user we want to connect as and where the server is located.</p><p>In your terminal window type the below. If it does not make sense how I have presented it,  have included a picture:</p><pre><code>ssh &lt;username&gt;@&lt;host/ip&gt;</code></pre><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/06/image-16.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>Depending on your config, you may either get presented with 1 of many results.<br>Below are common ones I see day to day and you may encounter the first time you SSH:</p><p>Password prompt: Do I really need to tell you?<br><code>Could not resolve hostname</code>: You may have typed it wrong<br></p><pre><code>The authenticity of host 'bread-exit-node-1.breadnet.co.uk (203.192.192.24)' can't be established.\nECDSA key fingerprint is fd:fd:d4:f9:77:fe:73:84:e1:55:00:ad:d6:6d:22:fe.\nAre you sure you want to continue connecting (yes/no)?</code></pre><p>Type <code>yes</code></p><p></p><p>Once it's gone though you will be presented with a linux shell. Go wild!</p><p>To get back to your local machine, type <code>exit</code> </p><hr><h2 id=\"phase-2-keys-\">Phase 2: Keys! </h2><p>Whilst SSH is secure if done correctly, you really want to avoid using a password to login as if someone gets your password they have the ability to login from anywhere regardless of what machine they are on. To combat this, we use SSH keys. </p><p>Sadly I only know Linux for this bit, but luckily the good folks at Ubuntu pulled though and saved me!</p><p>(guess that's another reason to move to linux)</p><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://ubuntu.com/tutorials/tutorial-ssh-keygen-on-windows#4-key-generation-with-openssh\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Generate SSH Keys on Windows 10 | Ubuntu</div><div class=\"kg-bookmark-description\">Ubuntu is an open source software operating system that runs from the desktop, to the cloud, to all your internet connected things.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://assets.ubuntu.com/v1/17b68252-apple-touch-icon-180x180-precomposed-ubuntu.png\" alt=\"\"><span class=\"kg-bookmark-author\">Ubuntu</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://assets.ubuntu.com/v1/ebdfffbf-Aubergine_suru_background.png\" alt=\"\"></div></a><figcaption>Using OpenSSH</figcaption></figure><figure class=\"kg-card kg-bookmark-card kg-card-hascaption\"><a class=\"kg-bookmark-container\" href=\"https://ubuntu.com/tutorials/tutorial-ssh-keygen-on-windows#5-key-generation-with-putty\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Generate SSH Keys on Windows 10 | Ubuntu</div><div class=\"kg-bookmark-description\">Ubuntu is an open source software operating system that runs from the desktop, to the cloud, to all your internet connected things.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://assets.ubuntu.com/v1/17b68252-apple-touch-icon-180x180-precomposed-ubuntu.png\" alt=\"\"><span class=\"kg-bookmark-author\">Ubuntu</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://assets.ubuntu.com/v1/ebdfffbf-Aubergine_suru_background.png\" alt=\"\"></div></a><figcaption>Putty</figcaption></figure><p>On linux, type</p><pre><code>ssh-keygen</code></pre><p>this will return something similar to</p><pre><code>stannardb@bread-d1:~$ ssh-keygen \nGenerating public/private rsa key pair.\nEnter file in which to save the key (/home/&lt;username&gt;/.ssh/id_rsa): </code></pre><p>Just press enter</p><blockquote>Important to read the next step</blockquote><p>If you are presented with the below. Press <code>n</code></p><pre><code>/home/&lt;username&gt;/.ssh/id_rsa already exists.\nOverwrite (y/n)?</code></pre><p>If not, then it will continue to asking you for a password. I suggest pick a strong one, it will encrypt the private keyflie on the disk. </p><p>Now you have your keys, lets take a second to explain them.<br>cd to the .ssh folder and list the contense</p><pre><code>cd .ssh\nls</code></pre><p>You will see 2 files at mimimum, one called <code>id_rsa</code> and <code>id_rsa.pub</code></p><blockquote>DO NOT EVER GIVE ANYONE <code>ID_RSA</code> </blockquote><p>The file named <code>id_rsa</code> is your private key and <code>id_rsa.pub</code> is your public key. Weird huh. Let me explain it quickly.</p><p>Imagine you have a post box at the front of your house, it has 2 doors. One on the top where people can put post, and one on the front for retrieving post. This postbox has 2 locks. One on the top flap and one on the big door to get post.</p><p>The public key can be compared to giving all your friends and the Postal workers a key each, to access the top flap so they can put post in to your postbox. You want as many people to have this so it makes recieving post simple.</p><p>Where as the private key is the key that opens the main flap to get out the post. You <strong>really</strong> don't want anyone to get this key as they will have full roam to your mailbox. </p><p>You would give your public key to your friends who have servers so you can login, and they do the same. </p><p>To drive the point home, imagine the private key is your underwear that your mom purchased, it has minnie the mouse on it. I don't know about you, but I don't want anyone to see them. </p><p>Now we have that out the way, we can talk about how it makes our lives easier.</p><p>We need to copy our keys to the remote server that we plan to login to. Luckilly (as usual) linux has a built in tool for this. For some reason, they named it <code>ssh-copy-id</code> , odd.</p><p>The syntax for this is:</p><pre><code>ssh-copy-id &lt;username&gt;@&lt;remote_server&gt;</code></pre><p>So if I wanted to copy my desktops keys to my exit node:</p><pre><code>ssh-copy-id jeff@bread-exit-node-1.breadnet.co.uk</code></pre><p>It will ask for a password, then once that's in you're good to go.</p><p>Try SSH to the host again, and you <em>hopefully</em> wont be asked for a password. </p><hr><h2 id=\"phase-3-config-file\">Phase 3: Config file</h2><p>So now that we have been using SSH for a while, it gets annoying having to type out the username, server name and sometimes port mapping (appending <code>-p &lt;port</code> if you wanted to know) </p><p>SSH can make use of a config file with a specific syntax to speed up connecting to a server. It allows something like the below where we can give our servers human names if you're in to that</p><pre><code>ssh jeffserver</code></pre><p>To achive this level of godly-ness we need to create the config file. </p><p>Run the below to create the file, or edit it if you already have one. </p><pre><code>nano ~/.ssh/config</code></pre><p>This will either open up a file with  a config or a blank file. </p><p>In the file we need to specify at minimum 3 things. What we will call it, the address and what user</p><pre><code>### domain name example\nHost india-exit\n  HostName bread-exit-node-1.breadnet.co.uk\n  User root\n  \n  ###IP address example\n  Host india-exit-ip\n  \tHostname 203.192.192.24\n  \tUser root\n  </code></pre><p>Feel free to copy the above for formatting. </p><blockquote>Just know you cant use spaces for names</blockquote><p>Now save the file with <code>ctrl + x</code> <code>y</code> <code>enter</code></p><p>Try ssh to the name you gave it. (This is the word that follows after <code>Host</code>)</p><p>It <em>should</em> work if all went well.</p><p></p><p>Now I know my windows friends are complaining 'but what about us' - Don't worry fam. </p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://security.stackexchange.com/questions/167952/copy-ssh-public-key-from-windows-to-ubuntu\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Copy SSH Public Key from Windows to Ubuntu</div><div class=\"kg-bookmark-description\">I am trying to get my ssh public key from my windows client to ubuntu host, but I have no idea how to. I tried to find the authorized_keys file, but had zero success. ssh-copy-id command did not work</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://cdn.sstatic.net/Sites/security/Img/apple-touch-icon.png?v&#x3D;14deb51d6304\" alt=\"\"><span class=\"kg-bookmark-author\">Information Security Stack Exchange</span><span class=\"kg-bookmark-publisher\">user15791</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://cdn.sstatic.net/Sites/security/Img/apple-touch-icon@2.png?v&#x3D;497726d850f9\" alt=\"\"></div></a></figure><p>Move your public key (id_rsa.pub) and your private key (id_rsa) to <code>C:\\Users\\yourUserName\\.ssh\\</code> create the .ssh folder if needed.</p><p><strong><strong>2.</strong></strong> On your windows host via cmder:</p><p><code>cp C:\\Users\\yourUserName\\.ssh\\id_rsa.pub C:\\Users\\yourUserName\\authorized_keys</code></p><p><strong><strong>3.</strong></strong> On your ubuntu host:</p><p><code>service ssh status</code> Start sshd if necessary</p><p><code>mkdir ~/.ssh/</code> (if it doesn't already exist)</p><p><strong><strong>4.</strong></strong> On your Windows host via cmder:</p><p><code>cd C:\\Users\\yourUserName\\</code></p><p><code>scp authorized_keys login-id@ubuntu-Host-Ip:~/.ssh</code></p><p><strong><strong>5.</strong></strong> On your Ubuntu host:</p><p><code>chmod 700 ~/.ssh</code></p><p><code>chmod 600 ~/.ssh/authorized_keys</code></p><p><strong><strong>6.</strong></strong> On your Windows host via cmder:</p><p>Test if you can ssh into your ubuntu host without login/pw auth. If yes:</p><p><code>rm C:\\Users\\yourUserName\\authorized_keys</code></p><hr><h2 id=\"phase-4-look-mom-no-password-\">Phase 4: Look mom, no password!</h2><p>Now this has the potential to really ruin your day if it goes wrong, so I suggest open 2 ssh sessions to the server. One you will leave in the background logged in and at a sudo terminal ( <code>sudo -s</code>) and one where you will do the work in. This is just in case something goes wrong! </p><blockquote>Before completing the below steps, ensure you can ssh to the server using keys only.</blockquote><p>We will start by disabling Password based authentication</p><p>On one of the sessions, type</p><pre><code>sudo nano /etc/ssh/sshd_config</code></pre><p>Int he file, locate the line <code>PasswordAuthentication</code> </p><p>Set it to <code>no</code> so it looks like the below</p><pre><code>PasswordAuthentication no</code></pre><p>Here comes the part that will ruin your day if goofed up. Just putting this out there but I accept to responsibility if you goof up. </p><pre><code>sudo service ssh restart</code></pre><p>Now logout (<code>exit</code>) and ssh to the server. If all is good, you should have no problem logging in. </p><p><strong>However, </strong>should you not be able to, just open the other window we had and revert the changes, restart the service and work out the problems you may have had!</p><p></p><p>To wrap it up:</p><p>You should now have SSH knowledge and a place to start in the world of managing linux based server's over SSH, how to setup and use keys and how to stop using passwords. </p><p></p><p>If you're ready to move on to something a little more complex, check out the below!</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"__GHOST_URL__/ssh-client-setup-using-keys/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">SSH Client setup using keys</div><div class=\"kg-bookmark-description\">The following steps will setup your ssh keypairs on your local machine, copy thepublic key to your server, and configure your ssh client to use a specificprivate key with a server alias. -------------------------------------------------------------------------------- First, generate your keypair…</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://breadnet.co.uk/favicon.png\" alt=\"\"><span class=\"kg-bookmark-author\">breadNET</span><span class=\"kg-bookmark-publisher\">thatonesysadmin</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://breadnet.co.uk/content/images/2020/08/matrix-2883623_1920.jpg\" alt=\"\"></div></a></figure><hr><p>You can hire me via <a href=\"https://www.upwork.com/freelancers/~01c61ee9802b94133e\">Upwork</a> or <a href=\"mailto:work@breadnet.co.uk\">emailing me</a> for weekend projects!</p>","comment_id":"5ee29a98544a0d4341b6a310","plaintext":"Sweet, you just got a linux server running on <insert cloud provider > but now you need to actually do something on it.\n\nYou tried to use the web console but now you need to paste something in... It doesn't work? Shit. Luckilly SSH was designed to allow for Secure SHell access to a server. (Hence SSH)\n\nWe will look in to the basics of how to connect, using keys, config files and going passwordless!  \n\nBasically stops you doing what the goons in the photo are doing, sitting next to the servers to manage them\n\n\nPhase 1: The connection\n\nNow that your fresh server is waiting for you to come in and unleash 12 levels of hell on it, we need to connect.\n\nDepending on what Operating system you are using, your milage may vary.\n\nIf you are on windows you can download a tool called Putty from here (their website if you don't trust my links)\nBut if memory servers correctly, windows 10 has SSH baked in to the cmd app.\n\nWhere as if you are on Linux, SSH is built in!\n\nStart by opening your terminal program. On ubuntu and ubuntu based OS' press ctrl + alt + t or on windows win + r then type cmd\n\nNext we will have to tell the computer what to do, what user we want to connect as and where the server is located.\n\nIn your terminal window type the below. If it does not make sense how I have presented it,  have included a picture:\n\nssh <username>@<host/ip>\n\nDepending on your config, you may either get presented with 1 of many results.\nBelow are common ones I see day to day and you may encounter the first time you SSH:\n\nPassword prompt: Do I really need to tell you?\nCould not resolve hostname: You may have typed it wrong\n\n\nThe authenticity of host 'bread-exit-node-1.breadnet.co.uk (203.192.192.24)' can't be established.\nECDSA key fingerprint is fd:fd:d4:f9:77:fe:73:84:e1:55:00:ad:d6:6d:22:fe.\nAre you sure you want to continue connecting (yes/no)?\n\nType yes\n\n\n\nOnce it's gone though you will be presented with a linux shell. Go wild!\n\nTo get back to your local machine, type exit\n\n\nPhase 2: Keys!\n\nWhilst SSH is secure if done correctly, you really want to avoid using a password to login as if someone gets your password they have the ability to login from anywhere regardless of what machine they are on. To combat this, we use SSH keys.\n\nSadly I only know Linux for this bit, but luckily the good folks at Ubuntu pulled though and saved me!\n\n(guess that's another reason to move to linux)\n\nGenerate SSH Keys on Windows 10 | UbuntuUbuntu is an open source software operating system that runs from the desktop, to the cloud, to all your internet connected things.UbuntuGenerate SSH Keys on Windows 10 | UbuntuUbuntu is an open source software operating system that runs from the desktop, to the cloud, to all your internet connected things.Ubuntu\n\nOn linux, type\n\nssh-keygen\n\nthis will return something similar to\n\nstannardb@bread-d1:~$ ssh-keygen \nGenerating public/private rsa key pair.\nEnter file in which to save the key (/home/<username>/.ssh/id_rsa): \n\nJust press enter\n\nImportant to read the next step\n\nIf you are presented with the below. Press n\n\n/home/<username>/.ssh/id_rsa already exists.\nOverwrite (y/n)?\n\nIf not, then it will continue to asking you for a password. I suggest pick a strong one, it will encrypt the private keyflie on the disk.\n\nNow you have your keys, lets take a second to explain them.\ncd to the .ssh folder and list the contense\n\ncd .ssh\nls\n\nYou will see 2 files at mimimum, one called id_rsa and id_rsa.pub\n\nDO NOT EVER GIVE ANYONE ID_RSA\n\nThe file named id_rsa is your private key and id_rsa.pub is your public key. Weird huh. Let me explain it quickly.\n\nImagine you have a post box at the front of your house, it has 2 doors. One on the top where people can put post, and one on the front for retrieving post. This postbox has 2 locks. One on the top flap and one on the big door to get post.\n\nThe public key can be compared to giving all your friends and the Postal workers a key each, to access the top flap so they can put post in to your postbox. You want as many people to have this so it makes recieving post simple.\n\nWhere as the private key is the key that opens the main flap to get out the post. You really don't want anyone to get this key as they will have full roam to your mailbox.\n\nYou would give your public key to your friends who have servers so you can login, and they do the same.\n\nTo drive the point home, imagine the private key is your underwear that your mom purchased, it has minnie the mouse on it. I don't know about you, but I don't want anyone to see them.\n\nNow we have that out the way, we can talk about how it makes our lives easier.\n\nWe need to copy our keys to the remote server that we plan to login to. Luckilly (as usual) linux has a built in tool for this. For some reason, they named it ssh-copy-id , odd.\n\nThe syntax for this is:\n\nssh-copy-id <username>@<remote_server>\n\nSo if I wanted to copy my desktops keys to my exit node:\n\nssh-copy-id jeff@bread-exit-node-1.breadnet.co.uk\n\nIt will ask for a password, then once that's in you're good to go.\n\nTry SSH to the host again, and you hopefully wont be asked for a password.\n\n\nPhase 3: Config file\n\nSo now that we have been using SSH for a while, it gets annoying having to type out the username, server name and sometimes port mapping (appending -p <port if you wanted to know)\n\nSSH can make use of a config file with a specific syntax to speed up connecting to a server. It allows something like the below where we can give our servers human names if you're in to that\n\nssh jeffserver\n\nTo achive this level of godly-ness we need to create the config file.\n\nRun the below to create the file, or edit it if you already have one.\n\nnano ~/.ssh/config\n\nThis will either open up a file with  a config or a blank file.\n\nIn the file we need to specify at minimum 3 things. What we will call it, the address and what user\n\n### domain name example\nHost india-exit\n  HostName bread-exit-node-1.breadnet.co.uk\n  User root\n  \n  ###IP address example\n  Host india-exit-ip\n  \tHostname 203.192.192.24\n  \tUser root\n  \n\nFeel free to copy the above for formatting.\n\nJust know you cant use spaces for names\n\nNow save the file with ctrl + x y enter\n\nTry ssh to the name you gave it. (This is the word that follows after Host)\n\nIt should work if all went well.\n\n\n\nNow I know my windows friends are complaining 'but what about us' - Don't worry fam.\n\nCopy SSH Public Key from Windows to UbuntuI am trying to get my ssh public key from my windows client to ubuntu host, but I have no idea how to. I tried to find the authorized_keys file, but had zero success. ssh-copy-id command did not workInformation Security Stack Exchangeuser15791\n\nMove your public key (id_rsa.pub) and your private key (id_rsa) to C:\\Users\\yourUserName\\.ssh\\ create the .ssh folder if needed.\n\n2. On your windows host via cmder:\n\ncp C:\\Users\\yourUserName\\.ssh\\id_rsa.pub C:\\Users\\yourUserName\\authorized_keys\n\n3. On your ubuntu host:\n\nservice ssh status Start sshd if necessary\n\nmkdir ~/.ssh/ (if it doesn't already exist)\n\n4. On your Windows host via cmder:\n\ncd C:\\Users\\yourUserName\\\n\nscp authorized_keys login-id@ubuntu-Host-Ip:~/.ssh\n\n5. On your Ubuntu host:\n\nchmod 700 ~/.ssh\n\nchmod 600 ~/.ssh/authorized_keys\n\n6. On your Windows host via cmder:\n\nTest if you can ssh into your ubuntu host without login/pw auth. If yes:\n\nrm C:\\Users\\yourUserName\\authorized_keys\n\n\nPhase 4: Look mom, no password!\n\nNow this has the potential to really ruin your day if it goes wrong, so I suggest open 2 ssh sessions to the server. One you will leave in the background logged in and at a sudo terminal ( sudo -s) and one where you will do the work in. This is just in case something goes wrong!\n\nBefore completing the below steps, ensure you can ssh to the server using keys only.\n\nWe will start by disabling Password based authentication\n\nOn one of the sessions, type\n\nsudo nano /etc/ssh/sshd_config\n\nInt he file, locate the line PasswordAuthentication\n\nSet it to no so it looks like the below\n\nPasswordAuthentication no\n\nHere comes the part that will ruin your day if goofed up. Just putting this out there but I accept to responsibility if you goof up.\n\nsudo service ssh restart\n\nNow logout (exit) and ssh to the server. If all is good, you should have no problem logging in.\n\nHowever, should you not be able to, just open the other window we had and revert the changes, restart the service and work out the problems you may have had!\n\n\n\nTo wrap it up:\n\nYou should now have SSH knowledge and a place to start in the world of managing linux based server's over SSH, how to setup and use keys and how to stop using passwords.\n\n\n\nIf you're ready to move on to something a little more complex, check out the below!\n\nSSH Client setup using keysThe following steps will setup your ssh keypairs on your local machine, copy thepublic key to your server, and configure your ssh client to use a specificprivate key with a server alias. -------------------------------------------------------------------------------- First, generate your keypair…breadNETthatonesysadmin\n\nYou can hire me via Upwork or emailing me for weekend projects!","feature_image":"https://images.unsplash.com/photo-1586772002345-339f8042a777?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=2000&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2020-06-11T20:56:56.000Z","updated_at":"2021-05-02T01:47:16.000Z","published_at":"2020-06-11T22:00:17.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"6425f3bc192c0c150413d597","uuid":"90816887-702f-4d67-897f-89ab32de56d6","title":"Moving to the cloud: Intro","slug":"moving-to-the-cloud-1","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/07/image-1.png\",\"alt\":\"Cost breakdown for Digital ocean vs OVH\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/07/Untitled-drawing.png\"}],[\"bookmark\",{\"url\":\"https://github.com/userbradley/breadnet-infra\",\"metadata\":{\"url\":\"https://github.com/userbradley/breadnet-infra\",\"title\":\"userbradley/breadnet-infra\",\"description\":\"Breadnet Infra setup though ansible. Contribute to userbradley/breadnet-infra development by creating an account on GitHub.\",\"author\":\"userbradley\",\"publisher\":\"GitHub\",\"thumbnail\":\"https://avatars2.githubusercontent.com/u/41597815?s=400&v=4\",\"icon\":\"https://github.githubassets.com/favicons/favicon.svg\"}}],[\"code\",{\"code\":\".\\n├── hosts\\n├── README.md\\n├── setup\\n│   ├── defaults\\n│   │   └── main.yml\\n│   ├── group_vars\\n│   │   ├── all.yml\\n│   │   ├── app1.yml\\n│   │   └── app2.yml\\n│   └── tasks\\n│       ├── bookstack.yml\\n│       ├── files\\n│       │   ├── bookstack_env.j2\\n│       │   ├── bookstack_nginx.j2\\n│       │   ├── kanboard_config.j2\\n│       │   ├── kan_nginx.j2\\n│       │   ├── matomo_nginx.j2\\n│       │   ├── status.env.j2\\n│       │   ├── status.j2\\n│       │   └── zabbix_apache2.j2\\n│       ├── ghost.yml\\n│       ├── - include: nginx.yml\\n│       ├── jellyfin.yml\\n│       ├── kanboard.yml\\n│       ├── main.yml\\n│       ├── matomo.yml\\n│       ├── mysql.yml\\n│       ├── nginx.yml\\n│       ├── php7.2.yml\\n│       ├── reboot.yml\\n│       ├── scripts\\n│       │   └── inscomp.sh\\n│       ├── status.yml\\n│       └── zabbix.yml\\n├── setup.yml\\n├── tasks\\n│   ├── files\\n│   │   ├── bookstack_env.j2\\n│   │   ├── bookstack_nginx.j2\\n│   │   ├── kanboard_config.j2\\n│   │   ├── kan_nginx.j2\\n│   │   ├── matomo_nginx.j2\\n│   │   ├── status.env.j2\\n│   │   ├── status.j2\\n│   │   └── zabbix_apache2.j2\\n│   ├── ghost.yml\\n│   ├── main.yml\\n│   └── scripts\\n│       └── inscomp.sh\\n└── test\\n    ├── defaults\\n    │   └── main.yml\\n    ├── hosts\\n    ├── roles\\n    │   ├── bookstack\\n    │   │   ├── defaults\\n    │   │   │   └── main.yml\\n    │   │   └── tasks\\n    │   │       └── main.yml\\n    │   ├── common\\n    │   │   └── tasks\\n    │   │       ├── mysql.yml\\n    │   │       └── php7.2.yml\\n    │   ├── defaults\\n    │   │   └── main.yml\\n    │   ├── jellyfin\\n    │   │   └── tasks\\n    │   │       └── main.yml\\n    │   ├── kanboard\\n    │   │   ├── defaults\\n    │   │   │   └── main.yml\\n    │   │   └── tasks\\n    │   │       └── main.yml\\n    │   ├── matomo\\n    │   │   ├── defaults\\n    │   │   │   └── main.yml\\n    │   │   └── tasks\\n    │   │       └── main.yml\\n    │   ├── mysql\\n    │   │   └── tasks\\n    │   │       └── main.yml\\n    │   ├── nginx\\n    │   │   ├── defaults\\n    │   │   │   └── main.yml\\n    │   │   └── tasks\\n    │   │       └── main.yml\\n    │   ├── reboot\\n    │   │   └── tasks\\n    │   │       └── main.yml\\n    │   ├── status\\n    │   │   ├── defaults\\n    │   │   │   └── main.yml\\n    │   │   └── tasks\\n    │   │       └── main.yml\\n    │   └── zabbix\\n    │       └── tasks\\n    │           └── main.yml\\n    └── setup.yml\"}],[\"code\",{\"code\":\"---\\n- include: reboot.yml\\n  become: yes\\n  when:\\n     - reboot|bool\\n       \\n- include: nginx.yml\\n  become: yes\\n  delegate_to: app2\\n  when: \\n     - inventory_hostname in groups['app1']\\n     #- inventory_hostname in groups['app2']\\n     - inventory_hostname == app[1]\\n     - inventory_hostname in groups['app1']|default([])\\n     - nginx|bool\\n  tags:\\n      - nginx\\n\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/07/image-2.png\",\"cardWidth\":\"\",\"caption\":\"A meme for all you cool kids\"}],[\"code\",{\"code\":\"---\\n    - name: Add PHP repository \\n      become: true\\n      apt_repository:\\n       repo: 'ppa:ondrej/php'\\n      tags: [php1]\\n\\n    - name: Update repositories\\n      become: true\\n      apt:\\n       update_cache: yes\\n      tags: [php2]\\n\\n    - name: Install php7.2 #Move to a global install\\n      become: true\\n      apt: name=php7.2 update_cache=yes state=latest\\n      tags: [php3]\\n\\n    - name: Install php7.2-cli #Move to a global install\\n      become: true\\n      apt: name=php7.2-cli update_cache=yes state=latest\\n      tags: [php4]\"}],[\"bookmark\",{\"url\":\"https://reddit.com/r/ansible\",\"metadata\":{\"url\":\"https://www.reddit.com/r/ansible/\",\"title\":\"r/ansible\",\"description\":\"r/ansible: Automation for the People! A Subreddit dedicated to fostering communication in the Ansible Community, includes Ansible, AWX, Ansible …\",\"author\":null,\"publisher\":\"reddit\",\"thumbnail\":\"https://b.thumbs.redditmedia.com/WmbHlRNHXOci-aUzBgHmKPMHRNvI2OtKF2XguHteO5A.png\",\"icon\":\"https://www.redditstatic.com/desktop2x/img/favicon/android-icon-192x192.png\"}}],[\"hr\",{}],[\"hr\",{}],[\"hr\",{}]],\"markups\":[[\"a\",[\"href\",\"https://ovh.com\"]],[\"strong\"],[\"code\"],[\"a\",[\"href\",\"https://www.upwork.com/freelancers/~01c61ee9802b94133e\"]],[\"a\",[\"href\",\"mailto:work@breadnet.co.uk\"]]],\"sections\":[[1,\"p\",[]],[1,\"p\",[[0,[],0,\"We will start this story with the facts. I am getting older, and I want to move out. As I get older, I am forced to accept more responsibilities, like paying for my own electricity. \"]]],[1,\"p\",[[0,[],0,\"As any selfhoster will tell you, this is what bleeds their wallets dry. \"]]],[1,\"p\",[[0,[],0,\"Moving to the cloud is not a decision I took lightly, there was alot of consideration put in to this. I was considering purchasing a newer server, probably a Dell r610 or something of that range and maxxing out the specs, but that pulls a fair bit of electricity, and where I plan to live, would also be in the same room as where I sleep. Fun\"]]],[1,\"p\",[[0,[],0,\"What cloud provider am I going with?\"]]],[1,\"p\",[[0,[],0,\"Well, glad you asked. I have decided to go with a french company called \"],[0,[0],1,\"OVH\"],[0,[],0,\" as their prices are pretty hard to beat (at-least from what I have seen) \"]]],[1,\"blockquote\",[[0,[],0,\"This article was written before they burnt down their Datacentre, so I am sort of re-evaluating my decisions... \"]]],[1,\"p\",[[0,[],0,\"One node costs £3.60 pcm (vat inc) compared to £6 from digital ocean for a similarly specced machine. Not bad eh?\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"Let's do a cost break down per cloud provider, using their minimum viable product as well as correctly sizing the nodes\"]]],[10,0],[1,\"p\",[[0,[],0,\"I was also considering doing something on GCP with pre-emptible vm's (They get shutdown at any time, moved around, basically abused, for a fraction of the cost)\"]]],[1,\"p\",[[0,[],0,\"Below is my ideal setup for moving my stuff off my host at home, to the cloud\"]]],[10,1],[1,\"p\",[[0,[],0,\"Most of the things I run are native web applications, hence why I can slap them on a really basic vm. But, applications like Zabbix and Jellyfin are full blown applications that need COMPUTE! hence why they are on a slightly more powerful host. \"]]],[1,\"p\",[[0,[],0,\"Now, the issue I face here is that, I need to in effect, install 13 web apps. I don't really fancy having to do them one by one, so I plan to automate the bajeebus out of it (well, as much as possible)\"]]],[1,\"p\",[[0,[],0,\"A nice thing about using something like Ansible is you can see \"],[0,[1],1,\"exactly\"],[0,[],0,\" what was done, so if something breaks, I have a rough idea of what the hell is up.\"]]],[1,\"p\",[[0,[],0,\"Another nice thing about ansible, is I can create a teardown playbook that dumps databases, copies config files etc to my local host, then allowing me to move cloud providers painlessly. \"]]],[1,\"p\",[[0,[],0,\"You can see my setup below. \"],[1,[],0,0],[0,[],0,\"It's most likely still work in progress, but you can copy parts if you so desire. \"]]],[1,\"p\",[[0,[],0,\"Now I am by no means an ansible expert, or a cicd engineer or what have you, but I can google and I'm pretty decent at it.\"]]],[10,2],[1,\"p\",[[0,[],0,\"This ansible playbook works methodically. Below are the steps I am striving towards:\"]]],[3,\"ol\",[[[0,[],0,\"Install nginx on \"],[0,[2],1,\"rev1\"],[0,[],0,\"\\t\"]],[[0,[],0,\"Install Nginx and mysql on \"],[0,[2],1,\"app1\"],[0,[],0,\" along with Analytics, Website, kanboard, status page and month server (used for reddit posts) \"]],[[0,[],0,\"Install Nginx and mysql on \"],[0,[2],1,\"app2\"],[0,[],0,\" along with Bookstack, Firefly, Passbolt, Nextcloud and may need to put docker in a container, this I will need to test\"]],[[0,[],0,\"Install Zabbix, Jellyfin and Minecraft on \"],[0,[2],1,\"app3\"]]]],[1,\"p\",[[0,[],0,\"So far I have faced a few issues, mainly down to my own stupidity, and some down to a simple lack of knowledge of ansible. \"]]],[1,\"p\",[[0,[],0,\"The first being how I was calling what to install on each server, I had some funky fike structure like you see below.\"]]],[1,\"p\",[[0,[],0,\"The issue was you would call the file \"],[0,[2],1,\"setup.yml\"],[0,[],0,\" which would call on the files in the folder called \"],[0,[2],1,\"setup\"],[0,[],0,\" which would then run their respective jobs. \"]]],[10,3],[1,\"p\",[[0,[],0,\"Now those of you who do ansible day in day our are probably wanting to swat my house, and I'm sorry - You wont like this next part.\"]]],[1,\"p\",[[0,[],0,\"My solution on what tasks were to be run per server was so bad, that it actually made me consider giving up.\"]]],[10,4],[1,\"p\",[[0,[],0,\"This was my attempt at getting it nginx to run on server \"],[0,[2],1,\"app1\"],[0,[],0,\" - Not great.\"]]],[1,\"p\",[[0,[],0,\"I asked the kind people of reddit and someone pointed me in the right direction, now everything actually works for a start. So if anyone finds my page from googling 'How to run different playbooks on different servers' (not even close to what I actually needed) then check out how I have done it on github. You're welcome\"]]],[1,\"p\",[[0,[],0,\"My second issue I faced was that I use Nginx as I just found it so much simpler to understand. And yes, apache2 and I had a good childhood growing up, but times change\"]]],[10,5],[1,\"p\",[[0,[],0,\"And for some reason Apache2 was being installed when ever I ran the install php7.2 task.\"]]],[1,\"p\",[[0,[],0,\"Let's take a look at the task for a second (in breif)\"]]],[10,6],[1,\"p\",[[0,[],0,\"Now those of you who have been dealing with linux and PHP longer than I've been alive will know exactly what the hell is going on here, something I did not.\"]]],[1,\"p\",[[0,[],0,\"So I went to the only place that keeps us Sysadmins employed, google.\"]]],[1,\"blockquote\",[[0,[],0,\"Does Apache2 auto install when installing PHP 7.2?\"]]],[1,\"p\",[[0,[],0,\"Yes - it does. See, I was installing \"],[0,[2],1,\"php7.2\"],[0,[],0,\" which has a package dependency of... you guessed it \"],[0,[2],1,\"libapache2-mod-php7.2\"],[0,[],0,\" which as far as I care to know, is apache2. \"]]],[1,\"p\",[[0,[],0,\"The way I figured this out was really painful, but I ran the playbook with \"],[0,[2],1,\"-t php1\"],[0,[],0,\" then\"],[1,[],0,1],[0,[],0,\" \"],[0,[2],1,\"-t php2\"],[0,[],0,\" and running \"],[0,[2],1,\"systemctl status apache2\"],[0,[],0,\" till I saw it kick back a response. \"]]],[1,\"p\",[[0,[],0,\"My tip for anyone else dealing with weird issues like this? Just segment everything down, read the logs, take it a step at a time. This is the nice thing with ansible is you can see exactly what it did, and there seems to be quite a nice community over at reddit. You dont want to lean on them to spoon feed you, but use them as your last resort! (eg: after page 2 of google)\"]]],[10,7],[10,8],[1,\"p\",[[0,[],0,\"Now we have the fluff of what I'm doing out of the way, we can finally start running things! \"]]],[10,9],[1,\"p\",[[0,[],0,\"Stay tuned for a part 2! \"]]],[1,\"p\",[[0,[],0,\"(Note from the Author: I will probably land up changing a lot of things)\"]]],[10,10],[1,\"p\",[[0,[],0,\"You can hire me via \"],[0,[3],1,\"Upwork\"],[0,[],0,\" or \"],[0,[4],1,\"emailing me\"],[0,[],0,\" for weekend projects!\"]]]],\"ghostVersion\":\"3.0\"}","lexical":null,"html":"<p></p><p>We will start this story with the facts. I am getting older, and I want to move out. As I get older, I am forced to accept more responsibilities, like paying for my own electricity. </p><p>As any selfhoster will tell you, this is what bleeds their wallets dry. </p><p>Moving to the cloud is not a decision I took lightly, there was alot of consideration put in to this. I was considering purchasing a newer server, probably a Dell r610 or something of that range and maxxing out the specs, but that pulls a fair bit of electricity, and where I plan to live, would also be in the same room as where I sleep. Fun</p><p>What cloud provider am I going with?</p><p>Well, glad you asked. I have decided to go with a french company called <a href=\"https://ovh.com\">OVH</a> as their prices are pretty hard to beat (at-least from what I have seen) </p><blockquote>This article was written before they burnt down their Datacentre, so I am sort of re-evaluating my decisions... </blockquote><p>One node costs £3.60 pcm (vat inc) compared to £6 from digital ocean for a similarly specced machine. Not bad eh?</p><p></p><p>Let's do a cost break down per cloud provider, using their minimum viable product as well as correctly sizing the nodes</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/07/image-1.png\" class=\"kg-image\" alt=\"Cost breakdown for Digital ocean vs OVH\" loading=\"lazy\"></figure><p>I was also considering doing something on GCP with pre-emptible vm's (They get shutdown at any time, moved around, basically abused, for a fraction of the cost)</p><p>Below is my ideal setup for moving my stuff off my host at home, to the cloud</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/07/Untitled-drawing.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>Most of the things I run are native web applications, hence why I can slap them on a really basic vm. But, applications like Zabbix and Jellyfin are full blown applications that need COMPUTE! hence why they are on a slightly more powerful host. </p><p>Now, the issue I face here is that, I need to in effect, install 13 web apps. I don't really fancy having to do them one by one, so I plan to automate the bajeebus out of it (well, as much as possible)</p><p>A nice thing about using something like Ansible is you can see <strong>exactly</strong> what was done, so if something breaks, I have a rough idea of what the hell is up.</p><p>Another nice thing about ansible, is I can create a teardown playbook that dumps databases, copies config files etc to my local host, then allowing me to move cloud providers painlessly. </p><p>You can see my setup below. <br>It's most likely still work in progress, but you can copy parts if you so desire. </p><p>Now I am by no means an ansible expert, or a cicd engineer or what have you, but I can google and I'm pretty decent at it.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://github.com/userbradley/breadnet-infra\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">userbradley/breadnet-infra</div><div class=\"kg-bookmark-description\">Breadnet Infra setup though ansible. Contribute to userbradley/breadnet-infra development by creating an account on GitHub.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://github.githubassets.com/favicons/favicon.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">userbradley</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://avatars2.githubusercontent.com/u/41597815?s&#x3D;400&amp;v&#x3D;4\" alt=\"\"></div></a></figure><p>This ansible playbook works methodically. Below are the steps I am striving towards:</p><ol><li>Install nginx on <code>rev1</code> </li><li>Install Nginx and mysql on <code>app1</code> along with Analytics, Website, kanboard, status page and month server (used for reddit posts) </li><li>Install Nginx and mysql on <code>app2</code> along with Bookstack, Firefly, Passbolt, Nextcloud and may need to put docker in a container, this I will need to test</li><li>Install Zabbix, Jellyfin and Minecraft on <code>app3</code></li></ol><p>So far I have faced a few issues, mainly down to my own stupidity, and some down to a simple lack of knowledge of ansible. </p><p>The first being how I was calling what to install on each server, I had some funky fike structure like you see below.</p><p>The issue was you would call the file <code>setup.yml</code> which would call on the files in the folder called <code>setup</code> which would then run their respective jobs. </p><pre><code>.\n├── hosts\n├── README.md\n├── setup\n│   ├── defaults\n│   │   └── main.yml\n│   ├── group_vars\n│   │   ├── all.yml\n│   │   ├── app1.yml\n│   │   └── app2.yml\n│   └── tasks\n│       ├── bookstack.yml\n│       ├── files\n│       │   ├── bookstack_env.j2\n│       │   ├── bookstack_nginx.j2\n│       │   ├── kanboard_config.j2\n│       │   ├── kan_nginx.j2\n│       │   ├── matomo_nginx.j2\n│       │   ├── status.env.j2\n│       │   ├── status.j2\n│       │   └── zabbix_apache2.j2\n│       ├── ghost.yml\n│       ├── - include: nginx.yml\n│       ├── jellyfin.yml\n│       ├── kanboard.yml\n│       ├── main.yml\n│       ├── matomo.yml\n│       ├── mysql.yml\n│       ├── nginx.yml\n│       ├── php7.2.yml\n│       ├── reboot.yml\n│       ├── scripts\n│       │   └── inscomp.sh\n│       ├── status.yml\n│       └── zabbix.yml\n├── setup.yml\n├── tasks\n│   ├── files\n│   │   ├── bookstack_env.j2\n│   │   ├── bookstack_nginx.j2\n│   │   ├── kanboard_config.j2\n│   │   ├── kan_nginx.j2\n│   │   ├── matomo_nginx.j2\n│   │   ├── status.env.j2\n│   │   ├── status.j2\n│   │   └── zabbix_apache2.j2\n│   ├── ghost.yml\n│   ├── main.yml\n│   └── scripts\n│       └── inscomp.sh\n└── test\n    ├── defaults\n    │   └── main.yml\n    ├── hosts\n    ├── roles\n    │   ├── bookstack\n    │   │   ├── defaults\n    │   │   │   └── main.yml\n    │   │   └── tasks\n    │   │       └── main.yml\n    │   ├── common\n    │   │   └── tasks\n    │   │       ├── mysql.yml\n    │   │       └── php7.2.yml\n    │   ├── defaults\n    │   │   └── main.yml\n    │   ├── jellyfin\n    │   │   └── tasks\n    │   │       └── main.yml\n    │   ├── kanboard\n    │   │   ├── defaults\n    │   │   │   └── main.yml\n    │   │   └── tasks\n    │   │       └── main.yml\n    │   ├── matomo\n    │   │   ├── defaults\n    │   │   │   └── main.yml\n    │   │   └── tasks\n    │   │       └── main.yml\n    │   ├── mysql\n    │   │   └── tasks\n    │   │       └── main.yml\n    │   ├── nginx\n    │   │   ├── defaults\n    │   │   │   └── main.yml\n    │   │   └── tasks\n    │   │       └── main.yml\n    │   ├── reboot\n    │   │   └── tasks\n    │   │       └── main.yml\n    │   ├── status\n    │   │   ├── defaults\n    │   │   │   └── main.yml\n    │   │   └── tasks\n    │   │       └── main.yml\n    │   └── zabbix\n    │       └── tasks\n    │           └── main.yml\n    └── setup.yml</code></pre><p>Now those of you who do ansible day in day our are probably wanting to swat my house, and I'm sorry - You wont like this next part.</p><p>My solution on what tasks were to be run per server was so bad, that it actually made me consider giving up.</p><pre><code>---\n- include: reboot.yml\n  become: yes\n  when:\n     - reboot|bool\n       \n- include: nginx.yml\n  become: yes\n  delegate_to: app2\n  when: \n     - inventory_hostname in groups['app1']\n     #- inventory_hostname in groups['app2']\n     - inventory_hostname == app[1]\n     - inventory_hostname in groups['app1']|default([])\n     - nginx|bool\n  tags:\n      - nginx\n</code></pre><p>This was my attempt at getting it nginx to run on server <code>app1</code> - Not great.</p><p>I asked the kind people of reddit and someone pointed me in the right direction, now everything actually works for a start. So if anyone finds my page from googling 'How to run different playbooks on different servers' (not even close to what I actually needed) then check out how I have done it on github. You're welcome</p><p>My second issue I faced was that I use Nginx as I just found it so much simpler to understand. And yes, apache2 and I had a good childhood growing up, but times change</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2020/07/image-2.png\" class=\"kg-image\" alt loading=\"lazy\"><figcaption>A meme for all you cool kids</figcaption></figure><p>And for some reason Apache2 was being installed when ever I ran the install php7.2 task.</p><p>Let's take a look at the task for a second (in breif)</p><pre><code>---\n    - name: Add PHP repository \n      become: true\n      apt_repository:\n       repo: 'ppa:ondrej/php'\n      tags: [php1]\n\n    - name: Update repositories\n      become: true\n      apt:\n       update_cache: yes\n      tags: [php2]\n\n    - name: Install php7.2 #Move to a global install\n      become: true\n      apt: name=php7.2 update_cache=yes state=latest\n      tags: [php3]\n\n    - name: Install php7.2-cli #Move to a global install\n      become: true\n      apt: name=php7.2-cli update_cache=yes state=latest\n      tags: [php4]</code></pre><p>Now those of you who have been dealing with linux and PHP longer than I've been alive will know exactly what the hell is going on here, something I did not.</p><p>So I went to the only place that keeps us Sysadmins employed, google.</p><blockquote>Does Apache2 auto install when installing PHP 7.2?</blockquote><p>Yes - it does. See, I was installing <code>php7.2</code> which has a package dependency of... you guessed it <code>libapache2-mod-php7.2</code> which as far as I care to know, is apache2. </p><p>The way I figured this out was really painful, but I ran the playbook with <code>-t php1</code> then<br> <code>-t php2</code> and running <code>systemctl status apache2</code> till I saw it kick back a response. </p><p>My tip for anyone else dealing with weird issues like this? Just segment everything down, read the logs, take it a step at a time. This is the nice thing with ansible is you can see exactly what it did, and there seems to be quite a nice community over at reddit. You dont want to lean on them to spoon feed you, but use them as your last resort! (eg: after page 2 of google)</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://reddit.com/r/ansible\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">r/ansible</div><div class=\"kg-bookmark-description\">r/ansible: Automation for the People! A Subreddit dedicated to fostering communication in the Ansible Community, includes Ansible, AWX, Ansible …</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://www.redditstatic.com/desktop2x/img/favicon/android-icon-192x192.png\" alt=\"\"><span class=\"kg-bookmark-author\">reddit</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://b.thumbs.redditmedia.com/WmbHlRNHXOci-aUzBgHmKPMHRNvI2OtKF2XguHteO5A.png\" alt=\"\"></div></a></figure><hr><p>Now we have the fluff of what I'm doing out of the way, we can finally start running things! </p><hr><p>Stay tuned for a part 2! </p><p>(Note from the Author: I will probably land up changing a lot of things)</p><hr><p>You can hire me via <a href=\"https://www.upwork.com/freelancers/~01c61ee9802b94133e\">Upwork</a> or <a href=\"mailto:work@breadnet.co.uk\">emailing me</a> for weekend projects!</p>","comment_id":"5f0f43c11c811d05f5b994c2","plaintext":"We will start this story with the facts. I am getting older, and I want to move out. As I get older, I am forced to accept more responsibilities, like paying for my own electricity.\n\nAs any selfhoster will tell you, this is what bleeds their wallets dry.\n\nMoving to the cloud is not a decision I took lightly, there was alot of consideration put in to this. I was considering purchasing a newer server, probably a Dell r610 or something of that range and maxxing out the specs, but that pulls a fair bit of electricity, and where I plan to live, would also be in the same room as where I sleep. Fun\n\nWhat cloud provider am I going with?\n\nWell, glad you asked. I have decided to go with a french company called OVH as their prices are pretty hard to beat (at-least from what I have seen)\n\nThis article was written before they burnt down their Datacentre, so I am sort of re-evaluating my decisions...\n\nOne node costs £3.60 pcm (vat inc) compared to £6 from digital ocean for a similarly specced machine. Not bad eh?\n\n\n\nLet's do a cost break down per cloud provider, using their minimum viable product as well as correctly sizing the nodes\n\nI was also considering doing something on GCP with pre-emptible vm's (They get shutdown at any time, moved around, basically abused, for a fraction of the cost)\n\nBelow is my ideal setup for moving my stuff off my host at home, to the cloud\n\nMost of the things I run are native web applications, hence why I can slap them on a really basic vm. But, applications like Zabbix and Jellyfin are full blown applications that need COMPUTE! hence why they are on a slightly more powerful host.\n\nNow, the issue I face here is that, I need to in effect, install 13 web apps. I don't really fancy having to do them one by one, so I plan to automate the bajeebus out of it (well, as much as possible)\n\nA nice thing about using something like Ansible is you can see exactly what was done, so if something breaks, I have a rough idea of what the hell is up.\n\nAnother nice thing about ansible, is I can create a teardown playbook that dumps databases, copies config files etc to my local host, then allowing me to move cloud providers painlessly.\n\nYou can see my setup below.\nIt's most likely still work in progress, but you can copy parts if you so desire.\n\nNow I am by no means an ansible expert, or a cicd engineer or what have you, but I can google and I'm pretty decent at it.\n\nuserbradley/breadnet-infraBreadnet Infra setup though ansible. Contribute to userbradley/breadnet-infra development by creating an account on GitHub.GitHubuserbradley\n\nThis ansible playbook works methodically. Below are the steps I am striving towards:\n\n 1. Install nginx on rev1 \n 2. Install Nginx and mysql on app1 along with Analytics, Website, kanboard, status page and month server (used for reddit posts)\n 3. Install Nginx and mysql on app2 along with Bookstack, Firefly, Passbolt, Nextcloud and may need to put docker in a container, this I will need to test\n 4. Install Zabbix, Jellyfin and Minecraft on app3\n\nSo far I have faced a few issues, mainly down to my own stupidity, and some down to a simple lack of knowledge of ansible.\n\nThe first being how I was calling what to install on each server, I had some funky fike structure like you see below.\n\nThe issue was you would call the file setup.yml which would call on the files in the folder called setup which would then run their respective jobs.\n\n.\n├── hosts\n├── README.md\n├── setup\n│   ├── defaults\n│   │   └── main.yml\n│   ├── group_vars\n│   │   ├── all.yml\n│   │   ├── app1.yml\n│   │   └── app2.yml\n│   └── tasks\n│       ├── bookstack.yml\n│       ├── files\n│       │   ├── bookstack_env.j2\n│       │   ├── bookstack_nginx.j2\n│       │   ├── kanboard_config.j2\n│       │   ├── kan_nginx.j2\n│       │   ├── matomo_nginx.j2\n│       │   ├── status.env.j2\n│       │   ├── status.j2\n│       │   └── zabbix_apache2.j2\n│       ├── ghost.yml\n│       ├── - include: nginx.yml\n│       ├── jellyfin.yml\n│       ├── kanboard.yml\n│       ├── main.yml\n│       ├── matomo.yml\n│       ├── mysql.yml\n│       ├── nginx.yml\n│       ├── php7.2.yml\n│       ├── reboot.yml\n│       ├── scripts\n│       │   └── inscomp.sh\n│       ├── status.yml\n│       └── zabbix.yml\n├── setup.yml\n├── tasks\n│   ├── files\n│   │   ├── bookstack_env.j2\n│   │   ├── bookstack_nginx.j2\n│   │   ├── kanboard_config.j2\n│   │   ├── kan_nginx.j2\n│   │   ├── matomo_nginx.j2\n│   │   ├── status.env.j2\n│   │   ├── status.j2\n│   │   └── zabbix_apache2.j2\n│   ├── ghost.yml\n│   ├── main.yml\n│   └── scripts\n│       └── inscomp.sh\n└── test\n    ├── defaults\n    │   └── main.yml\n    ├── hosts\n    ├── roles\n    │   ├── bookstack\n    │   │   ├── defaults\n    │   │   │   └── main.yml\n    │   │   └── tasks\n    │   │       └── main.yml\n    │   ├── common\n    │   │   └── tasks\n    │   │       ├── mysql.yml\n    │   │       └── php7.2.yml\n    │   ├── defaults\n    │   │   └── main.yml\n    │   ├── jellyfin\n    │   │   └── tasks\n    │   │       └── main.yml\n    │   ├── kanboard\n    │   │   ├── defaults\n    │   │   │   └── main.yml\n    │   │   └── tasks\n    │   │       └── main.yml\n    │   ├── matomo\n    │   │   ├── defaults\n    │   │   │   └── main.yml\n    │   │   └── tasks\n    │   │       └── main.yml\n    │   ├── mysql\n    │   │   └── tasks\n    │   │       └── main.yml\n    │   ├── nginx\n    │   │   ├── defaults\n    │   │   │   └── main.yml\n    │   │   └── tasks\n    │   │       └── main.yml\n    │   ├── reboot\n    │   │   └── tasks\n    │   │       └── main.yml\n    │   ├── status\n    │   │   ├── defaults\n    │   │   │   └── main.yml\n    │   │   └── tasks\n    │   │       └── main.yml\n    │   └── zabbix\n    │       └── tasks\n    │           └── main.yml\n    └── setup.yml\n\nNow those of you who do ansible day in day our are probably wanting to swat my house, and I'm sorry - You wont like this next part.\n\nMy solution on what tasks were to be run per server was so bad, that it actually made me consider giving up.\n\n---\n- include: reboot.yml\n  become: yes\n  when:\n     - reboot|bool\n       \n- include: nginx.yml\n  become: yes\n  delegate_to: app2\n  when: \n     - inventory_hostname in groups['app1']\n     #- inventory_hostname in groups['app2']\n     - inventory_hostname == app[1]\n     - inventory_hostname in groups['app1']|default([])\n     - nginx|bool\n  tags:\n      - nginx\n\n\nThis was my attempt at getting it nginx to run on server app1 - Not great.\n\nI asked the kind people of reddit and someone pointed me in the right direction, now everything actually works for a start. So if anyone finds my page from googling 'How to run different playbooks on different servers' (not even close to what I actually needed) then check out how I have done it on github. You're welcome\n\nMy second issue I faced was that I use Nginx as I just found it so much simpler to understand. And yes, apache2 and I had a good childhood growing up, but times change\n\nAnd for some reason Apache2 was being installed when ever I ran the install php7.2 task.\n\nLet's take a look at the task for a second (in breif)\n\n---\n    - name: Add PHP repository \n      become: true\n      apt_repository:\n       repo: 'ppa:ondrej/php'\n      tags: [php1]\n\n    - name: Update repositories\n      become: true\n      apt:\n       update_cache: yes\n      tags: [php2]\n\n    - name: Install php7.2 #Move to a global install\n      become: true\n      apt: name=php7.2 update_cache=yes state=latest\n      tags: [php3]\n\n    - name: Install php7.2-cli #Move to a global install\n      become: true\n      apt: name=php7.2-cli update_cache=yes state=latest\n      tags: [php4]\n\nNow those of you who have been dealing with linux and PHP longer than I've been alive will know exactly what the hell is going on here, something I did not.\n\nSo I went to the only place that keeps us Sysadmins employed, google.\n\nDoes Apache2 auto install when installing PHP 7.2?\n\nYes - it does. See, I was installing php7.2 which has a package dependency of... you guessed it libapache2-mod-php7.2 which as far as I care to know, is apache2.\n\nThe way I figured this out was really painful, but I ran the playbook with -t php1 then\n-t php2 and running systemctl status apache2 till I saw it kick back a response.\n\nMy tip for anyone else dealing with weird issues like this? Just segment everything down, read the logs, take it a step at a time. This is the nice thing with ansible is you can see exactly what it did, and there seems to be quite a nice community over at reddit. You dont want to lean on them to spoon feed you, but use them as your last resort! (eg: after page 2 of google)\n\nr/ansibler/ansible: Automation for the People! A Subreddit dedicated to fostering communication in the Ansible Community, includes Ansible, AWX, Ansible …reddit\n\nNow we have the fluff of what I'm doing out of the way, we can finally start running things!\n\nStay tuned for a part 2!\n\n(Note from the Author: I will probably land up changing a lot of things)\n\nYou can hire me via Upwork or emailing me for weekend projects!","feature_image":"https://images.unsplash.com/photo-1560182413-53772f3d7134?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=2000&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2020-07-15T17:58:25.000Z","updated_at":"2021-05-05T14:17:46.000Z","published_at":"2021-04-01T12:52:13.000Z","custom_excerpt":"How did I move all my servers to the cloud? Well - Ansible, automation and CI/CD!","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"6425f3bc192c0c150413d598","uuid":"3fa7efe4-2f72-471d-8644-0663e6f68598","title":"Cloud-init that works","slug":"cloud-init-that-works","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/08/image.png\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/08/image-2.png\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/08/image-3.png\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/08/image-4.png\",\"caption\":\"\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/08/image-5.png\",\"caption\":\"\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/08/image-6.png\"}],[\"code\",{\"code\":\"sudo -s\\nmount /dev/cdrom /mnt && bash /mnt/Linux/install.sh -n && reboot now\"}],[\"hr\",{}],[\"html\",{\"html\":\"<script async src=\\\"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js\\\"></script>\\n<ins class=\\\"adsbygoogle\\\"\\n     style=\\\"display:block; text-align:center;\\\"\\n     data-ad-layout=\\\"in-article\\\"\\n     data-ad-format=\\\"fluid\\\"\\n     data-ad-client=\\\"ca-pub-3875982740171072\\\"\\n     data-ad-slot=\\\"8884714694\\\"></ins>\\n<script>\\n     (adsbygoogle = window.adsbygoogle || []).push({});\\n</script>\"}],[\"code\",{\"code\":\"sudo -s\\napt-get install cloud-init cloud-initramfs-growroot\\n\"}],[\"code\",{\"code\":\"dpkg-reconfigure cloud-init\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/08/image-7.png\",\"caption\":\"ConfigDrive, OpenNebula, Azure and Openstack (I relase the photo is too small)\"}],[\"code\",{\"code\":\"cd /var/lib/cloud\\nrm -r instance\"}],[\"code\",{\"code\":\"apt-get update && apt-get upgrade -y && sudo shutdown now\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/08/image-8.png\"}],[\"hr\",{}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/08/image-10.png\"}],[\"code\",{\"code\":\"ssh ubuntu@IP \\n\"}],[\"hr\",{}]],\"markups\":[[\"code\"],[\"strong\"],[\"em\"],[\"a\",[\"href\",\"https://www.upwork.com/freelancers/~01c61ee9802b94133e\"]],[\"a\",[\"href\",\"mailto:work@breadnet.co.uk\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"For some reason this single task has stumped me for around 3 years till I had to basically wipe my home lab as I messed up some hard drive install procedure half juggling act. \"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"I will be writing these instructions for Xen Orchestra but almost 99% of this can be transfered to other virtualization servers. \"]]],[1,\"p\",[[0,[],0,\"First we will need to start with creating a VM on your server.\"]]],[1,\"p\",[[0,[],0,\"I am chosing Ubuntu 18.04 as this is the current Ubuntu image I trust. generally I will move on to the latest LTS release after it's been out for a year. \"]]],[1,\"p\",[[0,[],0,\"Create the VM with 1 core, 1gb ram and 10gb drive. Give it a name. \"]]],[10,0],[1,\"p\",[[0,[],0,\"Once it's done, run though the install procedure as you normally would till you get to the disk options. This is where it gets fun. We need to install everything on one partition to allow for disk resizing later on\"]]],[10,1],[1,\"p\",[[0,[],0,\"Select Manual here\"]]],[10,2],[1,\"p\",[[0,[],0,\"Select the disk then \"],[0,[0],1,\"Add Partition\"]]],[10,3],[1,\"p\",[[0,[],0,\"Type the avalible size (so in this case \"],[0,[0],1,\"9.998\"],[0,[],0,\" )then create\"]]],[1,\"p\",[[0,[],0,\"Finish the install as you usually would. \"]]],[10,4],[1,\"p\",[[0,[],0,\"Dont worry too much about SSH keys as this all gets wiped with \"],[0,[0],1,\"cloud-init\"]]],[1,\"p\",[[0,[],0,\"Let the install go on.\"]]],[1,\"p\",[[0,[],0,\"Once the install is completed, reboot and remove the drive if instructed to.\"]]],[1,\"p\",[[0,[],0,\"Login to the server though either xoa's web interface or via ssh. I find SSH is the best way.\"]]],[1,\"p\",[[0,[],0,\"We need to install the xen tools before we do anything. I have a one liner I tend to run that does this\"]]],[10,5],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"First mount the drive in xen orchestra, select \"],[0,[0],1,\"guest-tools.iso\"]]],[1,\"p\",[[0,[],0,\"Now once we're logged in, the one liner\"]]],[10,6],[1,\"p\",[[0,[],0,\"This does 3 things. \"]]],[3,\"ol\",[[[0,[],0,\"Mount the drive at /mnt\"]],[[0,[],0,\"Run the install.sh command siletly with no input from you needed\"]],[[0,[],0,\"reboots the machine\"]]]],[1,\"p\",[[0,[],0,\"Once rebooted, log back in and we can start with the actual cloud-init part\"]]],[10,7],[10,8],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"Now we can install cloud-init and the software needed to grow the disk when we want a larger disk\"]]],[10,9],[1,\"p\",[[0,[],0,\"Growroot package is needed to allow us to expand the disk on the template\"]]],[1,\"p\",[[0,[],0,\"Now that is done, here comes the part I really have 0 clue about, and just crossed my fingers and hoped for the best. I suggest you do this via ssh, and full screen\"]]],[10,10],[1,\"p\",[[0,[],0,\"Deselect them all, then match it to what I have below\"]]],[10,11],[1,\"blockquote\",[[0,[],0,\"I have had a very nice person (Who I cant name due to GDPR, love you bro) reach out to me to let me know the following, which I think is \"],[0,[1],1,\"very \"],[0,[],0,\"importat to know\"]]],[1,\"blockquote\",[[0,[],0,\"After multiple failed attempts, and some debugging I found that the latest(?) Ubuntu installer overrides the effects of the `dpkg-reconfigure` step with setting \"],[0,[2],1,\"datasource_list\"],[0,[],0,\" to \"],[0,[2],1,\"[None]\"],[0,[],0,\" in /etc/cloud/cloud.cfg.d/99-installer.cfg\"]]],[1,\"blockquote\",[[0,[],0,\"Removing the offending line (or the whole file) fixed my problems. This info may help other people who will find your post later.\"]]],[1,\"p\",[[0,[],0,\"Press tab, then \"],[0,[0],1,\"OK\"]]],[1,\"p\",[[0,[],0,\"Now we need to delete a folder that has caused me so much greif over the years\"]]],[10,12],[1,\"p\",[[0,[],0,\"Run an update and upgrade for good measure and then shutdown\"]]],[10,13],[1,\"p\",[[0,[],0,\"Once the VM is shut down you should see under \"],[0,[0],1,\"Advanced\"],[0,[],0,\" that you can \"],[0,[0],1,\"Convert to Template\"],[0,[],0,\" \"]]],[10,14],[1,\"p\",[[0,[],0,\"Click it. \"]]],[1,\"p\",[[0,[],0,\"Now when we go to create a new VM we type \"],[0,[0],1,\"Ubuntu cloud image\"],[0,[],0,\" for the template name.\"]]],[1,\"p\",[[0,[],0,\"I highly suggest you add your main computer's Public SSH key to XOA.\"]]],[10,15],[10,16],[1,\"p\",[[0,[],0,\"Now when you create the VM, rename the name and the disk. Add your SSH key and click create.\"]]],[1,\"p\",[[0,[],0,\"It shold take around about 30 seconds for the VM to be created and ready to use! \"]]],[1,\"p\",[[0,[],0,\"You can ssh using the below as it has your SSH keys added\"]]],[10,17],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"if you have questions drop me an email at webmaster at breadnet dot co dot uk\"]]],[10,18],[1,\"p\",[[0,[],0,\"You can hire me via \"],[0,[3],1,\"Upwork\"],[0,[],0,\" or \"],[0,[4],1,\"emailing me\"],[0,[],0,\" for weekend projects!\"]]]],\"ghostVersion\":\"3.0\"}","lexical":null,"html":"<p>For some reason this single task has stumped me for around 3 years till I had to basically wipe my home lab as I messed up some hard drive install procedure half juggling act. </p><p></p><p>I will be writing these instructions for Xen Orchestra but almost 99% of this can be transfered to other virtualization servers. </p><p>First we will need to start with creating a VM on your server.</p><p>I am chosing Ubuntu 18.04 as this is the current Ubuntu image I trust. generally I will move on to the latest LTS release after it's been out for a year. </p><p>Create the VM with 1 core, 1gb ram and 10gb drive. Give it a name. </p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/08/image.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>Once it's done, run though the install procedure as you normally would till you get to the disk options. This is where it gets fun. We need to install everything on one partition to allow for disk resizing later on</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/08/image-2.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>Select Manual here</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/08/image-3.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>Select the disk then <code>Add Partition</code></p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/08/image-4.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>Type the avalible size (so in this case <code>9.998</code> )then create</p><p>Finish the install as you usually would. </p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/08/image-5.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>Dont worry too much about SSH keys as this all gets wiped with <code>cloud-init</code></p><p>Let the install go on.</p><p>Once the install is completed, reboot and remove the drive if instructed to.</p><p>Login to the server though either xoa's web interface or via ssh. I find SSH is the best way.</p><p>We need to install the xen tools before we do anything. I have a one liner I tend to run that does this</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/08/image-6.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p></p><p>First mount the drive in xen orchestra, select <code>guest-tools.iso</code></p><p>Now once we're logged in, the one liner</p><pre><code>sudo -s\nmount /dev/cdrom /mnt &amp;&amp; bash /mnt/Linux/install.sh -n &amp;&amp; reboot now</code></pre><p>This does 3 things. </p><ol><li>Mount the drive at /mnt</li><li>Run the install.sh command siletly with no input from you needed</li><li>reboots the machine</li></ol><p>Once rebooted, log back in and we can start with the actual cloud-init part</p><hr><!--kg-card-begin: html--><script async src=\"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js\"></script>\n<ins class=\"adsbygoogle\"\n     style=\"display:block; text-align:center;\"\n     data-ad-layout=\"in-article\"\n     data-ad-format=\"fluid\"\n     data-ad-client=\"ca-pub-3875982740171072\"\n     data-ad-slot=\"8884714694\"></ins>\n<script>\n     (adsbygoogle = window.adsbygoogle || []).push({});\n</script><!--kg-card-end: html--><p></p><p>Now we can install cloud-init and the software needed to grow the disk when we want a larger disk</p><pre><code>sudo -s\napt-get install cloud-init cloud-initramfs-growroot\n</code></pre><p>Growroot package is needed to allow us to expand the disk on the template</p><p>Now that is done, here comes the part I really have 0 clue about, and just crossed my fingers and hoped for the best. I suggest you do this via ssh, and full screen</p><pre><code>dpkg-reconfigure cloud-init</code></pre><p>Deselect them all, then match it to what I have below</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2020/08/image-7.png\" class=\"kg-image\" alt loading=\"lazy\"><figcaption>ConfigDrive, OpenNebula, Azure and Openstack (I relase the photo is too small)</figcaption></figure><blockquote>I have had a very nice person (Who I cant name due to GDPR, love you bro) reach out to me to let me know the following, which I think is <strong>very </strong>importat to know</blockquote><blockquote>After multiple failed attempts, and some debugging I found that the latest(?) Ubuntu installer overrides the effects of the `dpkg-reconfigure` step with setting <em>datasource_list</em> to <em>[None]</em> in /etc/cloud/cloud.cfg.d/99-installer.cfg</blockquote><blockquote>Removing the offending line (or the whole file) fixed my problems. This info may help other people who will find your post later.</blockquote><p>Press tab, then <code>OK</code></p><p>Now we need to delete a folder that has caused me so much greif over the years</p><pre><code>cd /var/lib/cloud\nrm -r instance</code></pre><p>Run an update and upgrade for good measure and then shutdown</p><pre><code>apt-get update &amp;&amp; apt-get upgrade -y &amp;&amp; sudo shutdown now</code></pre><p>Once the VM is shut down you should see under <code>Advanced</code> that you can <code>Convert to Template</code> </p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/08/image-8.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>Click it. </p><p>Now when we go to create a new VM we type <code>Ubuntu cloud image</code> for the template name.</p><p>I highly suggest you add your main computer's Public SSH key to XOA.</p><hr><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/08/image-10.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>Now when you create the VM, rename the name and the disk. Add your SSH key and click create.</p><p>It shold take around about 30 seconds for the VM to be created and ready to use! </p><p>You can ssh using the below as it has your SSH keys added</p><pre><code>ssh ubuntu@IP \n</code></pre><p></p><p>if you have questions drop me an email at webmaster at breadnet dot co dot uk</p><hr><p>You can hire me via <a href=\"https://www.upwork.com/freelancers/~01c61ee9802b94133e\">Upwork</a> or <a href=\"mailto:work@breadnet.co.uk\">emailing me</a> for weekend projects!</p>","comment_id":"5f29a8cb1c811d05f5b99597","plaintext":"For some reason this single task has stumped me for around 3 years till I had to basically wipe my home lab as I messed up some hard drive install procedure half juggling act.\n\n\n\nI will be writing these instructions for Xen Orchestra but almost 99% of this can be transfered to other virtualization servers.\n\nFirst we will need to start with creating a VM on your server.\n\nI am chosing Ubuntu 18.04 as this is the current Ubuntu image I trust. generally I will move on to the latest LTS release after it's been out for a year.\n\nCreate the VM with 1 core, 1gb ram and 10gb drive. Give it a name.\n\nOnce it's done, run though the install procedure as you normally would till you get to the disk options. This is where it gets fun. We need to install everything on one partition to allow for disk resizing later on\n\nSelect Manual here\n\nSelect the disk then Add Partition\n\nType the avalible size (so in this case 9.998 )then create\n\nFinish the install as you usually would.\n\nDont worry too much about SSH keys as this all gets wiped with cloud-init\n\nLet the install go on.\n\nOnce the install is completed, reboot and remove the drive if instructed to.\n\nLogin to the server though either xoa's web interface or via ssh. I find SSH is the best way.\n\nWe need to install the xen tools before we do anything. I have a one liner I tend to run that does this\n\n\n\nFirst mount the drive in xen orchestra, select guest-tools.iso\n\nNow once we're logged in, the one liner\n\nsudo -s\nmount /dev/cdrom /mnt && bash /mnt/Linux/install.sh -n && reboot now\n\nThis does 3 things.\n\n 1. Mount the drive at /mnt\n 2. Run the install.sh command siletly with no input from you needed\n 3. reboots the machine\n\nOnce rebooted, log back in and we can start with the actual cloud-init part\n\n\n\n\n\nNow we can install cloud-init and the software needed to grow the disk when we want a larger disk\n\nsudo -s\napt-get install cloud-init cloud-initramfs-growroot\n\n\nGrowroot package is needed to allow us to expand the disk on the template\n\nNow that is done, here comes the part I really have 0 clue about, and just crossed my fingers and hoped for the best. I suggest you do this via ssh, and full screen\n\ndpkg-reconfigure cloud-init\n\nDeselect them all, then match it to what I have below\n\nI have had a very nice person (Who I cant name due to GDPR, love you bro) reach out to me to let me know the following, which I think is very importat to know\n\nAfter multiple failed attempts, and some debugging I found that the latest(?) Ubuntu installer overrides the effects of the `dpkg-reconfigure` step with setting datasource_list to [None] in /etc/cloud/cloud.cfg.d/99-installer.cfg\n\nRemoving the offending line (or the whole file) fixed my problems. This info may help other people who will find your post later.\n\nPress tab, then OK\n\nNow we need to delete a folder that has caused me so much greif over the years\n\ncd /var/lib/cloud\nrm -r instance\n\nRun an update and upgrade for good measure and then shutdown\n\napt-get update && apt-get upgrade -y && sudo shutdown now\n\nOnce the VM is shut down you should see under Advanced that you can Convert to Template\n\nClick it.\n\nNow when we go to create a new VM we type Ubuntu cloud image for the template name.\n\nI highly suggest you add your main computer's Public SSH key to XOA.\n\nNow when you create the VM, rename the name and the disk. Add your SSH key and click create.\n\nIt shold take around about 30 seconds for the VM to be created and ready to use!\n\nYou can ssh using the below as it has your SSH keys added\n\nssh ubuntu@IP \n\n\n\n\nif you have questions drop me an email at webmaster at breadnet dot co dot uk\n\nYou can hire me via Upwork or emailing me for weekend projects!","feature_image":"https://images.unsplash.com/photo-1451187580459-43490279c0fa?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=2000&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2020-08-04T18:28:27.000Z","updated_at":"2021-05-02T01:46:54.000Z","published_at":"2020-08-04T19:29:58.000Z","custom_excerpt":"Want to speed up the deployment of Linux servers on your Xen based server? Well I finally figured it out!","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"6425f3bc192c0c150413d599","uuid":"4d73a1fe-a675-48e2-9a5a-ae4713cff6e0","title":"SSH Client setup using keys","slug":"ssh-client-setup-using-keys","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<h4>The following steps will setup your ssh keypairs on your local machine, copy the public key to your server, and configure your ssh client to use a specific private key with a server alias.</h4>\\n\\n<hr>\\n\\n<p>First, generate your keypair, I generally name the keys with my username-service, username-hostname, or username-device, sometimes even a combination of the three.</p><br/>\\n\\n<p>To generate keys for a new server at <i>securedomain.com</i></p>\\n<br/>\\n\\n<code>ssh-keygen -t rsa -b 4096 -C 'email@securedomain.com'</code>\\n<br/>\\n\\n<h4><u>Things to keep in mind:</u></h4>\\n<ul>\\n    <li>The email does not have to be the same domain as the service/server you are connecting to</li>\\n    <li>The <code>-t rsa -b 4096</code> options are safe and will work on most servers, you can also use <code>-t ed25519</code> if you wish.</li>\\n</ul>\\n<br/>\\n\\n<p>Once you're in the ssh-keygen prompt it will ask you to provide a name for your keys</p>\\n\\n<p><b><u>NOTE:</u></b> You can also do this in the command itself using the <code>-f &lt;filename&gt;</code></p>\\n<br/>\\n\\n\\n<code>Generating public/private rsa key pair.\\nEnter file in which to save the key (/home/user/.ssh/id_rsa):<span style='color: red;'>&lt;new_key_name&gt;</span></code>\\n\\n<p>The <samp>ssh-keygen</samp> application will then, prompt you for a passphrase to secure your key. <i>(You don't have to, but it is recommended)</i>. If you don't wish to use a passphrase just press <kbd>enter</kbd> twice.</p>\\n\\n<hr>\\n\\n<p>Now, we need to copy our keys to the server, we do so by entering the following</p>\\n<br/>\\n\\n<code>ssh-copy-id -i .ssh/yourkeyfile username@hostname</code>\\n\\n<p>This will effectively copy your keyfile over to the new server in a secure fashion. More reading on this <a href=\\\"https://www.ssh.com/ssh/copy-id\\\">here</a>.</p>\\n\\n\\n<p>Next, you will want to add the key to your <samp>~/.ssh/config</samp> to be used automatically with the specified host it was created for; this saves the leg work of having to remember which key goes with which host, and also from having to type <code>-i /path/to/key</code> options with your ssh command.</p>\\n<br/>\\n\\n<p>To do this, first we need to edit our config, so open up <samp>~/.ssh/config</samp> in your preferred editor and enter the following:</p>\"}],[\"markdown\",{\"markdown\":\"```\\n# EXAMPLE\\n# This will setup the use of example as an alias for the FQDN of the server you want to connect to\\n\\nHost example\\n    Hostname example.securedomain.com\\n    AddKeysToAgent yes\\n    UseKeychain yes\\n    IdentityFile ~/.ssh/securedomain_username\\n```\"}],[\"html\",{\"html\":\"<p>In this config the following are true:</p>\\n\\n<ul>\\n     <li><code>Host</code> is an alias to the server we wish to connect to, it does not have to match the domain name.</li>\\n    <li><code>Hostname</code> is the actual FQDN of the server we wish to connect to</li>\\n    <li><code>AddKeysToAgent</code> tells ssh to add the specified keyfiles to our existing <a href=\\\"https://www.ssh.com/ssh/agent\\\">ssh-agent</a></li>\\n    <li><code>UseKeychain</code> tells ssh to utilize the <a href=\\\"https://www.techrepublic.com/article/configure-it-quick-use-keychain-to-simplify-ssh-connections/\\\">keychain</a>, which either starts the ssh-agent, or connects to an already running instance saving the trouble of typing the passphrase for a given key if you're logging in and out of a server frequently.</li>\\n    <li><code>IdentityFile</code> is your keyfile you wish to use for the host you are configuring; probably the key you just generated.</li>\\n</ul>\\n\\n<p>Using this config and example; we can now use the following command</p>\\n<br/>\\n\\n<p><code>ssh user@example</code></p>\\n\\n<p>Which using this config, in the background is expanded out to the following command</p>\\n<br/>\\n\\n<p><code>ssh -i ~/.ssh/securedomain_username user@example.securedomain.com</code></p>\"}],[\"hr\",{}]],\"markups\":[[\"a\",[\"href\",\"https://www.upwork.com/freelancers/~01c61ee9802b94133e\"]],[\"a\",[\"href\",\"mailto:work@breadnet.co.uk\"]]],\"sections\":[[10,0],[10,1],[10,2],[10,3],[1,\"p\",[[0,[],0,\"You can hire me via \"],[0,[0],1,\"Upwork\"],[0,[],0,\" or \"],[0,[1],1,\"emailing me\"],[0,[],0,\" for weekend projects!\"]]]],\"ghostVersion\":\"3.0\"}","lexical":null,"html":"<!--kg-card-begin: html--><h4>The following steps will setup your ssh keypairs on your local machine, copy the public key to your server, and configure your ssh client to use a specific private key with a server alias.</h4>\n\n<hr>\n\n<p>First, generate your keypair, I generally name the keys with my username-service, username-hostname, or username-device, sometimes even a combination of the three.</p><br/>\n\n<p>To generate keys for a new server at <i>securedomain.com</i></p>\n<br/>\n\n<code>ssh-keygen -t rsa -b 4096 -C 'email@securedomain.com'</code>\n<br/>\n\n<h4><u>Things to keep in mind:</u></h4>\n<ul>\n    <li>The email does not have to be the same domain as the service/server you are connecting to</li>\n    <li>The <code>-t rsa -b 4096</code> options are safe and will work on most servers, you can also use <code>-t ed25519</code> if you wish.</li>\n</ul>\n<br/>\n\n<p>Once you're in the ssh-keygen prompt it will ask you to provide a name for your keys</p>\n\n<p><b><u>NOTE:</u></b> You can also do this in the command itself using the <code>-f &lt;filename&gt;</code></p>\n<br/>\n\n\n<code>Generating public/private rsa key pair.\nEnter file in which to save the key (/home/user/.ssh/id_rsa):<span style='color: red;'>&lt;new_key_name&gt;</span></code>\n\n<p>The <samp>ssh-keygen</samp> application will then, prompt you for a passphrase to secure your key. <i>(You don't have to, but it is recommended)</i>. If you don't wish to use a passphrase just press <kbd>enter</kbd> twice.</p>\n\n<hr>\n\n<p>Now, we need to copy our keys to the server, we do so by entering the following</p>\n<br/>\n\n<code>ssh-copy-id -i .ssh/yourkeyfile username@hostname</code>\n\n<p>This will effectively copy your keyfile over to the new server in a secure fashion. More reading on this <a href=\"https://www.ssh.com/ssh/copy-id\">here</a>.</p>\n\n\n<p>Next, you will want to add the key to your <samp>~/.ssh/config</samp> to be used automatically with the specified host it was created for; this saves the leg work of having to remember which key goes with which host, and also from having to type <code>-i /path/to/key</code> options with your ssh command.</p>\n<br/>\n\n<p>To do this, first we need to edit our config, so open up <samp>~/.ssh/config</samp> in your preferred editor and enter the following:</p><!--kg-card-end: html--><!--kg-card-begin: markdown--><pre><code># EXAMPLE\n# This will setup the use of example as an alias for the FQDN of the server you want to connect to\n\nHost example\n    Hostname example.securedomain.com\n    AddKeysToAgent yes\n    UseKeychain yes\n    IdentityFile ~/.ssh/securedomain_username\n</code></pre>\n<!--kg-card-end: markdown--><!--kg-card-begin: html--><p>In this config the following are true:</p>\n\n<ul>\n     <li><code>Host</code> is an alias to the server we wish to connect to, it does not have to match the domain name.</li>\n    <li><code>Hostname</code> is the actual FQDN of the server we wish to connect to</li>\n    <li><code>AddKeysToAgent</code> tells ssh to add the specified keyfiles to our existing <a href=\"https://www.ssh.com/ssh/agent\">ssh-agent</a></li>\n    <li><code>UseKeychain</code> tells ssh to utilize the <a href=\"https://www.techrepublic.com/article/configure-it-quick-use-keychain-to-simplify-ssh-connections/\">keychain</a>, which either starts the ssh-agent, or connects to an already running instance saving the trouble of typing the passphrase for a given key if you're logging in and out of a server frequently.</li>\n    <li><code>IdentityFile</code> is your keyfile you wish to use for the host you are configuring; probably the key you just generated.</li>\n</ul>\n\n<p>Using this config and example; we can now use the following command</p>\n<br/>\n\n<p><code>ssh user@example</code></p>\n\n<p>Which using this config, in the background is expanded out to the following command</p>\n<br/>\n\n<p><code>ssh -i ~/.ssh/securedomain_username user@example.securedomain.com</code></p><!--kg-card-end: html--><hr><p>You can hire me via <a href=\"https://www.upwork.com/freelancers/~01c61ee9802b94133e\">Upwork</a> or <a href=\"mailto:work@breadnet.co.uk\">emailing me</a> for weekend projects!</p>","comment_id":"5f2f67a71c811d05f5b99610","plaintext":"The following steps will setup your ssh keypairs on your local machine, copy the public key to your server, and configure your ssh client to use a specific private key with a server alias.\n\n\n\n\n\nFirst, generate your keypair, I generally name the keys with my username-service, username-hostname, or username-device, sometimes even a combination of the three.\n\n\n\n\nTo generate keys for a new server at securedomain.com\n\n\n\n\n\nssh-keygen -t rsa -b 4096 -C 'email@securedomain.com'\n\n\n\n\n\nThings to keep in mind:\n\n\n * The email does not have to be the same domain as the service/server you are connecting to\n * The -t rsa -b 4096 options are safe and will work on most servers, you can also use -t ed25519 if you wish.\n\n\n\n\n\nOnce you're in the ssh-keygen prompt it will ask you to provide a name for your keys\n\n\n\nNOTE: You can also do this in the command itself using the -f <filename>\n\n\n\n\n\n\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/home/user/.ssh/id_rsa):<new_key_name>\n\n\n\nThe ssh-keygen application will then, prompt you for a passphrase to secure your key. (You don't have to, but it is recommended). If you don't wish to use a passphrase just press enter twice.\n\n\n\n\n\nNow, we need to copy our keys to the server, we do so by entering the following\n\n\n\n\n\nssh-copy-id -i .ssh/yourkeyfile username@hostname\n\n\n\nThis will effectively copy your keyfile over to the new server in a secure fashion. More reading on this here.\n\n\n\n\nNext, you will want to add the key to your ~/.ssh/config to be used automatically with the specified host it was created for; this saves the leg work of having to remember which key goes with which host, and also from having to type -i /path/to/key options with your ssh command.\n\n\n\n\n\nTo do this, first we need to edit our config, so open up ~/.ssh/config in your preferred editor and enter the following:\n\n# EXAMPLE\n# This will setup the use of example as an alias for the FQDN of the server you want to connect to\n\nHost example\n    Hostname example.securedomain.com\n    AddKeysToAgent yes\n    UseKeychain yes\n    IdentityFile ~/.ssh/securedomain_username\n\n\n\nIn this config the following are true:\n\n\n\n * Host is an alias to the server we wish to connect to, it does not have to match the domain name.\n * Hostname is the actual FQDN of the server we wish to connect to\n * AddKeysToAgent tells ssh to add the specified keyfiles to our existing ssh-agent\n * UseKeychain tells ssh to utilize the keychain, which either starts the ssh-agent, or connects to an already running instance saving the trouble of typing the passphrase for a given key if you're logging in and out of a server frequently.\n * IdentityFile is your keyfile you wish to use for the host you are configuring; probably the key you just generated.\n\n\n\nUsing this config and example; we can now use the following command\n\n\n\n\n\nssh user@example\n\n\n\nWhich using this config, in the background is expanded out to the following command\n\n\n\n\n\nssh -i ~/.ssh/securedomain_username user@example.securedomain.com\n\nYou can hire me via Upwork or emailing me for weekend projects!","feature_image":"https://images.unsplash.com/photo-1575908539614-ff89490f4a78?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=2000&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2020-08-09T03:04:07.000Z","updated_at":"2021-05-02T01:46:44.000Z","published_at":"2020-08-09T04:49:19.000Z","custom_excerpt":"Wanted to boost your escurity using ssh? Why not use keypairs!","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"6425f3bc192c0c150413d59a","uuid":"c6e073d0-7cc6-4a18-84bd-b57c7a3512c2","title":"Self hosted analytics","slug":"self-hosted-analytics","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"code\",{\"code\":\"nginx\\nmariadb-server mariadb-client\\nsoftware-properties-common\\n(repo) ppa:ondrej/php\\nphp7.2-fpm php7.2-common php7.2-sqlite php7.2-curl php7.2-intl php7.2-mbstring php7.2-xmlrpc php7.2-mysql php7.2-gd php7.2-xml php7.2-cli php7.2-zip\\nunzip\"}],[\"bookmark\",{\"url\":\"__GHOST_URL__/a-beginners-guide-to-ssh/\",\"metadata\":{\"url\":\"__GHOST_URL__/a-beginners-guide-to-ssh/\",\"title\":\"A beginners guide to SSH\",\"description\":\"Sweet, you just got a linux server running on <insert cloud provider > but now\\nyou need to actually do something on it. You tried to use the web console but now you need to paste something in... It\\ndoesn’t work? Shit. Luckilly SSH was designed to allow for Secure SHell access\\nto a server. (Hence S…\",\"author\":\"Bradley Stannard\",\"publisher\":\"breadNET\",\"thumbnail\":\"https://images.unsplash.com/photo-1586772002345-339f8042a777?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=2000&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ\",\"icon\":\"https://breadnet.co.uk/favicon.png\"}}],[\"hr\",{}],[\"code\",{\"code\":\"sudo -s\\n\"}],[\"hr\",{}],[\"code\",{\"code\":\"add-apt-repository ppa:ondrej/php\\napt-get update\"}],[\"code\",{\"code\":\"apt-get install nginx mariadb-server mariadb-client software-properties-common php7.2-fpm php7.2-common php7.2-sqlite php7.2-curl php7.2-intl php7.2-mbstring php7.2-xmlrpc php7.2-mysql php7.2-gd php7.2-xml php7.2-cli php7.2-zip unzip -y\"}],[\"code\",{\"code\":\"sudo nano /etc/php/7.2/fpm/php.ini\"}],[\"code\",{\"code\":\"file_uploads = On\\nallow_url_fopen = On\\nshort_open_tag = On\\ncgi.fix_pathinfo = 0\\nmemory_limit = 256M\\nupload_max_filesize = 100M\\nmax_execution_time = 360\\ndate.timezone = UTC\"}],[\"code\",{\"code\":\"systemctl restart nginx\"}],[\"hr\",{}],[\"code\",{\"code\":\"mysql_secure_installation\"}],[\"code\",{\"code\":\"Enter current password for root (enter for none): Just press the Enter\\n\\nSet root password? [Y/n]: Y\\n\\nNew password: Enter password\\n\\nRe-enter new password: Repeat password\\n\\nRemove anonymous users? [Y/n]: Y\\n\\nDisallow root login remotely? [Y/n]: Y\\n\\nRemove test database and access to it? [Y/n]:  Y\\n\\nReload privilege tables now? [Y/n]:  Y\"}],[\"code\",{\"code\":\"mysql -u root -p\"}],[\"code\",{\"code\":\"create database matomo;\"}],[\"code\",{\"code\":\"create user 'matomousr'@'localhost' identified by 'matomo_user_password';\"}],[\"code\",{\"code\":\"create user 'matomousr'@'<nginx_ip> identified by 'matomo_user_password';\"}],[\"code\",{\"code\":\"GRANT ALL ON matomo.* TO 'matomousr'@'<localhost or nginx server ip>' IDENTIFIED BY 'matomo_user_password' WITH GRANT OPTION;\"}],[\"code\",{\"code\":\"FLUSH PRIVILEGES;\\nEXIT;\"}],[\"hr\",{}],[\"code\",{\"code\":\"cd /tmp && wget https://builds.matomo.org/piwik.zip\\nunzip piwik.zip\\nsudo mv piwik /var/www/matomo\"}],[\"code\",{\"code\":\"sudo chown -R www-data:www-data /var/www/matomo/\\nsudo chmod -R 755 /var/www/matomo/\"}],[\"hr\",{}],[\"code\",{\"code\":\"nano /etc/nginx/sites-available/<subdomain>.yourdomain.tld\"}],[\"code\",{\"code\":\"server {\\n    listen 80;\\n    listen [::]:80;\\n    root /var/www/html/matomo;\\n    index  index.php index.html index.htm;\\n    server_name <subdomain>.yourdomain.tld www.<subdomain>.yourdomain.tld;\\n\\n    client_max_body_size 100M;\\n\\n    location / {\\n        try_files $uri /index.php?$query_string;\\n       }\\n\\n    location ~ \\\\.php$ {\\n    include snippets/fastcgi-php.conf;\\n    fastcgi_pass             unix:/var/run/php/php7.2-fpm.sock;\\n    fastcgi_param   SCRIPT_FILENAME $document_root$fastcgi_script_name;\\n     }\\n}\"}],[\"code\",{\"code\":\"ln -s /etc/nginx/sites-available/<subdomain>.yourdomain.tld /etc/nginx/sites-enabled/<subdomain>.yourdomain.tld\"}],[\"code\",{\"code\":\"systemctl restart nginx\"}],[\"hr\",{}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/08/image-11.png\",\"width\":485,\"height\":64}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/08/image-13.png\",\"width\":440,\"height\":220}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/08/image-14.png\",\"width\":1027,\"height\":238}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/08/image-15.png\",\"width\":1304,\"height\":133}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/08/image-16.png\",\"width\":1050,\"height\":404}],[\"hr\",{}]],\"markups\":[[\"code\"],[\"em\"],[\"a\",[\"href\",\"https://www.upwork.com/freelancers/~01c61ee9802b94133e\"]],[\"a\",[\"href\",\"mailto:work@breadnet.co.uk\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"The worst part about self hosting stuff is not having that seamless transition between applications built by google, microsoft and others, to being able to control the data your self.\"]]],[1,\"p\",[[0,[],0,\"My biggest annoyance was being able to see how many hits my page got, where people came to my site from, as well as what people were searching for. This allows me to know what people like (for example most people came to my site to see me use s3 with jellyfin and nginx reverse proxy) so I can write more posts for them\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"Today we will look at web analytics. A controversial topic depending on who you ask.\"]]],[1,\"p\",[[0,[],0,\"Let's get started.\"]]],[1,\"p\",[[0,[],0,\"We will be installing something called Matomo on to our web server. This server can be located on your server proxied to the outside world, or on a droplet or on AWS if you roll that way, It scales well.\"]]],[1,\"p\",[[0,[],0,\"What we will be installing:\"]]],[10,0],[1,\"p\",[[0,[],0,\"If you already have these things installed, that's great. You can just skip the part where we need to install them. On Ubuntu (The os for my server) it doesnt hurt to install something twice. \"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"Because I am lazy and dont like having to type \"],[0,[0],1,\"sudo\"],[0,[],0,\" at the start of every command, we will start by logging in as root. \"]]],[1,\"p\",[[0,[],0,\"If your server is remote, you may need to connect via ssh. I have a guide!\"]]],[10,1],[10,2],[1,\"p\",[[0,[],0,\"Login as root\"]]],[10,3],[10,4],[1,\"p\",[[0,[],0,\"Install all the software (in one go as I'm lazy and like doing it in blocks)\"]]],[1,\"p\",[[0,[],0,\"First add the PHP repo and apt-update\"]]],[10,5],[1,\"p\",[[0,[],0,\"Then all the rest we need!\"]]],[10,6],[1,\"p\",[[0,[],0,\"Once everything is installed, edit the \"],[0,[0],1,\"php7.2\"],[0,[],0,\" config file\"]]],[10,7],[1,\"p\",[[0,[],0,\"Find the below and change them\"]]],[10,8],[1,\"p\",[[0,[],0,\"Restart nginx\"]]],[10,9],[10,10],[1,\"p\",[[0,[],0,\"Now we can configure the database for Matomo.\"]]],[1,\"p\",[[0,[],0,\"if you already have mariadb or mysql configured, skip this step as I \"],[0,[1],1,\"think\"],[0,[],0,\" it can cause issues  ¯\\\\_(ツ)_/¯\"]]],[10,11],[1,\"p\",[[0,[],0,\"You will be prompted to answer the below:\"]]],[10,12],[1,\"p\",[[0,[],0,\"Sweet, now we need to create a database user as well as a database for Matomo to use\"]]],[1,\"p\",[[0,[],0,\"login to mysql\"]]],[10,13],[1,\"p\",[[0,[],0,\"Now create a database called matomo (You can call your's what ever you like)\"]]],[10,14],[1,\"p\",[[0,[],0,\"Now create a user called matomousr. Use step \"],[0,[0],1,\"A\"],[0,[],0,\" if you're running the database server on the same host, or step \"],[0,[0],1,\"B\"],[0,[],0,\" if a remote server will have matomo.\"]]],[1,\"h2\",[[0,[],0,\"A \"]]],[10,15],[1,\"h2\",[[0,[],0,\"B\"]]],[10,16],[1,\"p\",[[0,[],0,\"Now we need to grant permissions for \"],[0,[0],1,\"matomousr\"],[0,[],0,\" to use the database\"]]],[10,17],[1,\"p\",[[0,[],0,\"Now flush privileges and exit\"]]],[10,18],[10,19],[1,\"p\",[[0,[],0,\"Nice, you've made it this far. I promise it's alomsot over!\"]]],[1,\"p\",[[0,[],0,\"Download and install matomo - It used to be called 'Piwiki' so dont worry about the file name\"]]],[10,20],[1,\"p\",[[0,[],0,\"Now we need to fix the permissions\"]]],[10,21],[10,22],[1,\"p\",[[0,[],0,\"NGINX part\"]]],[1,\"p\",[[0,[],0,\"For simplicity I like to make my nginx config files names' reflect their site url. So the file would be \"],[0,[0],1,\"test.breadnet.co.uk\"],[0,[],0,\" would point to... well, you guessed it. No need to tell you :)\"]]],[1,\"p\",[[0,[],0,\"Before you move on, pick a subdomain for matomo to run on. I made the stupid decision of going 'What's the short version of analytics? huh, anal... cool.' so pick something like \"],[0,[0],1,\"tracking.domain\"],[0,[],0,\" or \"],[0,[0],1,\"watcher.domain\"]]],[10,23],[1,\"p\",[[0,[],0,\"In the open window paste:\"]]],[10,24],[1,\"p\",[[0,[],0,\"Save and exit (Ctrl + x, y, enter)\"]]],[1,\"p\",[[0,[],0,\"Now we canenable the site\"]]],[10,25],[1,\"p\",[[0,[],0,\"Restart nginx\"]]],[10,26],[1,\"p\",[[0,[],0,\"You will need to add a DNS A record (Or AAAA record if you're cool and use IPV6) pointing the \"],[0,[0],1,\"<subdomain>\"],[0,[],0,\" part to your server's IP address.\"]]],[1,\"p\",[[0,[],0,\"Opoen your browser and go to the url of your matomo instance and you'll need to run through the setup. It's straightforward.\"]]],[10,27],[1,\"p\",[[0,[],0,\"Adding a site is rather simple.\"]]],[1,\"p\",[[0,[],0,\"Once logged in, go to the gear icon at the top right:\"]]],[10,28],[1,\"p\",[[0,[],0,\"Then you will see a section called \"],[0,[0],1,\"Quick Links\"],[0,[],0,\" at the top.\"]]],[10,29],[1,\"p\",[[0,[],0,\"Click the \"],[0,[0],1,\"Add a new website\"]]],[1,\"p\",[[0,[],0,\"Select what the site is:\"]]],[10,30],[1,\"p\",[[0,[],0,\"Specify the Name and the URL, then scroll down and click save.\"]]],[1,\"p\",[[0,[],0,\"Once it's saved, click the \"],[0,[0],1,\"View tracking code\"],[0,[],0,\" section to get the secret sauce you need to add to your site\"]]],[10,31],[1,\"p\",[[0,[],0,\"if you scroll down you will see a box like the below. You need to add this to the header or footer of every page you want to track:\"]]],[10,32],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"If you have any issues or questions, feel free to get in contact!\"]]],[10,33],[1,\"p\",[[0,[],0,\"You can hire me via \"],[0,[2],1,\"Upwork\"],[0,[],0,\" or \"],[0,[3],1,\"emailing me\"],[0,[],0,\" for weekend projects!\"]]]],\"ghostVersion\":\"3.0\"}","lexical":null,"html":"<p>The worst part about self hosting stuff is not having that seamless transition between applications built by google, microsoft and others, to being able to control the data your self.</p><p>My biggest annoyance was being able to see how many hits my page got, where people came to my site from, as well as what people were searching for. This allows me to know what people like (for example most people came to my site to see me use s3 with jellyfin and nginx reverse proxy) so I can write more posts for them</p><p></p><p>Today we will look at web analytics. A controversial topic depending on who you ask.</p><p>Let's get started.</p><p>We will be installing something called Matomo on to our web server. This server can be located on your server proxied to the outside world, or on a droplet or on AWS if you roll that way, It scales well.</p><p>What we will be installing:</p><pre><code>nginx\nmariadb-server mariadb-client\nsoftware-properties-common\n(repo) ppa:ondrej/php\nphp7.2-fpm php7.2-common php7.2-sqlite php7.2-curl php7.2-intl php7.2-mbstring php7.2-xmlrpc php7.2-mysql php7.2-gd php7.2-xml php7.2-cli php7.2-zip\nunzip</code></pre><p>If you already have these things installed, that's great. You can just skip the part where we need to install them. On Ubuntu (The os for my server) it doesnt hurt to install something twice. </p><p></p><p>Because I am lazy and dont like having to type <code>sudo</code> at the start of every command, we will start by logging in as root. </p><p>If your server is remote, you may need to connect via ssh. I have a guide!</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"__GHOST_URL__/a-beginners-guide-to-ssh/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">A beginners guide to SSH</div><div class=\"kg-bookmark-description\">Sweet, you just got a linux server running on &lt;insert cloud provider &gt; but nowyou need to actually do something on it. You tried to use the web console but now you need to paste something in... Itdoesn’t work? Shit. Luckilly SSH was designed to allow for Secure SHell accessto a server. (Hence S…</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://breadnet.co.uk/favicon.png\" alt=\"\"><span class=\"kg-bookmark-author\">breadNET</span><span class=\"kg-bookmark-publisher\">Bradley Stannard</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://images.unsplash.com/photo-1586772002345-339f8042a777?ixlib&#x3D;rb-1.2.1&amp;q&#x3D;80&amp;fm&#x3D;jpg&amp;crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;w&#x3D;2000&amp;fit&#x3D;max&amp;ixid&#x3D;eyJhcHBfaWQiOjExNzczfQ\" alt=\"\"></div></a></figure><hr><p>Login as root</p><pre><code>sudo -s\n</code></pre><hr><p>Install all the software (in one go as I'm lazy and like doing it in blocks)</p><p>First add the PHP repo and apt-update</p><pre><code>add-apt-repository ppa:ondrej/php\napt-get update</code></pre><p>Then all the rest we need!</p><pre><code>apt-get install nginx mariadb-server mariadb-client software-properties-common php7.2-fpm php7.2-common php7.2-sqlite php7.2-curl php7.2-intl php7.2-mbstring php7.2-xmlrpc php7.2-mysql php7.2-gd php7.2-xml php7.2-cli php7.2-zip unzip -y</code></pre><p>Once everything is installed, edit the <code>php7.2</code> config file</p><pre><code>sudo nano /etc/php/7.2/fpm/php.ini</code></pre><p>Find the below and change them</p><pre><code>file_uploads = On\nallow_url_fopen = On\nshort_open_tag = On\ncgi.fix_pathinfo = 0\nmemory_limit = 256M\nupload_max_filesize = 100M\nmax_execution_time = 360\ndate.timezone = UTC</code></pre><p>Restart nginx</p><pre><code>systemctl restart nginx</code></pre><hr><p>Now we can configure the database for Matomo.</p><p>if you already have mariadb or mysql configured, skip this step as I <em>think</em> it can cause issues  ¯\\_(ツ)_/¯</p><pre><code>mysql_secure_installation</code></pre><p>You will be prompted to answer the below:</p><pre><code>Enter current password for root (enter for none): Just press the Enter\n\nSet root password? [Y/n]: Y\n\nNew password: Enter password\n\nRe-enter new password: Repeat password\n\nRemove anonymous users? [Y/n]: Y\n\nDisallow root login remotely? [Y/n]: Y\n\nRemove test database and access to it? [Y/n]:  Y\n\nReload privilege tables now? [Y/n]:  Y</code></pre><p>Sweet, now we need to create a database user as well as a database for Matomo to use</p><p>login to mysql</p><pre><code>mysql -u root -p</code></pre><p>Now create a database called matomo (You can call your's what ever you like)</p><pre><code>create database matomo;</code></pre><p>Now create a user called matomousr. Use step <code>A</code> if you're running the database server on the same host, or step <code>B</code> if a remote server will have matomo.</p><h2 id=\"a\">A </h2><pre><code>create user 'matomousr'@'localhost' identified by 'matomo_user_password';</code></pre><h2 id=\"b\">B</h2><pre><code>create user 'matomousr'@'&lt;nginx_ip&gt; identified by 'matomo_user_password';</code></pre><p>Now we need to grant permissions for <code>matomousr</code> to use the database</p><pre><code>GRANT ALL ON matomo.* TO 'matomousr'@'&lt;localhost or nginx server ip&gt;' IDENTIFIED BY 'matomo_user_password' WITH GRANT OPTION;</code></pre><p>Now flush privileges and exit</p><pre><code>FLUSH PRIVILEGES;\nEXIT;</code></pre><hr><p>Nice, you've made it this far. I promise it's alomsot over!</p><p>Download and install matomo - It used to be called 'Piwiki' so dont worry about the file name</p><pre><code>cd /tmp &amp;&amp; wget https://builds.matomo.org/piwik.zip\nunzip piwik.zip\nsudo mv piwik /var/www/matomo</code></pre><p>Now we need to fix the permissions</p><pre><code>sudo chown -R www-data:www-data /var/www/matomo/\nsudo chmod -R 755 /var/www/matomo/</code></pre><hr><p>NGINX part</p><p>For simplicity I like to make my nginx config files names' reflect their site url. So the file would be <code>test.breadnet.co.uk</code> would point to... well, you guessed it. No need to tell you :)</p><p>Before you move on, pick a subdomain for matomo to run on. I made the stupid decision of going 'What's the short version of analytics? huh, anal... cool.' so pick something like <code>tracking.domain</code> or <code>watcher.domain</code></p><pre><code>nano /etc/nginx/sites-available/&lt;subdomain&gt;.yourdomain.tld</code></pre><p>In the open window paste:</p><pre><code>server {\n    listen 80;\n    listen [::]:80;\n    root /var/www/html/matomo;\n    index  index.php index.html index.htm;\n    server_name &lt;subdomain&gt;.yourdomain.tld www.&lt;subdomain&gt;.yourdomain.tld;\n\n    client_max_body_size 100M;\n\n    location / {\n        try_files $uri /index.php?$query_string;\n       }\n\n    location ~ \\.php$ {\n    include snippets/fastcgi-php.conf;\n    fastcgi_pass             unix:/var/run/php/php7.2-fpm.sock;\n    fastcgi_param   SCRIPT_FILENAME $document_root$fastcgi_script_name;\n     }\n}</code></pre><p>Save and exit (Ctrl + x, y, enter)</p><p>Now we canenable the site</p><pre><code>ln -s /etc/nginx/sites-available/&lt;subdomain&gt;.yourdomain.tld /etc/nginx/sites-enabled/&lt;subdomain&gt;.yourdomain.tld</code></pre><p>Restart nginx</p><pre><code>systemctl restart nginx</code></pre><p>You will need to add a DNS A record (Or AAAA record if you're cool and use IPV6) pointing the <code>&lt;subdomain&gt;</code> part to your server's IP address.</p><p>Opoen your browser and go to the url of your matomo instance and you'll need to run through the setup. It's straightforward.</p><hr><p>Adding a site is rather simple.</p><p>Once logged in, go to the gear icon at the top right:</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/08/image-11.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"485\" height=\"64\"></figure><p>Then you will see a section called <code>Quick Links</code> at the top.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/08/image-13.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"440\" height=\"220\"></figure><p>Click the <code>Add a new website</code></p><p>Select what the site is:</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/08/image-14.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1027\" height=\"238\" srcset=\"__GHOST_URL__/content/images/size/w600/2020/08/image-14.png 600w, __GHOST_URL__/content/images/size/w1000/2020/08/image-14.png 1000w, __GHOST_URL__/content/images/2020/08/image-14.png 1027w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Specify the Name and the URL, then scroll down and click save.</p><p>Once it's saved, click the <code>View tracking code</code> section to get the secret sauce you need to add to your site</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/08/image-15.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1304\" height=\"133\" srcset=\"__GHOST_URL__/content/images/size/w600/2020/08/image-15.png 600w, __GHOST_URL__/content/images/size/w1000/2020/08/image-15.png 1000w, __GHOST_URL__/content/images/2020/08/image-15.png 1304w\" sizes=\"(min-width: 720px) 720px\"></figure><p>if you scroll down you will see a box like the below. You need to add this to the header or footer of every page you want to track:</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/08/image-16.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1050\" height=\"404\" srcset=\"__GHOST_URL__/content/images/size/w600/2020/08/image-16.png 600w, __GHOST_URL__/content/images/size/w1000/2020/08/image-16.png 1000w, __GHOST_URL__/content/images/2020/08/image-16.png 1050w\" sizes=\"(min-width: 720px) 720px\"></figure><p></p><p>If you have any issues or questions, feel free to get in contact!</p><hr><p>You can hire me via <a href=\"https://www.upwork.com/freelancers/~01c61ee9802b94133e\">Upwork</a> or <a href=\"mailto:work@breadnet.co.uk\">emailing me</a> for weekend projects!</p>","comment_id":"5f453990682355289d7cf266","plaintext":"The worst part about self hosting stuff is not having that seamless transition between applications built by google, microsoft and others, to being able to control the data your self.\n\nMy biggest annoyance was being able to see how many hits my page got, where people came to my site from, as well as what people were searching for. This allows me to know what people like (for example most people came to my site to see me use s3 with jellyfin and nginx reverse proxy) so I can write more posts for them\n\n\n\nToday we will look at web analytics. A controversial topic depending on who you ask.\n\nLet's get started.\n\nWe will be installing something called Matomo on to our web server. This server can be located on your server proxied to the outside world, or on a droplet or on AWS if you roll that way, It scales well.\n\nWhat we will be installing:\n\nnginx\nmariadb-server mariadb-client\nsoftware-properties-common\n(repo) ppa:ondrej/php\nphp7.2-fpm php7.2-common php7.2-sqlite php7.2-curl php7.2-intl php7.2-mbstring php7.2-xmlrpc php7.2-mysql php7.2-gd php7.2-xml php7.2-cli php7.2-zip\nunzip\n\nIf you already have these things installed, that's great. You can just skip the part where we need to install them. On Ubuntu (The os for my server) it doesnt hurt to install something twice.\n\n\n\nBecause I am lazy and dont like having to type sudo at the start of every command, we will start by logging in as root.\n\nIf your server is remote, you may need to connect via ssh. I have a guide!\n\nA beginners guide to SSHSweet, you just got a linux server running on <insert cloud provider > but nowyou need to actually do something on it. You tried to use the web console but now you need to paste something in... Itdoesn’t work? Shit. Luckilly SSH was designed to allow for Secure SHell accessto a server. (Hence S…breadNETBradley Stannard\n\nLogin as root\n\nsudo -s\n\n\nInstall all the software (in one go as I'm lazy and like doing it in blocks)\n\nFirst add the PHP repo and apt-update\n\nadd-apt-repository ppa:ondrej/php\napt-get update\n\nThen all the rest we need!\n\napt-get install nginx mariadb-server mariadb-client software-properties-common php7.2-fpm php7.2-common php7.2-sqlite php7.2-curl php7.2-intl php7.2-mbstring php7.2-xmlrpc php7.2-mysql php7.2-gd php7.2-xml php7.2-cli php7.2-zip unzip -y\n\nOnce everything is installed, edit the php7.2 config file\n\nsudo nano /etc/php/7.2/fpm/php.ini\n\nFind the below and change them\n\nfile_uploads = On\nallow_url_fopen = On\nshort_open_tag = On\ncgi.fix_pathinfo = 0\nmemory_limit = 256M\nupload_max_filesize = 100M\nmax_execution_time = 360\ndate.timezone = UTC\n\nRestart nginx\n\nsystemctl restart nginx\n\nNow we can configure the database for Matomo.\n\nif you already have mariadb or mysql configured, skip this step as I think it can cause issues  ¯\\_(ツ)_/¯\n\nmysql_secure_installation\n\nYou will be prompted to answer the below:\n\nEnter current password for root (enter for none): Just press the Enter\n\nSet root password? [Y/n]: Y\n\nNew password: Enter password\n\nRe-enter new password: Repeat password\n\nRemove anonymous users? [Y/n]: Y\n\nDisallow root login remotely? [Y/n]: Y\n\nRemove test database and access to it? [Y/n]:  Y\n\nReload privilege tables now? [Y/n]:  Y\n\nSweet, now we need to create a database user as well as a database for Matomo to use\n\nlogin to mysql\n\nmysql -u root -p\n\nNow create a database called matomo (You can call your's what ever you like)\n\ncreate database matomo;\n\nNow create a user called matomousr. Use step A if you're running the database server on the same host, or step B if a remote server will have matomo.\n\n\nA\n\ncreate user 'matomousr'@'localhost' identified by 'matomo_user_password';\n\n\nB\n\ncreate user 'matomousr'@'<nginx_ip> identified by 'matomo_user_password';\n\nNow we need to grant permissions for matomousr to use the database\n\nGRANT ALL ON matomo.* TO 'matomousr'@'<localhost or nginx server ip>' IDENTIFIED BY 'matomo_user_password' WITH GRANT OPTION;\n\nNow flush privileges and exit\n\nFLUSH PRIVILEGES;\nEXIT;\n\nNice, you've made it this far. I promise it's alomsot over!\n\nDownload and install matomo - It used to be called 'Piwiki' so dont worry about the file name\n\ncd /tmp && wget https://builds.matomo.org/piwik.zip\nunzip piwik.zip\nsudo mv piwik /var/www/matomo\n\nNow we need to fix the permissions\n\nsudo chown -R www-data:www-data /var/www/matomo/\nsudo chmod -R 755 /var/www/matomo/\n\nNGINX part\n\nFor simplicity I like to make my nginx config files names' reflect their site url. So the file would be test.breadnet.co.uk would point to... well, you guessed it. No need to tell you :)\n\nBefore you move on, pick a subdomain for matomo to run on. I made the stupid decision of going 'What's the short version of analytics? huh, anal... cool.' so pick something like tracking.domain or watcher.domain\n\nnano /etc/nginx/sites-available/<subdomain>.yourdomain.tld\n\nIn the open window paste:\n\nserver {\n    listen 80;\n    listen [::]:80;\n    root /var/www/html/matomo;\n    index  index.php index.html index.htm;\n    server_name <subdomain>.yourdomain.tld www.<subdomain>.yourdomain.tld;\n\n    client_max_body_size 100M;\n\n    location / {\n        try_files $uri /index.php?$query_string;\n       }\n\n    location ~ \\.php$ {\n    include snippets/fastcgi-php.conf;\n    fastcgi_pass             unix:/var/run/php/php7.2-fpm.sock;\n    fastcgi_param   SCRIPT_FILENAME $document_root$fastcgi_script_name;\n     }\n}\n\nSave and exit (Ctrl + x, y, enter)\n\nNow we canenable the site\n\nln -s /etc/nginx/sites-available/<subdomain>.yourdomain.tld /etc/nginx/sites-enabled/<subdomain>.yourdomain.tld\n\nRestart nginx\n\nsystemctl restart nginx\n\nYou will need to add a DNS A record (Or AAAA record if you're cool and use IPV6) pointing the <subdomain> part to your server's IP address.\n\nOpoen your browser and go to the url of your matomo instance and you'll need to run through the setup. It's straightforward.\n\nAdding a site is rather simple.\n\nOnce logged in, go to the gear icon at the top right:\n\nThen you will see a section called Quick Links at the top.\n\nClick the Add a new website\n\nSelect what the site is:\n\nSpecify the Name and the URL, then scroll down and click save.\n\nOnce it's saved, click the View tracking code section to get the secret sauce you need to add to your site\n\nif you scroll down you will see a box like the below. You need to add this to the header or footer of every page you want to track:\n\n\n\nIf you have any issues or questions, feel free to get in contact!\n\nYou can hire me via Upwork or emailing me for weekend projects!","feature_image":"https://images.unsplash.com/photo-1560472354-b33ff0c44a43?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=2000&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2020-08-25T16:17:20.000Z","updated_at":"2021-05-02T01:46:17.000Z","published_at":"2020-08-25T17:19:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"6425f3bc192c0c150413d59b","uuid":"f82bfe5c-c57a-4929-a50b-3fe183c8661f","title":"Cloud security","slug":"cloud-security","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"hr\",{}],[\"hr\",{}],[\"hr\",{}],[\"hr\",{}],[\"hr\",{}],[\"hr\",{}]],\"markups\":[[\"strong\"],[\"em\"],[\"a\",[\"href\",\"https://ico.org.uk/for-organisations/guide-to-data-protection/guide-to-the-general-data-protection-regulation-gdpr/\"]],[\"a\",[\"href\",\"https://www.upwork.com/freelancers/~01c61ee9802b94133e\"]],[\"a\",[\"href\",\"mailto:work@breadnet.co.uk\"]]],\"sections\":[[1,\"blockquote\",[[0,[],0,\"This is still being written but I think it's important you see it\"]]],[1,\"p\",[[0,[],0,\"Well, It's official. I am now a 'Cloud engineer' as far as my company is concerned. Woo!\"]]],[1,\"p\",[[0,[],0,\"On the topic of cloud, I should make my first post about something that is usually overloked by companies when migrating to the cloud: \"],[0,[0],1,\"Security\"]]],[1,\"p\",[[0,[],0,\"This is only a guidance and you should consult your SecOps team if you have one, or consider hiring consultants for this. You can get in contact and I can point you towards companies that are good at this.\"]]],[10,0],[1,\"p\",[[0,[],0,\"Hopefully this guidance will help you to figure out how confident you are in your clous services being able to securly handle your PII as well as private operatoinal data. \"]]],[1,\"p\",[[0,[],0,\"Overall this guidance will be built upon a collection of 14 cloud security principals.\"]]],[1,\"p\",[[0,[],0,\"Futher towards the bottom of this post you will find a section about 'Seperation and cloud security' \"]]],[1,\"p\",[[0,[],0,\"The extent of your security will be dependant on your responsiblities to yuor department and clients, but will vary significatly dependent on your industry. Most Public cloud providers will have reasonably good default security rules, but your responisbilites will be at largest when managing your own infrastrucute. (IaaS)\"]]],[10,1],[1,\"h3\",[[0,[],0,\"How this guidance will work\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"When you're trying to get a better and clearer picture of the risk you would be taking when adapting can, for the most part, be quite difficult to do jsut due to the complex nature of the cloud. I reccomend you use the cloud security principals later on in this post to better structure your analysis. Below you will first find the 8 step process, follwing this will determin which of the princpals are most effecting you and your requirments, and how cloud providers can meet them. \"],[1,[],0,0],[0,[],0,\"Most important is that the decisions youmake about the use and configuration of cloud based services \"],[0,[1],1,\"shoudl \"],[0,[],0,\"be part of your regular risk managment procedures. \"]]],[10,2],[1,\"h3\",[[0,[],0,\"Whos is this guidance for?\"]]],[1,\"p\",[[0,[],0,\"This guidance is aimed at Enterprise organisations but can be refrenced for Public sector, how ever you should consult your managment before acting.\"]]],[10,3],[1,\"h3\",[[0,[],0,\"Making a decision\"]]],[1,\"p\",[[0,[],0,\"You should work through the below steps which will help you identify cloud services that meet your secirity requirments for operating on the cloud.\"]]],[3,\"ol\",[[[0,[0],1,\"Understand operational requirments\"],[1,[],0,1],[0,[],0,\"Understand your ideal use case of the cloud service, consider issues like avaliblity and connectivity. Identify tisks that would be unacceptable should they be realised, and what is ok.\"]],[[0,[0],1,\"Understand your information\"],[1,[],0,2],[0,[],0,\"Identify the type, source and destination of the data that will be stored, processed and transpoted by the cloud service. Understand legal regulations and implications. An example would be storing data on EU citizens would fall under \"],[0,[2],1,\"GDPR\"]],[[0,[0],1,\"Determin relevant security principals\"],[1,[],0,3],[0,[],0,\"You know the business requirments, the risks you're willing to/not to take, you've got a clear picture of the information which will be exposed to the service.\"],[1,[],0,4],[1,[],0,5],[0,[],0,\"Now you will be able to determin which of the cloud security principals are most relevant.\"]],[[0,[0],1,\"Understand how principals are implimented.\"],[1,[],0,6],[0,[],0,\"Find out how your cloud service provider calimes to impliment your security principals that you've previouslt identifeid as relevant. They will have different approaches you should consider and may be able to provide you guidance.\"]],[[0,[0],1,\"Understand the assurnace offered\"],[1,[],0,7],[0,[],0,\"Can your cloud service provider demonstrate the principals that you have identified in step 3 been implimented correctly and to standard? You may want to use external vendors to help uou identify their claims\"]],[[0,[0],1,\"Identification of additional mitigation that can be applied\"],[1,[],0,8],[0,[],0,\"Consider any additioanl measures you or your organization can apply (as a consumer of cloud services) can help to reduce the risk of applications and information.\"]],[[0,[0],1,\"Consieder remaining risks\"],[1,[],0,9],[0,[],0,\"Having worked your way through the above steps, decide weather there are any remaining that need to be addressed, and their importance.\"]],[[0,[0],1,\"Continue to monitor and manage risks\"],[1,[],0,10],[0,[],0,\"Once deployed to the cloud, monthly review weather your services are still meeting your operational needs as well as securty needs.\"]]]],[10,4],[1,\"h1\",[[0,[],0,\"Implementing the Cloud Security Principles\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"Seeing as there are 14 principals, I will try and explain what they are, it's goal and how you're able to impliment them. Let's get a crack on!\"]]],[1,\"p\",[]],[1,\"h3\",[[0,[],0,\"1: Data in transit protection\"]]],[1,\"p\",[[0,[],0,\"Users data that is transisting networks should be adequatly secured to prevent tampering and eavesdropping.\"]]],[1,\"p\",[[0,[],0,\"It can be a combination of:\"]]],[3,\"ul\",[[[0,[],0,\"Networ protection: Denying access to networks, securing access layers\"]],[[0,[],0,\"Encryption: Denying attacker the ability to read the data in the first place\"]]]],[1,\"p\",[[0,[0],1,\"Goals\"],[0,[],0,\":\"]]],[1,\"p\",[[0,[],0,\"You should be confident that:\"]]],[3,\"ul\",[[[0,[],0,\"Data in transil is protected when leaving your control and to users\"]],[[0,[],0,\"Transit data is protected internally eg: App to webserver, API endpoints\"]]]],[1,\"p\",[[0,[0],1,\"Implimentation\"]]],[1,\"p\",[[0,[],0,\"There are many factors to remember when looking in to securing cloud endpoints, but it's imporatnt to remember that at some point the data will leave your VPC or cloud enviroment and will be presented to a user at some point down the line. Securing data in transit can be a hard one but there are key points to look at when securing these. We will first address where an attackor or bad actor would gain access. This could take the form of physical access to hardware, logical access if there is broken code bases or vulnerable software, or between the user and the service. \"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"A simple means to mediating this risk is to emply encryption between endpoins. This should be used between API interfaces, database connections and web conenctions. You would want to use TLS when:\"]]],[3,\"ul\",[[[0,[],0,\"Access to confidential data is required\"]],[[0,[],0,\"Support authhentication and access control.\"]]]],[1,\"p\",[[0,[],0,\"A section that can usually be overlooked would be onbaording and offboarding users. \"],[1,[],0,11],[0,[],0,\"These  processed can usually involve large ammounts of bulk data being moved around, be it copying files to a new starters computer, or offboarding a user where you are taking an image of their account or computer for legal holding. In this case it would be reffered to as 'Transient Data' which should be protected in line with:\"]]],[1,\"p\",[]],[1,\"h3\",[[0,[],0,\"2: Asset protection and resilience\"]]],[1,\"p\",[]],[10,5],[1,\"p\",[[0,[],0,\"You can hire me via \"],[0,[3],1,\"Upwork\"],[0,[],0,\" or \"],[0,[4],1,\"emailing me\"],[0,[],0,\" for weekend projects!\"]]]],\"ghostVersion\":\"3.0\"}","lexical":null,"html":"<blockquote>This is still being written but I think it's important you see it</blockquote><p>Well, It's official. I am now a 'Cloud engineer' as far as my company is concerned. Woo!</p><p>On the topic of cloud, I should make my first post about something that is usually overloked by companies when migrating to the cloud: <strong>Security</strong></p><p>This is only a guidance and you should consult your SecOps team if you have one, or consider hiring consultants for this. You can get in contact and I can point you towards companies that are good at this.</p><hr><p>Hopefully this guidance will help you to figure out how confident you are in your clous services being able to securly handle your PII as well as private operatoinal data. </p><p>Overall this guidance will be built upon a collection of 14 cloud security principals.</p><p>Futher towards the bottom of this post you will find a section about 'Seperation and cloud security' </p><p>The extent of your security will be dependant on your responsiblities to yuor department and clients, but will vary significatly dependent on your industry. Most Public cloud providers will have reasonably good default security rules, but your responisbilites will be at largest when managing your own infrastrucute. (IaaS)</p><hr><h3 id=\"how-this-guidance-will-work\">How this guidance will work</h3><p></p><p>When you're trying to get a better and clearer picture of the risk you would be taking when adapting can, for the most part, be quite difficult to do jsut due to the complex nature of the cloud. I reccomend you use the cloud security principals later on in this post to better structure your analysis. Below you will first find the 8 step process, follwing this will determin which of the princpals are most effecting you and your requirments, and how cloud providers can meet them. <br>Most important is that the decisions youmake about the use and configuration of cloud based services <em>shoudl </em>be part of your regular risk managment procedures. </p><hr><h3 id=\"whos-is-this-guidance-for\">Whos is this guidance for?</h3><p>This guidance is aimed at Enterprise organisations but can be refrenced for Public sector, how ever you should consult your managment before acting.</p><hr><h3 id=\"making-a-decision\">Making a decision</h3><p>You should work through the below steps which will help you identify cloud services that meet your secirity requirments for operating on the cloud.</p><ol><li><strong>Understand operational requirments</strong><br>Understand your ideal use case of the cloud service, consider issues like avaliblity and connectivity. Identify tisks that would be unacceptable should they be realised, and what is ok.</li><li><strong>Understand your information</strong><br>Identify the type, source and destination of the data that will be stored, processed and transpoted by the cloud service. Understand legal regulations and implications. An example would be storing data on EU citizens would fall under <a href=\"https://ico.org.uk/for-organisations/guide-to-data-protection/guide-to-the-general-data-protection-regulation-gdpr/\">GDPR</a></li><li><strong>Determin relevant security principals</strong><br>You know the business requirments, the risks you're willing to/not to take, you've got a clear picture of the information which will be exposed to the service.<br><br>Now you will be able to determin which of the cloud security principals are most relevant.</li><li><strong>Understand how principals are implimented.</strong><br>Find out how your cloud service provider calimes to impliment your security principals that you've previouslt identifeid as relevant. They will have different approaches you should consider and may be able to provide you guidance.</li><li><strong>Understand the assurnace offered</strong><br>Can your cloud service provider demonstrate the principals that you have identified in step 3 been implimented correctly and to standard? You may want to use external vendors to help uou identify their claims</li><li><strong>Identification of additional mitigation that can be applied</strong><br>Consider any additioanl measures you or your organization can apply (as a consumer of cloud services) can help to reduce the risk of applications and information.</li><li><strong>Consieder remaining risks</strong><br>Having worked your way through the above steps, decide weather there are any remaining that need to be addressed, and their importance.</li><li><strong>Continue to monitor and manage risks</strong><br>Once deployed to the cloud, monthly review weather your services are still meeting your operational needs as well as securty needs.</li></ol><hr><h1 id=\"implementing-the-cloud-security-principles\">Implementing the Cloud Security Principles</h1><p></p><p>Seeing as there are 14 principals, I will try and explain what they are, it's goal and how you're able to impliment them. Let's get a crack on!</p><p></p><h3 id=\"1-data-in-transit-protection\">1: Data in transit protection</h3><p>Users data that is transisting networks should be adequatly secured to prevent tampering and eavesdropping.</p><p>It can be a combination of:</p><ul><li>Networ protection: Denying access to networks, securing access layers</li><li>Encryption: Denying attacker the ability to read the data in the first place</li></ul><p><strong>Goals</strong>:</p><p>You should be confident that:</p><ul><li>Data in transil is protected when leaving your control and to users</li><li>Transit data is protected internally eg: App to webserver, API endpoints</li></ul><p><strong>Implimentation</strong></p><p>There are many factors to remember when looking in to securing cloud endpoints, but it's imporatnt to remember that at some point the data will leave your VPC or cloud enviroment and will be presented to a user at some point down the line. Securing data in transit can be a hard one but there are key points to look at when securing these. We will first address where an attackor or bad actor would gain access. This could take the form of physical access to hardware, logical access if there is broken code bases or vulnerable software, or between the user and the service. </p><p></p><p>A simple means to mediating this risk is to emply encryption between endpoins. This should be used between API interfaces, database connections and web conenctions. You would want to use TLS when:</p><ul><li>Access to confidential data is required</li><li>Support authhentication and access control.</li></ul><p>A section that can usually be overlooked would be onbaording and offboarding users. <br>These  processed can usually involve large ammounts of bulk data being moved around, be it copying files to a new starters computer, or offboarding a user where you are taking an image of their account or computer for legal holding. In this case it would be reffered to as 'Transient Data' which should be protected in line with:</p><p></p><h3 id=\"2-asset-protection-and-resilience\">2: Asset protection and resilience</h3><p></p><hr><p>You can hire me via <a href=\"https://www.upwork.com/freelancers/~01c61ee9802b94133e\">Upwork</a> or <a href=\"mailto:work@breadnet.co.uk\">emailing me</a> for weekend projects!</p>","comment_id":"5f6f364d682355289d7cf34c","plaintext":"This is still being written but I think it's important you see it\n\nWell, It's official. I am now a 'Cloud engineer' as far as my company is concerned. Woo!\n\nOn the topic of cloud, I should make my first post about something that is usually overloked by companies when migrating to the cloud: Security\n\nThis is only a guidance and you should consult your SecOps team if you have one, or consider hiring consultants for this. You can get in contact and I can point you towards companies that are good at this.\n\nHopefully this guidance will help you to figure out how confident you are in your clous services being able to securly handle your PII as well as private operatoinal data.\n\nOverall this guidance will be built upon a collection of 14 cloud security principals.\n\nFuther towards the bottom of this post you will find a section about 'Seperation and cloud security'\n\nThe extent of your security will be dependant on your responsiblities to yuor department and clients, but will vary significatly dependent on your industry. Most Public cloud providers will have reasonably good default security rules, but your responisbilites will be at largest when managing your own infrastrucute. (IaaS)\n\n\nHow this guidance will work\n\n\n\nWhen you're trying to get a better and clearer picture of the risk you would be taking when adapting can, for the most part, be quite difficult to do jsut due to the complex nature of the cloud. I reccomend you use the cloud security principals later on in this post to better structure your analysis. Below you will first find the 8 step process, follwing this will determin which of the princpals are most effecting you and your requirments, and how cloud providers can meet them.\nMost important is that the decisions youmake about the use and configuration of cloud based services shoudl be part of your regular risk managment procedures.\n\n\nWhos is this guidance for?\n\nThis guidance is aimed at Enterprise organisations but can be refrenced for Public sector, how ever you should consult your managment before acting.\n\n\nMaking a decision\n\nYou should work through the below steps which will help you identify cloud services that meet your secirity requirments for operating on the cloud.\n\n 1. Understand operational requirments\n    Understand your ideal use case of the cloud service, consider issues like avaliblity and connectivity. Identify tisks that would be unacceptable should they be realised, and what is ok.\n 2. Understand your information\n    Identify the type, source and destination of the data that will be stored, processed and transpoted by the cloud service. Understand legal regulations and implications. An example would be storing data on EU citizens would fall under GDPR\n 3. Determin relevant security principals\n    You know the business requirments, the risks you're willing to/not to take, you've got a clear picture of the information which will be exposed to the service.\n    \n    Now you will be able to determin which of the cloud security principals are most relevant.\n 4. Understand how principals are implimented.\n    Find out how your cloud service provider calimes to impliment your security principals that you've previouslt identifeid as relevant. They will have different approaches you should consider and may be able to provide you guidance.\n 5. Understand the assurnace offered\n    Can your cloud service provider demonstrate the principals that you have identified in step 3 been implimented correctly and to standard? You may want to use external vendors to help uou identify their claims\n 6. Identification of additional mitigation that can be applied\n    Consider any additioanl measures you or your organization can apply (as a consumer of cloud services) can help to reduce the risk of applications and information.\n 7. Consieder remaining risks\n    Having worked your way through the above steps, decide weather there are any remaining that need to be addressed, and their importance.\n 8. Continue to monitor and manage risks\n    Once deployed to the cloud, monthly review weather your services are still meeting your operational needs as well as securty needs.\n\n\nImplementing the Cloud Security Principles\n\n\n\nSeeing as there are 14 principals, I will try and explain what they are, it's goal and how you're able to impliment them. Let's get a crack on!\n\n\n\n\n1: Data in transit protection\n\nUsers data that is transisting networks should be adequatly secured to prevent tampering and eavesdropping.\n\nIt can be a combination of:\n\n * Networ protection: Denying access to networks, securing access layers\n * Encryption: Denying attacker the ability to read the data in the first place\n\nGoals:\n\nYou should be confident that:\n\n * Data in transil is protected when leaving your control and to users\n * Transit data is protected internally eg: App to webserver, API endpoints\n\nImplimentation\n\nThere are many factors to remember when looking in to securing cloud endpoints, but it's imporatnt to remember that at some point the data will leave your VPC or cloud enviroment and will be presented to a user at some point down the line. Securing data in transit can be a hard one but there are key points to look at when securing these. We will first address where an attackor or bad actor would gain access. This could take the form of physical access to hardware, logical access if there is broken code bases or vulnerable software, or between the user and the service.\n\n\n\nA simple means to mediating this risk is to emply encryption between endpoins. This should be used between API interfaces, database connections and web conenctions. You would want to use TLS when:\n\n * Access to confidential data is required\n * Support authhentication and access control.\n\nA section that can usually be overlooked would be onbaording and offboarding users.\nThese  processed can usually involve large ammounts of bulk data being moved around, be it copying files to a new starters computer, or offboarding a user where you are taking an image of their account or computer for legal holding. In this case it would be reffered to as 'Transient Data' which should be protected in line with:\n\n\n\n\n2: Asset protection and resilience\n\n\n\nYou can hire me via Upwork or emailing me for weekend projects!","feature_image":"https://images.unsplash.com/photo-1554769944-3138b076c38a?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=2000&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2020-09-26T12:38:37.000Z","updated_at":"2021-05-02T01:46:02.000Z","published_at":"2020-09-29T18:22:44.000Z","custom_excerpt":"Thinking of moving to the cloud or currently operating on the cloud? Find out how how secure you are.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"6425f3bc192c0c150413d59c","uuid":"6f570893-7240-4cbe-927f-21ececbb4e6d","title":"Terraform with OVH","slug":"terraform-ovh-openstack","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"bookmark\",{\"url\":\"https://github.com/userbradley/cTorrent\",\"metadata\":{\"url\":\"https://github.com/userbradley/cTorrent\",\"title\":\"userbradley/cTorrent\",\"description\":\"Automatic torrenting though OVH. Contribute to userbradley/cTorrent development by creating an account on GitHub.\",\"author\":\"userbradley\",\"publisher\":\"GitHub\",\"thumbnail\":\"https://avatars2.githubusercontent.com/u/41597815?s=400&v=4\",\"icon\":\"https://github.githubassets.com/favicons/favicon.svg\"}}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/12/image.png\",\"width\":792,\"height\":446,\"caption\":\"me using terraform to create infrastructure&nbsp;\"}],[\"code\",{\"code\":\"provider \\\"digitalocean\\\" {\\n  token = var.do_token\\n}\",\"caption\":\"provider.tf\"}],[\"code\",{\"code\":\"variable \\\"do_token\\\" {\\n  type = string\\n  description = \\\"API key to communicate to Digital Ocean with\\\"\\n}\",\"caption\":\"vars.tf\"}],[\"code\",{\"code\":\"do_token = \\\"loluthought58ead42e01f1a62d6f422004e69cd5ba775af3b09\\\"\",\"caption\":\"terraform.tfvars\"}],[\"code\",{\"code\":\"provider \\\"google\\\" {\\n  version = \\\"3.5.0\\\"\\n  credentials = file(\\\"terraform-c8b2b88693d4.json\\\")\\n\\n  project = \\\"absolute-access-271419\\\"\\n  region  = \\\"us-central1\\\"\\n  zone    = \\\"us-central1-c\\\"\\n}\"}],[\"code\",{\"code\":\"provider \\\"ovh\\\" {\\n  endpoint           = \\\"ovh-eu\\\"\\n  application_key    = \\\"yyyyyy\\\"\\n  application_secret = \\\"xxxxxxxxxxxxxx\\\"\\n  consumer_key       = \\\"zzzzzzzzzzzzzz\\\"\\n}\",\"language\":\"hcl\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/12/image-1.png\",\"width\":1922,\"height\":698,\"caption\":\"Horizon for OVH\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/12/image-2.png\",\"width\":309,\"height\":259}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/12/image-3.png\",\"width\":199,\"height\":263}],[\"code\",{\"code\":\"export OS_AUTH_URL=https://auth.cloud.ovh.net/v3\\n# With the addition of Keystone we have standardized on the term **project**\\n# as the entity that owns the resources.\\nexport OS_PROJECT_ID=thisisaplaceholder\\nexport OS_PROJECT_NAME=\\\"bunchofnumbersgohere\\\"\\nexport OS_USER_DOMAIN_NAME=\\\"Default\\\"\\nif [ -z \\\"$OS_USER_DOMAIN_NAME\\\" ]; then unset OS_USER_DOMAIN_NAME; fi\\nexport OS_PROJECT_DOMAIN_ID=\\\"default\\\"\\nif [ -z \\\"$OS_PROJECT_DOMAIN_ID\\\" ]; then unset OS_PROJECT_DOMAIN_ID; fi\\n# unset v2.0 items in case set\\nunset OS_TENANT_ID\\nunset OS_TENANT_NAME\\n# In addition to the owning entity (tenant), OpenStack stores the entity\\n# performing the action as the **user**.\\nexport OS_USERNAME=\\\"user-sike\\\"\\n# With Keystone you pass the keystone password.\\necho \\\"Please enter your OpenStack Password for project $OS_PROJECT_NAME as user $OS_USERNAME: \\\"\\nread -sr OS_PASSWORD_INPUT\\nexport OS_PASSWORD=whaddyatalkinabout\\n# If your configuration has multiple regions, we set that information here.\\n# OS_REGION_NAME is optional and only valid in certain environments.\\nexport OS_REGION_NAME=\\\"UK1\\\"\\n# Don't leave a blank variable, unset it if it was empty\\nif [ -z \\\"$OS_REGION_NAME\\\" ]; then unset OS_REGION_NAME; fi\\nexport OS_INTERFACE=public\\nexport OS_IDENTITY_API_VERSION=3\\n\",\"language\":\"bash\"}],[\"code\",{\"code\":\"provider \\\"openstack\\\" {\\n  auth_url = \\\"https://auth.cloud.ovh.net/v3\\\"\\n  alias = \\\"ovh\\\"\\n}\"}],[\"code\",{\"code\":\"provider \\\"openstack\\\" {\\n  auth_url = \\\"https://auth.cloud.ovh.net/v3\\\"\\n  alias = \\\"ovh\\\"\\n}\",\"caption\":\"provider.tf\"}],[\"code\",{\"code\":\"pub_file_location = \\\"/path/to/your/home/directory/.ssh/id_rsa.pub\\\"\",\"caption\":\"terraform.tfvars\"}],[\"code\",{\"code\":\"variable \\\"pub_file_location\\\" {\\n  type = string\\n}\",\"caption\":\"variables.tf\"}],[\"code\",{\"code\":\"resource \\\"openstack_compute_keypair_v2\\\" \\\"key\\\" {\\n  name       = \\\"SSH\\\"\\n  public_key = file(var.pub_file_location)\\n}\",\"caption\":\"ssh.tf\"}],[\"code\",{\"code\":\"➜ terraform plan\\nRefreshing Terraform state in-memory prior to plan...\\nThe refreshed state will be used to calculate this plan, but will not be\\npersisted to local or remote state storage.\\n\\n\\nError: One of 'auth_url' or 'cloud' must be specified\\n\\n  on <empty> line 0:\\n  (source code not available)\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/12/image-4.png\",\"width\":243,\"height\":207,\"cardWidth\":\"\"}],[\"code\",{\"code\":\"source /path/to/sh/file/we/downloaded/erlier/<name>.sh\"}],[\"hr\",{}]],\"markups\":[[\"a\",[\"href\",\"https://w3techs.com/technologies/details/ho-ovhsas\"]],[\"a\",[\"href\",\"https://www.grandviewresearch.com/industry-analysis/web-hosting-services-market#:~:text=The%20global%20web%20hosting%20services,Virtual%20Private%20Server%20(VPS).\"]],[\"a\",[\"href\",\"https://jira.breadnet.co.uk/projects/CTOR/issues/CTOR-15?filter=allissues\"]],[\"a\",[\"href\",\"https://www.openstack.org\"]],[\"a\",[\"href\",\"https://api.ovh.com/createToken/index.cgi?GET=/*&POST=/*&PUT=/*&DELETE=/*\"]],[\"a\",[\"href\",\"https://docs.openstack.org/horizon/latest/\"]],[\"a\",[\"href\",\"__GHOST_URL__/terraform-ovh-openstack/www.ovh.com/manager/public-cloud/\"]],[\"code\"],[\"a\",[\"href\",\"https://horizon.cloud.ovh.net/auth/login/?next=/\"]],[\"a\",[\"href\",\"https://registry.terraform.io/providers/terraform-provider-openstack/openstack/latest/docs\"]],[\"a\",[\"href\",\"https://www.upwork.com/freelancers/~01c61ee9802b94133e\"]],[\"a\",[\"href\",\"mailto:work@breadnet.co.uk\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"OVH is a french cloud provider that has (\"],[0,[0],1,\"According to w3techs\"],[0,[],0,\") 3.2% of the web hosting market. Whist this does seem small, remember the market is around \"],[0,[1],1,\"$56.7 billion\"],[0,[],0,\". So that's a decent piece of the cake. \"]]],[1,\"p\",[[0,[],0,\"Moving on.\"]]],[1,\"p\",[[0,[],0,\"I am working on a project that I will most likely abandon at some point called cTorrent\"]]],[10,0],[1,\"p\",[[0,[2],1,\"Which you can track the progress on Jira\"]]],[1,\"p\",[[0,[],0,\"Any way.\"]]],[1,\"p\",[[0,[],0,\"In this project, I need to create instances on OVH, firewall rules and volumes. I could manually go to the website and do this, but I want to be able to exert as minimal effort as possible and have the biggest reward possible. \"]]],[1,\"p\",[[0,[],0,\"So we use something called Terraform. \"],[1,[],0,0],[0,[],0,\"Below is an accurate representation of how using terraform will speed up your life\"]]],[10,1],[1,\"p\",[[0,[],0,\"So what is the issue here?\"]]],[1,\"p\",[[0,[],0,\"I found that no matter how much I tried, I could not get authentication to work. Usually you would set up the provider like below. Set the authentication scheme, so here we use an API key.\"]]],[10,2],[10,3],[10,4],[1,\"p\",[[0,[],0,\"Or with google:\"]]],[10,5],[1,\"p\",[[0,[],0,\"Here we set the provider as usual, then the credential file, as well as other fluff like where we want things to pop up in, and what compute zone. \"]]],[1,\"p\",[[0,[],0,\"With OVH it's different - Let me explain.\"]]],[1,\"p\",[[0,[],0,\"OVH uses a technology stack called Openstack, which you can read more about \"],[0,[3],1,\"here\"],[0,[],0,\". Openstack has many API endpoints we can interface with. But the part that caused confusion is the providers. \"]]],[1,\"p\",[[0,[],0,\"You can connect to OVH using their OVH provider on terraform, but for the life of me, I was unable to get authentication working. You were required to create an API key through \"],[0,[4],1,\"some backdoor looking website. \"],[0,[],0,\"Any way, below is what they expect your provider setup to look like:\"]]],[10,6],[1,\"p\",[[0,[],0,\"Now I would like to say, I am by no means an idiot, but this made me feel like an idiot. I just could not get it working.\"]]],[1,\"p\",[[0,[],0,\"Besides that, this provider did not actually support creating compute instances. Odd. \"]]],[1,\"p\",[[0,[],0,\"This is the point at which I had to use my brain and get creative - You remember how I said OVH uses Openstack? Well they make their Openstack endpoints public. \"]]],[1,\"p\",[[0,[],0,\"We will be connecting to something called \"],[0,[5],1,\"Horizon\"],[0,[],0,\", which is the dashboard project for Openstack. \"]]],[10,7],[1,\"p\",[[0,[],0,\"Now to get in to Horizon, we need to create a user.\"]]],[1,\"p\",[[0,[],0,\"Login to \"],[0,[6],1,\"OVH public cloud management portal\"],[0,[],0,\" with your account credentials. \"]]],[1,\"p\",[[0,[],0,\"Once here go to the far left, scroll down to \"],[0,[7],1,\"Users & Roles\"],[0,[],0,\" and create a new user. \"]]],[10,8],[1,\"p\",[[0,[],0,\"Pick \"],[0,[7],1,\"Administrator\"],[0,[],0,\" and then copy the username, and the password to a text file so you can come back to them for the next step.\"]]],[1,\"p\",[[0,[],0,\"Login to Horizon\"],[1,[],0,1],[0,[8],1,\"https://horizon.cloud.ovh.net/auth/login/?next=/\"]]],[1,\"p\",[[0,[],0,\"Paste the username and the password in to their respective fields. Once logged in, go to the top right then click \"],[0,[7],1,\"OpenStack RC File v3\"]]],[10,9],[1,\"p\",[[0,[],0,\"This will download a file which just has numbers and prepended by \"],[0,[7],1,\".sh\"],[0,[],0,\" \"]]],[1,\"p\",[[0,[],0,\"If you're lazy, you can edit this file and hard code the password in to it. To do so:\"]]],[1,\"p\",[[0,[],0,\"Open the file in an editor. On linux just use nano.\"]]],[1,\"p\",[[0,[],0,\"Where it says \"],[0,[7],1,\"export OS_PASSWORD=$OS_PASSWORD_INPUT\"],[0,[],0,\" change it so that \"],[0,[7],1,\"$OS_PASSWORD_INPUT\"],[0,[],0,\" is your password. \"]]],[1,\"p\",[[0,[],0,\"like below:\"]]],[10,10],[1,\"p\",[[0,[],0,\"The reason I've done this is because it sets environment variables for the username, region, password and some other fluff. \"]]],[1,\"p\",[[0,[],0,\"Once this is done, we need to change our provider from OVH, to Openstack. \"]]],[1,\"p\",[[0,[9],1,\"https://registry.terraform.io/providers/terraform-provider-openstack/openstack/latest/docs\"]]],[1,\"p\",[[0,[],0,\"Luckily, their documentation is beautiful so not hard to understand. \"]]],[1,\"p\",[[0,[],0,\"We need to set the provider like:\"]]],[10,11],[1,\"p\",[[0,[],0,\"Let's just create SSH keys to test this -  Format the files as below.\"]]],[10,12],[10,13],[10,14],[10,15],[1,\"p\",[[0,[],0,\"Now we can open the terminal, navigate to the folder and then run \"],[0,[7],1,\"terraform init\"],[0,[],0,\" \"]]],[1,\"p\",[[0,[],0,\"Now run \"],[0,[7],1,\"terraform plan\"]]],[1,\"p\",[[0,[],0,\"Here's where I had the issue that prompted me to write this.\"]]],[10,16],[1,\"p\",[[0,[],0,\"We have the \"],[0,[7],1,\"auth_url\"],[0,[],0,\" set tho? This stumped me for a while.\"]]],[10,17],[1,\"p\",[[0,[],0,\"In the current directory you working in, run\"]]],[10,18],[1,\"p\",[[0,[],0,\"Depending on if you did my cheeky hack, you can press enter, or if not paste the password from earlier. \"]]],[1,\"p\",[[0,[],0,\"Now run Terraform plan and you should see it creating the SSH key. \"]]],[1,\"p\",[[0,[],0,\"From here you're able to play around. Feel free to check out my Terraform code that I have written at like 3 am, so excuse the formatting. Still learning :)\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"Any questions please feel free to reach out to me on linkedin, or find my email address somewhere. \"]]],[10,19],[1,\"p\",[[0,[],0,\"You can hire me via \"],[0,[10],1,\"Upwork\"],[0,[],0,\" or \"],[0,[11],1,\"emailing me\"],[0,[],0,\" for weekend projects!\"]]]],\"ghostVersion\":\"3.0\"}","lexical":null,"html":"<p>OVH is a french cloud provider that has (<a href=\"https://w3techs.com/technologies/details/ho-ovhsas\">According to w3techs</a>) 3.2% of the web hosting market. Whist this does seem small, remember the market is around <a href=\"https://www.grandviewresearch.com/industry-analysis/web-hosting-services-market#:~:text=The%20global%20web%20hosting%20services,Virtual%20Private%20Server%20(VPS).\">$56.7 billion</a>. So that's a decent piece of the cake. </p><p>Moving on.</p><p>I am working on a project that I will most likely abandon at some point called cTorrent</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://github.com/userbradley/cTorrent\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">userbradley/cTorrent</div><div class=\"kg-bookmark-description\">Automatic torrenting though OVH. Contribute to userbradley/cTorrent development by creating an account on GitHub.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://github.githubassets.com/favicons/favicon.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">userbradley</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://avatars2.githubusercontent.com/u/41597815?s&#x3D;400&amp;v&#x3D;4\" alt=\"\"></div></a></figure><p><a href=\"https://jira.breadnet.co.uk/projects/CTOR/issues/CTOR-15?filter=allissues\">Which you can track the progress on Jira</a></p><p>Any way.</p><p>In this project, I need to create instances on OVH, firewall rules and volumes. I could manually go to the website and do this, but I want to be able to exert as minimal effort as possible and have the biggest reward possible. </p><p>So we use something called Terraform. <br>Below is an accurate representation of how using terraform will speed up your life</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2020/12/image.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"792\" height=\"446\" srcset=\"__GHOST_URL__/content/images/size/w600/2020/12/image.png 600w, __GHOST_URL__/content/images/2020/12/image.png 792w\" sizes=\"(min-width: 720px) 720px\"><figcaption>me using terraform to create infrastructure&nbsp;</figcaption></figure><p>So what is the issue here?</p><p>I found that no matter how much I tried, I could not get authentication to work. Usually you would set up the provider like below. Set the authentication scheme, so here we use an API key.</p><figure class=\"kg-card kg-code-card\"><pre><code>provider \"digitalocean\" {\n  token = var.do_token\n}</code></pre><figcaption>provider.tf</figcaption></figure><figure class=\"kg-card kg-code-card\"><pre><code>variable \"do_token\" {\n  type = string\n  description = \"API key to communicate to Digital Ocean with\"\n}</code></pre><figcaption>vars.tf</figcaption></figure><figure class=\"kg-card kg-code-card\"><pre><code>do_token = \"loluthought58ead42e01f1a62d6f422004e69cd5ba775af3b09\"</code></pre><figcaption>terraform.tfvars</figcaption></figure><p>Or with google:</p><pre><code>provider \"google\" {\n  version = \"3.5.0\"\n  credentials = file(\"terraform-c8b2b88693d4.json\")\n\n  project = \"absolute-access-271419\"\n  region  = \"us-central1\"\n  zone    = \"us-central1-c\"\n}</code></pre><p>Here we set the provider as usual, then the credential file, as well as other fluff like where we want things to pop up in, and what compute zone. </p><p>With OVH it's different - Let me explain.</p><p>OVH uses a technology stack called Openstack, which you can read more about <a href=\"https://www.openstack.org\">here</a>. Openstack has many API endpoints we can interface with. But the part that caused confusion is the providers. </p><p>You can connect to OVH using their OVH provider on terraform, but for the life of me, I was unable to get authentication working. You were required to create an API key through <a href=\"https://api.ovh.com/createToken/index.cgi?GET=/*&amp;POST=/*&amp;PUT=/*&amp;DELETE=/*\">some backdoor looking website. </a>Any way, below is what they expect your provider setup to look like:</p><pre><code class=\"language-hcl\">provider \"ovh\" {\n  endpoint           = \"ovh-eu\"\n  application_key    = \"yyyyyy\"\n  application_secret = \"xxxxxxxxxxxxxx\"\n  consumer_key       = \"zzzzzzzzzzzzzz\"\n}</code></pre><p>Now I would like to say, I am by no means an idiot, but this made me feel like an idiot. I just could not get it working.</p><p>Besides that, this provider did not actually support creating compute instances. Odd. </p><p>This is the point at which I had to use my brain and get creative - You remember how I said OVH uses Openstack? Well they make their Openstack endpoints public. </p><p>We will be connecting to something called <a href=\"https://docs.openstack.org/horizon/latest/\">Horizon</a>, which is the dashboard project for Openstack. </p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2020/12/image-1.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1922\" height=\"698\" srcset=\"__GHOST_URL__/content/images/size/w600/2020/12/image-1.png 600w, __GHOST_URL__/content/images/size/w1000/2020/12/image-1.png 1000w, __GHOST_URL__/content/images/size/w1600/2020/12/image-1.png 1600w, __GHOST_URL__/content/images/2020/12/image-1.png 1922w\" sizes=\"(min-width: 720px) 720px\"><figcaption>Horizon for OVH</figcaption></figure><p>Now to get in to Horizon, we need to create a user.</p><p>Login to <a href=\"__GHOST_URL__/terraform-ovh-openstack/www.ovh.com/manager/public-cloud/\">OVH public cloud management portal</a> with your account credentials. </p><p>Once here go to the far left, scroll down to <code>Users &amp; Roles</code> and create a new user. </p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/12/image-2.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"309\" height=\"259\"></figure><p>Pick <code>Administrator</code> and then copy the username, and the password to a text file so you can come back to them for the next step.</p><p>Login to Horizon<br><a href=\"https://horizon.cloud.ovh.net/auth/login/?next=/\">https://horizon.cloud.ovh.net/auth/login/?next=/</a></p><p>Paste the username and the password in to their respective fields. Once logged in, go to the top right then click <code>OpenStack RC File v3</code></p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/12/image-3.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"199\" height=\"263\"></figure><p>This will download a file which just has numbers and prepended by <code>.sh</code> </p><p>If you're lazy, you can edit this file and hard code the password in to it. To do so:</p><p>Open the file in an editor. On linux just use nano.</p><p>Where it says <code>export OS_PASSWORD=$OS_PASSWORD_INPUT</code> change it so that <code>$OS_PASSWORD_INPUT</code> is your password. </p><p>like below:</p><pre><code class=\"language-bash\">export OS_AUTH_URL=https://auth.cloud.ovh.net/v3\n# With the addition of Keystone we have standardized on the term **project**\n# as the entity that owns the resources.\nexport OS_PROJECT_ID=thisisaplaceholder\nexport OS_PROJECT_NAME=\"bunchofnumbersgohere\"\nexport OS_USER_DOMAIN_NAME=\"Default\"\nif [ -z \"$OS_USER_DOMAIN_NAME\" ]; then unset OS_USER_DOMAIN_NAME; fi\nexport OS_PROJECT_DOMAIN_ID=\"default\"\nif [ -z \"$OS_PROJECT_DOMAIN_ID\" ]; then unset OS_PROJECT_DOMAIN_ID; fi\n# unset v2.0 items in case set\nunset OS_TENANT_ID\nunset OS_TENANT_NAME\n# In addition to the owning entity (tenant), OpenStack stores the entity\n# performing the action as the **user**.\nexport OS_USERNAME=\"user-sike\"\n# With Keystone you pass the keystone password.\necho \"Please enter your OpenStack Password for project $OS_PROJECT_NAME as user $OS_USERNAME: \"\nread -sr OS_PASSWORD_INPUT\nexport OS_PASSWORD=whaddyatalkinabout\n# If your configuration has multiple regions, we set that information here.\n# OS_REGION_NAME is optional and only valid in certain environments.\nexport OS_REGION_NAME=\"UK1\"\n# Don't leave a blank variable, unset it if it was empty\nif [ -z \"$OS_REGION_NAME\" ]; then unset OS_REGION_NAME; fi\nexport OS_INTERFACE=public\nexport OS_IDENTITY_API_VERSION=3\n</code></pre><p>The reason I've done this is because it sets environment variables for the username, region, password and some other fluff. </p><p>Once this is done, we need to change our provider from OVH, to Openstack. </p><p><a href=\"https://registry.terraform.io/providers/terraform-provider-openstack/openstack/latest/docs\">https://registry.terraform.io/providers/terraform-provider-openstack/openstack/latest/docs</a></p><p>Luckily, their documentation is beautiful so not hard to understand. </p><p>We need to set the provider like:</p><pre><code>provider \"openstack\" {\n  auth_url = \"https://auth.cloud.ovh.net/v3\"\n  alias = \"ovh\"\n}</code></pre><p>Let's just create SSH keys to test this -  Format the files as below.</p><figure class=\"kg-card kg-code-card\"><pre><code>provider \"openstack\" {\n  auth_url = \"https://auth.cloud.ovh.net/v3\"\n  alias = \"ovh\"\n}</code></pre><figcaption>provider.tf</figcaption></figure><figure class=\"kg-card kg-code-card\"><pre><code>pub_file_location = \"/path/to/your/home/directory/.ssh/id_rsa.pub\"</code></pre><figcaption>terraform.tfvars</figcaption></figure><figure class=\"kg-card kg-code-card\"><pre><code>variable \"pub_file_location\" {\n  type = string\n}</code></pre><figcaption>variables.tf</figcaption></figure><figure class=\"kg-card kg-code-card\"><pre><code>resource \"openstack_compute_keypair_v2\" \"key\" {\n  name       = \"SSH\"\n  public_key = file(var.pub_file_location)\n}</code></pre><figcaption>ssh.tf</figcaption></figure><p>Now we can open the terminal, navigate to the folder and then run <code>terraform init</code> </p><p>Now run <code>terraform plan</code></p><p>Here's where I had the issue that prompted me to write this.</p><pre><code>➜ terraform plan\nRefreshing Terraform state in-memory prior to plan...\nThe refreshed state will be used to calculate this plan, but will not be\npersisted to local or remote state storage.\n\n\nError: One of 'auth_url' or 'cloud' must be specified\n\n  on &lt;empty&gt; line 0:\n  (source code not available)</code></pre><p>We have the <code>auth_url</code> set tho? This stumped me for a while.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/12/image-4.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"243\" height=\"207\"></figure><p>In the current directory you working in, run</p><pre><code>source /path/to/sh/file/we/downloaded/erlier/&lt;name&gt;.sh</code></pre><p>Depending on if you did my cheeky hack, you can press enter, or if not paste the password from earlier. </p><p>Now run Terraform plan and you should see it creating the SSH key. </p><p>From here you're able to play around. Feel free to check out my Terraform code that I have written at like 3 am, so excuse the formatting. Still learning :)</p><p></p><p>Any questions please feel free to reach out to me on linkedin, or find my email address somewhere. </p><hr><p>You can hire me via <a href=\"https://www.upwork.com/freelancers/~01c61ee9802b94133e\">Upwork</a> or <a href=\"mailto:work@breadnet.co.uk\">emailing me</a> for weekend projects!</p>","comment_id":"5fe75406682355289d7cf3da","plaintext":"OVH is a french cloud provider that has (According to w3techs) 3.2% of the web hosting market. Whist this does seem small, remember the market is around $56.7 billion. So that's a decent piece of the cake.\n\nMoving on.\n\nI am working on a project that I will most likely abandon at some point called cTorrent\n\nuserbradley/cTorrentAutomatic torrenting though OVH. Contribute to userbradley/cTorrent development by creating an account on GitHub.GitHubuserbradley\n\nWhich you can track the progress on Jira\n\nAny way.\n\nIn this project, I need to create instances on OVH, firewall rules and volumes. I could manually go to the website and do this, but I want to be able to exert as minimal effort as possible and have the biggest reward possible.\n\nSo we use something called Terraform.\nBelow is an accurate representation of how using terraform will speed up your life\n\nSo what is the issue here?\n\nI found that no matter how much I tried, I could not get authentication to work. Usually you would set up the provider like below. Set the authentication scheme, so here we use an API key.\n\nprovider \"digitalocean\" {\n  token = var.do_token\n}\n\nvariable \"do_token\" {\n  type = string\n  description = \"API key to communicate to Digital Ocean with\"\n}\n\ndo_token = \"loluthought58ead42e01f1a62d6f422004e69cd5ba775af3b09\"\n\nOr with google:\n\nprovider \"google\" {\n  version = \"3.5.0\"\n  credentials = file(\"terraform-c8b2b88693d4.json\")\n\n  project = \"absolute-access-271419\"\n  region  = \"us-central1\"\n  zone    = \"us-central1-c\"\n}\n\nHere we set the provider as usual, then the credential file, as well as other fluff like where we want things to pop up in, and what compute zone.\n\nWith OVH it's different - Let me explain.\n\nOVH uses a technology stack called Openstack, which you can read more about here. Openstack has many API endpoints we can interface with. But the part that caused confusion is the providers.\n\nYou can connect to OVH using their OVH provider on terraform, but for the life of me, I was unable to get authentication working. You were required to create an API key through some backdoor looking website. Any way, below is what they expect your provider setup to look like:\n\nprovider \"ovh\" {\n  endpoint           = \"ovh-eu\"\n  application_key    = \"yyyyyy\"\n  application_secret = \"xxxxxxxxxxxxxx\"\n  consumer_key       = \"zzzzzzzzzzzzzz\"\n}\n\nNow I would like to say, I am by no means an idiot, but this made me feel like an idiot. I just could not get it working.\n\nBesides that, this provider did not actually support creating compute instances. Odd.\n\nThis is the point at which I had to use my brain and get creative - You remember how I said OVH uses Openstack? Well they make their Openstack endpoints public.\n\nWe will be connecting to something called Horizon, which is the dashboard project for Openstack.\n\nNow to get in to Horizon, we need to create a user.\n\nLogin to OVH public cloud management portal with your account credentials.\n\nOnce here go to the far left, scroll down to Users & Roles and create a new user.\n\nPick Administrator and then copy the username, and the password to a text file so you can come back to them for the next step.\n\nLogin to Horizon\nhttps://horizon.cloud.ovh.net/auth/login/?next=/\n\nPaste the username and the password in to their respective fields. Once logged in, go to the top right then click OpenStack RC File v3\n\nThis will download a file which just has numbers and prepended by .sh\n\nIf you're lazy, you can edit this file and hard code the password in to it. To do so:\n\nOpen the file in an editor. On linux just use nano.\n\nWhere it says export OS_PASSWORD=$OS_PASSWORD_INPUT change it so that $OS_PASSWORD_INPUT is your password.\n\nlike below:\n\nexport OS_AUTH_URL=https://auth.cloud.ovh.net/v3\n# With the addition of Keystone we have standardized on the term **project**\n# as the entity that owns the resources.\nexport OS_PROJECT_ID=thisisaplaceholder\nexport OS_PROJECT_NAME=\"bunchofnumbersgohere\"\nexport OS_USER_DOMAIN_NAME=\"Default\"\nif [ -z \"$OS_USER_DOMAIN_NAME\" ]; then unset OS_USER_DOMAIN_NAME; fi\nexport OS_PROJECT_DOMAIN_ID=\"default\"\nif [ -z \"$OS_PROJECT_DOMAIN_ID\" ]; then unset OS_PROJECT_DOMAIN_ID; fi\n# unset v2.0 items in case set\nunset OS_TENANT_ID\nunset OS_TENANT_NAME\n# In addition to the owning entity (tenant), OpenStack stores the entity\n# performing the action as the **user**.\nexport OS_USERNAME=\"user-sike\"\n# With Keystone you pass the keystone password.\necho \"Please enter your OpenStack Password for project $OS_PROJECT_NAME as user $OS_USERNAME: \"\nread -sr OS_PASSWORD_INPUT\nexport OS_PASSWORD=whaddyatalkinabout\n# If your configuration has multiple regions, we set that information here.\n# OS_REGION_NAME is optional and only valid in certain environments.\nexport OS_REGION_NAME=\"UK1\"\n# Don't leave a blank variable, unset it if it was empty\nif [ -z \"$OS_REGION_NAME\" ]; then unset OS_REGION_NAME; fi\nexport OS_INTERFACE=public\nexport OS_IDENTITY_API_VERSION=3\n\n\nThe reason I've done this is because it sets environment variables for the username, region, password and some other fluff.\n\nOnce this is done, we need to change our provider from OVH, to Openstack.\n\nhttps://registry.terraform.io/providers/terraform-provider-openstack/openstack/latest/docs\n\nLuckily, their documentation is beautiful so not hard to understand.\n\nWe need to set the provider like:\n\nprovider \"openstack\" {\n  auth_url = \"https://auth.cloud.ovh.net/v3\"\n  alias = \"ovh\"\n}\n\nLet's just create SSH keys to test this -  Format the files as below.\n\nprovider \"openstack\" {\n  auth_url = \"https://auth.cloud.ovh.net/v3\"\n  alias = \"ovh\"\n}\n\npub_file_location = \"/path/to/your/home/directory/.ssh/id_rsa.pub\"\n\nvariable \"pub_file_location\" {\n  type = string\n}\n\nresource \"openstack_compute_keypair_v2\" \"key\" {\n  name       = \"SSH\"\n  public_key = file(var.pub_file_location)\n}\n\nNow we can open the terminal, navigate to the folder and then run terraform init\n\nNow run terraform plan\n\nHere's where I had the issue that prompted me to write this.\n\n➜ terraform plan\nRefreshing Terraform state in-memory prior to plan...\nThe refreshed state will be used to calculate this plan, but will not be\npersisted to local or remote state storage.\n\n\nError: One of 'auth_url' or 'cloud' must be specified\n\n  on <empty> line 0:\n  (source code not available)\n\nWe have the auth_url set tho? This stumped me for a while.\n\nIn the current directory you working in, run\n\nsource /path/to/sh/file/we/downloaded/erlier/<name>.sh\n\nDepending on if you did my cheeky hack, you can press enter, or if not paste the password from earlier.\n\nNow run Terraform plan and you should see it creating the SSH key.\n\nFrom here you're able to play around. Feel free to check out my Terraform code that I have written at like 3 am, so excuse the formatting. Still learning :)\n\n\n\nAny questions please feel free to reach out to me on linkedin, or find my email address somewhere.\n\nYou can hire me via Upwork or emailing me for weekend projects!","feature_image":"__GHOST_URL__/content/images/2020/12/og-image.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2020-12-26T15:17:26.000Z","updated_at":"2021-05-02T01:45:40.000Z","published_at":"2020-12-26T16:38:30.000Z","custom_excerpt":"Having issues terraforming OVH and Openstack? Well, I think I solved it for you. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"6425f3bc192c0c150413d59d","uuid":"61dd53bc-c56a-4481-a5b3-3af896dcd1f0","title":"Moving to LDAP","slug":"moving-to-ldap","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"hr\",{}],[\"hr\",{}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/01/image-1.png\",\"width\":948,\"height\":504}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/01/image-2.png\",\"width\":1507,\"height\":883}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/01/image-4.png\",\"width\":1033,\"height\":412}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/01/image-5.png\",\"width\":365,\"height\":313}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/01/image-6.png\",\"width\":345,\"height\":217}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/01/image-7.png\",\"width\":697,\"height\":140}],[\"markdown\",{\"markdown\":\"|Field name | what to put there |\\n| --- | --- |\\n| Name | Jumpcloud |\\n| Directory type | OpenLDAP |\\n| Hostname | ldap.jumpcloud.com |\\n| Port | 636 (use SSL |\\n| Username | bind username dn |\\n| Password | Password of bind dn user |\\n| Base DN | Copy thr org DN and add `ou=Users,` to the front |\\n| LDAP Permissions | Read only, with Local groups |\\n| Secure SSL | Tick box |\\n| Naive DN Matching | Tick box |\\n| Update group membership when logging in | Every time the user logs in |\\n| Synchronisation Interval (minutes) | 2 |\\n| User Object Class | inetorgperson |\\n| User Object Filter | (objectclass=inetorgperson) |\\n| User Name Attribute | uid |\\n| User Name RDN Attribute | uid |\\n| User First Name Attribute | givenName |\\n| User Last Name Attribute | sn |\\n| User Display Name Attribute | displayName |\\n| User Email Attribute | mail |\\n| User Password Attribute | userPassword |\\n| User Password Encryption | SHA |\\n| User Unique ID Attribute | entryUUID |\\n| Group Object Class | groupOfNames |\\n| Group Object Filter | (objectclass=groupOfNames) |\\n| Group Name Attribute | cn |\\n| Group Description Attribute | cn |\\n| Group Members Attribute | member |\\n| User Membership Attribute | memberOf |\\n\\n\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/01/image-8.png\",\"width\":895,\"height\":713}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/01/image-9.png\",\"width\":920,\"height\":531}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/01/image-10.png\",\"width\":699,\"height\":702}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/01/image-11.png\",\"width\":644,\"height\":303}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/01/image-12.png\",\"width\":690,\"height\":244}],[\"hr\",{}],[\"code\",{\"code\":\"APP_DEBUG=true #change to false once it works\\n\\nAUTH_METHOD=ldap\\nLDAP_SERVER=ldaps://ldap.jumpcloud.com:636\\nLDAP_BASE_DN=ou=Users,o=<org id>,dc=jumpcloud,dc=com\\nLDAP_DN=uid=bind,ou=Users,o=<org id>,dc=jumpcloud,dc=com\\nLDAP_PASS=<bind user password>\\nLDAP_USER_FILTER=(&(uid=${user}))\\nLDAP_VERSION=3\\nLDAP_ID_ATTRIBUTE=uid\\nLDAP_EMAIL_ATTRIBUTE=mail\\nLDAP_DISPLAY_NAME_ATTRIBUTE=cn\\nLDAP_USER_TO_GROUPS=true\\nLDAP_GROUP_ATTRIBUTE=\\\"memberOf\\\"\\nLDAP_REMOVE_FROM_GROUPS=false\\n\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/01/image-13.png\",\"width\":755,\"height\":142}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/01/image-14.png\",\"width\":1878,\"height\":1297}],[\"hr\",{}]],\"markups\":[[\"strong\"],[\"a\",[\"href\",\"https://en.wikipedia.org/wiki/Multi-factor_authentication\"]],[\"a\",[\"href\",\"__GHOST_URL__/tag/how-to/\"]],[\"a\",[\"href\",\"__GHOST_URL__/p/61dd53bc-c56a-4481-a5b3-3af896dcd1f0/jumpcloud.com/\"]],[\"em\"],[\"code\"],[\"a\",[\"href\",\"https://www.upwork.com/freelancers/~01c61ee9802b94133e\"]],[\"a\",[\"href\",\"mailto:work@breadnet.co.uk\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I run 2 \"],[0,[0],1,\"incredibly \"],[0,[],0,\"important pieces of software here at breadNET\"]]],[3,\"ol\",[[[0,[],0,\"Jira (Project management tool)\"]],[[0,[],0,\"Bookstack (Knowledge base articles) \"]]]],[1,\"p\",[[0,[],0,\"The annoyance comes from having to login to them with 2 separate accounts and a lack of \"],[0,[1],1,\"2 factor authentication\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"The aim of this blog post is less of a blog, and more of a How to, hence the \"],[0,[2],1,\"How to\"],[0,[],0,\" tag :) \"]]],[1,\"p\",[[0,[],0,\"First we will look in to the platform I have decided to go with, why I chose it and what other options there are. \"]]],[1,\"p\",[[0,[],0,\"Secondly we will look at setting up LDAP auth for Jira and then Bookstack as well as the issues I faced. \"]]],[10,0],[1,\"h3\",[[0,[],0,\"What LDAP system I have gone with?\"]]],[1,\"p\",[[0,[],0,\"At my old job back as a welpdesk engineer, I was tasked with looking in to a SaaS based platform for windows based authentication. I had come across a platform called \"],[0,[3],1,\"JumpCloud\"],[0,[],0,\" which is a cloud hosted directory platform which pride them selves on \\\"\"],[0,[4],1,\"An open directory platform for secure, friction-less access from any device to any resource, anywhere\"],[0,[],0,\"\\\"  and from the testing and deployments I have done, I can stand behind them and say this is indeed true.\"]]],[1,\"h3\",[[0,[],0,\"Why I chose Jumpcloud\"]]],[1,\"p\",[[0,[],0,\"There are a few reasons on why I chose to go with Jumpcloud over Okta and Microsoft Azure Active directory\"]]],[3,\"ol\",[[[0,[],0,\"Jumpcloud is free for up to 10 users, for ever\"]],[[0,[],0,\"I have experience with it\"]],[[0,[],0,\"I have had long conversations with their sales team, so I know the in's and out's of their platform\"]],[[0,[],0,\"It's free. \"]]]],[10,1],[1,\"h1\",[[0,[],0,\"Setting up Jira with Jumpcloud\"]]],[1,\"p\",[[0,[],0,\"The first step is to sign up to jumpcloud. It's free, so you don't even need to setup a credit card. \"]]],[1,\"p\",[[0,[],0,\"Our first step is to create a Bind user. This is the user that Jira and Bookstack will authenticate with to search the directory. \"]]],[1,\"p\",[[0,[],0,\"There are rules with this password that I have found make life easier\"]]],[3,\"ol\",[[[0,[],0,\"No funky charachters (!'@ etc)\"]],[[0,[],0,\"No \"],[0,[5],1,\"#\"],[0,[],0,\" as in \"],[0,[5],1,\".env\"],[0,[],0,\" files and most config files, this is a comment\"]]]],[1,\"p\",[[0,[],0,\"Create the user, don't bother with their name and stuff, just make the username \"],[0,[5],1,\"bind\"],[0,[],0,\" and set the password. Set their email as bind@example.org or a test email account your organisation has -  Ensure you set the password as below or you may face issues. \"]]],[10,2],[1,\"p\",[[0,[],0,\"Ensure that you click \"],[0,[5],1,\"Enable as LDAP Bind DN\"],[0,[],0,\" \"]]],[1,\"p\",[[0,[],0,\"Once done, click back in and copy the \"],[0,[5],1,\"Base DN\"]]],[1,\"p\",[[0,[],0,\"In my case it's \"],[0,[5],1,\"uid=bind,ou=Users,o=5ff9f63bb2433c378b9d8d18,dc=jumpcloud,dc=com\"]]],[1,\"p\",[[0,[],0,\"Once this is done, we can head over to the groups and start creating groups.\"]]],[1,\"p\",[[0,[],0,\"We need these groups to reflect their \"],[0,[0],1,\"exact name\"],[0,[],0,\" in jira. Example below:\"]]],[10,3],[1,\"p\",[[0,[],0,\"Two groups that we need to ensure we have is \"],[0,[5],1,\"jira-software-users\"],[0,[],0,\" and \"],[0,[5],1,\"jira-administrators\"],[0,[],0,\" as without these groups, adding new admins and users becomes pointless as we will need to then go in to jira and add them to the group, which kind of defeats the point of one source of truth.\"]]],[1,\"p\",[[0,[],0,\"Ensure that the group is added to the directory:\"]]],[10,4],[1,\"p\",[[0,[],0,\"Once this is done, we can go and create a user and then ensure we add them to 2 things, the LDAP directory, and then a jira group we created. \"]]],[1,\"p\",[[0,[],0,\"Finally, create a user, add them to the \"],[0,[5],1,\"jira-software-users\"],[0,[],0,\" group, and to the Directory. \"]]],[1,\"p\",[[0,[],0,\"Now we can move on to setting up jira. \"]]],[1,\"p\",[[0,[],0,\"Login to Jira server, click settings at the top then select \"],[0,[5],1,\"User Management\"]]],[10,5],[1,\"p\",[[0,[],0,\"On the left hand side, select \"],[0,[5],1,\"User Directories\"],[0,[],0,\" then click \"],[0,[5],1,\"Add Directory\"],[0,[],0,\" then select LDAP\"]]],[10,6],[1,\"p\",[[0,[],0,\"Ensure that we set it to use OpenLDAP\"]]],[10,7],[1,\"p\",[[0,[],0,\"I will break down each field and what to put in there\"]]],[10,8],[10,9],[10,10],[10,11],[10,12],[10,13],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"Then click save and test. \"]]],[1,\"p\",[[0,[],0,\"Go to user management and go to User Directories, then click Sync.\"]]],[1,\"p\",[[0,[],0,\"Nice! Now it's set up! \"]]],[1,\"p\",[[0,[],0,\"You should see the test user appear and be able to login with their username and password. \"]]],[1,\"p\",[[0,[],0,\"If you want to sync users that already have local jira accounts, to ldap, just create their user in Jumpcloud with their email address, username and a new password. \"]]],[1,\"p\",[[0,[],0,\"Synch the directory and then delete the local user. You should see a message saying that the user exists in 2 directories. If you don't see this, stop. Fix the synch. \"]]],[10,14],[1,\"h1\",[[0,[],0,\"Bookstack\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"Bookstack has some realllllly beautiful documentation so this was a pure breeze, but there's some issues that I came across.\"]]],[1,\"p\",[[0,[],0,\"I highly suggest that you start this with creating users and groups in Jumpcloud before making config changes.\"]]],[1,\"p\",[[0,[],0,\"Now we can ssh to the server and start to hack at the .env file.\"]]],[1,\"p\",[[0,[],0,\"Firstly login as the admin account to your instance, then go to users and your profile. \"]]],[1,\"p\",[[0,[],0,\"We need to add the below to the file.\"]]],[10,15],[1,\"p\",[[0,[],0,\"Replace \"],[0,[5],1,\"<org id>\"],[0,[],0,\" with your org id from jumpcloud and \"],[0,[5],1,\"<bind user password>\"],[0,[],0,\" with the bind user's password.\"]]],[1,\"p\",[[0,[],0,\"Restart nginx or apache or what ever you're running and login with a user account.\"]]],[1,\"p\",[[0,[],0,\"Refresh the page on Bookstack and you will see the below\"]]],[10,16],[1,\"p\",[[0,[],0,\"Fill the box out with the username that you've used in jumpcloud for your account, then open an incognito window and then try logging in with your jumucloud username\"]]],[1,\"p\",[[0,[],0,\"If you see this, then you're in trouble! Get in contact and I can see what I can do to help!\"]]],[10,17],[1,\"p\",[[0,[],0,\"Once you've verified that logins work, edit the \"],[0,[5],1,\".env\"],[0,[],0,\" file set \"],[0,[5],1,\"APP_DEBUG=false\"],[0,[],0,\" and restart nginx.\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"Congrats, you now have LDAP setup!\"]]],[10,18],[1,\"p\",[[0,[],0,\"You can hire me via \"],[0,[6],1,\"Upwork\"],[0,[],0,\" or \"],[0,[7],1,\"emailing me\"],[0,[],0,\" for weekend projects!\"]]]],\"ghostVersion\":\"3.0\"}","lexical":null,"html":"<p>I run 2 <strong>incredibly </strong>important pieces of software here at breadNET</p><ol><li>Jira (Project management tool)</li><li>Bookstack (Knowledge base articles) </li></ol><p>The annoyance comes from having to login to them with 2 separate accounts and a lack of <a href=\"https://en.wikipedia.org/wiki/Multi-factor_authentication\">2 factor authentication</a>.</p><p>The aim of this blog post is less of a blog, and more of a How to, hence the <a href=\"__GHOST_URL__/tag/how-to/\">How to</a> tag :) </p><p>First we will look in to the platform I have decided to go with, why I chose it and what other options there are. </p><p>Secondly we will look at setting up LDAP auth for Jira and then Bookstack as well as the issues I faced. </p><hr><h3 id=\"what-ldap-system-i-have-gone-with\">What LDAP system I have gone with?</h3><p>At my old job back as a welpdesk engineer, I was tasked with looking in to a SaaS based platform for windows based authentication. I had come across a platform called <a href=\"__GHOST_URL__/p/61dd53bc-c56a-4481-a5b3-3af896dcd1f0/jumpcloud.com/\">JumpCloud</a> which is a cloud hosted directory platform which pride them selves on \"<em>An open directory platform for secure, friction-less access from any device to any resource, anywhere</em>\"  and from the testing and deployments I have done, I can stand behind them and say this is indeed true.</p><h3 id=\"why-i-chose-jumpcloud\">Why I chose Jumpcloud</h3><p>There are a few reasons on why I chose to go with Jumpcloud over Okta and Microsoft Azure Active directory</p><ol><li>Jumpcloud is free for up to 10 users, for ever</li><li>I have experience with it</li><li>I have had long conversations with their sales team, so I know the in's and out's of their platform</li><li>It's free. </li></ol><hr><h1 id=\"setting-up-jira-with-jumpcloud\">Setting up Jira with Jumpcloud</h1><p>The first step is to sign up to jumpcloud. It's free, so you don't even need to setup a credit card. </p><p>Our first step is to create a Bind user. This is the user that Jira and Bookstack will authenticate with to search the directory. </p><p>There are rules with this password that I have found make life easier</p><ol><li>No funky charachters (!'@ etc)</li><li>No <code>#</code> as in <code>.env</code> files and most config files, this is a comment</li></ol><p>Create the user, don't bother with their name and stuff, just make the username <code>bind</code> and set the password. Set their email as bind@example.org or a test email account your organisation has -  Ensure you set the password as below or you may face issues. </p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2021/01/image-1.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"948\" height=\"504\" srcset=\"__GHOST_URL__/content/images/size/w600/2021/01/image-1.png 600w, __GHOST_URL__/content/images/2021/01/image-1.png 948w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Ensure that you click <code>Enable as LDAP Bind DN</code> </p><p>Once done, click back in and copy the <code>Base DN</code></p><p>In my case it's <code>uid=bind,ou=Users,o=5ff9f63bb2433c378b9d8d18,dc=jumpcloud,dc=com</code></p><p>Once this is done, we can head over to the groups and start creating groups.</p><p>We need these groups to reflect their <strong>exact name</strong> in jira. Example below:</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2021/01/image-2.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1507\" height=\"883\" srcset=\"__GHOST_URL__/content/images/size/w600/2021/01/image-2.png 600w, __GHOST_URL__/content/images/size/w1000/2021/01/image-2.png 1000w, __GHOST_URL__/content/images/2021/01/image-2.png 1507w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Two groups that we need to ensure we have is <code>jira-software-users</code> and <code>jira-administrators</code> as without these groups, adding new admins and users becomes pointless as we will need to then go in to jira and add them to the group, which kind of defeats the point of one source of truth.</p><p>Ensure that the group is added to the directory:</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2021/01/image-4.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1033\" height=\"412\" srcset=\"__GHOST_URL__/content/images/size/w600/2021/01/image-4.png 600w, __GHOST_URL__/content/images/size/w1000/2021/01/image-4.png 1000w, __GHOST_URL__/content/images/2021/01/image-4.png 1033w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Once this is done, we can go and create a user and then ensure we add them to 2 things, the LDAP directory, and then a jira group we created. </p><p>Finally, create a user, add them to the <code>jira-software-users</code> group, and to the Directory. </p><p>Now we can move on to setting up jira. </p><p>Login to Jira server, click settings at the top then select <code>User Management</code></p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2021/01/image-5.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"365\" height=\"313\"></figure><p>On the left hand side, select <code>User Directories</code> then click <code>Add Directory</code> then select LDAP</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2021/01/image-6.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"345\" height=\"217\"></figure><p>Ensure that we set it to use OpenLDAP</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2021/01/image-7.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"697\" height=\"140\" srcset=\"__GHOST_URL__/content/images/size/w600/2021/01/image-7.png 600w, __GHOST_URL__/content/images/2021/01/image-7.png 697w\"></figure><p>I will break down each field and what to put in there</p><!--kg-card-begin: markdown--><table>\n<thead>\n<tr>\n<th>Field name</th>\n<th>what to put there</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Name</td>\n<td>Jumpcloud</td>\n</tr>\n<tr>\n<td>Directory type</td>\n<td>OpenLDAP</td>\n</tr>\n<tr>\n<td>Hostname</td>\n<td>ldap.jumpcloud.com</td>\n</tr>\n<tr>\n<td>Port</td>\n<td>636 (use SSL</td>\n</tr>\n<tr>\n<td>Username</td>\n<td>bind username dn</td>\n</tr>\n<tr>\n<td>Password</td>\n<td>Password of bind dn user</td>\n</tr>\n<tr>\n<td>Base DN</td>\n<td>Copy thr org DN and add <code>ou=Users,</code> to the front</td>\n</tr>\n<tr>\n<td>LDAP Permissions</td>\n<td>Read only, with Local groups</td>\n</tr>\n<tr>\n<td>Secure SSL</td>\n<td>Tick box</td>\n</tr>\n<tr>\n<td>Naive DN Matching</td>\n<td>Tick box</td>\n</tr>\n<tr>\n<td>Update group membership when logging in</td>\n<td>Every time the user logs in</td>\n</tr>\n<tr>\n<td>Synchronisation Interval (minutes)</td>\n<td>2</td>\n</tr>\n<tr>\n<td>User Object Class</td>\n<td>inetorgperson</td>\n</tr>\n<tr>\n<td>User Object Filter</td>\n<td>(objectclass=inetorgperson)</td>\n</tr>\n<tr>\n<td>User Name Attribute</td>\n<td>uid</td>\n</tr>\n<tr>\n<td>User Name RDN Attribute</td>\n<td>uid</td>\n</tr>\n<tr>\n<td>User First Name Attribute</td>\n<td>givenName</td>\n</tr>\n<tr>\n<td>User Last Name Attribute</td>\n<td>sn</td>\n</tr>\n<tr>\n<td>User Display Name Attribute</td>\n<td>displayName</td>\n</tr>\n<tr>\n<td>User Email Attribute</td>\n<td>mail</td>\n</tr>\n<tr>\n<td>User Password Attribute</td>\n<td>userPassword</td>\n</tr>\n<tr>\n<td>User Password Encryption</td>\n<td>SHA</td>\n</tr>\n<tr>\n<td>User Unique ID Attribute</td>\n<td>entryUUID</td>\n</tr>\n<tr>\n<td>Group Object Class</td>\n<td>groupOfNames</td>\n</tr>\n<tr>\n<td>Group Object Filter</td>\n<td>(objectclass=groupOfNames)</td>\n</tr>\n<tr>\n<td>Group Name Attribute</td>\n<td>cn</td>\n</tr>\n<tr>\n<td>Group Description Attribute</td>\n<td>cn</td>\n</tr>\n<tr>\n<td>Group Members Attribute</td>\n<td>member</td>\n</tr>\n<tr>\n<td>User Membership Attribute</td>\n<td>memberOf</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: markdown--><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2021/01/image-8.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"895\" height=\"713\" srcset=\"__GHOST_URL__/content/images/size/w600/2021/01/image-8.png 600w, __GHOST_URL__/content/images/2021/01/image-8.png 895w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2021/01/image-9.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"920\" height=\"531\" srcset=\"__GHOST_URL__/content/images/size/w600/2021/01/image-9.png 600w, __GHOST_URL__/content/images/2021/01/image-9.png 920w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2021/01/image-10.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"699\" height=\"702\" srcset=\"__GHOST_URL__/content/images/size/w600/2021/01/image-10.png 600w, __GHOST_URL__/content/images/2021/01/image-10.png 699w\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2021/01/image-11.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"644\" height=\"303\" srcset=\"__GHOST_URL__/content/images/size/w600/2021/01/image-11.png 600w, __GHOST_URL__/content/images/2021/01/image-11.png 644w\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2021/01/image-12.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"690\" height=\"244\" srcset=\"__GHOST_URL__/content/images/size/w600/2021/01/image-12.png 600w, __GHOST_URL__/content/images/2021/01/image-12.png 690w\"></figure><p></p><p>Then click save and test. </p><p>Go to user management and go to User Directories, then click Sync.</p><p>Nice! Now it's set up! </p><p>You should see the test user appear and be able to login with their username and password. </p><p>If you want to sync users that already have local jira accounts, to ldap, just create their user in Jumpcloud with their email address, username and a new password. </p><p>Synch the directory and then delete the local user. You should see a message saying that the user exists in 2 directories. If you don't see this, stop. Fix the synch. </p><hr><h1 id=\"bookstack\">Bookstack</h1><p></p><p>Bookstack has some realllllly beautiful documentation so this was a pure breeze, but there's some issues that I came across.</p><p>I highly suggest that you start this with creating users and groups in Jumpcloud before making config changes.</p><p>Now we can ssh to the server and start to hack at the .env file.</p><p>Firstly login as the admin account to your instance, then go to users and your profile. </p><p>We need to add the below to the file.</p><pre><code>APP_DEBUG=true #change to false once it works\n\nAUTH_METHOD=ldap\nLDAP_SERVER=ldaps://ldap.jumpcloud.com:636\nLDAP_BASE_DN=ou=Users,o=&lt;org id&gt;,dc=jumpcloud,dc=com\nLDAP_DN=uid=bind,ou=Users,o=&lt;org id&gt;,dc=jumpcloud,dc=com\nLDAP_PASS=&lt;bind user password&gt;\nLDAP_USER_FILTER=(&amp;(uid=${user}))\nLDAP_VERSION=3\nLDAP_ID_ATTRIBUTE=uid\nLDAP_EMAIL_ATTRIBUTE=mail\nLDAP_DISPLAY_NAME_ATTRIBUTE=cn\nLDAP_USER_TO_GROUPS=true\nLDAP_GROUP_ATTRIBUTE=\"memberOf\"\nLDAP_REMOVE_FROM_GROUPS=false\n</code></pre><p>Replace <code>&lt;org id&gt;</code> with your org id from jumpcloud and <code>&lt;bind user password&gt;</code> with the bind user's password.</p><p>Restart nginx or apache or what ever you're running and login with a user account.</p><p>Refresh the page on Bookstack and you will see the below</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2021/01/image-13.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"755\" height=\"142\" srcset=\"__GHOST_URL__/content/images/size/w600/2021/01/image-13.png 600w, __GHOST_URL__/content/images/2021/01/image-13.png 755w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Fill the box out with the username that you've used in jumpcloud for your account, then open an incognito window and then try logging in with your jumucloud username</p><p>If you see this, then you're in trouble! Get in contact and I can see what I can do to help!</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2021/01/image-14.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1878\" height=\"1297\" srcset=\"__GHOST_URL__/content/images/size/w600/2021/01/image-14.png 600w, __GHOST_URL__/content/images/size/w1000/2021/01/image-14.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/01/image-14.png 1600w, __GHOST_URL__/content/images/2021/01/image-14.png 1878w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Once you've verified that logins work, edit the <code>.env</code> file set <code>APP_DEBUG=false</code> and restart nginx.</p><p></p><p>Congrats, you now have LDAP setup!</p><hr><p>You can hire me via <a href=\"https://www.upwork.com/freelancers/~01c61ee9802b94133e\">Upwork</a> or <a href=\"mailto:work@breadnet.co.uk\">emailing me</a> for weekend projects!</p>","comment_id":"5ff9e592682355289d7cf4ac","plaintext":"I run 2 incredibly important pieces of software here at breadNET\n\n 1. Jira (Project management tool)\n 2. Bookstack (Knowledge base articles)\n\nThe annoyance comes from having to login to them with 2 separate accounts and a lack of 2 factor authentication.\n\nThe aim of this blog post is less of a blog, and more of a How to, hence the How to tag :)\n\nFirst we will look in to the platform I have decided to go with, why I chose it and what other options there are.\n\nSecondly we will look at setting up LDAP auth for Jira and then Bookstack as well as the issues I faced.\n\n\nWhat LDAP system I have gone with?\n\nAt my old job back as a welpdesk engineer, I was tasked with looking in to a SaaS based platform for windows based authentication. I had come across a platform called JumpCloud which is a cloud hosted directory platform which pride them selves on \"An open directory platform for secure, friction-less access from any device to any resource, anywhere\"  and from the testing and deployments I have done, I can stand behind them and say this is indeed true.\n\n\nWhy I chose Jumpcloud\n\nThere are a few reasons on why I chose to go with Jumpcloud over Okta and Microsoft Azure Active directory\n\n 1. Jumpcloud is free for up to 10 users, for ever\n 2. I have experience with it\n 3. I have had long conversations with their sales team, so I know the in's and out's of their platform\n 4. It's free.\n\n\nSetting up Jira with Jumpcloud\n\nThe first step is to sign up to jumpcloud. It's free, so you don't even need to setup a credit card.\n\nOur first step is to create a Bind user. This is the user that Jira and Bookstack will authenticate with to search the directory.\n\nThere are rules with this password that I have found make life easier\n\n 1. No funky charachters (!'@ etc)\n 2. No # as in .env files and most config files, this is a comment\n\nCreate the user, don't bother with their name and stuff, just make the username bind and set the password. Set their email as bind@example.org or a test email account your organisation has -  Ensure you set the password as below or you may face issues.\n\nEnsure that you click Enable as LDAP Bind DN\n\nOnce done, click back in and copy the Base DN\n\nIn my case it's uid=bind,ou=Users,o=5ff9f63bb2433c378b9d8d18,dc=jumpcloud,dc=com\n\nOnce this is done, we can head over to the groups and start creating groups.\n\nWe need these groups to reflect their exact name in jira. Example below:\n\nTwo groups that we need to ensure we have is jira-software-users and jira-administrators as without these groups, adding new admins and users becomes pointless as we will need to then go in to jira and add them to the group, which kind of defeats the point of one source of truth.\n\nEnsure that the group is added to the directory:\n\nOnce this is done, we can go and create a user and then ensure we add them to 2 things, the LDAP directory, and then a jira group we created.\n\nFinally, create a user, add them to the jira-software-users group, and to the Directory.\n\nNow we can move on to setting up jira.\n\nLogin to Jira server, click settings at the top then select User Management\n\nOn the left hand side, select User Directories then click Add Directory then select LDAP\n\nEnsure that we set it to use OpenLDAP\n\nI will break down each field and what to put in there\n\n\n\n\nField name\nwhat to put there\n\n\n\n\nName\nJumpcloud\n\n\nDirectory type\nOpenLDAP\n\n\nHostname\nldap.jumpcloud.com\n\n\nPort\n636 (use SSL\n\n\nUsername\nbind username dn\n\n\nPassword\nPassword of bind dn user\n\n\nBase DN\nCopy thr org DN and add ou=Users, to the front\n\n\nLDAP Permissions\nRead only, with Local groups\n\n\nSecure SSL\nTick box\n\n\nNaive DN Matching\nTick box\n\n\nUpdate group membership when logging in\nEvery time the user logs in\n\n\nSynchronisation Interval (minutes)\n2\n\n\nUser Object Class\ninetorgperson\n\n\nUser Object Filter\n(objectclass=inetorgperson)\n\n\nUser Name Attribute\nuid\n\n\nUser Name RDN Attribute\nuid\n\n\nUser First Name Attribute\ngivenName\n\n\nUser Last Name Attribute\nsn\n\n\nUser Display Name Attribute\ndisplayName\n\n\nUser Email Attribute\nmail\n\n\nUser Password Attribute\nuserPassword\n\n\nUser Password Encryption\nSHA\n\n\nUser Unique ID Attribute\nentryUUID\n\n\nGroup Object Class\ngroupOfNames\n\n\nGroup Object Filter\n(objectclass=groupOfNames)\n\n\nGroup Name Attribute\ncn\n\n\nGroup Description Attribute\ncn\n\n\nGroup Members Attribute\nmember\n\n\nUser Membership Attribute\nmemberOf\n\n\n\n\n\n\n\nThen click save and test.\n\nGo to user management and go to User Directories, then click Sync.\n\nNice! Now it's set up!\n\nYou should see the test user appear and be able to login with their username and password.\n\nIf you want to sync users that already have local jira accounts, to ldap, just create their user in Jumpcloud with their email address, username and a new password.\n\nSynch the directory and then delete the local user. You should see a message saying that the user exists in 2 directories. If you don't see this, stop. Fix the synch.\n\n\nBookstack\n\n\n\nBookstack has some realllllly beautiful documentation so this was a pure breeze, but there's some issues that I came across.\n\nI highly suggest that you start this with creating users and groups in Jumpcloud before making config changes.\n\nNow we can ssh to the server and start to hack at the .env file.\n\nFirstly login as the admin account to your instance, then go to users and your profile.\n\nWe need to add the below to the file.\n\nAPP_DEBUG=true #change to false once it works\n\nAUTH_METHOD=ldap\nLDAP_SERVER=ldaps://ldap.jumpcloud.com:636\nLDAP_BASE_DN=ou=Users,o=<org id>,dc=jumpcloud,dc=com\nLDAP_DN=uid=bind,ou=Users,o=<org id>,dc=jumpcloud,dc=com\nLDAP_PASS=<bind user password>\nLDAP_USER_FILTER=(&(uid=${user}))\nLDAP_VERSION=3\nLDAP_ID_ATTRIBUTE=uid\nLDAP_EMAIL_ATTRIBUTE=mail\nLDAP_DISPLAY_NAME_ATTRIBUTE=cn\nLDAP_USER_TO_GROUPS=true\nLDAP_GROUP_ATTRIBUTE=\"memberOf\"\nLDAP_REMOVE_FROM_GROUPS=false\n\n\nReplace <org id> with your org id from jumpcloud and <bind user password> with the bind user's password.\n\nRestart nginx or apache or what ever you're running and login with a user account.\n\nRefresh the page on Bookstack and you will see the below\n\nFill the box out with the username that you've used in jumpcloud for your account, then open an incognito window and then try logging in with your jumucloud username\n\nIf you see this, then you're in trouble! Get in contact and I can see what I can do to help!\n\nOnce you've verified that logins work, edit the .env file set APP_DEBUG=false and restart nginx.\n\n\n\nCongrats, you now have LDAP setup!\n\nYou can hire me via Upwork or emailing me for weekend projects!","feature_image":"__GHOST_URL__/content/images/2021/01/maxresdefault.jpg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2021-01-09T17:19:14.000Z","updated_at":"2021-05-02T01:45:08.000Z","published_at":"2021-01-09T23:43:34.000Z","custom_excerpt":"Sounds easy, but how easy was it?","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"6425f3bc192c0c150413d59e","uuid":"b7a610aa-15a1-4bd4-a522-04ecab430351","title":"Moving to the cloud: Infrastructure","slug":"moving-to-the-cloud-2","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"bookmark\",{\"url\":\"__GHOST_URL__/cloud-migration-part-1/\",\"metadata\":{\"url\":\"__GHOST_URL__/cloud-migration-part-1/\",\"title\":\"breadNET Cloud Migration\",\"description\":\"How did I move all my servers to the cloud? Well - Ansible, automation and CI/CD!\",\"author\":\"Bradley Stannard\",\"publisher\":\"breadNET\",\"thumbnail\":\"https://images.unsplash.com/photo-1560182413-53772f3d7134?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=2000&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ\",\"icon\":\"https://breadnet.co.uk/favicon.png\"}}],[\"bookmark\",{\"url\":\"https://gitlab.breadnet.co.uk/terraform/breadnet-ovh\",\"metadata\":{\"url\":\"http://gitlab.breadnet.co.uk/terraform/breadnet-ovh\",\"title\":\"terraform / breadNET OVH\",\"description\":\"The OVH terraform for breadNET cloud servers\",\"author\":null,\"publisher\":\"GitLab\",\"thumbnail\":\"http://gitlab.breadnet.co.uk/assets/gitlab_logo-7ae504fe4f68fdebb3c2034e36621930cd36ea87924c11ff65dbcb8ed50dca58.png\",\"icon\":\"https://gitlab.breadnet.co.uk/assets/touch-icon-ipad-retina-8ebe416f5313483d9c1bc772b5bbe03ecad52a54eba443e5215a22caed2a16a2.png\"}}],[\"hr\",{}],[\"bookmark\",{\"url\":\"https://jira.breadnet.co.uk/browse/BCM-2\",\"metadata\":{\"url\":\"https://jira.breadnet.co.uk/browse/BCM-2\",\"title\":\"[BCM-2] List apps that need to be moved - breadNET Jira\",\"description\":null,\"author\":null,\"publisher\":\"JIRA\",\"thumbnail\":\"https://jira.breadnet.co.uk/secure/projectavatar?avatarId=10324\",\"icon\":\"https://jira.breadnet.co.uk/s/q6useh/814001/75e65367690ac553d207206292c2b417/_/jira-favicon-hires.png\"}}],[\"code\",{\"code\":\"traceroute to 1.1.1.1 (1.1.1.1), 64 hops max, 52 byte packets\\n 1  192.168.0.1 (192.168.0.1)  4.404 ms  2.788 ms  2.798 ms\\n 2  * * *\\n 3  brhm-core-2b-et-315-0.network.virginmedia.net (81.110.128.5)  19.415 ms  18.775 ms  18.458 ms\\n 4  * * *\\n 5  * * *\\n 6  tcma-ic-2-ae9-0.network.virginmedia.net (62.253.174.178)  20.490 ms  20.312 ms  17.615 ms\\n 7  162.158.32.254 (162.158.32.254)  32.277 ms  53.136 ms  38.476 ms\\n 8  162.158.32.9 (162.158.32.9)  31.605 ms  32.833 ms  35.764 ms\\n 9  one.one.one.one (1.1.1.1)  31.442 ms  31.381 ms  33.273 ms\",\"language\":\"bash\"}],[\"code\",{\"code\":\"resource \\\"openstack_compute_instance_v2\\\" \\\"instance\\\" {\\n  name = var.name\\n  flavor_name = \\\"s1-2\\\"\\n  key_pair = var.key\\n  image_name = \\\"Ubuntu 18.04\\\"\\n  security_groups = var.sg\\n  network {\\n    port = openstack_networking_port_v2.internet.id\\n  }\\n  network {\\n    port = openstack_networking_port_v2.internal.id\\n  }\\n}\\n\\nresource \\\"openstack_networking_port_v2\\\" \\\"internal\\\" {\\n  name           = \\\"${var.name}-backend\\\"\\n  network_id     = var.netid\\n  admin_state_up = \\\"true\\\"\\n\\n  fixed_ip {\\n    subnet_id  = var.subid\\n    ip_address = var.ip\\n  }\\n}\\n\\nresource \\\"openstack_networking_port_v2\\\" \\\"internet\\\" {\\n  name           = \\\"${var.name}-ext-net\\\"\\n  network_id     = \\\"6011fbc9-4cbf-46a4-8452-6890a340b60b\\\"\\n  admin_state_up = \\\"true\\\"\\n}\\n\",\"language\":\"HCL\"}],[\"bookmark\",{\"url\":\"https://gitlab.breadnet.co.uk/terraform/modules/ovh/instance-ports/-/tree/master\",\"metadata\":{\"url\":\"http://gitlab.breadnet.co.uk/terraform/modules/ovh/instance-ports/-/tree/master\",\"title\":\"Files · master · terraform / modules / ovh / Instance Ports\",\"description\":\"Instance with VPC ports\",\"author\":null,\"publisher\":\"GitLab\",\"thumbnail\":\"http://gitlab.breadnet.co.uk/assets/gitlab_logo-7ae504fe4f68fdebb3c2034e36621930cd36ea87924c11ff65dbcb8ed50dca58.png\",\"icon\":\"https://gitlab.breadnet.co.uk/assets/touch-icon-ipad-retina-8ebe416f5313483d9c1bc772b5bbe03ecad52a54eba443e5215a22caed2a16a2.png\"}}],[\"code\",{\"code\":\"module \\\"lb-rp\\\" {\\n  source = \\\"git::https://gitlab.breadnet.co.uk/terraform/modules/ovh/instance-ports.git\\\"\\n  ip = \\\"172.16.18.10\\\"\\n  key = openstack_compute_keypair_v2.computer.name\\n  name = \\\"lon-lb-rp\\\"\\n  netid = module.vpc.network_id\\n  sg = [\\n    openstack_compute_secgroup_v2.ssh.id,\\n    openstack_compute_secgroup_v2.icmp.id,\\n    openstack_compute_secgroup_v2.web.id\\n  ]\\n  subid = module.vpc.subnet_id\\n}\",\"language\":\"HCL\"}],[\"code\",{\"code\":\"module \\\"lb-rp\\\"{\\nsource = \\\"git::https://gitlab.breadnet.co.uk/terraform/modules/ovh/instance-ports.git\\\"\\n# code goes here\\n}\",\"caption\":\"Defines the module\"}],[\"code\",{\"code\":\"ip = \\\"172.16.18.10\\\"\\n\"}],[\"code\",{\"code\":\" key = openstack_compute_keypair_v2.computer.name\"}],[\"code\",{\"code\":\"name = \\\"lon-lb-rp\\\"\"}],[\"code\",{\"code\":\"netid = module.vpc.network_id\"}],[\"code\",{\"code\":\"  sg = [\\n    openstack_compute_secgroup_v2.ssh.id,\\n    openstack_compute_secgroup_v2.icmp.id,\\n    openstack_compute_secgroup_v2.web.id\\n  ]\"}],[\"code\",{\"code\":\"subid = module.vpc.subnet_id\"}],[\"bookmark\",{\"url\":\"https://www.reuters.com/article/us-france-ovh-fire-idUSKBN2B20NU\",\"metadata\":{\"url\":\"https://www.reuters.com/article/us-france-ovh-fire-idUSKBN2B20NU\",\"title\":\"Millions of websites offline after fire at French cloud services firm\",\"description\":\"A fire at a French cloud services firm has disrupted millions of websites, knocking out government agencies’ portals, banks, shops, news websites and taking out a chunk of the .FR web space, according to internet monitors.\",\"author\":\"Mathieu Rosemain, Raphael Satter\",\"publisher\":\"Reuters\",\"thumbnail\":\"https://static.reuters.com/resources/r/?m=02&d=20210310&t=2&i=1554440499&r=LYNXMPEH290XD&w=800\",\"icon\":\"https://www.reuters.com/article/_next/static/images/favicon-196x196-052cc719f1ac872e3544e51801338b46.png\"}}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/04/image.png\",\"width\":490,\"height\":485}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/04/image-1.png\",\"width\":251,\"height\":200}],[\"bookmark\",{\"url\":\"https://jira.breadnet.co.uk/browse/BCM-18\",\"metadata\":{\"url\":\"https://jira.breadnet.co.uk/browse/BCM-18\",\"title\":\"[BCM-18] Issue log - breadNET Jira\",\"description\":null,\"author\":null,\"publisher\":\"JIRA\",\"thumbnail\":\"https://jira.breadnet.co.uk/secure/projectavatar?avatarId=10324\",\"icon\":\"https://jira.breadnet.co.uk/s/q6useh/814001/75e65367690ac553d207206292c2b417/_/jira-favicon-hires.png\"}}],[\"markdown\",{\"markdown\":\"| Issue | Description | solved? |\\n| --- | --- | --- |\\n| Multiple IP address | I was trying to attach internal IP address and external IP address to an instance and depeding on the order you slap them in on, one doesnt get connected. | no |\\n| Moving to module and IP issues | This was sheer user error and not knowing how to then attach the Port (Which contains the IP address) to the instance | yes! |\\n| Second instance failing to create | This was a stange one. I was trying to create 2 instances and use the same port as I was able to do this with the ext network. Turns out you cant actually do this so you need to create a port per instance per network | yes |\\n| Firewall rules not being connected | I'm not even sure what causes this, but just doing a terraform apply agian works | eh, why not? |\"}],[\"code\",{\"code\":\"Error getting openstack_networking_network_v2 16ce390d-4fa1-4767-8372-f7b243f3ac89: Get \\\"https://network.compute.uk1.cloud.ovh.net/v2.0/networks/16ce390d-4fa1-4767-8372-f7b243f3ac89\\\": OpenStack connection error, retries exhausted. Aborting. Last error was: read tcp 192.168.0.11:64516->51.75.101.108:443: read: connection reset by peer\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/04/image-2.png\",\"width\":3252,\"height\":120}],[\"hr\",{}]],\"markups\":[[\"strong\"],[\"a\",[\"href\",\"https://www.ispreview.co.uk/talk/threads/virgin-media-business-500-35-voom-13-static-ips.37005/\"]],[\"a\",[\"href\",\"https://danielmiessler.com/study/security-by-obscurity/\"]],[\"code\"],[\"s\"],[\"a\",[\"href\",\"https://www.upwork.com/freelancers/~01c61ee9802b94133e\"]],[\"a\",[\"href\",\"mailto:work@breadnet.co.uk\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Welcome back!\"]]],[1,\"p\",[[0,[],0,\"If you're not seen it, I suggest you have a read of:\"]]],[10,0],[1,\"p\",[[0,[],0,\"In this installment of \\\"Something I did at 4am that really should have been done during the day\\\" we will be looking at the infrastructure that will be powering my internal services. \"]]],[1,\"p\",[[0,[],0,\"I had a lot of issues during this, but let's take a look at the code first.\"]]],[10,1],[1,\"p\",[[0,[],0,\"I host all my code on Gitlab, allows me more control and able to play with CI at some point. \"]]],[1,\"p\",[[0,[],0,\"I have decided that for this deployment that it's best to use a IaC (Infrastructure as code) approach to setting up and provisioning the instances.\"]]],[1,\"p\",[[0,[],0,\"Probably wondering why? \"]]],[1,\"p\",[[0,[],0,\"It seems like a long process at the time, but should OVH decided they want to burn down London, I can just add a variable in to the code for a different region, apply and restore from backups. Also, it allows for a single source of truth and being in git, better control of changes. Eventually I will move this to a pipeline on merge to master so I don't have to do any terraform applies my self!\"]]],[1,\"p\",[[0,[],0,\"Lucky for me most of my applications (Barring Jira and Jellyfin) are quite lightweight so don't require a lot of compute resources. This being said I have still built this around the ability to expand my server needs as time progresses. \"]]],[10,2],[1,\"h1\",[[0,[],0,\"changes\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"Now, about that last comment from the previous post about changing up a lot of things...\"]]],[10,3],[1,\"p\",[[0,[],0,\"Last time we looked at the services I plan to migrate, we had a lot more.\"]]],[1,\"p\",[[0,[],0,\"Since I wrote that post I have since moved out of my parents house, and have given them a deadline of end of Q2 to have the servers shutdown. Whilst this is quite a sad thing to do, once I am a house owner and chase all the promotions I can, I have plans to run my own local cloud. I would probably go down the Colocation route with something like Openstack as this would make moving my pre-existing infrastructure over quite easy as I am writing modules for Openstack. \"]]],[1,\"p\",[[0,[],0,\"That being said, I have taken a look at the services I use day to day and have come to the conclusion the only services I \"],[0,[0],1,\"need \"],[0,[],0,\"and rely on are:\"]]],[3,\"ul\",[[[0,[],0,\"Passbolt\"]],[[0,[],0,\"Jira\"]],[[0,[],0,\"Jellyfin\"]],[[0,[],0,\"Gitlab\"]],[[0,[],0,\"Unifi\"]]]],[1,\"p\",[[0,[],0,\"Heres where things get fun.\"]]],[1,\"p\",[[0,[],0,\"Jira suggests a minimum of 8gb worth of ram for running the instances and Jellyfin can run on a pi with direct streaming. Nice.\"]]],[1,\"p\",[[0,[],0,\"It would seem like I will need to migrate Jira to the atlassian cloud offering, but plan on putting this off for as long as possible. I may see if I can just run a small computer at my parents for this. Still - End goal is to have it migrated.\"]]],[1,\"h1\",[[0,[],0,\"the why\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"You're probably thinking, \\\"Why actually migrate?\\\" \"],[1,[],0,0],[0,[],0,\"Well, running 4 pieces of equipment at my parents house when I am not paying for connections or power or space is a little unfair. I would move the server to where I live now, but we don't have a garage and nor the stable internet connection, blame Virgin Media and their \"],[0,[1],1,\"shambolic implementation  of how to handle Static IP address' \"]]],[1,\"p\",[[0,[],0,\"Basically the way it works is the Virgin media router connects to their network, then you establish a GRE connection to  a virgin media data centre in one of the following: Birmingham/Leeds/Cambridge/Reading (according to our IP location) where traffic then flows to/from.\"]]],[1,\"p\",[[0,[],0,\"Now this is fine, but it adds an additional 30ms of latency on to ANYTHING, and really breaks things like Cloudflare DNS, Google DNS and anything that uses an anycast IP address as instead of connecting to the closest server geographically and per ping, we connect to stuff up in Birmingham \"]]],[10,4],[1,\"p\",[[0,[],0,\"Any way.\"]]],[1,\"p\",[[0,[],0,\"Let's look at the code.\"]]],[1,\"p\",[[0,[],0,\"I have opted to go with a minimum of 3 servers, 1 server will be a load balancer and reverse proxy, then 2 application servers. \"],[1,[],0,1],[0,[],0,\"Going this route allows me to terminate SSL on one instance, and firewall things off on one instance, then the 2 back end instances don't need to be messed with as far as installing additional software goes. It also makes backups a lot easier (we will cover this in a second)\"]]],[1,\"p\",[[0,[],0,\"I don't want to go in to the nitty gritty details too much as I want to do my best to do a little \"],[0,[2],1,\"\\\"Security through obscurity\\\"\"],[0,[],0,\" and clean opsec practices. \"]]],[1,\"p\",[[0,[],0,\"I am going to be identifying what the best reverse proxy is to run:\"]]],[3,\"ul\",[[[0,[],0,\"HaProxy\"]],[[0,[],0,\"Nginx\"]],[[0,[],0,\"Varnish Proxy\"],[1,[],0,2],[0,[],0,\"This is a POS as it doesn't do SSL\"]]]],[1,\"p\",[[0,[],0,\"So if we're being honest, the pick is between:\"]]],[3,\"ul\",[[[0,[],0,\"HAProxy\"]],[[0,[],0,\"Nginx\"]]]],[1,\"p\",[[0,[],0,\"For the time being, I will roll Nginx as it's something I have been using since I (about to annoy a lot of old school fold) discovered apache2 isn't cool anymore. \"]]],[1,\"h1\",[[0,[],0,\"actually looking at the code now \"]]],[1,\"p\",[[0,[],0,\"I know, I said let's look at the code like 3 times now...\"]]],[1,\"p\",[[0,[],0,\"I have decided to go down the Modules route for Terraform as this is something I need to learn for work.\"]]],[1,\"p\",[[0,[],0,\"One snag I came across was I am creating a VPC and then connecting the instances to them via a port. For each instance I need to create a port, then link it to the instance, as well as \"],[0,[3],1,\"Ext-Net\"],[0,[],0,\" as OVH call it. \"]]],[10,5],[10,6],[1,\"p\",[[0,[],0,\"By slapping the instance creation in to a module, I dont have to create a port block each time I create an instance.\"]]],[1,\"p\",[[0,[],0,\"Now it looks like the below to create a new instance:\"]]],[10,7],[1,\"p\",[[0,[],0,\"Let's break this down as I know you care:\"]]],[10,8],[1,\"p\",[[0,[],0,\"Here we are defining the module, and where it can be found. Originally I was working with the modules in file, but towards the end I plan to use these modules often, so I moved them to git. \"]]],[10,9],[1,\"p\",[[0,[],0,\"This is defining the internal IP on the VPC\"]]],[10,10],[1,\"p\",[[0,[],0,\"This defines what SSH key I want to use to access the instances\"]]],[10,11],[1,\"p\",[[0,[],0,\"Guess what this does\"]]],[10,12],[1,\"p\",[[0,[],0,\"This one is fun, this connects the instance to the port with the correct network ID for the VPC. In order to do this, in the VPC module I had to define an output. \"]]],[10,13],[1,\"p\",[[0,[],0,\"These are the firewall rules for the instance\"]]],[10,14],[1,\"p\",[[0,[],0,\"Much like netid, this is from the VPC module, connecting it to the VPC with the correct Subnet address.\"]]],[1,\"p\",[[0,[],0,\"All in all, that creates an instance, attaches it to the correct VPC and subnet, gives it an IP address, gets a public IP address, allows me to ssh, ping and also inbound 80 and 443.\"]]],[1,\"p\",[[0,[],0,\"Now this is done, we can take a look at something which is important after the recent events...\"]]],[10,15],[1,\"p\",[[0,[],0,\"Yeah - Big Whoops.\"]]],[1,\"h1\",[[0,[],0,\"backups\"]]],[1,\"p\",[[0,[],0,\"Now I know people see the cloud as a magical place, but bro. Really? Just because it's far far away from your physical control, doesn't mean that problems still don't happen. \"]]],[1,\"p\",[[0,[],0,\"Meme time:\"]]],[10,16],[1,\"p\",[]],[10,17],[1,\"p\",[[0,[],0,\"Now we have that out of our system...\"]]],[1,\"p\",[[0,[],0,\"Let's have a little think about backups.\"]]],[1,\"p\",[[0,[],0,\"Currently I have a (lowely janky) system that works REALLY well. Let me explain:\"]]],[3,\"ul\",[[[0,[],0,\"Backups of all servers go to backuppc (currently) running internally\"]],[[0,[],0,\"The server running backuppc has a cron job syncing the data to Wasabi\"]],[[0,[],0,\"Mail server, dbserver and reverse all backup their files to individual buckets on wasabi.\"]],[[0,[],0,\"VM's are snapshotted daily and pushed to Wasabi nightly. They are also exported to a removable harddrive for daily recovery if needed.\"]],[[0,[],0,\"All in all, there is about 2tb worth of data that is being shuffled around. \"]]]],[1,\"p\",[[0,[],0,\"Now with us moving to the cloud, we are able to use backup services built in to OVH... But after the fire, I don't really want to have to rely on data being stored in the same data centre to be able to spin up my resources should the place flood or some strange event happen. \"]]],[1,\"p\",[[0,[],0,\"My plan:\"]]],[3,\"ul\",[[[0,[],0,\"Jellyfin media is served from s3, bucket replication to RO bucket.\"]],[[0,[],0,\"At my current address, we have 300m down/20 up, I will purchase a raspberry Pi and run backuppc from it\"]],[[0,[],0,\"Backuppc on pi also has a cron job to move files to s3 bucket\"]],[[0,[],0,\"Servers push their files to s3\"]],[[0,[],0,\"Config files are all done via Ansible or Git (ci/cd)\"]]]],[1,\"p\",[[0,[],0,\"The whole idea around this painful process is should OVH and my house decide to show the world they vape, I have \"],[0,[0],1,\"all\"],[0,[],0,\" the files on S3\"]]],[1,\"p\",[[0,[],0,\"Should OVH and the data centre in the netherlands (where s3 is) decide they don't wan't to be a data centre and instead become a warehouse, we have the files locally. \"]]],[1,\"p\",[[0,[],0,\"NOW - Should My house and also NL decide they <fill in some comic way of saying offline> we have the live servers, which I will snapshot and export these snapshots to my local computer. Max 30gb.\"]]],[1,\"h1\",[[0,[],0,\"Issues\"]]],[1,\"p\",[[0,[],0,\"Now the keen eyed among you who had a little scratch around Jira saw this:\"]]],[10,18],[1,\"p\",[[0,[],0,\"I kept a log of the \"],[0,[4],1,\"bullshit\"],[0,[],0,\" strange issues I came across for your entertainment.\"]]],[1,\"p\",[[0,[],0,\"let's get started. \"]]],[10,19],[10,20],[1,\"p\",[[0,[],0,\"Ah this one popped up MANY times - Looks like this and it made me want to slap someone at OVH.\"]]],[10,21],[1,\"p\",[[0,[],0,\"This is OVH being OVH. Just re-apply your terraform and you're good.\"]]],[1,\"p\",[[0,[],0,\"There is one more issues I need to solve, and that is getting the instances to pick up the VPC IP address, but I think I may need to do this via Ansible or Manually.\"]]],[1,\"p\",[[0,[],0,\"I want to avoid having to do as much as possible manually...\"]]],[1,\"p\",[[0,[],0,\"This leads me to on to the next post where I will be discussing the Ansible behind the services I will be running, and how I plan to move all this to a CI/CD pipeline (or something like that)\"]]],[10,22],[1,\"p\",[[0,[],0,\"You can hire me via \"],[0,[5],1,\"Upwork\"],[0,[],0,\" or \"],[0,[6],1,\"emailing me\"],[0,[],0,\" for weekend projects!\"]]]],\"ghostVersion\":\"3.0\"}","lexical":null,"html":"<p>Welcome back!</p><p>If you're not seen it, I suggest you have a read of:</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"__GHOST_URL__/cloud-migration-part-1/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">breadNET Cloud Migration</div><div class=\"kg-bookmark-description\">How did I move all my servers to the cloud? Well - Ansible, automation and CI/CD!</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://breadnet.co.uk/favicon.png\" alt=\"\"><span class=\"kg-bookmark-author\">breadNET</span><span class=\"kg-bookmark-publisher\">Bradley Stannard</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://images.unsplash.com/photo-1560182413-53772f3d7134?ixlib&#x3D;rb-1.2.1&amp;q&#x3D;80&amp;fm&#x3D;jpg&amp;crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;w&#x3D;2000&amp;fit&#x3D;max&amp;ixid&#x3D;eyJhcHBfaWQiOjExNzczfQ\" alt=\"\"></div></a></figure><p>In this installment of \"Something I did at 4am that really should have been done during the day\" we will be looking at the infrastructure that will be powering my internal services. </p><p>I had a lot of issues during this, but let's take a look at the code first.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://gitlab.breadnet.co.uk/terraform/breadnet-ovh\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">terraform / breadNET OVH</div><div class=\"kg-bookmark-description\">The OVH terraform for breadNET cloud servers</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://gitlab.breadnet.co.uk/assets/touch-icon-ipad-retina-8ebe416f5313483d9c1bc772b5bbe03ecad52a54eba443e5215a22caed2a16a2.png\" alt=\"\"><span class=\"kg-bookmark-author\">GitLab</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"http://gitlab.breadnet.co.uk/assets/gitlab_logo-7ae504fe4f68fdebb3c2034e36621930cd36ea87924c11ff65dbcb8ed50dca58.png\" alt=\"\"></div></a></figure><p>I host all my code on Gitlab, allows me more control and able to play with CI at some point. </p><p>I have decided that for this deployment that it's best to use a IaC (Infrastructure as code) approach to setting up and provisioning the instances.</p><p>Probably wondering why? </p><p>It seems like a long process at the time, but should OVH decided they want to burn down London, I can just add a variable in to the code for a different region, apply and restore from backups. Also, it allows for a single source of truth and being in git, better control of changes. Eventually I will move this to a pipeline on merge to master so I don't have to do any terraform applies my self!</p><p>Lucky for me most of my applications (Barring Jira and Jellyfin) are quite lightweight so don't require a lot of compute resources. This being said I have still built this around the ability to expand my server needs as time progresses. </p><hr><h1 id=\"changes\">changes</h1><p></p><p>Now, about that last comment from the previous post about changing up a lot of things...</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jira.breadnet.co.uk/browse/BCM-2\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">[BCM-2] List apps that need to be moved - breadNET Jira</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jira.breadnet.co.uk/s/q6useh/814001/75e65367690ac553d207206292c2b417/_/jira-favicon-hires.png\" alt=\"\"><span class=\"kg-bookmark-author\">JIRA</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jira.breadnet.co.uk/secure/projectavatar?avatarId&#x3D;10324\" alt=\"\"></div></a></figure><p>Last time we looked at the services I plan to migrate, we had a lot more.</p><p>Since I wrote that post I have since moved out of my parents house, and have given them a deadline of end of Q2 to have the servers shutdown. Whilst this is quite a sad thing to do, once I am a house owner and chase all the promotions I can, I have plans to run my own local cloud. I would probably go down the Colocation route with something like Openstack as this would make moving my pre-existing infrastructure over quite easy as I am writing modules for Openstack. </p><p>That being said, I have taken a look at the services I use day to day and have come to the conclusion the only services I <strong>need </strong>and rely on are:</p><ul><li>Passbolt</li><li>Jira</li><li>Jellyfin</li><li>Gitlab</li><li>Unifi</li></ul><p>Heres where things get fun.</p><p>Jira suggests a minimum of 8gb worth of ram for running the instances and Jellyfin can run on a pi with direct streaming. Nice.</p><p>It would seem like I will need to migrate Jira to the atlassian cloud offering, but plan on putting this off for as long as possible. I may see if I can just run a small computer at my parents for this. Still - End goal is to have it migrated.</p><h1 id=\"the-why\">the why</h1><p></p><p>You're probably thinking, \"Why actually migrate?\" <br>Well, running 4 pieces of equipment at my parents house when I am not paying for connections or power or space is a little unfair. I would move the server to where I live now, but we don't have a garage and nor the stable internet connection, blame Virgin Media and their <a href=\"https://www.ispreview.co.uk/talk/threads/virgin-media-business-500-35-voom-13-static-ips.37005/\">shambolic implementation  of how to handle Static IP address' </a></p><p>Basically the way it works is the Virgin media router connects to their network, then you establish a GRE connection to  a virgin media data centre in one of the following: Birmingham/Leeds/Cambridge/Reading (according to our IP location) where traffic then flows to/from.</p><p>Now this is fine, but it adds an additional 30ms of latency on to ANYTHING, and really breaks things like Cloudflare DNS, Google DNS and anything that uses an anycast IP address as instead of connecting to the closest server geographically and per ping, we connect to stuff up in Birmingham </p><pre><code class=\"language-bash\">traceroute to 1.1.1.1 (1.1.1.1), 64 hops max, 52 byte packets\n 1  192.168.0.1 (192.168.0.1)  4.404 ms  2.788 ms  2.798 ms\n 2  * * *\n 3  brhm-core-2b-et-315-0.network.virginmedia.net (81.110.128.5)  19.415 ms  18.775 ms  18.458 ms\n 4  * * *\n 5  * * *\n 6  tcma-ic-2-ae9-0.network.virginmedia.net (62.253.174.178)  20.490 ms  20.312 ms  17.615 ms\n 7  162.158.32.254 (162.158.32.254)  32.277 ms  53.136 ms  38.476 ms\n 8  162.158.32.9 (162.158.32.9)  31.605 ms  32.833 ms  35.764 ms\n 9  one.one.one.one (1.1.1.1)  31.442 ms  31.381 ms  33.273 ms</code></pre><p>Any way.</p><p>Let's look at the code.</p><p>I have opted to go with a minimum of 3 servers, 1 server will be a load balancer and reverse proxy, then 2 application servers. <br>Going this route allows me to terminate SSL on one instance, and firewall things off on one instance, then the 2 back end instances don't need to be messed with as far as installing additional software goes. It also makes backups a lot easier (we will cover this in a second)</p><p>I don't want to go in to the nitty gritty details too much as I want to do my best to do a little <a href=\"https://danielmiessler.com/study/security-by-obscurity/\">\"Security through obscurity\"</a> and clean opsec practices. </p><p>I am going to be identifying what the best reverse proxy is to run:</p><ul><li>HaProxy</li><li>Nginx</li><li>Varnish Proxy<br>This is a POS as it doesn't do SSL</li></ul><p>So if we're being honest, the pick is between:</p><ul><li>HAProxy</li><li>Nginx</li></ul><p>For the time being, I will roll Nginx as it's something I have been using since I (about to annoy a lot of old school fold) discovered apache2 isn't cool anymore. </p><h1 id=\"actually-looking-at-the-code-now\">actually looking at the code now </h1><p>I know, I said let's look at the code like 3 times now...</p><p>I have decided to go down the Modules route for Terraform as this is something I need to learn for work.</p><p>One snag I came across was I am creating a VPC and then connecting the instances to them via a port. For each instance I need to create a port, then link it to the instance, as well as <code>Ext-Net</code> as OVH call it. </p><pre><code class=\"language-HCL\">resource \"openstack_compute_instance_v2\" \"instance\" {\n  name = var.name\n  flavor_name = \"s1-2\"\n  key_pair = var.key\n  image_name = \"Ubuntu 18.04\"\n  security_groups = var.sg\n  network {\n    port = openstack_networking_port_v2.internet.id\n  }\n  network {\n    port = openstack_networking_port_v2.internal.id\n  }\n}\n\nresource \"openstack_networking_port_v2\" \"internal\" {\n  name           = \"${var.name}-backend\"\n  network_id     = var.netid\n  admin_state_up = \"true\"\n\n  fixed_ip {\n    subnet_id  = var.subid\n    ip_address = var.ip\n  }\n}\n\nresource \"openstack_networking_port_v2\" \"internet\" {\n  name           = \"${var.name}-ext-net\"\n  network_id     = \"6011fbc9-4cbf-46a4-8452-6890a340b60b\"\n  admin_state_up = \"true\"\n}\n</code></pre><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://gitlab.breadnet.co.uk/terraform/modules/ovh/instance-ports/-/tree/master\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Files · master · terraform / modules / ovh / Instance Ports</div><div class=\"kg-bookmark-description\">Instance with VPC ports</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://gitlab.breadnet.co.uk/assets/touch-icon-ipad-retina-8ebe416f5313483d9c1bc772b5bbe03ecad52a54eba443e5215a22caed2a16a2.png\" alt=\"\"><span class=\"kg-bookmark-author\">GitLab</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"http://gitlab.breadnet.co.uk/assets/gitlab_logo-7ae504fe4f68fdebb3c2034e36621930cd36ea87924c11ff65dbcb8ed50dca58.png\" alt=\"\"></div></a></figure><p>By slapping the instance creation in to a module, I dont have to create a port block each time I create an instance.</p><p>Now it looks like the below to create a new instance:</p><pre><code class=\"language-HCL\">module \"lb-rp\" {\n  source = \"git::https://gitlab.breadnet.co.uk/terraform/modules/ovh/instance-ports.git\"\n  ip = \"172.16.18.10\"\n  key = openstack_compute_keypair_v2.computer.name\n  name = \"lon-lb-rp\"\n  netid = module.vpc.network_id\n  sg = [\n    openstack_compute_secgroup_v2.ssh.id,\n    openstack_compute_secgroup_v2.icmp.id,\n    openstack_compute_secgroup_v2.web.id\n  ]\n  subid = module.vpc.subnet_id\n}</code></pre><p>Let's break this down as I know you care:</p><figure class=\"kg-card kg-code-card\"><pre><code>module \"lb-rp\"{\nsource = \"git::https://gitlab.breadnet.co.uk/terraform/modules/ovh/instance-ports.git\"\n# code goes here\n}</code></pre><figcaption>Defines the module</figcaption></figure><p>Here we are defining the module, and where it can be found. Originally I was working with the modules in file, but towards the end I plan to use these modules often, so I moved them to git. </p><pre><code>ip = \"172.16.18.10\"\n</code></pre><p>This is defining the internal IP on the VPC</p><pre><code> key = openstack_compute_keypair_v2.computer.name</code></pre><p>This defines what SSH key I want to use to access the instances</p><pre><code>name = \"lon-lb-rp\"</code></pre><p>Guess what this does</p><pre><code>netid = module.vpc.network_id</code></pre><p>This one is fun, this connects the instance to the port with the correct network ID for the VPC. In order to do this, in the VPC module I had to define an output. </p><pre><code>  sg = [\n    openstack_compute_secgroup_v2.ssh.id,\n    openstack_compute_secgroup_v2.icmp.id,\n    openstack_compute_secgroup_v2.web.id\n  ]</code></pre><p>These are the firewall rules for the instance</p><pre><code>subid = module.vpc.subnet_id</code></pre><p>Much like netid, this is from the VPC module, connecting it to the VPC with the correct Subnet address.</p><p>All in all, that creates an instance, attaches it to the correct VPC and subnet, gives it an IP address, gets a public IP address, allows me to ssh, ping and also inbound 80 and 443.</p><p>Now this is done, we can take a look at something which is important after the recent events...</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://www.reuters.com/article/us-france-ovh-fire-idUSKBN2B20NU\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Millions of websites offline after fire at French cloud services firm</div><div class=\"kg-bookmark-description\">A fire at a French cloud services firm has disrupted millions of websites, knocking out government agencies’ portals, banks, shops, news websites and taking out a chunk of the .FR web space, according to internet monitors.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://www.reuters.com/article/_next/static/images/favicon-196x196-052cc719f1ac872e3544e51801338b46.png\" alt=\"\"><span class=\"kg-bookmark-author\">Reuters</span><span class=\"kg-bookmark-publisher\">Mathieu Rosemain, Raphael Satter</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://static.reuters.com/resources/r/?m&#x3D;02&amp;d&#x3D;20210310&amp;t&#x3D;2&amp;i&#x3D;1554440499&amp;r&#x3D;LYNXMPEH290XD&amp;w&#x3D;800\" alt=\"\"></div></a></figure><p>Yeah - Big Whoops.</p><h1 id=\"backups\">backups</h1><p>Now I know people see the cloud as a magical place, but bro. Really? Just because it's far far away from your physical control, doesn't mean that problems still don't happen. </p><p>Meme time:</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2021/04/image.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"490\" height=\"485\"></figure><p></p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2021/04/image-1.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"251\" height=\"200\"></figure><p>Now we have that out of our system...</p><p>Let's have a little think about backups.</p><p>Currently I have a (lowely janky) system that works REALLY well. Let me explain:</p><ul><li>Backups of all servers go to backuppc (currently) running internally</li><li>The server running backuppc has a cron job syncing the data to Wasabi</li><li>Mail server, dbserver and reverse all backup their files to individual buckets on wasabi.</li><li>VM's are snapshotted daily and pushed to Wasabi nightly. They are also exported to a removable harddrive for daily recovery if needed.</li><li>All in all, there is about 2tb worth of data that is being shuffled around. </li></ul><p>Now with us moving to the cloud, we are able to use backup services built in to OVH... But after the fire, I don't really want to have to rely on data being stored in the same data centre to be able to spin up my resources should the place flood or some strange event happen. </p><p>My plan:</p><ul><li>Jellyfin media is served from s3, bucket replication to RO bucket.</li><li>At my current address, we have 300m down/20 up, I will purchase a raspberry Pi and run backuppc from it</li><li>Backuppc on pi also has a cron job to move files to s3 bucket</li><li>Servers push their files to s3</li><li>Config files are all done via Ansible or Git (ci/cd)</li></ul><p>The whole idea around this painful process is should OVH and my house decide to show the world they vape, I have <strong>all</strong> the files on S3</p><p>Should OVH and the data centre in the netherlands (where s3 is) decide they don't wan't to be a data centre and instead become a warehouse, we have the files locally. </p><p>NOW - Should My house and also NL decide they &lt;fill in some comic way of saying offline&gt; we have the live servers, which I will snapshot and export these snapshots to my local computer. Max 30gb.</p><h1 id=\"issues\">Issues</h1><p>Now the keen eyed among you who had a little scratch around Jira saw this:</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://jira.breadnet.co.uk/browse/BCM-18\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">[BCM-18] Issue log - breadNET Jira</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://jira.breadnet.co.uk/s/q6useh/814001/75e65367690ac553d207206292c2b417/_/jira-favicon-hires.png\" alt=\"\"><span class=\"kg-bookmark-author\">JIRA</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://jira.breadnet.co.uk/secure/projectavatar?avatarId&#x3D;10324\" alt=\"\"></div></a></figure><p>I kept a log of the <s>bullshit</s> strange issues I came across for your entertainment.</p><p>let's get started. </p><!--kg-card-begin: markdown--><table>\n<thead>\n<tr>\n<th>Issue</th>\n<th>Description</th>\n<th>solved?</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Multiple IP address</td>\n<td>I was trying to attach internal IP address and external IP address to an instance and depeding on the order you slap them in on, one doesnt get connected.</td>\n<td>no</td>\n</tr>\n<tr>\n<td>Moving to module and IP issues</td>\n<td>This was sheer user error and not knowing how to then attach the Port (Which contains the IP address) to the instance</td>\n<td>yes!</td>\n</tr>\n<tr>\n<td>Second instance failing to create</td>\n<td>This was a stange one. I was trying to create 2 instances and use the same port as I was able to do this with the ext network. Turns out you cant actually do this so you need to create a port per instance per network</td>\n<td>yes</td>\n</tr>\n<tr>\n<td>Firewall rules not being connected</td>\n<td>I'm not even sure what causes this, but just doing a terraform apply agian works</td>\n<td>eh, why not?</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: markdown--><pre><code>Error getting openstack_networking_network_v2 16ce390d-4fa1-4767-8372-f7b243f3ac89: Get \"https://network.compute.uk1.cloud.ovh.net/v2.0/networks/16ce390d-4fa1-4767-8372-f7b243f3ac89\": OpenStack connection error, retries exhausted. Aborting. Last error was: read tcp 192.168.0.11:64516-&gt;51.75.101.108:443: read: connection reset by peer</code></pre><p>Ah this one popped up MANY times - Looks like this and it made me want to slap someone at OVH.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2021/04/image-2.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"74\" srcset=\"__GHOST_URL__/content/images/size/w600/2021/04/image-2.png 600w, __GHOST_URL__/content/images/size/w1000/2021/04/image-2.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/04/image-2.png 1600w, __GHOST_URL__/content/images/size/w2400/2021/04/image-2.png 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><p>This is OVH being OVH. Just re-apply your terraform and you're good.</p><p>There is one more issues I need to solve, and that is getting the instances to pick up the VPC IP address, but I think I may need to do this via Ansible or Manually.</p><p>I want to avoid having to do as much as possible manually...</p><p>This leads me to on to the next post where I will be discussing the Ansible behind the services I will be running, and how I plan to move all this to a CI/CD pipeline (or something like that)</p><hr><p>You can hire me via <a href=\"https://www.upwork.com/freelancers/~01c61ee9802b94133e\">Upwork</a> or <a href=\"mailto:work@breadnet.co.uk\">emailing me</a> for weekend projects!</p>","comment_id":"606925d8bea1f916bfb8403d","plaintext":"Welcome back!\n\nIf you're not seen it, I suggest you have a read of:\n\nbreadNET Cloud MigrationHow did I move all my servers to the cloud? Well - Ansible, automation and CI/CD!breadNETBradley Stannard\n\nIn this installment of \"Something I did at 4am that really should have been done during the day\" we will be looking at the infrastructure that will be powering my internal services.\n\nI had a lot of issues during this, but let's take a look at the code first.\n\nterraform / breadNET OVHThe OVH terraform for breadNET cloud serversGitLab\n\nI host all my code on Gitlab, allows me more control and able to play with CI at some point.\n\nI have decided that for this deployment that it's best to use a IaC (Infrastructure as code) approach to setting up and provisioning the instances.\n\nProbably wondering why?\n\nIt seems like a long process at the time, but should OVH decided they want to burn down London, I can just add a variable in to the code for a different region, apply and restore from backups. Also, it allows for a single source of truth and being in git, better control of changes. Eventually I will move this to a pipeline on merge to master so I don't have to do any terraform applies my self!\n\nLucky for me most of my applications (Barring Jira and Jellyfin) are quite lightweight so don't require a lot of compute resources. This being said I have still built this around the ability to expand my server needs as time progresses.\n\n\nchanges\n\n\n\nNow, about that last comment from the previous post about changing up a lot of things...\n\n[BCM-2] List apps that need to be moved - breadNET JiraJIRA\n\nLast time we looked at the services I plan to migrate, we had a lot more.\n\nSince I wrote that post I have since moved out of my parents house, and have given them a deadline of end of Q2 to have the servers shutdown. Whilst this is quite a sad thing to do, once I am a house owner and chase all the promotions I can, I have plans to run my own local cloud. I would probably go down the Colocation route with something like Openstack as this would make moving my pre-existing infrastructure over quite easy as I am writing modules for Openstack.\n\nThat being said, I have taken a look at the services I use day to day and have come to the conclusion the only services I need and rely on are:\n\n * Passbolt\n * Jira\n * Jellyfin\n * Gitlab\n * Unifi\n\nHeres where things get fun.\n\nJira suggests a minimum of 8gb worth of ram for running the instances and Jellyfin can run on a pi with direct streaming. Nice.\n\nIt would seem like I will need to migrate Jira to the atlassian cloud offering, but plan on putting this off for as long as possible. I may see if I can just run a small computer at my parents for this. Still - End goal is to have it migrated.\n\n\nthe why\n\n\n\nYou're probably thinking, \"Why actually migrate?\"\nWell, running 4 pieces of equipment at my parents house when I am not paying for connections or power or space is a little unfair. I would move the server to where I live now, but we don't have a garage and nor the stable internet connection, blame Virgin Media and their shambolic implementation  of how to handle Static IP address'\n\nBasically the way it works is the Virgin media router connects to their network, then you establish a GRE connection to  a virgin media data centre in one of the following: Birmingham/Leeds/Cambridge/Reading (according to our IP location) where traffic then flows to/from.\n\nNow this is fine, but it adds an additional 30ms of latency on to ANYTHING, and really breaks things like Cloudflare DNS, Google DNS and anything that uses an anycast IP address as instead of connecting to the closest server geographically and per ping, we connect to stuff up in Birmingham\n\ntraceroute to 1.1.1.1 (1.1.1.1), 64 hops max, 52 byte packets\n 1  192.168.0.1 (192.168.0.1)  4.404 ms  2.788 ms  2.798 ms\n 2  * * *\n 3  brhm-core-2b-et-315-0.network.virginmedia.net (81.110.128.5)  19.415 ms  18.775 ms  18.458 ms\n 4  * * *\n 5  * * *\n 6  tcma-ic-2-ae9-0.network.virginmedia.net (62.253.174.178)  20.490 ms  20.312 ms  17.615 ms\n 7  162.158.32.254 (162.158.32.254)  32.277 ms  53.136 ms  38.476 ms\n 8  162.158.32.9 (162.158.32.9)  31.605 ms  32.833 ms  35.764 ms\n 9  one.one.one.one (1.1.1.1)  31.442 ms  31.381 ms  33.273 ms\n\nAny way.\n\nLet's look at the code.\n\nI have opted to go with a minimum of 3 servers, 1 server will be a load balancer and reverse proxy, then 2 application servers.\nGoing this route allows me to terminate SSL on one instance, and firewall things off on one instance, then the 2 back end instances don't need to be messed with as far as installing additional software goes. It also makes backups a lot easier (we will cover this in a second)\n\nI don't want to go in to the nitty gritty details too much as I want to do my best to do a little \"Security through obscurity\" and clean opsec practices.\n\nI am going to be identifying what the best reverse proxy is to run:\n\n * HaProxy\n * Nginx\n * Varnish Proxy\n   This is a POS as it doesn't do SSL\n\nSo if we're being honest, the pick is between:\n\n * HAProxy\n * Nginx\n\nFor the time being, I will roll Nginx as it's something I have been using since I (about to annoy a lot of old school fold) discovered apache2 isn't cool anymore.\n\n\nactually looking at the code now\n\nI know, I said let's look at the code like 3 times now...\n\nI have decided to go down the Modules route for Terraform as this is something I need to learn for work.\n\nOne snag I came across was I am creating a VPC and then connecting the instances to them via a port. For each instance I need to create a port, then link it to the instance, as well as Ext-Net as OVH call it.\n\nresource \"openstack_compute_instance_v2\" \"instance\" {\n  name = var.name\n  flavor_name = \"s1-2\"\n  key_pair = var.key\n  image_name = \"Ubuntu 18.04\"\n  security_groups = var.sg\n  network {\n    port = openstack_networking_port_v2.internet.id\n  }\n  network {\n    port = openstack_networking_port_v2.internal.id\n  }\n}\n\nresource \"openstack_networking_port_v2\" \"internal\" {\n  name           = \"${var.name}-backend\"\n  network_id     = var.netid\n  admin_state_up = \"true\"\n\n  fixed_ip {\n    subnet_id  = var.subid\n    ip_address = var.ip\n  }\n}\n\nresource \"openstack_networking_port_v2\" \"internet\" {\n  name           = \"${var.name}-ext-net\"\n  network_id     = \"6011fbc9-4cbf-46a4-8452-6890a340b60b\"\n  admin_state_up = \"true\"\n}\n\n\nFiles · master · terraform / modules / ovh / Instance PortsInstance with VPC portsGitLab\n\nBy slapping the instance creation in to a module, I dont have to create a port block each time I create an instance.\n\nNow it looks like the below to create a new instance:\n\nmodule \"lb-rp\" {\n  source = \"git::https://gitlab.breadnet.co.uk/terraform/modules/ovh/instance-ports.git\"\n  ip = \"172.16.18.10\"\n  key = openstack_compute_keypair_v2.computer.name\n  name = \"lon-lb-rp\"\n  netid = module.vpc.network_id\n  sg = [\n    openstack_compute_secgroup_v2.ssh.id,\n    openstack_compute_secgroup_v2.icmp.id,\n    openstack_compute_secgroup_v2.web.id\n  ]\n  subid = module.vpc.subnet_id\n}\n\nLet's break this down as I know you care:\n\nmodule \"lb-rp\"{\nsource = \"git::https://gitlab.breadnet.co.uk/terraform/modules/ovh/instance-ports.git\"\n# code goes here\n}\n\nHere we are defining the module, and where it can be found. Originally I was working with the modules in file, but towards the end I plan to use these modules often, so I moved them to git.\n\nip = \"172.16.18.10\"\n\n\nThis is defining the internal IP on the VPC\n\n key = openstack_compute_keypair_v2.computer.name\n\nThis defines what SSH key I want to use to access the instances\n\nname = \"lon-lb-rp\"\n\nGuess what this does\n\nnetid = module.vpc.network_id\n\nThis one is fun, this connects the instance to the port with the correct network ID for the VPC. In order to do this, in the VPC module I had to define an output.\n\n  sg = [\n    openstack_compute_secgroup_v2.ssh.id,\n    openstack_compute_secgroup_v2.icmp.id,\n    openstack_compute_secgroup_v2.web.id\n  ]\n\nThese are the firewall rules for the instance\n\nsubid = module.vpc.subnet_id\n\nMuch like netid, this is from the VPC module, connecting it to the VPC with the correct Subnet address.\n\nAll in all, that creates an instance, attaches it to the correct VPC and subnet, gives it an IP address, gets a public IP address, allows me to ssh, ping and also inbound 80 and 443.\n\nNow this is done, we can take a look at something which is important after the recent events...\n\nMillions of websites offline after fire at French cloud services firmA fire at a French cloud services firm has disrupted millions of websites, knocking out government agencies’ portals, banks, shops, news websites and taking out a chunk of the .FR web space, according to internet monitors.ReutersMathieu Rosemain, Raphael Satter\n\nYeah - Big Whoops.\n\n\nbackups\n\nNow I know people see the cloud as a magical place, but bro. Really? Just because it's far far away from your physical control, doesn't mean that problems still don't happen.\n\nMeme time:\n\n\n\nNow we have that out of our system...\n\nLet's have a little think about backups.\n\nCurrently I have a (lowely janky) system that works REALLY well. Let me explain:\n\n * Backups of all servers go to backuppc (currently) running internally\n * The server running backuppc has a cron job syncing the data to Wasabi\n * Mail server, dbserver and reverse all backup their files to individual buckets on wasabi.\n * VM's are snapshotted daily and pushed to Wasabi nightly. They are also exported to a removable harddrive for daily recovery if needed.\n * All in all, there is about 2tb worth of data that is being shuffled around.\n\nNow with us moving to the cloud, we are able to use backup services built in to OVH... But after the fire, I don't really want to have to rely on data being stored in the same data centre to be able to spin up my resources should the place flood or some strange event happen.\n\nMy plan:\n\n * Jellyfin media is served from s3, bucket replication to RO bucket.\n * At my current address, we have 300m down/20 up, I will purchase a raspberry Pi and run backuppc from it\n * Backuppc on pi also has a cron job to move files to s3 bucket\n * Servers push their files to s3\n * Config files are all done via Ansible or Git (ci/cd)\n\nThe whole idea around this painful process is should OVH and my house decide to show the world they vape, I have all the files on S3\n\nShould OVH and the data centre in the netherlands (where s3 is) decide they don't wan't to be a data centre and instead become a warehouse, we have the files locally.\n\nNOW - Should My house and also NL decide they <fill in some comic way of saying offline> we have the live servers, which I will snapshot and export these snapshots to my local computer. Max 30gb.\n\n\nIssues\n\nNow the keen eyed among you who had a little scratch around Jira saw this:\n\n[BCM-18] Issue log - breadNET JiraJIRA\n\nI kept a log of the bullshit strange issues I came across for your entertainment.\n\nlet's get started.\n\n\n\n\nIssue\nDescription\nsolved?\n\n\n\n\nMultiple IP address\nI was trying to attach internal IP address and external IP address to an instance and depeding on the order you slap them in on, one doesnt get connected.\nno\n\n\nMoving to module and IP issues\nThis was sheer user error and not knowing how to then attach the Port (Which contains the IP address) to the instance\nyes!\n\n\nSecond instance failing to create\nThis was a stange one. I was trying to create 2 instances and use the same port as I was able to do this with the ext network. Turns out you cant actually do this so you need to create a port per instance per network\nyes\n\n\nFirewall rules not being connected\nI'm not even sure what causes this, but just doing a terraform apply agian works\neh, why not?\n\n\n\n\n\nError getting openstack_networking_network_v2 16ce390d-4fa1-4767-8372-f7b243f3ac89: Get \"https://network.compute.uk1.cloud.ovh.net/v2.0/networks/16ce390d-4fa1-4767-8372-f7b243f3ac89\": OpenStack connection error, retries exhausted. Aborting. Last error was: read tcp 192.168.0.11:64516->51.75.101.108:443: read: connection reset by peer\n\nAh this one popped up MANY times - Looks like this and it made me want to slap someone at OVH.\n\nThis is OVH being OVH. Just re-apply your terraform and you're good.\n\nThere is one more issues I need to solve, and that is getting the instances to pick up the VPC IP address, but I think I may need to do this via Ansible or Manually.\n\nI want to avoid having to do as much as possible manually...\n\nThis leads me to on to the next post where I will be discussing the Ansible behind the services I will be running, and how I plan to move all this to a CI/CD pipeline (or something like that)\n\nYou can hire me via Upwork or emailing me for weekend projects!","feature_image":"https://images.unsplash.com/photo-1555066931-4365d14bab8c?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwxMTc3M3wwfDF8c2VhcmNofDEzfHxjb2RlfGVufDB8fHx8MTYxNzUwMzcyMA&ixlib=rb-1.2.1&q=80&w=2000","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2021-04-04T02:35:04.000Z","updated_at":"2021-05-02T01:44:09.000Z","published_at":"2021-04-04T04:26:20.000Z","custom_excerpt":"Part 2 of moving to the cloud - Let's talk about IaC","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"6425f3bc192c0c150413d59f","uuid":"d7793028-b3c3-4e8d-9f01-1bee7fb2d34f","title":"What's behind breadNET","slug":"what-it-takes-to-run-breadnet","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"hr\",{}],[\"hr\",{}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/04/image-3.png\",\"width\":958,\"height\":298}],[\"markdown\",{\"markdown\":\"\\n*  Ghost\\n    * This is what runs my beautiful site\\n* Bookstack\\n    * KB and how to articles \\n* Gitlab\\n    * Source code and config managemnt lives here\\n* Jira\\n    * Project managment software and a good attempt to organize my life\\n* Jellyfin \\n    * Media server for all my legally sourced movies\\n* Grocy\\n    * Manages my food\\n* firefly-iii\\n    * Manages and makes me feel bad for spending money\\n* Passbolt\\n    * Password manager\\n* Matomo\\n    * Provides website analytics\\n* AWX\\n    * Ansible tower for server stuff and updates\"}],[\"markdown\",{\"markdown\":\"| Item | Count | Cost | Occurrence | Total Monthly | Total Yearly |\\n| --- | --- | --- | --- | --- | --- |\\n| Digital Ocean Droplet | 2 | $5 | Monthly | $10 | $120 | \\n| OVH Instance | 3 | £2.99 | Monthly | £10.76 | £129 |\\n| Wasabi Storage | idk | ($6 to 12) let's say $9 | Monthly | ~$9 | $108\"}],[\"bookmark\",{\"url\":\"__GHOST_URL__/cloud-migration-part-1/\",\"metadata\":{\"url\":\"__GHOST_URL__/cloud-migration-part-1/\",\"title\":\"breadNET Cloud Migration\",\"description\":\"How did I move all my servers to the cloud? Well - Ansible, automation and CI/CD!\",\"author\":\"Bradley Stannard\",\"publisher\":\"breadNET\",\"thumbnail\":\"https://images.unsplash.com/photo-1560182413-53772f3d7134?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=2000&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ\",\"icon\":\"https://breadnet.co.uk/favicon.png\"}}],[\"bookmark\",{\"url\":\"__GHOST_URL__/moving-to-the-cloud-2/\",\"metadata\":{\"url\":\"__GHOST_URL__/moving-to-the-cloud-2/\",\"title\":\"Moving to the cloud: Infrastructure\",\"description\":\"Part 2 of moving to the cloud - Let’s talk about IaC\",\"author\":\"Bradley Stannard\",\"publisher\":\"breadNET\",\"thumbnail\":\"https://images.unsplash.com/photo-1555066931-4365d14bab8c?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwxMTc3M3wwfDF8c2VhcmNofDEzfHxjb2RlfGVufDB8fHx8MTYxNzUwMzcyMA&ixlib=rb-1.2.1&q=80&w=2000\",\"icon\":\"https://breadnet.co.uk/favicon.png\"}}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/04/breaddns.png\",\"width\":3113,\"height\":1337}],[\"code\",{\"code\":\"version: '1.0'\\nstages:\\n  - checkout\\n  - prepare\\n  - deploy\\nsteps:\\n  main_clone:\\n    title: Cloning main repository...\\n    stage: checkout\\n    type: git-clone\\n    repo: '<bang your repo url here>'\\n    revision: master\\n    git: gitlab\\n  SetupAuth:\\n    image: alpine:3.9\\n    title: Configuring Auth\\n    stage: prepare\\n    commands:\\n      - export TF_VAR_cloudflare_email=$CLOUDFLARE_EMAIL\\n      - export TF_VAR_cloudflare_api_key=$CLOUDFLARE_API_KEY\\n  DeployWithTerraform:\\n    image: hashicorp/terraform:light\\n    title: Deploying Terraform plan\\n    stage: deploy\\n    commands:\\n      - terraform init -backend-config=\\\"token=\\\"$token\\\"\\\"\\n      - terraform apply -auto-approve\\n\",\"language\":\"yml\",\"caption\":\"https://gitlab.breadnet.co.uk/cicd/terraform/-/blob/master/codefresh.yml\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/04/Terraform-deployments.png\",\"width\":1600,\"height\":583}],[\"hr\",{}]],\"markups\":[[\"a\",[\"href\",\"https://anthonynsimon.com/blog/one-man-saas-architecture/?utm_source=breadnet.co.uk\"]],[\"a\",[\"href\",\"https://jake.nyc/words/tools-and-services-i-use-to-run-my-saas/\"]],[\"strong\"],[\"a\",[\"href\",\"https://cloudflare.com\"]],[\"a\",[\"href\",\"https://m.do.co/c/77be3c3aa96c\"]],[\"a\",[\"href\",\"https://ovh.com\"]],[\"a\",[\"href\",\"https://codefresh.io/\"]],[\"a\",[\"href\",\"https://app.terraform.io/\"]],[\"a\",[\"href\",\"https://wasabi.com\"]],[\"a\",[\"href\",\"https://namecheap.com\"]],[\"a\",[\"href\",\"__GHOST_URL__/p/d7793028-b3c3-4e8d-9f01-1bee7fb2d34f/terraform.io/\"]],[\"a\",[\"href\",\"https://www.ansible.com\"]],[\"a\",[\"href\",\"https://gitlab.breadnet.co.uk/explore/\"]],[\"a\",[\"href\",\"https://www.reuters.com/article/us-france-ovh-fire-idUSKBN2B20NU\"]],[\"a\",[\"href\",\"https://www.datacenterknowledge.com/archives/2012/07/03/multiple-generator-failures-caused-amazon-outage\"]],[\"a\",[\"href\",\"https://www.zdnet.com/article/it-wasnt-just-you-why-google-suffered-widespread-outages/\"]],[\"s\"],[\"a\",[\"href\",\"http://backuppc.sourceforge.net\\\\\"]],[\"a\",[\"href\",\"https://www.youtube.com/watch?v=fC7oUOUEEi4\"]],[\"a\",[\"href\",\"__GHOST_URL__/what-it-takes-to-run-breadnet/zabbix.com\"]],[\"a\",[\"href\",\"https://www.datadoghq.com\"]],[\"a\",[\"href\",\"https://www.librenms.org\"]],[\"em\"],[\"a\",[\"href\",\"https://www.upwork.com/freelancers/~01c61ee9802b94133e\"]],[\"a\",[\"href\",\"mailto:work@breadnet.co.uk\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Ever since I read \\\"\"],[0,[0],1,\"The architecture behind a one man SaaS\"],[0,[],0,\"\\\" and \\\"\"],[0,[1],1,\"Tools and services I use to run my SaaS\"],[0,[],0,\"\\\" I thought it would be cool to write about what powers breadNET and how I have things setup!\"]]],[1,\"p\",[]],[1,\"h3\",[[0,[],0,\"Let's start.\"]]],[1,\"p\",[[0,[],0,\"We will first take a look at what  breadNET is, as the about breadNET page is pretty bad and is pending a re-write. \"]]],[1,\"p\",[[0,[],0,\"breadNET (Yes, it's typed like that) started as my home lab project and business venture where I would host FOSS software like Kanboard, Bookstack, Jellyfin, passbolt etc. Basically the stuff I use day to day. Sadly this never took and another company came in and took this opportunity, gap in the market if you will, and did a pretty good job of it. Kudos!\"]]],[1,\"blockquote\",[[0,[],0,\"You can still by all means contact me to have me host these things for you for the cost of the server! Contact me via email or linkedin or what ever and we can work something out!\"]]],[1,\"p\",[[0,[],0,\"From there I decided just to change the site to a blog as for my job (just like everyone who works in IT) I spend a lot of time on google, and wanted to give back to the communities who rely on resources for help with things. \"]]],[1,\"p\",[[0,[],0,\"Enough chit-chat, let's dive in!\"]]],[10,0],[1,\"p\",[[0,[],0,\"We will break this down in to a few categories:\"]]],[3,\"ul\",[[[0,[],0,\"Hosted solutions\"]],[[0,[],0,\"Config management\"]],[[0,[],0,\"Servers and Software\"]],[[0,[],0,\"Backups\"]],[[0,[],0,\"Logging/ Monitoring\"]],[[0,[],0,\"Applications\"]],[[0,[],0,\"Cost\"]],[[0,[],0,\"Workflows\"]]]],[10,1],[1,\"h2\",[[0,[],0,\"Hosted Solutions\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"There are things that are just not best to host your self when you're looking for the best uptime avalible. \"]]],[1,\"p\",[[0,[2,3],2,\"Coudflare\"],[0,[],0,\" : DNS and DDOS protection\"]]],[1,\"p\",[[0,[4,2],2,\"Digitalocean\"],[0,[],0,\" : VPS hosting for mail server and web server (Highly recommend) \"]]],[1,\"p\",[[0,[2,5],2,\"OVH\"],[0,[],0,\" : Internal app hosting \"]]],[1,\"p\",[[0,[2,6],2,\"Codefresh\"],[0,[],0,\" : Ci/Cd pipelines\"]]],[1,\"p\",[[0,[7,2],1,\"Terraform\"],[0,[],1,\" cloud\"],[0,[],0,\" : Remote state for Terraform \"]]],[1,\"p\",[[0,[2,8],2,\"Wasabi\"],[0,[],0,\" : S3 compliant backups for cheap (but \"],[0,[2],1,\"very\"],[0,[],0,\" reliable)\"]]],[1,\"p\",[[0,[2,9],2,\"Namecheap\"],[0,[],0,\" : Really good pricing for domains and my GO TO for anything domain related (Except DNS) \"]]],[1,\"h2\",[[0,[],0,\"Config Management\"]]],[1,\"p\",[[0,[],0,\"This is the bane of my existence. In my ideal world anything I do I should be able to delete it and have it up and running again on Monday. (Let's be honest, this is more around me messing something up lol)\"]]],[1,\"blockquote\",[[0,[],0,\"\\\"Even if you lose all one day, you can build all over again if you retain your calm!\\\" - Thuan Pham, former CTO of Uber.\"]]],[1,\"p\",[[0,[10,2],2,\"Terraform\"],[0,[],0,\" :  This is what I use for creating cloud deployments, all the way from a load balancer to a database as a service, terraform can do it\"]]],[1,\"p\",[[0,[11,2],2,\"Ansible\"],[0,[],0,\" : This is what I use for provisioning my servers and getting them up to operating standards. Also use it on a cron job to keep all my servers up to date.\"]]],[1,\"p\",[[0,[12,2],2,\"Gitlab\"],[0,[],0,\" : This is where all my code lives. I don;t know why I use this over github, but I like it :) \"]]],[1,\"h2\",[[0,[],0,\"Servers and software\"]]],[1,\"p\",[[0,[],0,\"This little section is about the servers and software that power this place\"]]],[1,\"p\",[[0,[2],1,\"Ubuntu\"],[0,[],0,\" : The choice OS for any server I deploy. This is what I grew up with, and this is what I know very well.\"]]],[1,\"p\",[[0,[2],1,\"nginx\"],[0,[],0,\" : Once again, this is what I grew up with and know well. This powers EVERY web server I have ever deployed. Unless it's apache then that wast me!\"]]],[1,\"p\",[[0,[2],1,\"mariadb\"],[0,[],0,\" : This is my go to database engine for any database that I require. If an application allows me to use mariadb, you bet I will use it!\"]]],[1,\"p\",[[0,[2],1,\"rclone\"],[0,[],0,\" : This is an important piece of software I use for synching data between many different services, s3, drive, gcs etc...\"]]],[1,\"p\",[[0,[2],1,\"intelliJ\"],[0,[],0,\" : This is the most beautiful IDE I have ever used, strongly recommend \"]]],[1,\"p\",[[0,[2],1,\"direnv\"],[0,[],0,\" : Allows setting environment variables per directory, great for terraform and projects that need env variables \"]]],[1,\"h2\",[[0,[],0,\"Backups\"]]],[1,\"p\",[[0,[],0,\"This is the most important part of any business or lab. Without backups, nothing is really important. \"]]],[10,2],[1,\"p\",[[0,[],0,\"I follow the 3-2-1 rule, and I suggest you do! \"],[1,[],0,0],[0,[],0,\"The off site backups are designed to be used if something was to \"],[0,[13],1,\"burn down \"],[0,[],0,\"or the \"],[0,[14],1,\"backup generators not actually doing what they're meant to\"],[0,[],0,\" or \"],[0,[15],1,\"routers just going \\\"nah bro\\\"\"],[0,[],0,\" - Okay, I'm done \"],[0,[16],1,\"shitting on the cloud\"],[0,[],0,\" proving why you should have many backups!\"]]],[1,\"p\",[[0,[17,2],2,\"backuppc\"],[0,[],0,\" : I know, the site looks bad and the UI is old, but boy does this software haul ass. Highly reccomend\"]]],[1,\"p\",[[0,[2],1,\"S3\"],[0,[],0,\" : See Wasabi from Above\"]]],[1,\"p\",[[0,[18,2],2,\"Raspberry pi and a harddrive and a solid connection\"],[0,[],0,\": This serves as the UK backup location \"]]],[1,\"h2\",[[0,[],0,\"Monitoring/ Logging\"]]],[1,\"p\",[[0,[],0,\"This is one of those things that are often overlooked, but when \"],[0,[16],1,\"shit hits the fan\"],[0,[],0,\" things go wrong, being able to look at a graph and point to a spike and go \\\"yeah that's \"],[0,[16],1,\"fucked\"],[0,[],0,\" broken\\\" really helps, especially if you're able to then dial down in to each service and see what's happening. \"]]],[1,\"p\",[[0,[19,2],2,\"Zabbix\"],[0,[],0,\" : Providing agent metrics, mtr, snmp and everything I can jam in to it, in one place as well as alerting\"]]],[1,\"p\",[[0,[20,2],2,\"Datadog\"],[0,[],0,\" : Monitoring for cloud environments, little pricey but free tier is DECENT \"]]],[1,\"p\",[[0,[21,2],2,\"libreNMS\"],[0,[],0,\" : as I move all resources to the cloud, this will be decommissioned, but really good for network monitoring where Zabbix just wont cut it. \"]]],[1,\"p\",[[0,[2],1,\"Elastic stack\"],[0,[],0,\" : Coming soon! (I think?) \"]]],[1,\"h2\",[[0,[],0,\"Applications\"]]],[1,\"p\",[[0,[],0,\"This is the stuff I use day to day, and will happily host for you if you pay me to do it. \"]]],[10,3],[1,\"h2\",[[0,[],0,\"Cost\"]]],[1,\"p\",[[0,[],0,\"I've never done an exact break down but a rough estimate would look like\"]]],[10,4],[1,\"p\",[[0,[],0,\"So all in all, it costs me around about £295 at the time of writing this \"],[1,[],0,1],[0,[22],1,\"(April the 10th at 3:33am like an idiot, my (new) girlfriend will be here in like 8 hours and this is what I decide to do... let's see how long she can survive seeing someone who works in IT and takes their hobbies very very seriously) \"]]],[1,\"p\",[[0,[],0,\"Now the reason I don't know about Wasabi is due to it being how much I use and delete per month. They don't charge upload and download so I can do that as much as I want, more so for storage and if you delete 1TB tomorrow, you pay for that TB for 3 months. Eh, sucks but i'm yet to find a better offering that is so simple. \"]]],[1,\"h2\",[[0,[],0,\"Workflows\"]]],[1,\"p\",[[0,[],0,\"This is a strange one to write about as I am constantly learning new technology and moving things around, but let's look at an example that we're currently working on!\"]]],[1,\"p\",[[0,[],0,\"Moving my sheeeet to the cloud!\"]]],[1,\"p\",[[0,[],0,\"(shameless self plug below)\"]]],[10,5],[10,6],[1,\"p\",[[0,[],0,\"I have decided that I want to be able to simply deploy DNS records with minimal pain and agg, and to do this it would be best to do it though Terraform and then if I hadn't over complicated it enough, decided to automate the process of actually deploying it!\"]]],[1,\"p\",[[0,[],0,\"For this I have used Terraform, Gitlab and Codefresh\"]]],[10,7],[1,\"p\",[[0,[],0,\"Below is an example of the codefresh.yml \"]]],[10,8],[1,\"p\",[[0,[],0,\"Second workflow would be creating infrastructure for a deployment\"]]],[10,9],[1,\"p\",[[0,[],0,\"This way I ensure that terraform is uniform, and where the module already exists, I don't have to fart around with some strange issues.\"]]],[1,\"p\",[[0,[],0,\"Future plans are to fully opensource all code I write and move any secrets to environment vars so modules can be used anywhere!\"]]],[1,\"p\",[[0,[],0,\"My end goal here is to have everything under git control and cicd so I just describe something as code and then boom, it appears 3 minutes later.  \"]]],[10,10],[1,\"p\",[[0,[],0,\"You can hire me via \"],[0,[23],1,\"Upwork\"],[0,[],0,\" or \"],[0,[24],1,\"emailing me\"],[0,[],0,\" for weekend projects!\"]]]],\"ghostVersion\":\"3.0\"}","lexical":null,"html":"<p>Ever since I read \"<a href=\"https://anthonynsimon.com/blog/one-man-saas-architecture/?utm_source=breadnet.co.uk\">The architecture behind a one man SaaS</a>\" and \"<a href=\"https://jake.nyc/words/tools-and-services-i-use-to-run-my-saas/\">Tools and services I use to run my SaaS</a>\" I thought it would be cool to write about what powers breadNET and how I have things setup!</p><p></p><h3 id=\"let-s-start-\">Let's start.</h3><p>We will first take a look at what  breadNET is, as the about breadNET page is pretty bad and is pending a re-write. </p><p>breadNET (Yes, it's typed like that) started as my home lab project and business venture where I would host FOSS software like Kanboard, Bookstack, Jellyfin, passbolt etc. Basically the stuff I use day to day. Sadly this never took and another company came in and took this opportunity, gap in the market if you will, and did a pretty good job of it. Kudos!</p><blockquote>You can still by all means contact me to have me host these things for you for the cost of the server! Contact me via email or linkedin or what ever and we can work something out!</blockquote><p>From there I decided just to change the site to a blog as for my job (just like everyone who works in IT) I spend a lot of time on google, and wanted to give back to the communities who rely on resources for help with things. </p><p>Enough chit-chat, let's dive in!</p><hr><p>We will break this down in to a few categories:</p><ul><li>Hosted solutions</li><li>Config management</li><li>Servers and Software</li><li>Backups</li><li>Logging/ Monitoring</li><li>Applications</li><li>Cost</li><li>Workflows</li></ul><hr><h2 id=\"hosted-solutions\">Hosted Solutions</h2><p></p><p>There are things that are just not best to host your self when you're looking for the best uptime avalible. </p><p><strong><a href=\"https://cloudflare.com\">Coudflare</a></strong> : DNS and DDOS protection</p><p><a href=\"https://m.do.co/c/77be3c3aa96c\"><strong>Digitalocean</strong></a> : VPS hosting for mail server and web server (Highly recommend) </p><p><strong><a href=\"https://ovh.com\">OVH</a></strong> : Internal app hosting </p><p><strong><a href=\"https://codefresh.io/\">Codefresh</a></strong> : Ci/Cd pipelines</p><p><a href=\"https://app.terraform.io/\"><strong>Terraform</strong> cloud</a> : Remote state for Terraform </p><p><strong><a href=\"https://wasabi.com\">Wasabi</a></strong> : S3 compliant backups for cheap (but <strong>very</strong> reliable)</p><p><strong><a href=\"https://namecheap.com\">Namecheap</a></strong> : Really good pricing for domains and my GO TO for anything domain related (Except DNS) </p><h2 id=\"config-management\">Config Management</h2><p>This is the bane of my existence. In my ideal world anything I do I should be able to delete it and have it up and running again on Monday. (Let's be honest, this is more around me messing something up lol)</p><blockquote>\"Even if you lose all one day, you can build all over again if you retain your calm!\" - Thuan Pham, former CTO of Uber.</blockquote><p><a href=\"__GHOST_URL__/p/d7793028-b3c3-4e8d-9f01-1bee7fb2d34f/terraform.io/\"><strong>Terraform</strong></a> :  This is what I use for creating cloud deployments, all the way from a load balancer to a database as a service, terraform can do it</p><p><a href=\"https://www.ansible.com\"><strong>Ansible</strong></a> : This is what I use for provisioning my servers and getting them up to operating standards. Also use it on a cron job to keep all my servers up to date.</p><p><a href=\"https://gitlab.breadnet.co.uk/explore/\"><strong>Gitlab</strong></a> : This is where all my code lives. I don;t know why I use this over github, but I like it :) </p><h2 id=\"servers-and-software\">Servers and software</h2><p>This little section is about the servers and software that power this place</p><p><strong>Ubuntu</strong> : The choice OS for any server I deploy. This is what I grew up with, and this is what I know very well.</p><p><strong>nginx</strong> : Once again, this is what I grew up with and know well. This powers EVERY web server I have ever deployed. Unless it's apache then that wast me!</p><p><strong>mariadb</strong> : This is my go to database engine for any database that I require. If an application allows me to use mariadb, you bet I will use it!</p><p><strong>rclone</strong> : This is an important piece of software I use for synching data between many different services, s3, drive, gcs etc...</p><p><strong>intelliJ</strong> : This is the most beautiful IDE I have ever used, strongly recommend </p><p><strong>direnv</strong> : Allows setting environment variables per directory, great for terraform and projects that need env variables </p><h2 id=\"backups\">Backups</h2><p>This is the most important part of any business or lab. Without backups, nothing is really important. </p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2021/04/image-3.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"958\" height=\"298\" srcset=\"__GHOST_URL__/content/images/size/w600/2021/04/image-3.png 600w, __GHOST_URL__/content/images/2021/04/image-3.png 958w\" sizes=\"(min-width: 720px) 720px\"></figure><p>I follow the 3-2-1 rule, and I suggest you do! <br>The off site backups are designed to be used if something was to <a href=\"https://www.reuters.com/article/us-france-ovh-fire-idUSKBN2B20NU\">burn down </a>or the <a href=\"https://www.datacenterknowledge.com/archives/2012/07/03/multiple-generator-failures-caused-amazon-outage\">backup generators not actually doing what they're meant to</a> or <a href=\"https://www.zdnet.com/article/it-wasnt-just-you-why-google-suffered-widespread-outages/\">routers just going \"nah bro\"</a> - Okay, I'm done <s>shitting on the cloud</s> proving why you should have many backups!</p><p><a href=\"http://backuppc.sourceforge.net\\\"><strong>backuppc</strong></a> : I know, the site looks bad and the UI is old, but boy does this software haul ass. Highly reccomend</p><p><strong>S3</strong> : See Wasabi from Above</p><p><a href=\"https://www.youtube.com/watch?v=fC7oUOUEEi4\"><strong>Raspberry pi and a harddrive and a solid connection</strong></a>: This serves as the UK backup location </p><h2 id=\"monitoring-logging\">Monitoring/ Logging</h2><p>This is one of those things that are often overlooked, but when <s>shit hits the fan</s> things go wrong, being able to look at a graph and point to a spike and go \"yeah that's <s>fucked</s> broken\" really helps, especially if you're able to then dial down in to each service and see what's happening. </p><p><a href=\"__GHOST_URL__/what-it-takes-to-run-breadnet/zabbix.com\"><strong>Zabbix</strong></a> : Providing agent metrics, mtr, snmp and everything I can jam in to it, in one place as well as alerting</p><p><a href=\"https://www.datadoghq.com\"><strong>Datadog</strong></a> : Monitoring for cloud environments, little pricey but free tier is DECENT </p><p><a href=\"https://www.librenms.org\"><strong>libreNMS</strong></a> : as I move all resources to the cloud, this will be decommissioned, but really good for network monitoring where Zabbix just wont cut it. </p><p><strong>Elastic stack</strong> : Coming soon! (I think?) </p><h2 id=\"applications\">Applications</h2><p>This is the stuff I use day to day, and will happily host for you if you pay me to do it. </p><!--kg-card-begin: markdown--><ul>\n<li>Ghost\n<ul>\n<li>This is what runs my beautiful site</li>\n</ul>\n</li>\n<li>Bookstack\n<ul>\n<li>KB and how to articles</li>\n</ul>\n</li>\n<li>Gitlab\n<ul>\n<li>Source code and config managemnt lives here</li>\n</ul>\n</li>\n<li>Jira\n<ul>\n<li>Project managment software and a good attempt to organize my life</li>\n</ul>\n</li>\n<li>Jellyfin\n<ul>\n<li>Media server for all my legally sourced movies</li>\n</ul>\n</li>\n<li>Grocy\n<ul>\n<li>Manages my food</li>\n</ul>\n</li>\n<li>firefly-iii\n<ul>\n<li>Manages and makes me feel bad for spending money</li>\n</ul>\n</li>\n<li>Passbolt\n<ul>\n<li>Password manager</li>\n</ul>\n</li>\n<li>Matomo\n<ul>\n<li>Provides website analytics</li>\n</ul>\n</li>\n<li>AWX\n<ul>\n<li>Ansible tower for server stuff and updates</li>\n</ul>\n</li>\n</ul>\n<!--kg-card-end: markdown--><h2 id=\"cost\">Cost</h2><p>I've never done an exact break down but a rough estimate would look like</p><!--kg-card-begin: markdown--><table>\n<thead>\n<tr>\n<th>Item</th>\n<th>Count</th>\n<th>Cost</th>\n<th>Occurrence</th>\n<th>Total Monthly</th>\n<th>Total Yearly</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Digital Ocean Droplet</td>\n<td>2</td>\n<td>$5</td>\n<td>Monthly</td>\n<td>$10</td>\n<td>$120</td>\n</tr>\n<tr>\n<td>OVH Instance</td>\n<td>3</td>\n<td>£2.99</td>\n<td>Monthly</td>\n<td>£10.76</td>\n<td>£129</td>\n</tr>\n<tr>\n<td>Wasabi Storage</td>\n<td>idk</td>\n<td>($6 to 12) let's say $9</td>\n<td>Monthly</td>\n<td>~$9</td>\n<td>$108</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: markdown--><p>So all in all, it costs me around about £295 at the time of writing this <br><em>(April the 10th at 3:33am like an idiot, my (new) girlfriend will be here in like 8 hours and this is what I decide to do... let's see how long she can survive seeing someone who works in IT and takes their hobbies very very seriously) </em></p><p>Now the reason I don't know about Wasabi is due to it being how much I use and delete per month. They don't charge upload and download so I can do that as much as I want, more so for storage and if you delete 1TB tomorrow, you pay for that TB for 3 months. Eh, sucks but i'm yet to find a better offering that is so simple. </p><h2 id=\"workflows\">Workflows</h2><p>This is a strange one to write about as I am constantly learning new technology and moving things around, but let's look at an example that we're currently working on!</p><p>Moving my sheeeet to the cloud!</p><p>(shameless self plug below)</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"__GHOST_URL__/cloud-migration-part-1/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">breadNET Cloud Migration</div><div class=\"kg-bookmark-description\">How did I move all my servers to the cloud? Well - Ansible, automation and CI/CD!</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://breadnet.co.uk/favicon.png\" alt=\"\"><span class=\"kg-bookmark-author\">breadNET</span><span class=\"kg-bookmark-publisher\">Bradley Stannard</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://images.unsplash.com/photo-1560182413-53772f3d7134?ixlib&#x3D;rb-1.2.1&amp;q&#x3D;80&amp;fm&#x3D;jpg&amp;crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;w&#x3D;2000&amp;fit&#x3D;max&amp;ixid&#x3D;eyJhcHBfaWQiOjExNzczfQ\" alt=\"\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"__GHOST_URL__/moving-to-the-cloud-2/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Moving to the cloud: Infrastructure</div><div class=\"kg-bookmark-description\">Part 2 of moving to the cloud - Let’s talk about IaC</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://breadnet.co.uk/favicon.png\" alt=\"\"><span class=\"kg-bookmark-author\">breadNET</span><span class=\"kg-bookmark-publisher\">Bradley Stannard</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://images.unsplash.com/photo-1555066931-4365d14bab8c?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;MnwxMTc3M3wwfDF8c2VhcmNofDEzfHxjb2RlfGVufDB8fHx8MTYxNzUwMzcyMA&amp;ixlib&#x3D;rb-1.2.1&amp;q&#x3D;80&amp;w&#x3D;2000\" alt=\"\"></div></a></figure><p>I have decided that I want to be able to simply deploy DNS records with minimal pain and agg, and to do this it would be best to do it though Terraform and then if I hadn't over complicated it enough, decided to automate the process of actually deploying it!</p><p>For this I have used Terraform, Gitlab and Codefresh</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2021/04/breaddns.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"859\" srcset=\"__GHOST_URL__/content/images/size/w600/2021/04/breaddns.png 600w, __GHOST_URL__/content/images/size/w1000/2021/04/breaddns.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/04/breaddns.png 1600w, __GHOST_URL__/content/images/size/w2400/2021/04/breaddns.png 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Below is an example of the codefresh.yml </p><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-yml\">version: '1.0'\nstages:\n  - checkout\n  - prepare\n  - deploy\nsteps:\n  main_clone:\n    title: Cloning main repository...\n    stage: checkout\n    type: git-clone\n    repo: '&lt;bang your repo url here&gt;'\n    revision: master\n    git: gitlab\n  SetupAuth:\n    image: alpine:3.9\n    title: Configuring Auth\n    stage: prepare\n    commands:\n      - export TF_VAR_cloudflare_email=$CLOUDFLARE_EMAIL\n      - export TF_VAR_cloudflare_api_key=$CLOUDFLARE_API_KEY\n  DeployWithTerraform:\n    image: hashicorp/terraform:light\n    title: Deploying Terraform plan\n    stage: deploy\n    commands:\n      - terraform init -backend-config=\"token=\"$token\"\"\n      - terraform apply -auto-approve\n</code></pre><figcaption>https://gitlab.breadnet.co.uk/cicd/terraform/-/blob/master/codefresh.yml</figcaption></figure><p>Second workflow would be creating infrastructure for a deployment</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2021/04/Terraform-deployments.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1600\" height=\"583\" srcset=\"__GHOST_URL__/content/images/size/w600/2021/04/Terraform-deployments.png 600w, __GHOST_URL__/content/images/size/w1000/2021/04/Terraform-deployments.png 1000w, __GHOST_URL__/content/images/2021/04/Terraform-deployments.png 1600w\" sizes=\"(min-width: 720px) 720px\"></figure><p>This way I ensure that terraform is uniform, and where the module already exists, I don't have to fart around with some strange issues.</p><p>Future plans are to fully opensource all code I write and move any secrets to environment vars so modules can be used anywhere!</p><p>My end goal here is to have everything under git control and cicd so I just describe something as code and then boom, it appears 3 minutes later.  </p><hr><p>You can hire me via <a href=\"https://www.upwork.com/freelancers/~01c61ee9802b94133e\">Upwork</a> or <a href=\"mailto:work@breadnet.co.uk\">emailing me</a> for weekend projects!</p>","comment_id":"6070ffffbea1f916bfb84174","plaintext":"Ever since I read \"The architecture behind a one man SaaS\" and \"Tools and services I use to run my SaaS\" I thought it would be cool to write about what powers breadNET and how I have things setup!\n\n\n\n\nLet's start.\n\nWe will first take a look at what  breadNET is, as the about breadNET page is pretty bad and is pending a re-write.\n\nbreadNET (Yes, it's typed like that) started as my home lab project and business venture where I would host FOSS software like Kanboard, Bookstack, Jellyfin, passbolt etc. Basically the stuff I use day to day. Sadly this never took and another company came in and took this opportunity, gap in the market if you will, and did a pretty good job of it. Kudos!\n\nYou can still by all means contact me to have me host these things for you for the cost of the server! Contact me via email or linkedin or what ever and we can work something out!\n\nFrom there I decided just to change the site to a blog as for my job (just like everyone who works in IT) I spend a lot of time on google, and wanted to give back to the communities who rely on resources for help with things.\n\nEnough chit-chat, let's dive in!\n\nWe will break this down in to a few categories:\n\n * Hosted solutions\n * Config management\n * Servers and Software\n * Backups\n * Logging/ Monitoring\n * Applications\n * Cost\n * Workflows\n\n\nHosted Solutions\n\n\n\nThere are things that are just not best to host your self when you're looking for the best uptime avalible.\n\nCoudflare : DNS and DDOS protection\n\nDigitalocean : VPS hosting for mail server and web server (Highly recommend)\n\nOVH : Internal app hosting\n\nCodefresh : Ci/Cd pipelines\n\nTerraform cloud : Remote state for Terraform\n\nWasabi : S3 compliant backups for cheap (but very reliable)\n\nNamecheap : Really good pricing for domains and my GO TO for anything domain related (Except DNS)\n\n\nConfig Management\n\nThis is the bane of my existence. In my ideal world anything I do I should be able to delete it and have it up and running again on Monday. (Let's be honest, this is more around me messing something up lol)\n\n\"Even if you lose all one day, you can build all over again if you retain your calm!\" - Thuan Pham, former CTO of Uber.\n\nTerraform :  This is what I use for creating cloud deployments, all the way from a load balancer to a database as a service, terraform can do it\n\nAnsible : This is what I use for provisioning my servers and getting them up to operating standards. Also use it on a cron job to keep all my servers up to date.\n\nGitlab : This is where all my code lives. I don;t know why I use this over github, but I like it :)\n\n\nServers and software\n\nThis little section is about the servers and software that power this place\n\nUbuntu : The choice OS for any server I deploy. This is what I grew up with, and this is what I know very well.\n\nnginx : Once again, this is what I grew up with and know well. This powers EVERY web server I have ever deployed. Unless it's apache then that wast me!\n\nmariadb : This is my go to database engine for any database that I require. If an application allows me to use mariadb, you bet I will use it!\n\nrclone : This is an important piece of software I use for synching data between many different services, s3, drive, gcs etc...\n\nintelliJ : This is the most beautiful IDE I have ever used, strongly recommend\n\ndirenv : Allows setting environment variables per directory, great for terraform and projects that need env variables\n\n\nBackups\n\nThis is the most important part of any business or lab. Without backups, nothing is really important.\n\nI follow the 3-2-1 rule, and I suggest you do!\nThe off site backups are designed to be used if something was to burn down or the backup generators not actually doing what they're meant to or routers just going \"nah bro\" - Okay, I'm done shitting on the cloud proving why you should have many backups!\n\nbackuppc : I know, the site looks bad and the UI is old, but boy does this software haul ass. Highly reccomend\n\nS3 : See Wasabi from Above\n\nRaspberry pi and a harddrive and a solid connection: This serves as the UK backup location\n\n\nMonitoring/ Logging\n\nThis is one of those things that are often overlooked, but when shit hits the fan things go wrong, being able to look at a graph and point to a spike and go \"yeah that's fucked broken\" really helps, especially if you're able to then dial down in to each service and see what's happening.\n\nZabbix : Providing agent metrics, mtr, snmp and everything I can jam in to it, in one place as well as alerting\n\nDatadog : Monitoring for cloud environments, little pricey but free tier is DECENT\n\nlibreNMS : as I move all resources to the cloud, this will be decommissioned, but really good for network monitoring where Zabbix just wont cut it.\n\nElastic stack : Coming soon! (I think?)\n\n\nApplications\n\nThis is the stuff I use day to day, and will happily host for you if you pay me to do it.\n\n * Ghost\n   \n   * This is what runs my beautiful site\n   \n * Bookstack\n   \n   * KB and how to articles\n   \n * Gitlab\n   \n   * Source code and config managemnt lives here\n   \n * Jira\n   \n   * Project managment software and a good attempt to organize my life\n   \n * Jellyfin\n   \n   * Media server for all my legally sourced movies\n   \n * Grocy\n   \n   * Manages my food\n   \n * firefly-iii\n   \n   * Manages and makes me feel bad for spending money\n   \n * Passbolt\n   \n   * Password manager\n   \n * Matomo\n   \n   * Provides website analytics\n   \n * AWX\n   \n   * Ansible tower for server stuff and updates\n   \n\n\n\nCost\n\nI've never done an exact break down but a rough estimate would look like\n\n\n\n\nItem\nCount\nCost\nOccurrence\nTotal Monthly\nTotal Yearly\n\n\n\n\nDigital Ocean Droplet\n2\n$5\nMonthly\n$10\n$120\n\n\nOVH Instance\n3\n£2.99\nMonthly\n£10.76\n£129\n\n\nWasabi Storage\nidk\n($6 to 12) let's say $9\nMonthly\n~$9\n$108\n\n\n\n\n\nSo all in all, it costs me around about £295 at the time of writing this\n(April the 10th at 3:33am like an idiot, my (new) girlfriend will be here in like 8 hours and this is what I decide to do... let's see how long she can survive seeing someone who works in IT and takes their hobbies very very seriously)\n\nNow the reason I don't know about Wasabi is due to it being how much I use and delete per month. They don't charge upload and download so I can do that as much as I want, more so for storage and if you delete 1TB tomorrow, you pay for that TB for 3 months. Eh, sucks but i'm yet to find a better offering that is so simple.\n\n\nWorkflows\n\nThis is a strange one to write about as I am constantly learning new technology and moving things around, but let's look at an example that we're currently working on!\n\nMoving my sheeeet to the cloud!\n\n(shameless self plug below)\n\nbreadNET Cloud MigrationHow did I move all my servers to the cloud? Well - Ansible, automation and CI/CD!breadNETBradley StannardMoving to the cloud: InfrastructurePart 2 of moving to the cloud - Let’s talk about IaCbreadNETBradley Stannard\n\nI have decided that I want to be able to simply deploy DNS records with minimal pain and agg, and to do this it would be best to do it though Terraform and then if I hadn't over complicated it enough, decided to automate the process of actually deploying it!\n\nFor this I have used Terraform, Gitlab and Codefresh\n\nBelow is an example of the codefresh.yml\n\nversion: '1.0'\nstages:\n  - checkout\n  - prepare\n  - deploy\nsteps:\n  main_clone:\n    title: Cloning main repository...\n    stage: checkout\n    type: git-clone\n    repo: '<bang your repo url here>'\n    revision: master\n    git: gitlab\n  SetupAuth:\n    image: alpine:3.9\n    title: Configuring Auth\n    stage: prepare\n    commands:\n      - export TF_VAR_cloudflare_email=$CLOUDFLARE_EMAIL\n      - export TF_VAR_cloudflare_api_key=$CLOUDFLARE_API_KEY\n  DeployWithTerraform:\n    image: hashicorp/terraform:light\n    title: Deploying Terraform plan\n    stage: deploy\n    commands:\n      - terraform init -backend-config=\"token=\"$token\"\"\n      - terraform apply -auto-approve\n\n\nSecond workflow would be creating infrastructure for a deployment\n\nThis way I ensure that terraform is uniform, and where the module already exists, I don't have to fart around with some strange issues.\n\nFuture plans are to fully opensource all code I write and move any secrets to environment vars so modules can be used anywhere!\n\nMy end goal here is to have everything under git control and cicd so I just describe something as code and then boom, it appears 3 minutes later.  \n\nYou can hire me via Upwork or emailing me for weekend projects!","feature_image":"https://images.unsplash.com/photo-1616551569669-b60598758c4f?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwxMTc3M3wwfDF8c2VhcmNofDZ8fHRlYXIlMjBkb3dufGVufDB8fHx8MTYxODAyNDM2NQ&ixlib=rb-1.2.1&q=80&w=2000","featured":1,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2021-04-10T01:31:43.000Z","updated_at":"2023-03-26T14:42:13.000Z","published_at":"2021-04-10T03:15:01.000Z","custom_excerpt":"Let's take a peak behind the scenes at breadNET and see what it takes to keep this well(ish) oiled machine chugging","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"6425f3bc192c0c150413d5a0","uuid":"c95147f7-a5a4-4fcc-af9d-67ebde8ef74c","title":"Self hosters guide to the cloud","slug":"self-hosters-guide-to-the-cloud","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"hr\",{}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/04/image-7.png\",\"width\":500,\"height\":756,\"caption\":\"I will be writing a blog post shortly about that\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/04/image-8.png\",\"width\":583,\"height\":411}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/04/image-9.png\",\"width\":455,\"height\":737}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/04/image-10.png\",\"width\":443,\"height\":226}],[\"code\",{\"code\":\"Dear sir,\\n\\nI write to you to request one server with a gigabyte of ram, 1TB data and 2vcpu\\n\\nSigned,\\n\\nBradley\"}],[\"bookmark\",{\"url\":\"https://gitlab.breadnet.co.uk/terraform/modules/ovh\",\"metadata\":{\"url\":\"https://gitlab.breadnet.co.uk/terraform/modules/ovh\",\"title\":\"ovh\",\"description\":\"GitLab Enterprise Edition\",\"author\":null,\"publisher\":\"GitLab\",\"thumbnail\":\"https://gitlab.breadnet.co.uk/assets/gitlab_logo-7ae504fe4f68fdebb3c2034e36621930cd36ea87924c11ff65dbcb8ed50dca58.png\",\"icon\":\"https://gitlab.breadnet.co.uk/assets/touch-icon-ipad-retina-8ebe416f5313483d9c1bc772b5bbe03ecad52a54eba443e5215a22caed2a16a2.png\"}}],[\"hr\",{}],[\"hr\",{}]],\"markups\":[[\"s\"],[\"a\",[\"href\",\"https://stackoverflow.com/questions/53527277/is-it-possible-to-run-containers-on-android-devices\"]],[\"code\"],[\"a\",[\"href\",\"https://gitlab.breadnet.co.uk/terraform/modules/ovh/instance-ports/-/blob/074ac784d6c9533a7de7c4c4de32ecaa0c6f72f7/main.tf\"]],[\"a\",[\"href\",\"https://www.packer.io\"]],[\"a\",[\"href\",\"https://www.reuters.com/article/us-france-ovh-fire-idUSKBN2B20NU\"]],[\"a\",[\"href\",\"https://m.do.co/c/77be3c3aa96c\"]],[\"a\",[\"href\",\"zerotier.com\"]],[\"a\",[\"href\",\"https://www.upwork.com/freelancers/~01c61ee9802b94133e\"]],[\"a\",[\"href\",\"mailto:work@breadnet.co.uk\"]]],\"sections\":[[1,\"p\",[]],[3,\"ul\",[[[0,[],0,\"services to offer\"]],[[0,[],0,\"how it's different\"]],[[0,[],0,\"ways to manage it\"]],[[0,[],0,\"best practices\"]],[[0,[],0,\"how to make the most of it\"]],[[0,[],0,\"companies I suggest\"]]]],[1,\"p\",[[0,[],0,\"I'll be honest with eneryone here, I never thought I would ever write something like this. \"]]],[1,\"p\",[[0,[],0,\"The time has come, I am moving my servers to the cloud. \"]]],[1,\"p\",[[0,[],0,\"I still am a selfhoster as I rely on my own knowledge and experience to operate all my services and ensure that they are online for the people who have come to rely on them.\"]]],[1,\"p\",[[0,[],0,\"The reason I am moving to the cloud is since moving out of my parents, I need to decom all my on premise infrastructure as the network connection isn't the best and also it's at my parent house. \"]]],[1,\"p\",[[0,[],0,\"Let's take a look in to the reasons you would want to move your lab to the cloud (If you plan on still labbing), how to set it up and then how to maintain it.\"]]],[10,0],[1,\"p\",[[0,[],0,\"A lot of cloud providers will have different plans and offerings. Let's focus real quick on the terminology they use, as some of it is literal \"],[0,[0],1,\"bullshit\"],[0,[],0,\" \"]]],[1,\"p\",[[0,[],0,\"IaaS : Infrastructure as a service:\"],[1,[],0,0],[0,[],0,\"\\tThis would be renting the servers, network (yeah you can do that), storage etc from the \\tprovider\"]]],[1,\"p\",[[0,[],0,\"PaaS : Platform as a service:\"],[1,[],0,1],[0,[],0,\"\\tThis is delivering a platform to the user, so the developer just runs their code on their \\t     service and don't have to manage the infrastrucute. I'm not sure what their underlying \\t     tech is, but I'm probably going to go with K8 \"]]],[1,\"p\",[[0,[],0,\"SaaS : Software as a service:\"],[1,[],0,2],[0,[],0,\"\\tThis would be using something like Google drive, SAP etc. You use the software as a \\t     service\"]]],[1,\"p\",[[0,[],0,\"DBaaS : Database as a service\"],[1,[],0,3],[0,[],0,\"\\tIf you run a lot of applications that need a database, I suggest you use your cloud providers database offering as it's got backups usually and runs as a cluster with HA. \"]]],[1,\"p\",[[0,[],0,\"You will want to look at each cloud providers offerings. One thing to note is that you can actually land up causing cloud lock in. \"]]],[1,\"p\",[[0,[],0,\"<Place holder till I write about that> \"]]],[10,1],[1,\"p\",[[0,[],0,\"You can avoid vendor lock in (which in this case is a cloud provider) by ensuring you have a planned exist strategy, as well as ensuring that you have 100% ownership of your data. \"]]],[1,\"p\",[[0,[],0,\"If you are developing an application, use docker! You can run docker on basically anything \"],[0,[1],1,\"(well, almost)\"],[0,[],0,\" you then can bang it in to K8 (which every cool cloud provider will have) \"]]],[1,\"p\",[[0,[],0,\"Finally once you've picked your cloud provider you then have the whole setting 'it' up.\"],[1,[],0,4],[0,[],0,\"'it' being what ever you're hosting at home.\"]]],[1,\"p\",[[0,[],0,\"The common misconception I've seen with people and the cloud is that:\"]]],[10,2],[1,\"p\",[[0,[],0,\"no. Just no. Stop. I can understand that thought process if you were new to computers, but you guys running a datacetnre in your mom's living room, no. \"]]],[1,\"p\",[[0,[],0,\"Let me introduce you to a little friend called 'Firewalls' \"]]],[1,\"p\",[[0,[],0,\"At an absolute minimum, your cloud provider will have a form of managed firewall service where you can click and point. \"]]],[1,\"p\",[[0,[],0,\"My favourite way of this, which GCP does quite well is:\"]]],[1,\"p\",[[0,[],0,\"You create the firewall rules first. So we will create one allowing http/s access to our instances, and then we say we want to add it to any instance that has the tag of \"],[0,[2],1,\"web\"],[0,[],0,\" assigned under the network section:\"]]],[10,3],[1,\"p\",[[0,[],0,\"then under our instance we get this:\"]]],[10,4],[1,\"p\",[[0,[],0,\"Point in case, if you are still worried about people getting to your servers, then you can make a career out of that as a cyber security expert. Start studying my friend!\"]]],[1,\"p\",[[0,[],0,\"Now we have that out the way, we will talk about how to actually deploy.\"]]],[1,\"p\",[[0,[],0,\"Running on the cloud has some perks. You get access to things like API's where you can tell the cloud:\"]]],[10,5],[1,\"p\",[[0,[],0,\"(Side note, if you want to see how to actually do it: --> \"],[0,[3],1,\"Here\"],[0,[],0,\" <– )\"]]],[1,\"p\",[[0,[],0,\"and it will give you a server. \"]]],[1,\"p\",[[0,[],0,\"Now you're probably wondering why I mention this? Why cant I just do things normally? Ah well! You can manage everything as code now. You can use something like Terraform and Codefresh with Git to manage your infrastructure. \"]]],[1,\"p\",[[0,[],0,\"If you're using Terraform, I strongly suggest you learn to create modules so you can have a quick and standardised way to create instances and stuff. Feel free to checkout my modules I have created for OVH/ Openstack:\"]]],[10,6],[1,\"p\",[[0,[],0,\"The use case for this is repeatability and simplicity. If I want to deploy another instance to my stack, I simply create another module request in \"],[0,[2],1,\"main.tf\"],[0,[],0,\" and then it's done. \"]]],[1,\"p\",[[0,[],0,\"A nice example of this is my DNS records are all managed from Cloudflare, through terraform. Once I push to master on gitlab, Codefresh picks up the changes and deploys them. \"]]],[1,\"p\",[[0,[],0,\"At work we are Codefresh partners, but I use them personally as they are actually really good. (Just a little disclaimer) \"]]],[1,\"p\",[[0,[],0,\"From here, now that your infrastrucute is created, you need to provision your instances.\"]]],[1,\"p\",[[0,[],0,\"I suggest using something like an Ansible playbook or bash scripts (if push comes to shove) \"]]],[1,\"p\",[[0,[],0,\"If you're going to be working on Digital ocean, I suggest learning \"],[0,[4],1,\"Packer\"],[0,[],0,\" for image creation from a playbook/ script.\"]]],[10,7],[1,\"p\",[[0,[],0,\"Lastly there is backups and migrations. \"]]],[1,\"p\",[[0,[],0,\"A lot of things that people forget with running on the cloud is backups as it's out of their sight and mind.\"]]],[1,\"p\",[[0,[],0,\"The recent fire that was \"],[0,[5],1,\"experienced by OVH speaks to this\"],[0,[],0,\" - Just because it's in the cloud, doesn't mean that it's not still your responsibility to maintain it. \"]]],[1,\"p\",[[0,[],0,\"For this, I recommend daily snapshots of the instance, as well as file based backups to something like S3. I personally recommend something like Wasabi storage as they are pretty cheap.\"]]],[1,\"p\",[[0,[],0,\"Daily snapshots are managed by your cloud provider, but you can also use something like backuppc running on a pi at home with a 2tb drive (This is what I do to follow the 3-2-1 rule)\"]]],[1,\"p\",[[0,[],0,\"For example from my mail server to Wasabi is over a 200g connection so daily file backups are easy. \"]]],[1,\"p\",[[0,[],0,\"Oh that's something I forgot to mention about the cloud is the bandwidth. You will really enjoy this alot. \"]]],[1,\"p\",[]],[1,\"h3\",[[0,[],0,\"Companies I suggest\"]]],[1,\"p\",[]],[1,\"p\",[[0,[6],1,\"Digital ocean for hosting\"]]],[1,\"p\",[[0,[],0,\"OVH for hosting\"]]],[1,\"p\",[[0,[],0,\"Gitlab for code and Cicd\"]]],[1,\"p\",[[0,[],0,\"Codefresh for hosted CiCd\"]]],[1,\"p\",[[0,[],0,\"Cloudflare for DNS\"]]],[1,\"p\",[[0,[],0,\"Ubuntu as an OS provider \"]]],[1,\"p\",[[0,[7],1,\"Zerotier for Mesh overlay \"]]],[1,\"h3\",[[0,[],0,\"How to make the most of the cloud?\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"Having since moved to my own place, I don't have the luxury of running my own servers so I have had to throw everything on the cloud. \"]]],[1,\"p\",[[0,[],0,\"The nice thing here is that if you need a resource for say, 30 minutes, you only pay for 30 minutes. \"]]],[1,\"p\",[[0,[],0,\"I sometimes use a throw away instance (created and destroyed via terraform) for downloading large files quickly. I will download it to the instance, work on it there and push it back, utilising the bandwidth available.\"]]],[1,\"p\",[[0,[],0,\"OVH and most cloud providers don't block ports 6881-6889 6969 so if you're hosting linux ISO's for the community to download faster, god for you! <3 \"]]],[1,\"p\",[[0,[],0,\"Finally, you have the platform to help and inspire people, host a website with Ghost/ wordpress/ CMS of your choice! \"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"Safe hosting and don't open anything other than tcp:80/443 to the web!\"]]],[10,8],[1,\"p\",[[0,[],0,\"You can hire me via \"],[0,[8],1,\"Upwork\"],[0,[],0,\" or \"],[0,[9],1,\"emailing me\"],[0,[],0,\" for weekend projects!\"]]]],\"ghostVersion\":\"3.0\"}","lexical":null,"html":"<p></p><ul><li>services to offer</li><li>how it's different</li><li>ways to manage it</li><li>best practices</li><li>how to make the most of it</li><li>companies I suggest</li></ul><p>I'll be honest with eneryone here, I never thought I would ever write something like this. </p><p>The time has come, I am moving my servers to the cloud. </p><p>I still am a selfhoster as I rely on my own knowledge and experience to operate all my services and ensure that they are online for the people who have come to rely on them.</p><p>The reason I am moving to the cloud is since moving out of my parents, I need to decom all my on premise infrastructure as the network connection isn't the best and also it's at my parent house. </p><p>Let's take a look in to the reasons you would want to move your lab to the cloud (If you plan on still labbing), how to set it up and then how to maintain it.</p><hr><p>A lot of cloud providers will have different plans and offerings. Let's focus real quick on the terminology they use, as some of it is literal <s>bullshit</s> </p><p>IaaS : Infrastructure as a service:<br> This would be renting the servers, network (yeah you can do that), storage etc from the  provider</p><p>PaaS : Platform as a service:<br> This is delivering a platform to the user, so the developer just runs their code on their       service and don't have to manage the infrastrucute. I'm not sure what their underlying       tech is, but I'm probably going to go with K8 </p><p>SaaS : Software as a service:<br> This would be using something like Google drive, SAP etc. You use the software as a       service</p><p>DBaaS : Database as a service<br> If you run a lot of applications that need a database, I suggest you use your cloud providers database offering as it's got backups usually and runs as a cluster with HA. </p><p>You will want to look at each cloud providers offerings. One thing to note is that you can actually land up causing cloud lock in. </p><p>&lt;Place holder till I write about that&gt; </p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2021/04/image-7.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"500\" height=\"756\"><figcaption>I will be writing a blog post shortly about that</figcaption></figure><p>You can avoid vendor lock in (which in this case is a cloud provider) by ensuring you have a planned exist strategy, as well as ensuring that you have 100% ownership of your data. </p><p>If you are developing an application, use docker! You can run docker on basically anything <a href=\"https://stackoverflow.com/questions/53527277/is-it-possible-to-run-containers-on-android-devices\">(well, almost)</a> you then can bang it in to K8 (which every cool cloud provider will have) </p><p>Finally once you've picked your cloud provider you then have the whole setting 'it' up.<br>'it' being what ever you're hosting at home.</p><p>The common misconception I've seen with people and the cloud is that:</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2021/04/image-8.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"583\" height=\"411\"></figure><p>no. Just no. Stop. I can understand that thought process if you were new to computers, but you guys running a datacetnre in your mom's living room, no. </p><p>Let me introduce you to a little friend called 'Firewalls' </p><p>At an absolute minimum, your cloud provider will have a form of managed firewall service where you can click and point. </p><p>My favourite way of this, which GCP does quite well is:</p><p>You create the firewall rules first. So we will create one allowing http/s access to our instances, and then we say we want to add it to any instance that has the tag of <code>web</code> assigned under the network section:</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2021/04/image-9.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"455\" height=\"737\"></figure><p>then under our instance we get this:</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2021/04/image-10.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"443\" height=\"226\"></figure><p>Point in case, if you are still worried about people getting to your servers, then you can make a career out of that as a cyber security expert. Start studying my friend!</p><p>Now we have that out the way, we will talk about how to actually deploy.</p><p>Running on the cloud has some perks. You get access to things like API's where you can tell the cloud:</p><pre><code>Dear sir,\n\nI write to you to request one server with a gigabyte of ram, 1TB data and 2vcpu\n\nSigned,\n\nBradley</code></pre><p>(Side note, if you want to see how to actually do it: --&gt; <a href=\"https://gitlab.breadnet.co.uk/terraform/modules/ovh/instance-ports/-/blob/074ac784d6c9533a7de7c4c4de32ecaa0c6f72f7/main.tf\">Here</a> &lt;– )</p><p>and it will give you a server. </p><p>Now you're probably wondering why I mention this? Why cant I just do things normally? Ah well! You can manage everything as code now. You can use something like Terraform and Codefresh with Git to manage your infrastructure. </p><p>If you're using Terraform, I strongly suggest you learn to create modules so you can have a quick and standardised way to create instances and stuff. Feel free to checkout my modules I have created for OVH/ Openstack:</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://gitlab.breadnet.co.uk/terraform/modules/ovh\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">ovh</div><div class=\"kg-bookmark-description\">GitLab Enterprise Edition</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://gitlab.breadnet.co.uk/assets/touch-icon-ipad-retina-8ebe416f5313483d9c1bc772b5bbe03ecad52a54eba443e5215a22caed2a16a2.png\" alt=\"\"><span class=\"kg-bookmark-author\">GitLab</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://gitlab.breadnet.co.uk/assets/gitlab_logo-7ae504fe4f68fdebb3c2034e36621930cd36ea87924c11ff65dbcb8ed50dca58.png\" alt=\"\"></div></a></figure><p>The use case for this is repeatability and simplicity. If I want to deploy another instance to my stack, I simply create another module request in <code>main.tf</code> and then it's done. </p><p>A nice example of this is my DNS records are all managed from Cloudflare, through terraform. Once I push to master on gitlab, Codefresh picks up the changes and deploys them. </p><p>At work we are Codefresh partners, but I use them personally as they are actually really good. (Just a little disclaimer) </p><p>From here, now that your infrastrucute is created, you need to provision your instances.</p><p>I suggest using something like an Ansible playbook or bash scripts (if push comes to shove) </p><p>If you're going to be working on Digital ocean, I suggest learning <a href=\"https://www.packer.io\">Packer</a> for image creation from a playbook/ script.</p><hr><p>Lastly there is backups and migrations. </p><p>A lot of things that people forget with running on the cloud is backups as it's out of their sight and mind.</p><p>The recent fire that was <a href=\"https://www.reuters.com/article/us-france-ovh-fire-idUSKBN2B20NU\">experienced by OVH speaks to this</a> - Just because it's in the cloud, doesn't mean that it's not still your responsibility to maintain it. </p><p>For this, I recommend daily snapshots of the instance, as well as file based backups to something like S3. I personally recommend something like Wasabi storage as they are pretty cheap.</p><p>Daily snapshots are managed by your cloud provider, but you can also use something like backuppc running on a pi at home with a 2tb drive (This is what I do to follow the 3-2-1 rule)</p><p>For example from my mail server to Wasabi is over a 200g connection so daily file backups are easy. </p><p>Oh that's something I forgot to mention about the cloud is the bandwidth. You will really enjoy this alot. </p><p></p><h3 id=\"companies-i-suggest\">Companies I suggest</h3><p></p><p><a href=\"https://m.do.co/c/77be3c3aa96c\">Digital ocean for hosting</a></p><p>OVH for hosting</p><p>Gitlab for code and Cicd</p><p>Codefresh for hosted CiCd</p><p>Cloudflare for DNS</p><p>Ubuntu as an OS provider </p><p><a href=\"zerotier.com\">Zerotier for Mesh overlay </a></p><h3 id=\"how-to-make-the-most-of-the-cloud\">How to make the most of the cloud?</h3><p></p><p>Having since moved to my own place, I don't have the luxury of running my own servers so I have had to throw everything on the cloud. </p><p>The nice thing here is that if you need a resource for say, 30 minutes, you only pay for 30 minutes. </p><p>I sometimes use a throw away instance (created and destroyed via terraform) for downloading large files quickly. I will download it to the instance, work on it there and push it back, utilising the bandwidth available.</p><p>OVH and most cloud providers don't block ports 6881-6889 6969 so if you're hosting linux ISO's for the community to download faster, god for you! &lt;3 </p><p>Finally, you have the platform to help and inspire people, host a website with Ghost/ wordpress/ CMS of your choice! </p><p></p><p>Safe hosting and don't open anything other than tcp:80/443 to the web!</p><hr><p>You can hire me via <a href=\"https://www.upwork.com/freelancers/~01c61ee9802b94133e\">Upwork</a> or <a href=\"mailto:work@breadnet.co.uk\">emailing me</a> for weekend projects!</p>","comment_id":"607108d5bea1f916bfb841f8","plaintext":" * services to offer\n * how it's different\n * ways to manage it\n * best practices\n * how to make the most of it\n * companies I suggest\n\nI'll be honest with eneryone here, I never thought I would ever write something like this.\n\nThe time has come, I am moving my servers to the cloud.\n\nI still am a selfhoster as I rely on my own knowledge and experience to operate all my services and ensure that they are online for the people who have come to rely on them.\n\nThe reason I am moving to the cloud is since moving out of my parents, I need to decom all my on premise infrastructure as the network connection isn't the best and also it's at my parent house.\n\nLet's take a look in to the reasons you would want to move your lab to the cloud (If you plan on still labbing), how to set it up and then how to maintain it.\n\nA lot of cloud providers will have different plans and offerings. Let's focus real quick on the terminology they use, as some of it is literal bullshit\n\nIaaS : Infrastructure as a service:\n This would be renting the servers, network (yeah you can do that), storage etc from the  provider\n\nPaaS : Platform as a service:\n This is delivering a platform to the user, so the developer just runs their code on their       service and don't have to manage the infrastrucute. I'm not sure what their underlying       tech is, but I'm probably going to go with K8\n\nSaaS : Software as a service:\n This would be using something like Google drive, SAP etc. You use the software as a       service\n\nDBaaS : Database as a service\n If you run a lot of applications that need a database, I suggest you use your cloud providers database offering as it's got backups usually and runs as a cluster with HA.\n\nYou will want to look at each cloud providers offerings. One thing to note is that you can actually land up causing cloud lock in.\n\n<Place holder till I write about that>\n\nYou can avoid vendor lock in (which in this case is a cloud provider) by ensuring you have a planned exist strategy, as well as ensuring that you have 100% ownership of your data.\n\nIf you are developing an application, use docker! You can run docker on basically anything (well, almost) you then can bang it in to K8 (which every cool cloud provider will have)\n\nFinally once you've picked your cloud provider you then have the whole setting 'it' up.\n'it' being what ever you're hosting at home.\n\nThe common misconception I've seen with people and the cloud is that:\n\nno. Just no. Stop. I can understand that thought process if you were new to computers, but you guys running a datacetnre in your mom's living room, no.\n\nLet me introduce you to a little friend called 'Firewalls'\n\nAt an absolute minimum, your cloud provider will have a form of managed firewall service where you can click and point.\n\nMy favourite way of this, which GCP does quite well is:\n\nYou create the firewall rules first. So we will create one allowing http/s access to our instances, and then we say we want to add it to any instance that has the tag of web assigned under the network section:\n\nthen under our instance we get this:\n\nPoint in case, if you are still worried about people getting to your servers, then you can make a career out of that as a cyber security expert. Start studying my friend!\n\nNow we have that out the way, we will talk about how to actually deploy.\n\nRunning on the cloud has some perks. You get access to things like API's where you can tell the cloud:\n\nDear sir,\n\nI write to you to request one server with a gigabyte of ram, 1TB data and 2vcpu\n\nSigned,\n\nBradley\n\n(Side note, if you want to see how to actually do it: --> Here <– )\n\nand it will give you a server.\n\nNow you're probably wondering why I mention this? Why cant I just do things normally? Ah well! You can manage everything as code now. You can use something like Terraform and Codefresh with Git to manage your infrastructure.\n\nIf you're using Terraform, I strongly suggest you learn to create modules so you can have a quick and standardised way to create instances and stuff. Feel free to checkout my modules I have created for OVH/ Openstack:\n\novhGitLab Enterprise EditionGitLab\n\nThe use case for this is repeatability and simplicity. If I want to deploy another instance to my stack, I simply create another module request in main.tf and then it's done.\n\nA nice example of this is my DNS records are all managed from Cloudflare, through terraform. Once I push to master on gitlab, Codefresh picks up the changes and deploys them.\n\nAt work we are Codefresh partners, but I use them personally as they are actually really good. (Just a little disclaimer)\n\nFrom here, now that your infrastrucute is created, you need to provision your instances.\n\nI suggest using something like an Ansible playbook or bash scripts (if push comes to shove)\n\nIf you're going to be working on Digital ocean, I suggest learning Packer for image creation from a playbook/ script.\n\nLastly there is backups and migrations.\n\nA lot of things that people forget with running on the cloud is backups as it's out of their sight and mind.\n\nThe recent fire that was experienced by OVH speaks to this - Just because it's in the cloud, doesn't mean that it's not still your responsibility to maintain it.\n\nFor this, I recommend daily snapshots of the instance, as well as file based backups to something like S3. I personally recommend something like Wasabi storage as they are pretty cheap.\n\nDaily snapshots are managed by your cloud provider, but you can also use something like backuppc running on a pi at home with a 2tb drive (This is what I do to follow the 3-2-1 rule)\n\nFor example from my mail server to Wasabi is over a 200g connection so daily file backups are easy.\n\nOh that's something I forgot to mention about the cloud is the bandwidth. You will really enjoy this alot.\n\n\n\n\nCompanies I suggest\n\n\n\nDigital ocean for hosting\n\nOVH for hosting\n\nGitlab for code and Cicd\n\nCodefresh for hosted CiCd\n\nCloudflare for DNS\n\nUbuntu as an OS provider\n\nZerotier for Mesh overlay\n\n\nHow to make the most of the cloud?\n\n\n\nHaving since moved to my own place, I don't have the luxury of running my own servers so I have had to throw everything on the cloud.\n\nThe nice thing here is that if you need a resource for say, 30 minutes, you only pay for 30 minutes.\n\nI sometimes use a throw away instance (created and destroyed via terraform) for downloading large files quickly. I will download it to the instance, work on it there and push it back, utilising the bandwidth available.\n\nOVH and most cloud providers don't block ports 6881-6889 6969 so if you're hosting linux ISO's for the community to download faster, god for you! <3\n\nFinally, you have the platform to help and inspire people, host a website with Ghost/ wordpress/ CMS of your choice!\n\n\n\nSafe hosting and don't open anything other than tcp:80/443 to the web!\n\nYou can hire me via Upwork or emailing me for weekend projects!","feature_image":"__GHOST_URL__/content/images/2021/04/cloud.jpg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2021-04-10T02:09:25.000Z","updated_at":"2021-05-05T14:10:57.000Z","published_at":"2021-04-26T23:00:00.000Z","custom_excerpt":"Think 'the hitchhiker's guide to the galaxy' but for the cloud, and for those of us who self host","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"6425f3bc192c0c150413d5a1","uuid":"2c34771d-6399-4a8b-bbba-50ac24ae8590","title":"Hey!","slug":"bradley-stannard","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}]],\"cards\":[[\"hr\",{}],[\"html\",{\"html\":\"<centre><img src=\\\"https://lh3.googleusercontent.com/pw/ACtC-3dcmvrXc7ywh6plPElzeOgW0qnnN2zPIu4GCAVOP-TJmYEMt-J1qFU3R68xKAld0BBIvTjsKXJoUcKskMfJdmpu6D-InWoOI9EK3iBH4RpfWF9HJG5qNE0QSjTtdh7UehLmixwKNQnZZQiJA8KC0NDRDQ=w2486-h1864-no\\\" ></centre>\"}],[\"callout\",{\"calloutEmoji\":\"💡\",\"calloutText\":\"So you know a user's ethernet cable is the issue but they wont check it?<br>Tell them <em>\\\"Data only flows one way, can you please unplug it and flip it around\\\"</em><br>You're welcome\",\"backgroundColor\":\"grey\"}],[\"hr\",{}],[\"hr\",{}]],\"markups\":[[\"a\",[\"href\",\"https://asbindia.org\"]],[\"em\"],[\"a\",[\"href\",\"https://bradley.breadnet.co.uk/companies/pah/\"]],[\"a\",[\"href\",\"https://www.upwork.com/freelancers/~01c61ee9802b94133e\"]],[\"a\",[\"href\",\"mailto:website@breadnet.co.uk\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I see you're curious about who exactly I am?\"]]],[1,\"p\",[[0,[],0,\"TL;DR:\"]]],[3,\"ul\",[[[0,[],0,\"Nerd\"]],[[0,[],0,\"Started on the help desk\"]],[[0,[],0,\"Climbed my way up to the cloud (pun intended)\"]],[[0,[],0,\"DevOps/ Platform engineer at the UK's largest pet care company\"]]]],[10,0],[1,\"p\",[[0,[],0,\"I got in to IT when I was studying at the \"],[0,[0],1,\"American School of Bombay\"],[0,[],0,\" and was inspired by my Tech teacher (Mr. J. Laub) who taught me the basics of how the network at school worked, as well as Savio from the tech desk who patiently explained every answer to my questions. If it wasn't for moving to India and these people, I don't know where I would be now. \"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"My big break in to IT was after I finished school in 2018, I worked at Focus International, in the warehouse picking client orders. \"]]],[1,\"p\",[[0,[],0,\"I know, you're wondering what Ellesse shirts and shoes has to do with IT, well, this job provided me with the money to purchase servers and a 48U server rack.  From here I really started to learn linux, and moved my daily driver over to Linux mint to force my self to learn things. I'm not sure why I got fancy monitors for my computer when I spend 90% of my time in a terminal window, but eh.\"]]],[10,1],[1,\"p\",[[0,[],0,\"Moving on.\"]]],[1,\"p\",[[0,[],0,\"I had to enroll in education till I was 18, so I decided to go to technical college and do a Level 3 BTEC in IT. \"]]],[1,\"p\",[[0,[],0,\"I absolutely hated it. At risk of sounding like a complete jerk and know it all, these lectures were below me. It was so bad to the point that I discovered that you can vlan hop and was able to get on to management VLAN's before reporting my findings to the college.\"]]],[1,\"p\",[[0,[],0,\"After around 3-4 months (I forget) I \"],[0,[1],1,\"dropped out\"],[0,[],0,\" and found an apprenticeship in network engineering, which is what I really wanted to go in to. The interviews went well and I started in January. \"]]],[1,\"p\",[[0,[],0,\"I had my hopes and dreams crushed when on my first day it was explained to me that I would in fact not be engineering networks, but instead a level 1 Helpdesk \"],[0,[1],1,\"engineer.\"]]],[1,\"p\",[[0,[1],1,\"No\"],[0,[],0,\" this was not a setback, despite the negative note. I can now say with 100% confidence that everyone who works in IT should work on the Helpdesk at some point. It teaches you problem solving as well as knowing some sneaky tricks to get users to listen to you.\"]]],[10,2],[1,\"p\",[[0,[],0,\"After about a year and a bit, I was fedup. I wanted to become a linux system administrator, but was using windows on a day to day basis. Not helping my cause\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"I was approached by a recruiter from Texas who was looking for Junior system administrators. I had nothing to lose so jumped ship.\"]]],[1,\"p\",[[0,[],0,\"After about 6-8 months (Exact numbers escape me) I was put on a client project and told to basically learn cloud. I never had any intention to go in to cloud, in fact I wanted to become a NOC engineer.\"]]],[1,\"p\",[[0,[],0,\"I had my title changed from Junior systems engineer to Cloud Engineer after proving I was reliable and delivered.\"]]],[1,\"p\",[[0,[],0,\"I spent a lot of time working on client engagement and getting my GCP Professional Cloud Architect certification.\"]]],[1,\"p\",[[0,[],0,\"After a year and 6 months, I decided the time was right to move on to a new company.\"]]],[1,\"h3\",[[0,[],0,\"Roll on finance\"]]],[1,\"p\",[[0,[],0,\"I moved to another cloud consultancy, this time it was a german company. I seem to have a thing for International companies.\"]]],[1,\"p\",[[0,[],0,\"Stayed there for 6 months (I am not proud to admit this) before I was head hunted for the company I am currently at.\"]]],[1,\"p\",[[0,[],0,\"If you want to see what I am up to, you can check my \"],[0,[2],1,\"CV\"]]],[10,3],[1,\"h3\",[[0,[],0,\"My career goal?\"]]],[1,\"p\",[[1,[],0,0],[0,[],0,\" Not sure, what I do know is I want to become a cloud god, start a company that is the one stop shop for any start up, we have developers, UI/UX teams, cloud engineers, you name it, we're the place to go to launch your startup! \"]]],[10,4],[1,\"p\",[[0,[],0,\"As always, you can \"],[0,[3],1,\"contract me\"],[0,[],0,\" if you want help or \"],[0,[4],1,\"contact me\"],[0,[],0,\" if you just want to chat! \"]]],[1,\"p\",[]]],\"ghostVersion\":\"3.0\"}","lexical":null,"html":"<p>I see you're curious about who exactly I am?</p><p>TL;DR:</p><ul><li>Nerd</li><li>Started on the help desk</li><li>Climbed my way up to the cloud (pun intended)</li><li>DevOps/ Platform engineer at the UK's largest pet care company</li></ul><hr><p>I got in to IT when I was studying at the <a href=\"https://asbindia.org\">American School of Bombay</a> and was inspired by my Tech teacher (Mr. J. Laub) who taught me the basics of how the network at school worked, as well as Savio from the tech desk who patiently explained every answer to my questions. If it wasn't for moving to India and these people, I don't know where I would be now. </p><p></p><p>My big break in to IT was after I finished school in 2018, I worked at Focus International, in the warehouse picking client orders. </p><p>I know, you're wondering what Ellesse shirts and shoes has to do with IT, well, this job provided me with the money to purchase servers and a 48U server rack.  From here I really started to learn linux, and moved my daily driver over to Linux mint to force my self to learn things. I'm not sure why I got fancy monitors for my computer when I spend 90% of my time in a terminal window, but eh.</p><!--kg-card-begin: html--><centre><img src=\"https://lh3.googleusercontent.com/pw/ACtC-3dcmvrXc7ywh6plPElzeOgW0qnnN2zPIu4GCAVOP-TJmYEMt-J1qFU3R68xKAld0BBIvTjsKXJoUcKskMfJdmpu6D-InWoOI9EK3iBH4RpfWF9HJG5qNE0QSjTtdh7UehLmixwKNQnZZQiJA8KC0NDRDQ=w2486-h1864-no\" ></centre><!--kg-card-end: html--><p>Moving on.</p><p>I had to enroll in education till I was 18, so I decided to go to technical college and do a Level 3 BTEC in IT. </p><p>I absolutely hated it. At risk of sounding like a complete jerk and know it all, these lectures were below me. It was so bad to the point that I discovered that you can vlan hop and was able to get on to management VLAN's before reporting my findings to the college.</p><p>After around 3-4 months (I forget) I <em>dropped out</em> and found an apprenticeship in network engineering, which is what I really wanted to go in to. The interviews went well and I started in January. </p><p>I had my hopes and dreams crushed when on my first day it was explained to me that I would in fact not be engineering networks, but instead a level 1 Helpdesk <em>engineer.</em></p><p><em>No</em> this was not a setback, despite the negative note. I can now say with 100% confidence that everyone who works in IT should work on the Helpdesk at some point. It teaches you problem solving as well as knowing some sneaky tricks to get users to listen to you.</p><div class=\"kg-card kg-callout-card kg-callout-card-grey\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">So you know a user's ethernet cable is the issue but they wont check it?<br>Tell them <em>\"Data only flows one way, can you please unplug it and flip it around\"</em><br>You're welcome</div></div><p>After about a year and a bit, I was fedup. I wanted to become a linux system administrator, but was using windows on a day to day basis. Not helping my cause</p><p></p><p>I was approached by a recruiter from Texas who was looking for Junior system administrators. I had nothing to lose so jumped ship.</p><p>After about 6-8 months (Exact numbers escape me) I was put on a client project and told to basically learn cloud. I never had any intention to go in to cloud, in fact I wanted to become a NOC engineer.</p><p>I had my title changed from Junior systems engineer to Cloud Engineer after proving I was reliable and delivered.</p><p>I spent a lot of time working on client engagement and getting my GCP Professional Cloud Architect certification.</p><p>After a year and 6 months, I decided the time was right to move on to a new company.</p><h3 id=\"roll-on-finance\">Roll on finance</h3><p>I moved to another cloud consultancy, this time it was a german company. I seem to have a thing for International companies.</p><p>Stayed there for 6 months (I am not proud to admit this) before I was head hunted for the company I am currently at.</p><p>If you want to see what I am up to, you can check my <a href=\"https://bradley.breadnet.co.uk/companies/pah/\">CV</a></p><hr><h3 id=\"my-career-goal\">My career goal?</h3><p><br> Not sure, what I do know is I want to become a cloud god, start a company that is the one stop shop for any start up, we have developers, UI/UX teams, cloud engineers, you name it, we're the place to go to launch your startup! </p><hr><p>As always, you can <a href=\"https://www.upwork.com/freelancers/~01c61ee9802b94133e\">contract me</a> if you want help or <a href=\"mailto:website@breadnet.co.uk\">contact me</a> if you just want to chat! </p>","comment_id":"6071196dbea1f916bfb842c5","plaintext":"I see you're curious about who exactly I am?\n\nTL;DR:\n\n * Nerd\n * Started on the help desk\n * Climbed my way up to the cloud (pun intended)\n * DevOps/ Platform engineer at the UK's largest pet care company\n\nI got in to IT when I was studying at the American School of Bombay and was inspired by my Tech teacher (Mr. J. Laub) who taught me the basics of how the network at school worked, as well as Savio from the tech desk who patiently explained every answer to my questions. If it wasn't for moving to India and these people, I don't know where I would be now.\n\n\n\nMy big break in to IT was after I finished school in 2018, I worked at Focus International, in the warehouse picking client orders.\n\nI know, you're wondering what Ellesse shirts and shoes has to do with IT, well, this job provided me with the money to purchase servers and a 48U server rack.  From here I really started to learn linux, and moved my daily driver over to Linux mint to force my self to learn things. I'm not sure why I got fancy monitors for my computer when I spend 90% of my time in a terminal window, but eh.\n\nMoving on.\n\nI had to enroll in education till I was 18, so I decided to go to technical college and do a Level 3 BTEC in IT.\n\nI absolutely hated it. At risk of sounding like a complete jerk and know it all, these lectures were below me. It was so bad to the point that I discovered that you can vlan hop and was able to get on to management VLAN's before reporting my findings to the college.\n\nAfter around 3-4 months (I forget) I dropped out and found an apprenticeship in network engineering, which is what I really wanted to go in to. The interviews went well and I started in January.\n\nI had my hopes and dreams crushed when on my first day it was explained to me that I would in fact not be engineering networks, but instead a level 1 Helpdesk engineer.\n\nNo this was not a setback, despite the negative note. I can now say with 100% confidence that everyone who works in IT should work on the Helpdesk at some point. It teaches you problem solving as well as knowing some sneaky tricks to get users to listen to you.\n\n💡So you know a user's ethernet cable is the issue but they wont check it?\nTell them \"Data only flows one way, can you please unplug it and flip it around\"\nYou're welcome\n\nAfter about a year and a bit, I was fedup. I wanted to become a linux system administrator, but was using windows on a day to day basis. Not helping my cause\n\n\n\nI was approached by a recruiter from Texas who was looking for Junior system administrators. I had nothing to lose so jumped ship.\n\nAfter about 6-8 months (Exact numbers escape me) I was put on a client project and told to basically learn cloud. I never had any intention to go in to cloud, in fact I wanted to become a NOC engineer.\n\nI had my title changed from Junior systems engineer to Cloud Engineer after proving I was reliable and delivered.\n\nI spent a lot of time working on client engagement and getting my GCP Professional Cloud Architect certification.\n\nAfter a year and 6 months, I decided the time was right to move on to a new company.\n\n\nRoll on finance\n\nI moved to another cloud consultancy, this time it was a german company. I seem to have a thing for International companies.\n\nStayed there for 6 months (I am not proud to admit this) before I was head hunted for the company I am currently at.\n\nIf you want to see what I am up to, you can check my CV\n\n\nMy career goal?\n\n\nNot sure, what I do know is I want to become a cloud god, start a company that is the one stop shop for any start up, we have developers, UI/UX teams, cloud engineers, you name it, we're the place to go to launch your startup!\n\nAs always, you can contract me if you want help or contact me if you just want to chat!","feature_image":"__GHOST_URL__/content/images/2021/04/960EE757-998C-465A-8AC3-0B2A40CE0112-62956-0000098774ED1DDC.jpeg","featured":0,"type":"page","status":"draft","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2021-04-10T03:20:13.000Z","updated_at":"2025-04-02T12:31:23.000Z","published_at":"2021-04-10T03:39:49.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"6425f3bc192c0c150413d5a2","uuid":"cfa3ad30-3669-41a8-b16d-3929e2d068dd","title":"How I manage my DNS","slug":"dns-terraform-cloudflare","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"hr\",{}],[\"markdown\",{\"markdown\":\"* Domain\\n    * Namecheap \\n* DNS Hosting\\n    * Cloudflare\\n* Git Hosting\\n    * Selfhosted GIT server\\n* Pipeline\\n    * Codefresh\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/05/4waycycle-02.png\",\"width\":9443,\"height\":2084,\"caption\":\"The \\\"pipeline\\\"\"}],[\"code\",{\"code\":\"stannardb@bread-l1:~$ nslookup\\n> unifi.breadnet.co.uk\\nServer:\\t\\t127.0.0.53\\nAddress:\\t127.0.0.53#53\\n\\nNon-authoritative answer:\\nunifi.breadnet.co.uk\\tcanonical name = unifi.breadinfra.net.\\nName:\\tunifi.breadinfra.net\\nAddress: 51.77.109.126\\n> \\n\"}],[\"code\",{\"code\":\"#main.tf\\nresource \\\"cloudflare_record\\\" \\\"a\\\" {\\n  zone_id = \\\"<redacted>\\\"\\n  name    = var.name\\n  type    = \\\"A\\\"\\n  ttl     = \\\"1\\\"\\n  proxied = var.proxied\\n  value   = var.value\\n}\\n\\n#outputs.tf\\noutput \\\"id\\\" {\\n  value = cloudflare_record.a.id\\n}\\n\\noutput \\\"name\\\" {\\n  value = cloudflare_record.a.name\\n}\\n\\noutput \\\"hostname\\\" {\\n  value = cloudflare_record.a.hostname\\n}\\n\\n#provider.tf\\nterraform {\\n  required_providers {\\n    cloudflare = {\\n      source = \\\"cloudflare/cloudflare\\\"\\n      version = \\\"~> 2.0\\\"\\n    }\\n  }\\n}\\n\\n#variables.tf\\nvariable \\\"name\\\" {\\n  type = string\\n}\\nvariable \\\"value\\\" {\\n  type = string\\n}\\nvariable \\\"proxied\\\" {\\n  type = string\\n}\\n\"}],[\"code\",{\"code\":\"module \\\"hosted_on\\\" {\\n  source = \\\"git::ssh://git@<redacted>a.git\\\"\\n  name = \\\"hosted.on\\\"\\n  proxied = \\\"false\\\"\\n  value = var.<redacted>\"}],[\"code\",{\"code\":\"# module.hosted_on.cloudflare_record.a:\\nresource \\\"cloudflare_record\\\" \\\"a\\\" {\\n    created_on  = \\\"2021-04-22T20:49:44.673066Z\\\"\\n    data        = {}\\n    hostname    = \\\"hosted.on.breadinfra.net\\\"\\n    id          = \\\"<redacted>\\\"\\n    metadata    = {\\n        \\\"auto_added\\\"             = \\\"false\\\"\\n        \\\"managed_by_apps\\\"        = \\\"false\\\"\\n        \\\"managed_by_argo_tunnel\\\" = \\\"false\\\"\\n        \\\"source\\\"                 = \\\"primary\\\"\\n    }\\n    modified_on = \\\"2021-04-22T20:49:44.673066Z\\\"\\n    name        = \\\"hosted.on\\\"\\n    proxiable   = true\\n    proxied     = false\\n    ttl         = 1\\n    type        = \\\"A\\\"\\n    value       = \\\"<redacted>\\\"\\n    zone_id     = \\\"<redacted>\\\"\\n}\\n\"}],[\"code\",{\"code\":\"module \\\"bookstack\\\" {\\n  source = \\\"git::ssh://git@<redacted>/cname.git\\\"\\n  name = \\\"bookstack\\\"\\n  proxied = \\\"true\\\"\\n  value = module.reverse.hostname\\n}\"}],[\"code\",{\"code\":\"> bookstack.breadnet.co.uk\\nServer:\\t\\t127.0.0.53\\nAddress:\\t127.0.0.53#53\\n\\nNon-authoritative answer:\\nName:\\tbookstack.breadnet.co.uk\\nAddress: 104.21.43.73\\nName:\\tbookstack.breadnet.co.uk\\nAddress: 172.67.222.140\\nName:\\tbookstack.breadnet.co.uk\\nAddress: 2606:4700:3036::ac43:de8c\\nName:\\tbookstack.breadnet.co.uk\\nAddress: 2606:4700:3037::6815:2b49\\n> \"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/05/image.png\",\"width\":1301,\"height\":260}],[\"code\",{\"code\":\"version: '1.0'\\nstages:\\n  - checkout\\n  - prepare\\n  - deploy\\nsteps:\\n  main_clone:\\n    title: Git clone\\n    image: alpine/git:latest\\n    stage: checkout\\n    commands:\\n      - mkdir -p ~/.ssh\\n      - echo \\\"${SSH_KEY}\\\" | base64 -d > ~/.ssh/id_rsa\\n      - chmod 600 ~/.ssh/id_rsa\\n      - echo \\\"${gitlab_known_host}\\\" > ~/.ssh/known_hosts\\n      - rm -rvf *dns*\\n      - git clone git@<redacted>/dns.git\\n      - mv dns/* .\\n  SetupAuth:\\n    image: alpine:3.9\\n    title: Configuring Auth\\n    stage: prepare\\n    commands:\\n      - export TF_VAR_cloudflare_email=$CLOUDFLARE_EMAIL\\n      - export TF_VAR_cloudflare_api_key=$CLOUDFLARE_API_KEY\\n  DeployWithTerraform:\\n    image: hashicorp/terraform:light\\n    title: Deploying Terraform plan\\n    stage: deploy\\n    commands:\\n      - mkdir -p ~/.ssh\\n      - echo \\\"${SSH_KEY}\\\" | base64 -d > ~/.ssh/id_rsa\\n      - chmod 600 ~/.ssh/id_rsa\\n      - echo \\\"${gitlab_known_host}\\\" > ~/.ssh/known_hosts\\n      - terraform init -backend-config=\\\"token=\\\"$token\\\"\\\"\\n      - terraform apply -auto-approve\"}],[\"code\",{\"code\":\"{\\n  \\\"credentials\\\": {\\n    \\\"app.terraform.io\\\": {\\n      \\\"token\\\": \\\"<redacted ya cheeky bugger>\\\"\\n    }\\n  }\\n\"}],[\"hr\",{}]],\"markups\":[[\"a\",[\"href\",\"https://www.reddit.com/r/msp/comments/4j9spm/why_the_hell_do_web_designers_take_control_of_dns/\"]],[\"code\"],[\"a\",[\"href\",\"__GHOST_URL__/what-it-takes-to-run-breadnet/\"]],[\"a\",[\"href\",\"__GHOST_URL__/dns-terraform-cloudflare/kirstylawrie.com/\"]],[\"a\",[\"href\",\"https://www.upwork.com/freelancers/~01c61ee9802b94133e\"]],[\"a\",[\"href\",\"mailto:work@breadnet.co.uk\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Managing DNS can be a complicated thing for a large company, many departments changing things. \"]]],[1,\"p\",[[0,[],0,\"Let's be honest here, at some point someone somewhere asked a web developer to update a DNS record and then landed up wiping out an mx record (I made this up, \"],[0,[0],1,\"but it turns out someone did it\"],[0,[],0,\")\"]]],[1,\"p\",[[0,[],0,\"I chose to complicate my DNS as much as possible by moving the name servers from the registrar, to cloud flare, then importing the records in to Terraform, then moving things like IP address' of servers to variables, then got bored and moved everything over to cnames, then got bored again and moved to modules. - I know :) \"]]],[1,\"p\",[]],[10,0],[1,\"blockquote\",[[0,[],0,\"this post is a little confusing to follow, so if you get lost or it's just too confusing, please reach out!\"]]],[1,\"h2\",[[0,[],0,\"The why?\"]]],[1,\"p\",[[0,[],0,\"Back in 2020 I set my self a goal, get everything that it takes to manage breadNET in to a git repo. This spanned all the way from creating servers in the cloud to server config and app install scripts to DNS. \"]]],[1,\"p\",[[0,[],0,\"Here's where it gets interesting. I hadn't heard about terraform before April 2020 so I was trying to go about it in some strange way by using a bash script I was working on, and then pull from the cloudflare API blah blah blah. Simple story - would not have worked.\"]]],[1,\"p\",[[0,[],0,\"I want everything in git so should my server fall off the face of the earth, all it takes is running the playbook/ \"],[0,[1],1,\"terraform apply\"],[0,[],0,\", restore the databse backups and files and we're good to go!\"]]],[1,\"h2\",[[0,[],0,\"Dribble about my DNS\"]]],[1,\"p\",[[0,[],0,\"So like most people who host things at loss (\"],[0,[2],1,\"see how much\"],[0,[],0,\") using free services is a must. \"]]],[10,1],[1,\"p\",[]],[1,\"h2\",[[0,[],0,\"The How\"]]],[1,\"p\",[[0,[],0,\"Buckle up, this gets way too complicated for what it really is!\"]]],[10,2],[1,\"p\",[[0,[],0,\"I manage my DNS through Cloudflare, using terraform which gets applied on a push to master via a service called Codefresh. \"]]],[1,\"p\",[[0,[],0,\"Let's start at the DNS level.\"]]],[1,\"p\",[[0,[],0,\"I chose to use Cloudflare for DNS as they provide a proxy level to your services so I am able to serve more people with less resources! They also have DDOS protection which I laughed at, but I've come under a few DDOS attacks. Not sure why but hey-ho!\"]]],[1,\"p\",[[0,[],0,\"My records are set out in a way that all records that end with \"],[0,[1],1,\".breadnet.co.uk\"],[0,[],0,\" are cnames! Even the main domain is a cname. \"]]],[1,\"p\",[[0,[],0,\"These cnames point to a second domain which follows a simple layout\"]]],[1,\"p\",[[0,[1],1,\"<server purpose>.breadinfra.net\"],[0,[],0,\" so the reverse proxy server is \"],[0,[1],1,\"reverse.breadinfra.net\"],[0,[],0,\" \"]]],[1,\"p\",[[0,[],0,\"This means that my nslookups now look like this:\"]]],[10,3],[1,\"p\",[[0,[],0,\"I chose to do it this way as it allows me to only have to update the server record is I decide to move it. It originally came from when I was managing DNS through the UI. I have something like 75 records pointing to a few servers\"]]],[1,\"p\",[[0,[],0,\"Here comes the terraform!\"]]],[1,\"p\",[[0,[],0,\"Due to this abstraction I manage it through terraform using modules. Below is what a module looks like\"]]],[10,4],[1,\"p\",[[0,[],0,\"These modules allow me to repeat as little as possible when creating a new record. \"]]],[1,\"p\",[[0,[],0,\"If I want to create an A record, it looks like:\"]]],[10,5],[1,\"p\",[[0,[],0,\"This then creates a record which cloudflare sees as:\"]]],[10,6],[1,\"p\",[[0,[],0,\"Then when I create a cname it looks like\"]]],[10,7],[1,\"p\",[[0,[],0,\"Here we are passing the output of the module that created the A record for the reverse server (notice it's the hostname) - then we create it's fqdn, which will be \"],[0,[1],1,\"bookstack.breadnet.co.uk\"],[0,[],0,\" and set it to proxy through cloudflare's network\"]]],[10,8],[1,\"p\",[[0,[],0,\"Once I've got all the records created, I then push it to my private git server where codefresh picks it up and runs a pipeline on it\"]]],[1,\"p\",[[0,[],0,\"This is what it looks like from the UI\"]]],[10,9],[1,\"p\",[[0,[],0,\"There are 3 steps. Each step caused me hours of lost sleep just because\"]]],[3,\"ul\",[[[0,[],0,\"This was my first time using codefresh\"]],[[0,[],0,\"I don't know how to write the \"],[0,[1],1,\"codefresh.yml\"],[0,[],0,\" file\"]],[[0,[],0,\"I didn't know how to auth to my git server\"]],[[0,[],0,\"I didn't know how to auth to Terraform cloud for remote state\"]]]],[1,\"p\",[[0,[],0,\"Before we continue, let me show you the file:\"]]],[10,10],[1,\"p\",[[0,[],0,\"Let's break it down\"]]],[1,\"p\",[[0,[],0,\"It's broken in to 3 steps:\"]]],[3,\"ul\",[[[0,[],0,\"checkout\"]],[[0,[],0,\"Prepare\"]],[[0,[],0,\"Deploy\"]]]],[1,\"p\",[[0,[],0,\"The checkout step was where I had the most issues, I had to git clone from my private git server behind a firewall. \"]]],[1,\"p\",[[0,[],0,\"I had to open the firewall to codefresh's IP range and then configure SSH keys.\"]]],[1,\"p\",[[0,[],0,\"I did the SSH keys by setting a secret in codefresh, generating a random SSH key on my laptop, echoing the private key to base64 ( \"],[0,[1],1,\"cat ./ssh/<key> | base64\"],[0,[],0,\" ) and then putting that base64 encoded private key in to codefresh. It's important that you set it as a secret and then delete that private key from your laptop. \"]]],[1,\"p\",[[0,[],0,\"In your git platform, create a user and add that user to your repo, and then add the public key to their account.\"]]],[1,\"p\",[[0,[],0,\"I do a \"],[0,[1],1,\"rm\"],[0,[],0,\" because codefresh has persistence between running and it was causing issues with not updating the files. I would rather pull each time and waste 3 seconds than cause terraform state drift. \"]]],[1,\"p\",[[0,[],0,\"The \"],[0,[1],1,\"config auth\"],[0,[],0,\" stage just sets the cloudlfare API key as an environment variable so I'm not storing it in the terraform code. \"]]],[1,\"p\",[[0,[],0,\"Finally there is the deploy phase.\"]]],[1,\"p\",[[0,[],0,\"I use terraform cloud to manage my remote state file, this just helps as if I was to run this on my computer, I can just run when ever needed as long as I don't delete the file, I'm all good!\"]]],[1,\"p\",[[0,[],0,\"Where as running in an ephemeral environment, you want the persistence to be outside the platform. \"]]],[1,\"p\",[[0,[],0,\"You need to authenticate with a token to the terraform cloud which is usually stored under \"],[0,[1],1,\"$HOME/.terraform.d/credentials.tfrc.json\"],[0,[],0,\" and looks like\"]]],[10,11],[1,\"p\",[[0,[],0,\"so I had the idea to try and pass it to the command as \"],[0,[1],1,\"--backend-config\"],[0,[],0,\" and low and behold, it worked!\"]]],[1,\"p\",[[0,[],0,\"Codefresh then runs the full thing, auto approves this apply and puts it in to action.\"]]],[1,\"p\",[[0,[],0,\"From committing the code to the records being created takes about 50 seconds, which allows me to do other things, like finish the nginx config file, or queue up the command to get an LE certificate. \"]]],[1,\"p\",[[0,[],0,\"If any part of this doesn't make sense, please reach out to me!\"]]],[10,12],[1,\"p\",[[0,[],0,\"Illustrations by \"],[0,[3],1,\"Kirsty Lawrie\"]]],[1,\"p\",[[0,[],0,\"You can hire me via \"],[0,[4],1,\"Upwork\"],[0,[],0,\" or \"],[0,[5],1,\"emailing me\"],[0,[],0,\" for weekend projects!\"]]]],\"ghostVersion\":\"3.0\"}","lexical":null,"html":"<p>Managing DNS can be a complicated thing for a large company, many departments changing things. </p><p>Let's be honest here, at some point someone somewhere asked a web developer to update a DNS record and then landed up wiping out an mx record (I made this up, <a href=\"https://www.reddit.com/r/msp/comments/4j9spm/why_the_hell_do_web_designers_take_control_of_dns/\">but it turns out someone did it</a>)</p><p>I chose to complicate my DNS as much as possible by moving the name servers from the registrar, to cloud flare, then importing the records in to Terraform, then moving things like IP address' of servers to variables, then got bored and moved everything over to cnames, then got bored again and moved to modules. - I know :) </p><p></p><hr><blockquote>this post is a little confusing to follow, so if you get lost or it's just too confusing, please reach out!</blockquote><h2 id=\"the-why\">The why?</h2><p>Back in 2020 I set my self a goal, get everything that it takes to manage breadNET in to a git repo. This spanned all the way from creating servers in the cloud to server config and app install scripts to DNS. </p><p>Here's where it gets interesting. I hadn't heard about terraform before April 2020 so I was trying to go about it in some strange way by using a bash script I was working on, and then pull from the cloudflare API blah blah blah. Simple story - would not have worked.</p><p>I want everything in git so should my server fall off the face of the earth, all it takes is running the playbook/ <code>terraform apply</code>, restore the databse backups and files and we're good to go!</p><h2 id=\"dribble-about-my-dns\">Dribble about my DNS</h2><p>So like most people who host things at loss (<a href=\"__GHOST_URL__/what-it-takes-to-run-breadnet/\">see how much</a>) using free services is a must. </p><!--kg-card-begin: markdown--><ul>\n<li>Domain\n<ul>\n<li>Namecheap</li>\n</ul>\n</li>\n<li>DNS Hosting\n<ul>\n<li>Cloudflare</li>\n</ul>\n</li>\n<li>Git Hosting\n<ul>\n<li>Selfhosted GIT server</li>\n</ul>\n</li>\n<li>Pipeline\n<ul>\n<li>Codefresh</li>\n</ul>\n</li>\n</ul>\n<!--kg-card-end: markdown--><p></p><h2 id=\"the-how\">The How</h2><p>Buckle up, this gets way too complicated for what it really is!</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2021/05/4waycycle-02.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"441\" srcset=\"__GHOST_URL__/content/images/size/w600/2021/05/4waycycle-02.png 600w, __GHOST_URL__/content/images/size/w1000/2021/05/4waycycle-02.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/05/4waycycle-02.png 1600w, __GHOST_URL__/content/images/size/w2400/2021/05/4waycycle-02.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption>The \"pipeline\"</figcaption></figure><p>I manage my DNS through Cloudflare, using terraform which gets applied on a push to master via a service called Codefresh. </p><p>Let's start at the DNS level.</p><p>I chose to use Cloudflare for DNS as they provide a proxy level to your services so I am able to serve more people with less resources! They also have DDOS protection which I laughed at, but I've come under a few DDOS attacks. Not sure why but hey-ho!</p><p>My records are set out in a way that all records that end with <code>.breadnet.co.uk</code> are cnames! Even the main domain is a cname. </p><p>These cnames point to a second domain which follows a simple layout</p><p><code>&lt;server purpose&gt;.breadinfra.net</code> so the reverse proxy server is <code>reverse.breadinfra.net</code> </p><p>This means that my nslookups now look like this:</p><pre><code>stannardb@bread-l1:~$ nslookup\n&gt; unifi.breadnet.co.uk\nServer:\t\t127.0.0.53\nAddress:\t127.0.0.53#53\n\nNon-authoritative answer:\nunifi.breadnet.co.uk\tcanonical name = unifi.breadinfra.net.\nName:\tunifi.breadinfra.net\nAddress: 51.77.109.126\n&gt; \n</code></pre><p>I chose to do it this way as it allows me to only have to update the server record is I decide to move it. It originally came from when I was managing DNS through the UI. I have something like 75 records pointing to a few servers</p><p>Here comes the terraform!</p><p>Due to this abstraction I manage it through terraform using modules. Below is what a module looks like</p><pre><code>#main.tf\nresource \"cloudflare_record\" \"a\" {\n  zone_id = \"&lt;redacted&gt;\"\n  name    = var.name\n  type    = \"A\"\n  ttl     = \"1\"\n  proxied = var.proxied\n  value   = var.value\n}\n\n#outputs.tf\noutput \"id\" {\n  value = cloudflare_record.a.id\n}\n\noutput \"name\" {\n  value = cloudflare_record.a.name\n}\n\noutput \"hostname\" {\n  value = cloudflare_record.a.hostname\n}\n\n#provider.tf\nterraform {\n  required_providers {\n    cloudflare = {\n      source = \"cloudflare/cloudflare\"\n      version = \"~&gt; 2.0\"\n    }\n  }\n}\n\n#variables.tf\nvariable \"name\" {\n  type = string\n}\nvariable \"value\" {\n  type = string\n}\nvariable \"proxied\" {\n  type = string\n}\n</code></pre><p>These modules allow me to repeat as little as possible when creating a new record. </p><p>If I want to create an A record, it looks like:</p><pre><code>module \"hosted_on\" {\n  source = \"git::ssh://git@&lt;redacted&gt;a.git\"\n  name = \"hosted.on\"\n  proxied = \"false\"\n  value = var.&lt;redacted&gt;</code></pre><p>This then creates a record which cloudflare sees as:</p><pre><code># module.hosted_on.cloudflare_record.a:\nresource \"cloudflare_record\" \"a\" {\n    created_on  = \"2021-04-22T20:49:44.673066Z\"\n    data        = {}\n    hostname    = \"hosted.on.breadinfra.net\"\n    id          = \"&lt;redacted&gt;\"\n    metadata    = {\n        \"auto_added\"             = \"false\"\n        \"managed_by_apps\"        = \"false\"\n        \"managed_by_argo_tunnel\" = \"false\"\n        \"source\"                 = \"primary\"\n    }\n    modified_on = \"2021-04-22T20:49:44.673066Z\"\n    name        = \"hosted.on\"\n    proxiable   = true\n    proxied     = false\n    ttl         = 1\n    type        = \"A\"\n    value       = \"&lt;redacted&gt;\"\n    zone_id     = \"&lt;redacted&gt;\"\n}\n</code></pre><p>Then when I create a cname it looks like</p><pre><code>module \"bookstack\" {\n  source = \"git::ssh://git@&lt;redacted&gt;/cname.git\"\n  name = \"bookstack\"\n  proxied = \"true\"\n  value = module.reverse.hostname\n}</code></pre><p>Here we are passing the output of the module that created the A record for the reverse server (notice it's the hostname) - then we create it's fqdn, which will be <code>bookstack.breadnet.co.uk</code> and set it to proxy through cloudflare's network</p><pre><code>&gt; bookstack.breadnet.co.uk\nServer:\t\t127.0.0.53\nAddress:\t127.0.0.53#53\n\nNon-authoritative answer:\nName:\tbookstack.breadnet.co.uk\nAddress: 104.21.43.73\nName:\tbookstack.breadnet.co.uk\nAddress: 172.67.222.140\nName:\tbookstack.breadnet.co.uk\nAddress: 2606:4700:3036::ac43:de8c\nName:\tbookstack.breadnet.co.uk\nAddress: 2606:4700:3037::6815:2b49\n&gt; </code></pre><p>Once I've got all the records created, I then push it to my private git server where codefresh picks it up and runs a pipeline on it</p><p>This is what it looks like from the UI</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2021/05/image.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1301\" height=\"260\" srcset=\"__GHOST_URL__/content/images/size/w600/2021/05/image.png 600w, __GHOST_URL__/content/images/size/w1000/2021/05/image.png 1000w, __GHOST_URL__/content/images/2021/05/image.png 1301w\" sizes=\"(min-width: 720px) 720px\"></figure><p>There are 3 steps. Each step caused me hours of lost sleep just because</p><ul><li>This was my first time using codefresh</li><li>I don't know how to write the <code>codefresh.yml</code> file</li><li>I didn't know how to auth to my git server</li><li>I didn't know how to auth to Terraform cloud for remote state</li></ul><p>Before we continue, let me show you the file:</p><pre><code>version: '1.0'\nstages:\n  - checkout\n  - prepare\n  - deploy\nsteps:\n  main_clone:\n    title: Git clone\n    image: alpine/git:latest\n    stage: checkout\n    commands:\n      - mkdir -p ~/.ssh\n      - echo \"${SSH_KEY}\" | base64 -d &gt; ~/.ssh/id_rsa\n      - chmod 600 ~/.ssh/id_rsa\n      - echo \"${gitlab_known_host}\" &gt; ~/.ssh/known_hosts\n      - rm -rvf *dns*\n      - git clone git@&lt;redacted&gt;/dns.git\n      - mv dns/* .\n  SetupAuth:\n    image: alpine:3.9\n    title: Configuring Auth\n    stage: prepare\n    commands:\n      - export TF_VAR_cloudflare_email=$CLOUDFLARE_EMAIL\n      - export TF_VAR_cloudflare_api_key=$CLOUDFLARE_API_KEY\n  DeployWithTerraform:\n    image: hashicorp/terraform:light\n    title: Deploying Terraform plan\n    stage: deploy\n    commands:\n      - mkdir -p ~/.ssh\n      - echo \"${SSH_KEY}\" | base64 -d &gt; ~/.ssh/id_rsa\n      - chmod 600 ~/.ssh/id_rsa\n      - echo \"${gitlab_known_host}\" &gt; ~/.ssh/known_hosts\n      - terraform init -backend-config=\"token=\"$token\"\"\n      - terraform apply -auto-approve</code></pre><p>Let's break it down</p><p>It's broken in to 3 steps:</p><ul><li>checkout</li><li>Prepare</li><li>Deploy</li></ul><p>The checkout step was where I had the most issues, I had to git clone from my private git server behind a firewall. </p><p>I had to open the firewall to codefresh's IP range and then configure SSH keys.</p><p>I did the SSH keys by setting a secret in codefresh, generating a random SSH key on my laptop, echoing the private key to base64 ( <code>cat ./ssh/&lt;key&gt; | base64</code> ) and then putting that base64 encoded private key in to codefresh. It's important that you set it as a secret and then delete that private key from your laptop. </p><p>In your git platform, create a user and add that user to your repo, and then add the public key to their account.</p><p>I do a <code>rm</code> because codefresh has persistence between running and it was causing issues with not updating the files. I would rather pull each time and waste 3 seconds than cause terraform state drift. </p><p>The <code>config auth</code> stage just sets the cloudlfare API key as an environment variable so I'm not storing it in the terraform code. </p><p>Finally there is the deploy phase.</p><p>I use terraform cloud to manage my remote state file, this just helps as if I was to run this on my computer, I can just run when ever needed as long as I don't delete the file, I'm all good!</p><p>Where as running in an ephemeral environment, you want the persistence to be outside the platform. </p><p>You need to authenticate with a token to the terraform cloud which is usually stored under <code>$HOME/.terraform.d/credentials.tfrc.json</code> and looks like</p><pre><code>{\n  \"credentials\": {\n    \"app.terraform.io\": {\n      \"token\": \"&lt;redacted ya cheeky bugger&gt;\"\n    }\n  }\n</code></pre><p>so I had the idea to try and pass it to the command as <code>--backend-config</code> and low and behold, it worked!</p><p>Codefresh then runs the full thing, auto approves this apply and puts it in to action.</p><p>From committing the code to the records being created takes about 50 seconds, which allows me to do other things, like finish the nginx config file, or queue up the command to get an LE certificate. </p><p>If any part of this doesn't make sense, please reach out to me!</p><hr><p>Illustrations by <a href=\"__GHOST_URL__/dns-terraform-cloudflare/kirstylawrie.com/\">Kirsty Lawrie</a></p><p>You can hire me via <a href=\"https://www.upwork.com/freelancers/~01c61ee9802b94133e\">Upwork</a> or <a href=\"mailto:work@breadnet.co.uk\">emailing me</a> for weekend projects!</p>","comment_id":"609172c5bea1f916bfb8445c","plaintext":"Managing DNS can be a complicated thing for a large company, many departments changing things.\n\nLet's be honest here, at some point someone somewhere asked a web developer to update a DNS record and then landed up wiping out an mx record (I made this up, but it turns out someone did it)\n\nI chose to complicate my DNS as much as possible by moving the name servers from the registrar, to cloud flare, then importing the records in to Terraform, then moving things like IP address' of servers to variables, then got bored and moved everything over to cnames, then got bored again and moved to modules. - I know :)\n\n\n\nthis post is a little confusing to follow, so if you get lost or it's just too confusing, please reach out!\n\n\nThe why?\n\nBack in 2020 I set my self a goal, get everything that it takes to manage breadNET in to a git repo. This spanned all the way from creating servers in the cloud to server config and app install scripts to DNS.\n\nHere's where it gets interesting. I hadn't heard about terraform before April 2020 so I was trying to go about it in some strange way by using a bash script I was working on, and then pull from the cloudflare API blah blah blah. Simple story - would not have worked.\n\nI want everything in git so should my server fall off the face of the earth, all it takes is running the playbook/ terraform apply, restore the databse backups and files and we're good to go!\n\n\nDribble about my DNS\n\nSo like most people who host things at loss (see how much) using free services is a must.\n\n * Domain\n   \n   * Namecheap\n   \n * DNS Hosting\n   \n   * Cloudflare\n   \n * Git Hosting\n   \n   * Selfhosted GIT server\n   \n * Pipeline\n   \n   * Codefresh\n   \n\n\n\n\n\nThe How\n\nBuckle up, this gets way too complicated for what it really is!\n\nI manage my DNS through Cloudflare, using terraform which gets applied on a push to master via a service called Codefresh.\n\nLet's start at the DNS level.\n\nI chose to use Cloudflare for DNS as they provide a proxy level to your services so I am able to serve more people with less resources! They also have DDOS protection which I laughed at, but I've come under a few DDOS attacks. Not sure why but hey-ho!\n\nMy records are set out in a way that all records that end with .breadnet.co.uk are cnames! Even the main domain is a cname.\n\nThese cnames point to a second domain which follows a simple layout\n\n<server purpose>.breadinfra.net so the reverse proxy server is reverse.breadinfra.net\n\nThis means that my nslookups now look like this:\n\nstannardb@bread-l1:~$ nslookup\n> unifi.breadnet.co.uk\nServer:\t\t127.0.0.53\nAddress:\t127.0.0.53#53\n\nNon-authoritative answer:\nunifi.breadnet.co.uk\tcanonical name = unifi.breadinfra.net.\nName:\tunifi.breadinfra.net\nAddress: 51.77.109.126\n> \n\n\nI chose to do it this way as it allows me to only have to update the server record is I decide to move it. It originally came from when I was managing DNS through the UI. I have something like 75 records pointing to a few servers\n\nHere comes the terraform!\n\nDue to this abstraction I manage it through terraform using modules. Below is what a module looks like\n\n#main.tf\nresource \"cloudflare_record\" \"a\" {\n  zone_id = \"<redacted>\"\n  name    = var.name\n  type    = \"A\"\n  ttl     = \"1\"\n  proxied = var.proxied\n  value   = var.value\n}\n\n#outputs.tf\noutput \"id\" {\n  value = cloudflare_record.a.id\n}\n\noutput \"name\" {\n  value = cloudflare_record.a.name\n}\n\noutput \"hostname\" {\n  value = cloudflare_record.a.hostname\n}\n\n#provider.tf\nterraform {\n  required_providers {\n    cloudflare = {\n      source = \"cloudflare/cloudflare\"\n      version = \"~> 2.0\"\n    }\n  }\n}\n\n#variables.tf\nvariable \"name\" {\n  type = string\n}\nvariable \"value\" {\n  type = string\n}\nvariable \"proxied\" {\n  type = string\n}\n\n\nThese modules allow me to repeat as little as possible when creating a new record.\n\nIf I want to create an A record, it looks like:\n\nmodule \"hosted_on\" {\n  source = \"git::ssh://git@<redacted>a.git\"\n  name = \"hosted.on\"\n  proxied = \"false\"\n  value = var.<redacted>\n\nThis then creates a record which cloudflare sees as:\n\n# module.hosted_on.cloudflare_record.a:\nresource \"cloudflare_record\" \"a\" {\n    created_on  = \"2021-04-22T20:49:44.673066Z\"\n    data        = {}\n    hostname    = \"hosted.on.breadinfra.net\"\n    id          = \"<redacted>\"\n    metadata    = {\n        \"auto_added\"             = \"false\"\n        \"managed_by_apps\"        = \"false\"\n        \"managed_by_argo_tunnel\" = \"false\"\n        \"source\"                 = \"primary\"\n    }\n    modified_on = \"2021-04-22T20:49:44.673066Z\"\n    name        = \"hosted.on\"\n    proxiable   = true\n    proxied     = false\n    ttl         = 1\n    type        = \"A\"\n    value       = \"<redacted>\"\n    zone_id     = \"<redacted>\"\n}\n\n\nThen when I create a cname it looks like\n\nmodule \"bookstack\" {\n  source = \"git::ssh://git@<redacted>/cname.git\"\n  name = \"bookstack\"\n  proxied = \"true\"\n  value = module.reverse.hostname\n}\n\nHere we are passing the output of the module that created the A record for the reverse server (notice it's the hostname) - then we create it's fqdn, which will be bookstack.breadnet.co.uk and set it to proxy through cloudflare's network\n\n> bookstack.breadnet.co.uk\nServer:\t\t127.0.0.53\nAddress:\t127.0.0.53#53\n\nNon-authoritative answer:\nName:\tbookstack.breadnet.co.uk\nAddress: 104.21.43.73\nName:\tbookstack.breadnet.co.uk\nAddress: 172.67.222.140\nName:\tbookstack.breadnet.co.uk\nAddress: 2606:4700:3036::ac43:de8c\nName:\tbookstack.breadnet.co.uk\nAddress: 2606:4700:3037::6815:2b49\n> \n\nOnce I've got all the records created, I then push it to my private git server where codefresh picks it up and runs a pipeline on it\n\nThis is what it looks like from the UI\n\nThere are 3 steps. Each step caused me hours of lost sleep just because\n\n * This was my first time using codefresh\n * I don't know how to write the codefresh.yml file\n * I didn't know how to auth to my git server\n * I didn't know how to auth to Terraform cloud for remote state\n\nBefore we continue, let me show you the file:\n\nversion: '1.0'\nstages:\n  - checkout\n  - prepare\n  - deploy\nsteps:\n  main_clone:\n    title: Git clone\n    image: alpine/git:latest\n    stage: checkout\n    commands:\n      - mkdir -p ~/.ssh\n      - echo \"${SSH_KEY}\" | base64 -d > ~/.ssh/id_rsa\n      - chmod 600 ~/.ssh/id_rsa\n      - echo \"${gitlab_known_host}\" > ~/.ssh/known_hosts\n      - rm -rvf *dns*\n      - git clone git@<redacted>/dns.git\n      - mv dns/* .\n  SetupAuth:\n    image: alpine:3.9\n    title: Configuring Auth\n    stage: prepare\n    commands:\n      - export TF_VAR_cloudflare_email=$CLOUDFLARE_EMAIL\n      - export TF_VAR_cloudflare_api_key=$CLOUDFLARE_API_KEY\n  DeployWithTerraform:\n    image: hashicorp/terraform:light\n    title: Deploying Terraform plan\n    stage: deploy\n    commands:\n      - mkdir -p ~/.ssh\n      - echo \"${SSH_KEY}\" | base64 -d > ~/.ssh/id_rsa\n      - chmod 600 ~/.ssh/id_rsa\n      - echo \"${gitlab_known_host}\" > ~/.ssh/known_hosts\n      - terraform init -backend-config=\"token=\"$token\"\"\n      - terraform apply -auto-approve\n\nLet's break it down\n\nIt's broken in to 3 steps:\n\n * checkout\n * Prepare\n * Deploy\n\nThe checkout step was where I had the most issues, I had to git clone from my private git server behind a firewall.\n\nI had to open the firewall to codefresh's IP range and then configure SSH keys.\n\nI did the SSH keys by setting a secret in codefresh, generating a random SSH key on my laptop, echoing the private key to base64 ( cat ./ssh/<key> | base64 ) and then putting that base64 encoded private key in to codefresh. It's important that you set it as a secret and then delete that private key from your laptop.\n\nIn your git platform, create a user and add that user to your repo, and then add the public key to their account.\n\nI do a rm because codefresh has persistence between running and it was causing issues with not updating the files. I would rather pull each time and waste 3 seconds than cause terraform state drift.\n\nThe config auth stage just sets the cloudlfare API key as an environment variable so I'm not storing it in the terraform code.\n\nFinally there is the deploy phase.\n\nI use terraform cloud to manage my remote state file, this just helps as if I was to run this on my computer, I can just run when ever needed as long as I don't delete the file, I'm all good!\n\nWhere as running in an ephemeral environment, you want the persistence to be outside the platform.\n\nYou need to authenticate with a token to the terraform cloud which is usually stored under $HOME/.terraform.d/credentials.tfrc.json and looks like\n\n{\n  \"credentials\": {\n    \"app.terraform.io\": {\n      \"token\": \"<redacted ya cheeky bugger>\"\n    }\n  }\n\n\nso I had the idea to try and pass it to the command as --backend-config and low and behold, it worked!\n\nCodefresh then runs the full thing, auto approves this apply and puts it in to action.\n\nFrom committing the code to the records being created takes about 50 seconds, which allows me to do other things, like finish the nginx config file, or queue up the command to get an LE certificate.\n\nIf any part of this doesn't make sense, please reach out to me!\n\nIllustrations by Kirsty Lawrie\n\nYou can hire me via Upwork or emailing me for weekend projects!","feature_image":"__GHOST_URL__/content/images/2021/05/Image_20191002_222359-1-1.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2021-05-04T16:13:57.000Z","updated_at":"2021-05-24T20:00:10.000Z","published_at":"2021-05-24T19:57:00.000Z","custom_excerpt":"What's the best way to over engineer DNS record management? CI/CD - let me tell you","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"6425f3bc192c0c150413d5a3","uuid":"1965a3b4-47e6-4f03-b473-ad37d423ffcb","title":"Decrease EBS volume (AWS)","slug":"shrink-ebs-aws","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"hr\",{}],[\"hr\",{}],[\"markdown\",{\"markdown\":\"* Install ncdu on the instance (This is just to see the current size of all the files)\\n* Change directory to / and run sudo ncdu \\n* Create the new volume \\n    * Choose the region you wish to deploy it to\\n    * Choose the size and the type\\n     * Add the relevant tags\\n     * `name` : `new-vol`\\n    \"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/06/image.png\",\"width\":1316,\"height\":744,\"caption\":\"Creating the new VOL\"}],[\"markdown\",{\"markdown\":\"* Shutdown the instance (`sudo shutdown` on the instance)\\n* Right click the new Volume\\n* Click `Attach Volume`\\n* Choose the instance either by Name or ID (I prefer ID)\"}],[\"markdown\",{\"markdown\":\"* Check if the volume has got data on it (Can never be too sure)\\n    * `sudo file -s /dev/xvdf`\\n        * It should return `/dev/xvdf: data`\\n        * If it does not, stop and re-assess the situation\\n* Format the volume\\n    * `sudo mkfs -t ext4 /dev/xvdf`\"}],[\"markdown\",{\"markdown\":\"* Create the directory to mount the disk to\\n    * `mkdir /mnt/new-vol`\\n* `sudo mount /dev/xvdf /mnt/new-vol`\"}],[\"code\",{\"code\":\"rsync -axv / /mnt/new-vol/\"}],[\"hr\",{}],[\"markdown\",{\"markdown\":\"* Instal `grub` on the new volume\\n    * `grub-install --root-directory=/mnt/new-volume --force /dev/xvdf`\\n    * (centos 7+) `grub2-install --root-directory=/mnt/new-volume --force /dev/xvdf`\\n* Unmount the directory \\n    * `sudo umount /mnt/new-vol`\\n* Check the UUID using `blkid` (make note of this, we need it below)\\n* `tune2fs -U <uuid from ^> /dev/xvdf`\\n* Check the volume lable from the old volume using `sudo e2label /dev/xvda1` : Should return something like `cloudimg-rootfs`\\n* Replace the volume label with the old value with `e2label /dev/xvdf cloudimg-rootfs`\\n\\n\"}],[\"code\",{\"code\":\"sudo shutdown\\n\"}],[\"hr\",{}]],\"markups\":[[\"code\"],[\"a\",[\"href\",\"https://en.wikipedia.org/wiki/Cover_your_ass\"]],[\"strong\"],[\"u\"]],\"sections\":[[1,\"p\",[[0,[],0,\"It's rare for me to blog about AWS considering I am mainly GCP focused, but this stumped me enough to the point I think it can be of use.\"]]],[1,\"p\",[[0,[],0,\"Let's start\"]]],[10,0],[1,\"p\",[[0,[],0,\"AWS doesn't allow you to shrink an EBS volume, so our only way to do it is to create a new volume, move everything over then remap it. \"]]],[1,\"p\",[]],[1,\"h3\",[[0,[],0,\"Getting started\"]]],[1,\"p\",[[0,[],0,\"We will assume:\"]]],[3,\"ol\",[[[0,[],0,\"An instance running us us-east-1e\"]],[[0,[],0,\"300GB EBS volume named \"],[0,[0],1,\"old\"]],[[0,[],0,\"We want to create a new one of 100gb (we will call it \"],[0,[0],1,\"new-vol\"],[0,[],0,\")\"]]]],[1,\"p\",[[0,[],0,\"We need to shutdown the instance to prevent issues and inconsistencies. \"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"I suggest snapshotting the old volume before doing any work. This is just a \"],[0,[1],1,\"CYA\"],[0,[],0,\" precaution! (Remember to delete it later to save costs)\"]]],[10,1],[1,\"p\",[]],[10,2],[10,3],[10,4],[1,\"h3\",[[0,[],0,\"Now we need to start the instance and format the Volume\"]]],[10,5],[1,\"h3\",[[0,[],0,\"Mount the new volume to the instance\"]]],[10,6],[1,\"h3\",[[0,[],0,\"Copy data to new Volume\"]]],[10,7],[1,\"p\",[[0,[],0,\"This will take a while, so get on with some other work\"]]],[10,8],[1,\"h3\",[[0,[],0,\"Once the copy has finished:\"]]],[10,9],[1,\"h3\",[[0,[],0,\"Shutdown the instance\"]]],[10,10],[1,\"p\",[[0,[],0,\"Detach the old volume and the new volume from the AWS EC2 console\"]]],[1,\"p\",[[0,[],0,\"Attach the new volume (Best to do it by UUID of the EC2 Instance) to \"],[0,[0],1,\"/dev/sda1\"]]],[1,\"p\",[[0,[],0,\"Start the EC2 instance and SSH\"]]],[10,11],[1,\"h2\",[[0,[],0,\"Closing notes\"]]],[1,\"p\",[[0,[],0,\"Your milage may vary, we are assuming that you have adequate linux experience\"]]],[1,\"p\",[[0,[],0,\"I take no responsibility to any damage caused to your instances during this process, you knew the risks by doing this in prod, you take the fall for it. Learn from it and \"],[0,[2,3],2,\"don't do it in prod again \"]]],[1,\"p\",[]]],\"ghostVersion\":\"3.0\"}","lexical":null,"html":"<p>It's rare for me to blog about AWS considering I am mainly GCP focused, but this stumped me enough to the point I think it can be of use.</p><p>Let's start</p><hr><p>AWS doesn't allow you to shrink an EBS volume, so our only way to do it is to create a new volume, move everything over then remap it. </p><p></p><h3 id=\"getting-started\">Getting started</h3><p>We will assume:</p><ol><li>An instance running us us-east-1e</li><li>300GB EBS volume named <code>old</code></li><li>We want to create a new one of 100gb (we will call it <code>new-vol</code>)</li></ol><p>We need to shutdown the instance to prevent issues and inconsistencies. </p><p></p><p>I suggest snapshotting the old volume before doing any work. This is just a <a href=\"https://en.wikipedia.org/wiki/Cover_your_ass\">CYA</a> precaution! (Remember to delete it later to save costs)</p><hr><p></p><!--kg-card-begin: markdown--><ul>\n<li>Install ncdu on the instance (This is just to see the current size of all the files)</li>\n<li>Change directory to / and run sudo ncdu</li>\n<li>Create the new volume\n<ul>\n<li>Choose the region you wish to deploy it to</li>\n<li>Choose the size and the type</li>\n<li>Add the relevant tags</li>\n<li><code>name</code> : <code>new-vol</code></li>\n</ul>\n</li>\n</ul>\n<!--kg-card-end: markdown--><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2021/06/image.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1316\" height=\"744\" srcset=\"__GHOST_URL__/content/images/size/w600/2021/06/image.png 600w, __GHOST_URL__/content/images/size/w1000/2021/06/image.png 1000w, __GHOST_URL__/content/images/2021/06/image.png 1316w\" sizes=\"(min-width: 720px) 720px\"><figcaption>Creating the new VOL</figcaption></figure><!--kg-card-begin: markdown--><ul>\n<li>Shutdown the instance (<code>sudo shutdown</code> on the instance)</li>\n<li>Right click the new Volume</li>\n<li>Click <code>Attach Volume</code></li>\n<li>Choose the instance either by Name or ID (I prefer ID)</li>\n</ul>\n<!--kg-card-end: markdown--><h3 id=\"now-we-need-to-start-the-instance-and-format-the-volume\">Now we need to start the instance and format the Volume</h3><!--kg-card-begin: markdown--><ul>\n<li>Check if the volume has got data on it (Can never be too sure)\n<ul>\n<li><code>sudo file -s /dev/xvdf</code>\n<ul>\n<li>It should return <code>/dev/xvdf: data</code></li>\n<li>If it does not, stop and re-assess the situation</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Format the volume\n<ul>\n<li><code>sudo mkfs -t ext4 /dev/xvdf</code></li>\n</ul>\n</li>\n</ul>\n<!--kg-card-end: markdown--><h3 id=\"mount-the-new-volume-to-the-instance\">Mount the new volume to the instance</h3><!--kg-card-begin: markdown--><ul>\n<li>Create the directory to mount the disk to\n<ul>\n<li><code>mkdir /mnt/new-vol</code></li>\n</ul>\n</li>\n<li><code>sudo mount /dev/xvdf /mnt/new-vol</code></li>\n</ul>\n<!--kg-card-end: markdown--><h3 id=\"copy-data-to-new-volume\">Copy data to new Volume</h3><pre><code>rsync -axv / /mnt/new-vol/</code></pre><p>This will take a while, so get on with some other work</p><hr><h3 id=\"once-the-copy-has-finished-\">Once the copy has finished:</h3><!--kg-card-begin: markdown--><ul>\n<li>Instal <code>grub</code> on the new volume\n<ul>\n<li><code>grub-install --root-directory=/mnt/new-volume --force /dev/xvdf</code></li>\n<li>(centos 7+) <code>grub2-install --root-directory=/mnt/new-volume --force /dev/xvdf</code></li>\n</ul>\n</li>\n<li>Unmount the directory\n<ul>\n<li><code>sudo umount /mnt/new-vol</code></li>\n</ul>\n</li>\n<li>Check the UUID using <code>blkid</code> (make note of this, we need it below)</li>\n<li><code>tune2fs -U &lt;uuid from ^&gt; /dev/xvdf</code></li>\n<li>Check the volume lable from the old volume using <code>sudo e2label /dev/xvda1</code> : Should return something like <code>cloudimg-rootfs</code></li>\n<li>Replace the volume label with the old value with <code>e2label /dev/xvdf cloudimg-rootfs</code></li>\n</ul>\n<!--kg-card-end: markdown--><h3 id=\"shutdown-the-instance\">Shutdown the instance</h3><pre><code>sudo shutdown\n</code></pre><p>Detach the old volume and the new volume from the AWS EC2 console</p><p>Attach the new volume (Best to do it by UUID of the EC2 Instance) to <code>/dev/sda1</code></p><p>Start the EC2 instance and SSH</p><hr><h2 id=\"closing-notes\">Closing notes</h2><p>Your milage may vary, we are assuming that you have adequate linux experience</p><p>I take no responsibility to any damage caused to your instances during this process, you knew the risks by doing this in prod, you take the fall for it. Learn from it and <strong><u>don't do it in prod again </u></strong></p>","comment_id":"60dafbf6bea1f916bfb8456d","plaintext":"It's rare for me to blog about AWS considering I am mainly GCP focused, but this stumped me enough to the point I think it can be of use.\n\nLet's start\n\nAWS doesn't allow you to shrink an EBS volume, so our only way to do it is to create a new volume, move everything over then remap it.\n\n\n\n\nGetting started\n\nWe will assume:\n\n 1. An instance running us us-east-1e\n 2. 300GB EBS volume named old\n 3. We want to create a new one of 100gb (we will call it new-vol)\n\nWe need to shutdown the instance to prevent issues and inconsistencies.\n\n\n\nI suggest snapshotting the old volume before doing any work. This is just a CYA precaution! (Remember to delete it later to save costs)\n\n\n\n * Install ncdu on the instance (This is just to see the current size of all the files)\n * Change directory to / and run sudo ncdu\n * Create the new volume\n   \n   * Choose the region you wish to deploy it to\n   * Choose the size and the type\n   * Add the relevant tags\n   * name : new-vol\n   \n\n\n * Shutdown the instance (sudo shutdown on the instance)\n * Right click the new Volume\n * Click Attach Volume\n * Choose the instance either by Name or ID (I prefer ID)\n\n\n\nNow we need to start the instance and format the Volume\n\n * Check if the volume has got data on it (Can never be too sure)\n   \n   * sudo file -s /dev/xvdf\n     \n     * It should return /dev/xvdf: data\n     * If it does not, stop and re-assess the situation\n     \n   \n * Format the volume\n   \n   * sudo mkfs -t ext4 /dev/xvdf\n   \n\n\n\nMount the new volume to the instance\n\n * Create the directory to mount the disk to\n   \n   * mkdir /mnt/new-vol\n   \n * sudo mount /dev/xvdf /mnt/new-vol\n\n\n\nCopy data to new Volume\n\nrsync -axv / /mnt/new-vol/\n\nThis will take a while, so get on with some other work\n\n\nOnce the copy has finished:\n\n * Instal grub on the new volume\n   \n   * grub-install --root-directory=/mnt/new-volume --force /dev/xvdf\n   * (centos 7+) grub2-install --root-directory=/mnt/new-volume --force /dev/xvdf\n   \n * Unmount the directory\n   \n   * sudo umount /mnt/new-vol\n   \n * Check the UUID using blkid (make note of this, we need it below)\n * tune2fs -U <uuid from ^> /dev/xvdf\n * Check the volume lable from the old volume using sudo e2label /dev/xvda1 : Should return something like cloudimg-rootfs\n * Replace the volume label with the old value with e2label /dev/xvdf cloudimg-rootfs\n\n\n\nShutdown the instance\n\nsudo shutdown\n\n\nDetach the old volume and the new volume from the AWS EC2 console\n\nAttach the new volume (Best to do it by UUID of the EC2 Instance) to /dev/sda1\n\nStart the EC2 instance and SSH\n\n\nClosing notes\n\nYour milage may vary, we are assuming that you have adequate linux experience\n\nI take no responsibility to any damage caused to your instances during this process, you knew the risks by doing this in prod, you take the fall for it. Learn from it and don't do it in prod again","feature_image":"https://images.unsplash.com/photo-1484662020986-75935d2ebc66?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwxMTc3M3wwfDF8c2VhcmNofDR8fGRpc2t8ZW58MHx8fHwxNjI0OTY1MzUz&ixlib=rb-1.2.1&q=80&w=2000","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2021-06-29T10:54:46.000Z","updated_at":"2021-07-14T10:02:50.000Z","published_at":"2021-06-29T11:17:05.000Z","custom_excerpt":"Need to shrink an EBS volume? Well, it's not that simple. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"6425f3bc192c0c150413d5a4","uuid":"5e61ae27-9a74-40a6-aaee-646278558597","title":"Why I chose to migrate my mail server","slug":"leaving-selfhosted-mail","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"hr\",{}],[\"hr\",{}],[\"hr\",{}],[\"hr\",{}]],\"markups\":[[\"strong\"],[\"a\",[\"href\",\"https://www.upwork.com/freelancers/~01c61ee9802b94133e\"]],[\"a\",[\"href\",\"mailto:website@breadnet.co.uk\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Well, once again, this isn't something I expected to be writing about but here we are!\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"So if you've read any of my posts, you'd know that I self hosted my own mail server. At the time of writing, that server had been online for 2 years. I first installed it in 2018, and it's done well! \"]]],[1,\"p\",[[0,[],0,\"Now, the main question here is why I've decided to migrate it, and where to?\"]]],[10,0],[1,\"p\",[[0,[],0,\"So let's look at the pros vs cons of self hosting your own mail server:\"]]],[3,\"ol\",[[[0,[],0,\"First and foremost, it teaches you \"],[0,[0],1,\"so much \"],[0,[],0,\"about how email deliverability works, and how temperamental it is. \"]],[[0,[],0,\"Increased privacy, the emails reside on \"],[0,[0],1,\"your \"],[0,[],0,\"server that you control. No one else has access to the server (If you've set it up correctly) \"]],[[0,[],0,\"As many accounts as you want for the price of the server per month\"],[1,[],0,0],[0,[],0,\"This is nice to have, you can give friends and family email accounts and the milboxes can be as big as you want\"]],[[0,[],0,\"Faster release tracks - Seeing as you're self hosting the server, you can upgrade and install new software when ever you want\"]],[[0,[],0,\"Flexing rights, being able to tell people \\\"Yeah, I host my own email server\\\" \"]]]],[1,\"p\",[[0,[],0,\"There are more, but this is all I can think of at the moment\"]]],[10,1],[1,\"p\",[[0,[],0,\"Now lets look at the cons\"]]],[3,\"ol\",[[[0,[],0,\"Reliability - If your server goes down, your emails aren't accessable\"]],[[0,[],0,\"If your server goes down, you cant access emails\"]],[[0,[],0,\"You are the only support staff for your email domain\"]],[[0,[0],0,\"the \"],[0,[],0,\"amount\"],[0,[],1,\" of hackers and spammers\"]],[[0,[],0,\"Hard to secure \"],[0,[0],1,\"properly \"],[0,[],0,\"(it can be done, but it's a never ending road of \\\"what if I enable this\\\"\"]],[[0,[],0,\"Sender score - How will gmail office 365 etc inbox your mail (spoiler: spam)\"]],[[0,[],0,\"Updates sometimes break things\"]],[[0,[],0,\"You're responsible for \"],[0,[0],1,\"good \"],[0,[],0,\"backups \"]]]],[1,\"p\",[[0,[],0,\"Once again, there are many more, but these are the main ones I can think of\"]]],[10,2],[1,\"p\",[[0,[],0,\"As someone who hosted their mail server for \"],[0,[0],1,\"2 \"],[0,[],0,\"years, I think I have enough experience here to say it was fun, but the novelty wears off after about 6 months. Let me detail some of the issues I had:\"]]],[3,\"ol\",[[[0,[],0,\"IP address was blacklisted, provider wouldn't change IP address\"]],[[0,[],0,\"Provider had to be convinced to open port 25 etc\"]],[[0,[],0,\"Certificates that auto-renew, you need to restart postfix \"]],[[0,[],0,\"Stopping and starting your mail server each time you make a change to the config\"]],[[0,[],0,\"Log files aren't always useful and sometimes lead you down a rabbit hole\"]],[[0,[],0,\"Lots of things to customize (and then break)\"]],[[0,[],0,\"Server reboots would cause bounces in mail\"]],[[0,[],0,\"Sender reputation was hard to get right\"]],[[0,[],0,\"Inboxing was a \"],[0,[0],1,\"nightmare \"],[0,[],0,\"on both google and microsoft\"],[1,[],0,1],[0,[],0,\"(If someone from ms or google is reading this, fight me, 1v1 me in club penguin)\"]],[[0,[],0,\"Emails would have weird formatting, attachments wouldn't display correctly. \"]]]],[1,\"p\",[[0,[],0,\"Now I know from this article, it looks like I've spent most of the time taking a dump on hosting your own mail server, and whilst that is true, I highly reccomend it. If you or your team are responsible for emails, be it spam filtering or office 365 - You should run your own mail server.\"]]],[1,\"p\",[[0,[],0,\"Running your own mail server is great, albeit a little annoying and you need to be on top of things like backups etc, the learning is great!\"]]],[1,\"p\",[[0,[],0,\"In my opinion the best place to run your own mail server is for \"],[0,[0],1,\"internal only use\"],[0,[],0,\", so this could be you testing zabbix integrations, or just having an internal mail system for your IOT, or your team to shoot the shit at work and play.\"]]],[1,\"h2\",[[0,[],0,\"Cost\"]]],[1,\"p\",[[0,[],0,\"I was running my email on Digital Ocean for 2 years at $6 a month, which at the time of writing this (aug152021) it's around £4.33 a month for a server where as Office exchange online plan 1, is £3.60 \"]]],[1,\"p\",[[0,[],0,\"2 years running cost of Self hosting: 103.92\"],[1,[],0,2],[0,[],0,\"2 years running cost of M$ hosting: 86.4\"]]],[1,\"p\",[[0,[],0,\"Savings by M$ hosting it: 17.52\"]]],[1,\"p\",[[0,[],0,\"Whilst the 17.52 doesn't really seem like a big number, when we look at the hours it took to get working, we're looking at around about 80 hours over the 2 years, and considering I charge $17 per hour for contracting, that's $1360 worth of charges\"]]],[1,\"p\",[[0,[],0,\"vs\"]]],[1,\"p\",[[0,[],0,\"2 hours setup on Office 365: $34\"]]],[10,3],[1,\"p\",[[0,[],0,\"Closing notes:\"]]],[1,\"p\",[[0,[],0,\"Despite the higher cost and time taken to self host, I highly recommend it, most blogs will tell you don't do it, but I really think this should be every sysadmin's weekend project! It's fun and something usable!\"]]],[1,\"p\",[[0,[],0,\"As always, you can \"],[0,[1],1,\"contract me\"],[0,[],0,\" if you want help or \"],[0,[2],1,\"contact me\"]]],[1,\"p\",[]]],\"ghostVersion\":\"3.0\"}","lexical":null,"html":"<p>Well, once again, this isn't something I expected to be writing about but here we are!</p><p></p><p>So if you've read any of my posts, you'd know that I self hosted my own mail server. At the time of writing, that server had been online for 2 years. I first installed it in 2018, and it's done well! </p><p>Now, the main question here is why I've decided to migrate it, and where to?</p><hr><p>So let's look at the pros vs cons of self hosting your own mail server:</p><ol><li>First and foremost, it teaches you <strong>so much </strong>about how email deliverability works, and how temperamental it is. </li><li>Increased privacy, the emails reside on <strong>your </strong>server that you control. No one else has access to the server (If you've set it up correctly) </li><li>As many accounts as you want for the price of the server per month<br>This is nice to have, you can give friends and family email accounts and the milboxes can be as big as you want</li><li>Faster release tracks - Seeing as you're self hosting the server, you can upgrade and install new software when ever you want</li><li>Flexing rights, being able to tell people \"Yeah, I host my own email server\" </li></ol><p>There are more, but this is all I can think of at the moment</p><hr><p>Now lets look at the cons</p><ol><li>Reliability - If your server goes down, your emails aren't accessable</li><li>If your server goes down, you cant access emails</li><li>You are the only support staff for your email domain</li><li><strong>the amount of hackers and spammers</strong></li><li>Hard to secure <strong>properly </strong>(it can be done, but it's a never ending road of \"what if I enable this\"</li><li>Sender score - How will gmail office 365 etc inbox your mail (spoiler: spam)</li><li>Updates sometimes break things</li><li>You're responsible for <strong>good </strong>backups </li></ol><p>Once again, there are many more, but these are the main ones I can think of</p><hr><p>As someone who hosted their mail server for <strong>2 </strong>years, I think I have enough experience here to say it was fun, but the novelty wears off after about 6 months. Let me detail some of the issues I had:</p><ol><li>IP address was blacklisted, provider wouldn't change IP address</li><li>Provider had to be convinced to open port 25 etc</li><li>Certificates that auto-renew, you need to restart postfix </li><li>Stopping and starting your mail server each time you make a change to the config</li><li>Log files aren't always useful and sometimes lead you down a rabbit hole</li><li>Lots of things to customize (and then break)</li><li>Server reboots would cause bounces in mail</li><li>Sender reputation was hard to get right</li><li>Inboxing was a <strong>nightmare </strong>on both google and microsoft<br>(If someone from ms or google is reading this, fight me, 1v1 me in club penguin)</li><li>Emails would have weird formatting, attachments wouldn't display correctly. </li></ol><p>Now I know from this article, it looks like I've spent most of the time taking a dump on hosting your own mail server, and whilst that is true, I highly reccomend it. If you or your team are responsible for emails, be it spam filtering or office 365 - You should run your own mail server.</p><p>Running your own mail server is great, albeit a little annoying and you need to be on top of things like backups etc, the learning is great!</p><p>In my opinion the best place to run your own mail server is for <strong>internal only use</strong>, so this could be you testing zabbix integrations, or just having an internal mail system for your IOT, or your team to shoot the shit at work and play.</p><h2 id=\"cost\">Cost</h2><p>I was running my email on Digital Ocean for 2 years at $6 a month, which at the time of writing this (aug152021) it's around £4.33 a month for a server where as Office exchange online plan 1, is £3.60 </p><p>2 years running cost of Self hosting: 103.92<br>2 years running cost of M$ hosting: 86.4</p><p>Savings by M$ hosting it: 17.52</p><p>Whilst the 17.52 doesn't really seem like a big number, when we look at the hours it took to get working, we're looking at around about 80 hours over the 2 years, and considering I charge $17 per hour for contracting, that's $1360 worth of charges</p><p>vs</p><p>2 hours setup on Office 365: $34</p><hr><p>Closing notes:</p><p>Despite the higher cost and time taken to self host, I highly recommend it, most blogs will tell you don't do it, but I really think this should be every sysadmin's weekend project! It's fun and something usable!</p><p>As always, you can <a href=\"https://www.upwork.com/freelancers/~01c61ee9802b94133e\">contract me</a> if you want help or <a href=\"mailto:website@breadnet.co.uk\">contact me</a></p>","comment_id":"6118638c17e5727444dc7629","plaintext":"Well, once again, this isn't something I expected to be writing about but here we are!\n\n\n\nSo if you've read any of my posts, you'd know that I self hosted my own mail server. At the time of writing, that server had been online for 2 years. I first installed it in 2018, and it's done well!\n\nNow, the main question here is why I've decided to migrate it, and where to?\n\nSo let's look at the pros vs cons of self hosting your own mail server:\n\n 1. First and foremost, it teaches you so much about how email deliverability works, and how temperamental it is.\n 2. Increased privacy, the emails reside on your server that you control. No one else has access to the server (If you've set it up correctly)\n 3. As many accounts as you want for the price of the server per month\n    This is nice to have, you can give friends and family email accounts and the milboxes can be as big as you want\n 4. Faster release tracks - Seeing as you're self hosting the server, you can upgrade and install new software when ever you want\n 5. Flexing rights, being able to tell people \"Yeah, I host my own email server\"\n\nThere are more, but this is all I can think of at the moment\n\nNow lets look at the cons\n\n 1. Reliability - If your server goes down, your emails aren't accessable\n 2. If your server goes down, you cant access emails\n 3. You are the only support staff for your email domain\n 4. the amount of hackers and spammers\n 5. Hard to secure properly (it can be done, but it's a never ending road of \"what if I enable this\"\n 6. Sender score - How will gmail office 365 etc inbox your mail (spoiler: spam)\n 7. Updates sometimes break things\n 8. You're responsible for good backups\n\nOnce again, there are many more, but these are the main ones I can think of\n\nAs someone who hosted their mail server for 2 years, I think I have enough experience here to say it was fun, but the novelty wears off after about 6 months. Let me detail some of the issues I had:\n\n 1.  IP address was blacklisted, provider wouldn't change IP address\n 2.  Provider had to be convinced to open port 25 etc\n 3.  Certificates that auto-renew, you need to restart postfix\n 4.  Stopping and starting your mail server each time you make a change to the config\n 5.  Log files aren't always useful and sometimes lead you down a rabbit hole\n 6.  Lots of things to customize (and then break)\n 7.  Server reboots would cause bounces in mail\n 8.  Sender reputation was hard to get right\n 9.  Inboxing was a nightmare on both google and microsoft\n     (If someone from ms or google is reading this, fight me, 1v1 me in club penguin)\n 10. Emails would have weird formatting, attachments wouldn't display correctly.\n\nNow I know from this article, it looks like I've spent most of the time taking a dump on hosting your own mail server, and whilst that is true, I highly reccomend it. If you or your team are responsible for emails, be it spam filtering or office 365 - You should run your own mail server.\n\nRunning your own mail server is great, albeit a little annoying and you need to be on top of things like backups etc, the learning is great!\n\nIn my opinion the best place to run your own mail server is for internal only use, so this could be you testing zabbix integrations, or just having an internal mail system for your IOT, or your team to shoot the shit at work and play.\n\n\nCost\n\nI was running my email on Digital Ocean for 2 years at $6 a month, which at the time of writing this (aug152021) it's around £4.33 a month for a server where as Office exchange online plan 1, is £3.60\n\n2 years running cost of Self hosting: 103.92\n2 years running cost of M$ hosting: 86.4\n\nSavings by M$ hosting it: 17.52\n\nWhilst the 17.52 doesn't really seem like a big number, when we look at the hours it took to get working, we're looking at around about 80 hours over the 2 years, and considering I charge $17 per hour for contracting, that's $1360 worth of charges\n\nvs\n\n2 hours setup on Office 365: $34\n\nClosing notes:\n\nDespite the higher cost and time taken to self host, I highly recommend it, most blogs will tell you don't do it, but I really think this should be every sysadmin's weekend project! It's fun and something usable!\n\nAs always, you can contract me if you want help or contact me","feature_image":"https://images.unsplash.com/photo-1603539279542-e7cf76a92801?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwxMTc3M3wwfDF8c2VhcmNofDJ8fGVtYWlsfGVufDB8fHx8MTYyODk4ODQ0MQ&ixlib=rb-1.2.1&q=80&w=2000","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2021-08-15T00:45:00.000Z","updated_at":"2021-08-15T01:19:18.000Z","published_at":"2021-08-15T01:19:18.000Z","custom_excerpt":"Is it worth self hosting email?","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"6425f3bc192c0c150413d5a5","uuid":"52416baf-1604-43e6-bc86-481a37477289","title":"ZeroTrust and you","slug":"zerotrust-and-you","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}]],\"cards\":[[\"markdown\",{\"markdown\":\"* Duo Beyond\\n* Perimeter81\\n* Forcepoint Dynamic Security Solutions\\n* Prove\\n* Google BeyondCorp\\n* Microsoft 365 Azure\\n* Okta Identity Cloud \\n* Palo Alto Networks\\n* Proofpoint Meta\\n* Unisys Stealth\\n* Cloudflare Teams access\\n* Pritunl Zero\\n* Teleport\"}],[\"markdown\",{\"markdown\":\"* Allow SSH Bastion\\n* Allow web connections\\n* 2FA\\n* Central user management\\n* Simple to use\\n* Run on minimal hardware (s1-2 on OVH)\"}],[\"bookmark\",{\"url\":\"https://github.com/userbradley/zerotrust-public\",\"metadata\":{\"url\":\"https://github.com/userbradley/zerotrust-public\",\"title\":\"GitHub - userbradley/zerotrust-public\",\"description\":\"Contribute to userbradley/zerotrust-public development by creating an account on GitHub.\",\"author\":\"userbradley\",\"publisher\":\"GitHub\",\"thumbnail\":\"https://opengraph.githubassets.com/2308e21c024e7a78416d8c208b1ab6c624b50f62905581ef7298b47a62472324/userbradley/zerotrust-public\",\"icon\":\"https://github.githubassets.com/favicons/favicon.svg\"}}],[\"bookmark\",{\"url\":\"https://docs.pritunl.com/docs/pritunl-zero\",\"metadata\":{\"url\":\"https://docs.pritunl.com/docs/pritunl-zero\",\"title\":\"Getting Started SSH\",\"description\":\"Install and configure Pritunl Zero SSH certificates with two-factor authentication\",\"author\":null,\"publisher\":\"Pritunl\",\"thumbnail\":\"https://files.readme.io/306844c-ssh0.png\",\"icon\":\"https://files.readme.io/gkjYhYaZQkyKjQTOFpIn_favicon.ico\"}}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/12/image.png\",\"width\":2240,\"height\":1910}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/12/image-1.png\",\"width\":2162,\"height\":1596}],[\"code\",{\"code\":\"check_host_cert: certificate signature algorithm ssh-rsa: signature algorithm not supported\\nReceived disconnect from <ip> port 9800:2: Too many authentication failures\\nDisconnected from <ip> port 9800\\nkex_exchange_identification: Connection closed by remote host\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/12/image-2.png\",\"width\":389,\"height\":350}],[\"code\",{\"code\":\"Host 198.244.155.107\\n    ProxyJump bastion@<bastionaddress>:9800\\n\\n host reverse1-lon\\n hostname reverse1.lon.eu.breadnet.co.uk\\n user root\\n ProxyJump bastion@<bastionaddress>:9800\",\"language\":\"bash\"}],[\"markdown\",{\"markdown\":\"* Lines `1-2`\\n    * Connects to the host 198...\\n    * Username is your logged in user\\n    * Uses the ssh key defined when you setup the local pritunl-ssh\\n\\n* Rest of the file\\n    * Using a simple name\\n    * Logs in as root\\n    * Jumps via the bastion\\n    * Same SSH key as the above\"}],[\"hr\",{}],[\"hr\",{}]],\"markups\":[[\"a\",[\"href\",\"https://www.okta.com/sites/default/files/2021-07/WPR-2021-ZeroTrust-070821.pdf\"]],[\"a\",[\"href\",\"https://zero.pritunl.com\"]],[\"a\",[\"href\",\"__GHOST_URL__/terraform-ovh-openstack/\"]],[\"code\"],[\"a\",[\"href\",\"https://docs.docker.com/get-docker/\"]]],\"sections\":[[1,\"p\",[]],[1,\"h2\",[[0,[],0,\"Firstly what is zero trust\"]]],[1,\"p\",[[0,[],0,\"So from what I can tell, zero trust is a process of trusting nothing, not even the network you're on! It's designed to help with the \\\"Digital transition to the cloud\\\", which especially with the big 'rona around, this is good!\"]]],[1,\"p\",[[0,[],0,\"We're seeing more business' adopting a zero trust model as employees are working from home, and Corporate IT teams are scrambling to enable workers to access internal servers and applications. \"]]],[1,\"p\",[[0,[0],1,\"According to Okta\"],[0,[],0,\", zero trust has increased as a priority for 78% of business!   \"]]],[1,\"h3\",[[0,[],0,\"The TLDR;\"]]],[1,\"p\",[[0,[],0,\"Zero trust is exposing internal services behind a login page that has secure communication to the backend, or using hardware keys on computers. Be this using SSO or federated systems like LDAP behind the proxy, we call this zero trust. \"]]],[1,\"p\",[]],[1,\"h2\",[[0,[],0,\"Options on the market\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"As with anything, there are several options that one can use. \"]]],[10,0],[1,\"p\",[[0,[],0,\"Each one had it's fare share of good as well as bad things about it.\"]]],[1,\"p\",[[0,[],0,\"I needed my solution to:\"]]],[10,1],[1,\"h2\",[[0,[],0,\"Option I went with\"]]],[1,\"p\",[[0,[],0,\"Based on my above requirements, I've decided to go with \"],[0,[1],1,\"Pritunl Zero\"]]],[1,\"p\",[[0,[],0,\"It allows SSH, Web, 2FA on both web interfaces and the admin UI, Users are managed via the webUI, and stored in MongoDB and it's pretty simple to use!\"]]],[1,\"h2\",[[0,[],0,\"Implementation\"]]],[1,\"p\",[[0,[],0,\"All my code for this can be located on the below github repo:\"]]],[10,2],[1,\"p\",[[0,[],0,\"For getting started on using terraform with OVH, follow \"],[0,[2],1,\"This guide \"]]],[1,\"p\",[[0,[],0,\"Once setup with terraform, set the \"],[0,[3],1,\"me\"],[0,[],0,\" variables in \"],[0,[3],1,\"terraform.tfvars\"],[0,[],0,\" if on a mac, or edit the file path under \"],[0,[3],1,\"ssh.tf\"],[0,[],0,\" to the full path of your SSH key. \"]]],[1,\"p\",[[0,[],0,\"Once that's up, you need to SSH in to the server, copy the \"],[0,[3],1,\"install.sh\"],[0,[],0,\" file and run it! \"]]],[1,\"p\",[[0,[],0,\"Finally get the password with \"],[0,[3],1,\"sudo pritunl-zero default-password\"]]],[1,\"p\",[[0,[],0,\"If you plan on running an SSH bastion, you need to install Docker, which can be found \"],[0,[4],1,\"here\"]]],[1,\"p\",[[0,[],0,\"Full setup can be found below:\"]]],[10,3],[1,\"h2\",[[0,[],0,\"Issues I've had\"]]],[1,\"p\",[[0,[],0,\"So there have been a few teething issues I've had!\"]]],[3,\"ol\",[[[0,[],0,\"Installing the bastion service doesn't detail installing docker, the logs reflect this: \"]]]],[10,4],[1,\"p\",[[0,[],0,\"As soon as Docker was installed:\"]]],[10,5],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"2. SSH not working after certificate has expired\"]]],[1,\"p\",[[0,[],0,\"So the specific error you'd get here is: \"]]],[10,6],[1,\"p\",[[0,[],0,\"The simple way to solve this is run \"],[0,[3],1,\"pritunl-ssh\"],[0,[],0,\" and allow the new key on the web interface\"]]],[10,7],[1,\"p\",[[0,[],0,\"3. Config files are confusing!\"],[1,[],0,0],[0,[],0,\"\\tSo this was more on me than anything, and I'm hoping that by me showing you how to write one, as well as where the username comes from etc\"]]],[1,\"p\",[[0,[],0,\"Example config file:\"]]],[10,8],[10,9],[1,\"p\",[]],[10,10],[1,\"p\",[[0,[],0,\"Closing notes:\"]]],[1,\"p\",[[0,[],0,\"So this was a really short one, but I really wanted to share this as there's sub-minimal documentation around this topic and specifically around pritunl-zero.\"]]],[1,\"p\",[[0,[],0,\"If you have any issues or confused with anything please feel free to reach out to me! \"]]],[10,11],[1,\"p\",[[0,[],0,\"So I've added the ability to subscribe to my site... I know no one will, but I thought I would try and let yall enjoy it? It will just be a link to the latest post! \"]]]],\"ghostVersion\":\"3.0\"}","lexical":null,"html":"<p></p><h2 id=\"firstly-what-is-zero-trust\">Firstly what is zero trust</h2><p>So from what I can tell, zero trust is a process of trusting nothing, not even the network you're on! It's designed to help with the \"Digital transition to the cloud\", which especially with the big 'rona around, this is good!</p><p>We're seeing more business' adopting a zero trust model as employees are working from home, and Corporate IT teams are scrambling to enable workers to access internal servers and applications. </p><p><a href=\"https://www.okta.com/sites/default/files/2021-07/WPR-2021-ZeroTrust-070821.pdf\">According to Okta</a>, zero trust has increased as a priority for 78% of business!   </p><h3 id=\"the-tldr-\">The TLDR;</h3><p>Zero trust is exposing internal services behind a login page that has secure communication to the backend, or using hardware keys on computers. Be this using SSO or federated systems like LDAP behind the proxy, we call this zero trust. </p><p></p><h2 id=\"options-on-the-market\">Options on the market</h2><p></p><p>As with anything, there are several options that one can use. </p><!--kg-card-begin: markdown--><ul>\n<li>Duo Beyond</li>\n<li>Perimeter81</li>\n<li>Forcepoint Dynamic Security Solutions</li>\n<li>Prove</li>\n<li>Google BeyondCorp</li>\n<li>Microsoft 365 Azure</li>\n<li>Okta Identity Cloud</li>\n<li>Palo Alto Networks</li>\n<li>Proofpoint Meta</li>\n<li>Unisys Stealth</li>\n<li>Cloudflare Teams access</li>\n<li>Pritunl Zero</li>\n<li>Teleport</li>\n</ul>\n<!--kg-card-end: markdown--><p>Each one had it's fare share of good as well as bad things about it.</p><p>I needed my solution to:</p><!--kg-card-begin: markdown--><ul>\n<li>Allow SSH Bastion</li>\n<li>Allow web connections</li>\n<li>2FA</li>\n<li>Central user management</li>\n<li>Simple to use</li>\n<li>Run on minimal hardware (s1-2 on OVH)</li>\n</ul>\n<!--kg-card-end: markdown--><h2 id=\"option-i-went-with\">Option I went with</h2><p>Based on my above requirements, I've decided to go with <a href=\"https://zero.pritunl.com\">Pritunl Zero</a></p><p>It allows SSH, Web, 2FA on both web interfaces and the admin UI, Users are managed via the webUI, and stored in MongoDB and it's pretty simple to use!</p><h2 id=\"implementation\">Implementation</h2><p>All my code for this can be located on the below github repo:</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://github.com/userbradley/zerotrust-public\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">GitHub - userbradley/zerotrust-public</div><div class=\"kg-bookmark-description\">Contribute to userbradley/zerotrust-public development by creating an account on GitHub.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://github.githubassets.com/favicons/favicon.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">userbradley</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://opengraph.githubassets.com/2308e21c024e7a78416d8c208b1ab6c624b50f62905581ef7298b47a62472324/userbradley/zerotrust-public\" alt=\"\"></div></a></figure><p>For getting started on using terraform with OVH, follow <a href=\"__GHOST_URL__/terraform-ovh-openstack/\">This guide </a></p><p>Once setup with terraform, set the <code>me</code> variables in <code>terraform.tfvars</code> if on a mac, or edit the file path under <code>ssh.tf</code> to the full path of your SSH key. </p><p>Once that's up, you need to SSH in to the server, copy the <code>install.sh</code> file and run it! </p><p>Finally get the password with <code>sudo pritunl-zero default-password</code></p><p>If you plan on running an SSH bastion, you need to install Docker, which can be found <a href=\"https://docs.docker.com/get-docker/\">here</a></p><p>Full setup can be found below:</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://docs.pritunl.com/docs/pritunl-zero\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Getting Started SSH</div><div class=\"kg-bookmark-description\">Install and configure Pritunl Zero SSH certificates with two-factor authentication</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://files.readme.io/gkjYhYaZQkyKjQTOFpIn_favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">Pritunl</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://files.readme.io/306844c-ssh0.png\" alt=\"\"></div></a></figure><h2 id=\"issues-i-ve-had\">Issues I've had</h2><p>So there have been a few teething issues I've had!</p><ol><li>Installing the bastion service doesn't detail installing docker, the logs reflect this: </li></ol><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2021/12/image.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"1705\" srcset=\"__GHOST_URL__/content/images/size/w600/2021/12/image.png 600w, __GHOST_URL__/content/images/size/w1000/2021/12/image.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/12/image.png 1600w, __GHOST_URL__/content/images/2021/12/image.png 2240w\" sizes=\"(min-width: 720px) 720px\"></figure><p>As soon as Docker was installed:</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2021/12/image-1.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"1476\" srcset=\"__GHOST_URL__/content/images/size/w600/2021/12/image-1.png 600w, __GHOST_URL__/content/images/size/w1000/2021/12/image-1.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/12/image-1.png 1600w, __GHOST_URL__/content/images/2021/12/image-1.png 2162w\" sizes=\"(min-width: 720px) 720px\"></figure><p></p><p>2. SSH not working after certificate has expired</p><p>So the specific error you'd get here is: </p><pre><code>check_host_cert: certificate signature algorithm ssh-rsa: signature algorithm not supported\nReceived disconnect from &lt;ip&gt; port 9800:2: Too many authentication failures\nDisconnected from &lt;ip&gt; port 9800\nkex_exchange_identification: Connection closed by remote host</code></pre><p>The simple way to solve this is run <code>pritunl-ssh</code> and allow the new key on the web interface</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2021/12/image-2.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"389\" height=\"350\"></figure><p>3. Config files are confusing!<br> So this was more on me than anything, and I'm hoping that by me showing you how to write one, as well as where the username comes from etc</p><p>Example config file:</p><pre><code class=\"language-bash\">Host 198.244.155.107\n    ProxyJump bastion@&lt;bastionaddress&gt;:9800\n\n host reverse1-lon\n hostname reverse1.lon.eu.breadnet.co.uk\n user root\n ProxyJump bastion@&lt;bastionaddress&gt;:9800</code></pre><!--kg-card-begin: markdown--><ul>\n<li>\n<p>Lines <code>1-2</code></p>\n<ul>\n<li>Connects to the host 198...</li>\n<li>Username is your logged in user</li>\n<li>Uses the ssh key defined when you setup the local pritunl-ssh</li>\n</ul>\n</li>\n<li>\n<p>Rest of the file</p>\n<ul>\n<li>Using a simple name</li>\n<li>Logs in as root</li>\n<li>Jumps via the bastion</li>\n<li>Same SSH key as the above</li>\n</ul>\n</li>\n</ul>\n<!--kg-card-end: markdown--><p></p><hr><p>Closing notes:</p><p>So this was a really short one, but I really wanted to share this as there's sub-minimal documentation around this topic and specifically around pritunl-zero.</p><p>If you have any issues or confused with anything please feel free to reach out to me! </p><hr><p>So I've added the ability to subscribe to my site... I know no one will, but I thought I would try and let yall enjoy it? It will just be a link to the latest post! </p>","comment_id":"61b39cc317e5727444dc76c0","plaintext":"Firstly what is zero trust\n\nSo from what I can tell, zero trust is a process of trusting nothing, not even the network you're on! It's designed to help with the \"Digital transition to the cloud\", which especially with the big 'rona around, this is good!\n\nWe're seeing more business' adopting a zero trust model as employees are working from home, and Corporate IT teams are scrambling to enable workers to access internal servers and applications.\n\nAccording to Okta, zero trust has increased as a priority for 78% of business!  \n\n\nThe TLDR;\n\nZero trust is exposing internal services behind a login page that has secure communication to the backend, or using hardware keys on computers. Be this using SSO or federated systems like LDAP behind the proxy, we call this zero trust.\n\n\n\n\nOptions on the market\n\n\n\nAs with anything, there are several options that one can use.\n\n * Duo Beyond\n * Perimeter81\n * Forcepoint Dynamic Security Solutions\n * Prove\n * Google BeyondCorp\n * Microsoft 365 Azure\n * Okta Identity Cloud\n * Palo Alto Networks\n * Proofpoint Meta\n * Unisys Stealth\n * Cloudflare Teams access\n * Pritunl Zero\n * Teleport\n\n\nEach one had it's fare share of good as well as bad things about it.\n\nI needed my solution to:\n\n * Allow SSH Bastion\n * Allow web connections\n * 2FA\n * Central user management\n * Simple to use\n * Run on minimal hardware (s1-2 on OVH)\n\n\n\nOption I went with\n\nBased on my above requirements, I've decided to go with Pritunl Zero\n\nIt allows SSH, Web, 2FA on both web interfaces and the admin UI, Users are managed via the webUI, and stored in MongoDB and it's pretty simple to use!\n\n\nImplementation\n\nAll my code for this can be located on the below github repo:\n\nGitHub - userbradley/zerotrust-publicContribute to userbradley/zerotrust-public development by creating an account on GitHub.GitHubuserbradley\n\nFor getting started on using terraform with OVH, follow This guide\n\nOnce setup with terraform, set the me variables in terraform.tfvars if on a mac, or edit the file path under ssh.tf to the full path of your SSH key.\n\nOnce that's up, you need to SSH in to the server, copy the install.sh file and run it!\n\nFinally get the password with sudo pritunl-zero default-password\n\nIf you plan on running an SSH bastion, you need to install Docker, which can be found here\n\nFull setup can be found below:\n\nGetting Started SSHInstall and configure Pritunl Zero SSH certificates with two-factor authenticationPritunl\n\n\nIssues I've had\n\nSo there have been a few teething issues I've had!\n\n 1. Installing the bastion service doesn't detail installing docker, the logs reflect this:\n\nAs soon as Docker was installed:\n\n\n\n2. SSH not working after certificate has expired\n\nSo the specific error you'd get here is:\n\ncheck_host_cert: certificate signature algorithm ssh-rsa: signature algorithm not supported\nReceived disconnect from <ip> port 9800:2: Too many authentication failures\nDisconnected from <ip> port 9800\nkex_exchange_identification: Connection closed by remote host\n\nThe simple way to solve this is run pritunl-ssh and allow the new key on the web interface\n\n3. Config files are confusing!\n So this was more on me than anything, and I'm hoping that by me showing you how to write one, as well as where the username comes from etc\n\nExample config file:\n\nHost 198.244.155.107\n    ProxyJump bastion@<bastionaddress>:9800\n\n host reverse1-lon\n hostname reverse1.lon.eu.breadnet.co.uk\n user root\n ProxyJump bastion@<bastionaddress>:9800\n\n * \n   \n   \n   Lines 1-2\n   \n   \n   * Connects to the host 198...\n   * Username is your logged in user\n   * Uses the ssh key defined when you setup the local pritunl-ssh\n   \n * \n   \n   \n   Rest of the file\n   \n   \n   * Using a simple name\n   * Logs in as root\n   * Jumps via the bastion\n   * Same SSH key as the above\n   \n\n\n\n\nClosing notes:\n\nSo this was a really short one, but I really wanted to share this as there's sub-minimal documentation around this topic and specifically around pritunl-zero.\n\nIf you have any issues or confused with anything please feel free to reach out to me!\n\nSo I've added the ability to subscribe to my site... I know no one will, but I thought I would try and let yall enjoy it? It will just be a link to the latest post!","feature_image":"__GHOST_URL__/content/images/2021/12/618b007756edd86ecb738143_zero-trust.svg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2021-12-10T18:30:27.000Z","updated_at":"2021-12-14T01:43:09.000Z","published_at":"2021-12-14T00:34:52.000Z","custom_excerpt":"What is zerotrust, and how does one trust.. zero?","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"6425f3bc192c0c150413d5a6","uuid":"c32caeb5-434c-4a23-a4ee-d153b0ab5499","title":"Showcasing your knowledge","slug":"showcasing-your-knowledge","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}]],\"cards\":[[\"hr\",{}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/12/image-3.png\",\"width\":496,\"height\":322}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/12/image-4.png\",\"width\":489,\"height\":488}],[\"code\",{\"code\":\"FROM nginx:alpine\\nCOPY site/ /usr/share/nginx/html\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/12/image-5.png\",\"width\":1628,\"height\":374}],[\"hr\",{}],[\"hr\",{}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/12/image-6.png\",\"width\":544,\"height\":668,\"cardWidth\":\"\"}],[\"bookmark\",{\"type\":\"bookmark\",\"url\":\"https://github.com/bysd-project/portfolio-assignment\",\"metadata\":{\"url\":\"https://github.com/bysd-project/portfolio-assignment\",\"title\":\"GitHub - bysd-project/portfolio-assignment\",\"description\":\"Contribute to bysd-project/portfolio-assignment development by creating an account on GitHub.\",\"author\":\"sd-project\",\"publisher\":\"GitHub\",\"thumbnail\":\"https://opengraph.githubassets.com/8f004369117ca694b11fe0ba282659c57304c8e7c266273442ee7a8d6acf4f9a/bysd-project/portfolio-assignment\",\"icon\":\"https://github.githubassets.com/favicons/favicon.svg\"}}],[\"hr\",{}]],\"markups\":[[\"em\"],[\"code\"],[\"a\",[\"href\",\"__GHOST_URL__/showcasing-your-knowledge/bradley.breadnet.co.uk\"]],[\"a\",[\"href\",\"__GHOST_URL__/dns-terraform-cloudflare/\"]],[\"a\",[\"href\",\"https://cloud.google.com/container-registry/\"]],[\"strong\"]],\"sections\":[[1,\"p\",[[0,[],0,\"Being in IT is... \"],[0,[0],1,\"fun\"]]],[1,\"p\",[[0,[],0,\"As with any job, you naturally move around and, if you're in IT, the second you open Linkedin to recruiters, 10,000 recruiters will blow your inbox up trying to get you. And boy, do they harass you! Wow.\"]]],[1,\"p\",[[0,[],0,\"See, I've had my fare share of interviews, and I know what I want (cough slack cough jira), and don't want - Windows. \"]]],[1,\"p\",[[0,[],0,\"So in order to speed up the process of recruiters talking to me, and also to have a place to showcase my work, I decided it was a cool idea to make a \"],[0,[1],1,\"readme.md\"],[0,[],0,\" file almost, detailing what I do, past work and past projects.\"]]],[1,\"p\",[[0,[],0,\"Enter: \"],[0,[2],1,\"bradley.breadnet.co.uk\"]]],[1,\"p\",[[0,[],0,\"Now this site looks pretty simple, and really, it is. But the magic is how it's come to exist.\"]]],[1,\"p\",[[0,[],0,\"(recruiters get out your bingo sheet)\"]]],[10,0],[1,\"h2\",[[0,[],0,\"Mkdocs\"]]],[1,\"p\",[[0,[],0,\"Mkdocs is the platform on which the site is built, this is a project based documentation platform that renders \"],[0,[1],1,\"md\"],[0,[],0,\" files in to a static site. \"]]],[10,1],[1,\"p\",[[0,[],0,\"I decided to go with this over something like Hugo was simple: I find Hugo overly complicated for what it is, and there is a lack of flexibility if you don't know a reasonable amount of WebDev, then it's just lost\"]]],[10,2],[1,\"p\",[[0,[],0,\"It's SUPER simple to develop with, you just write a config file and then boom bow, run \"],[0,[1],1,\"mkdocs serve\"],[0,[],0,\" and you can see the live site\"]]],[1,\"h2\",[[0,[],0,\"Git\"]]],[1,\"p\",[[0,[],0,\"Seeing as all the files are \"],[0,[1],1,\".md\"],[0,[],0,\", it only makes sense to store this all in Github. \"]]],[1,\"p\",[[0,[],0,\"Now here's the catch. What we've done is build a Codefresh Pipeline (\"],[0,[3],1,\"Same things used for terraform\"],[0,[],0,\") that on a commit, builds the site and then deploys it. \"]]],[1,\"h2\",[[0,[],0,\"Docker\"]]],[1,\"p\",[[0,[],0,\"Now we have the pipeline running, we mentioned that it 'Builds the site' - This is managed as a docker container. This has to be the most simple docker contaier in the world...\"]]],[10,3],[1,\"p\",[[0,[],0,\"That's it! \"]]],[1,\"p\",[[0,[],0,\"This docker container is then pushed to \"],[0,[4],1,\"gcr.io\"],[0,[],0,\" for a later stage!\"]]],[1,\"p\",[[0,[],0,\"We also use docker for building the site, as there is a mkdocs docker container. If you're new to the world of cicd, most things need to run as a container in order to be used on a cicd pipeline!\"]]],[1,\"h2\",[[0,[],0,\"Terraform\"]]],[1,\"p\",[[0,[],0,\"This is the 'later stage'\"],[1,[],0,0],[0,[],0,\"To be able to host this site as cheaply as possible, I've selected to run on \"],[0,[4],1,\"google cloud run\"],[0,[],0,\", which is a super pain free means to running simple containers in production, and scale automatically.\"]]],[1,\"p\",[[0,[],0,\"Terraform uses the git commit ID as the container ID, and variables for what to deploy. This is actually pulled from the Codefresh Pipeline\"]]],[1,\"h2\",[[0,[],0,\"CICD\"]]],[1,\"p\",[[0,[],0,\"Here's the coolest part I think!\"]]],[1,\"p\",[[0,[],0,\"The pipeline looks like below\"]]],[10,4],[1,\"p\",[[0,[],0,\"Mkdocs build, docker build and deploy are the coolest parts.\"]]],[1,\"p\",[[0,[],0,\"mkdocs build connects to the local volume codefresh presnets, builds the static files to a directory, then the docker build pulls these files over, build the container and automatically pushes to GCR! \"]]],[1,\"p\",[[0,[],0,\"Finally the deploy steps are as you'd expect. The first one deploys to terraform, then the second one copies the files to my webserver where you've probably visited at some point! \"]]],[10,5],[1,\"h2\",[[0,[],0,\"The coolest part?\"]]],[1,\"p\",[[0,[],0,\"Everything is managed as code.\"]]],[1,\"p\",[[0,[],0,\"This includes but not limited to:\"]]],[3,\"ul\",[[[0,[],0,\"GCP API Enablement\"]],[[0,[],0,\"GCP Service account creation\"]],[[0,[],0,\"DNS Config\"]],[[0,[],0,\"Site\"]],[[0,[],0,\"Docker Containers\"]],[[0,[],0,\"Pipeline setup\"]]]],[1,\"p\",[]],[10,6],[1,\"p\",[[0,[],0,\"Closing notes:\"]]],[1,\"p\",[[0,[],0,\"The best way that I've seen to present your knowledge to recruiters as well as companies etc, is a site! I've got more comments about my site and my portfolio than my CV, as you're boxed in to this one page paradox where you're expected to \"],[0,[5],1,\"only \"],[0,[],0,\"fill out a page, and miss things!\"]]],[1,\"p\",[[0,[],0,\"A good example of this, is the skills matrix on the home page, this shows anyone what I know, and how well!\"]]],[10,7],[1,\"p\",[[0,[],0,\"This is also a really good project to learn cloudrun, terraform and CICD - And the best part is you have an actual end product that you can use, and present to people! If you want to have a crack at it, you can find a \\\"Homework\\\" assignment at the link below\"]]],[10,8],[10,9],[1,\"p\",[[0,[],0,\"So I've added the ability to subscribe to my site... I know no one will, but I thought I would try and let yall enjoy it? It will just be a link to the latest post! \"]]]],\"ghostVersion\":\"3.0\"}","lexical":null,"html":"<p>Being in IT is... <em>fun</em></p><p>As with any job, you naturally move around and, if you're in IT, the second you open Linkedin to recruiters, 10,000 recruiters will blow your inbox up trying to get you. And boy, do they harass you! Wow.</p><p>See, I've had my fare share of interviews, and I know what I want (cough slack cough jira), and don't want - Windows. </p><p>So in order to speed up the process of recruiters talking to me, and also to have a place to showcase my work, I decided it was a cool idea to make a <code>readme.md</code> file almost, detailing what I do, past work and past projects.</p><p>Enter: <a href=\"__GHOST_URL__/showcasing-your-knowledge/bradley.breadnet.co.uk\">bradley.breadnet.co.uk</a></p><p>Now this site looks pretty simple, and really, it is. But the magic is how it's come to exist.</p><p>(recruiters get out your bingo sheet)</p><hr><h2 id=\"mkdocs\">Mkdocs</h2><p>Mkdocs is the platform on which the site is built, this is a project based documentation platform that renders <code>md</code> files in to a static site. </p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2021/12/image-3.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"496\" height=\"322\"></figure><p>I decided to go with this over something like Hugo was simple: I find Hugo overly complicated for what it is, and there is a lack of flexibility if you don't know a reasonable amount of WebDev, then it's just lost</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2021/12/image-4.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"489\" height=\"488\"></figure><p>It's SUPER simple to develop with, you just write a config file and then boom bow, run <code>mkdocs serve</code> and you can see the live site</p><h2 id=\"git\">Git</h2><p>Seeing as all the files are <code>.md</code>, it only makes sense to store this all in Github. </p><p>Now here's the catch. What we've done is build a Codefresh Pipeline (<a href=\"__GHOST_URL__/dns-terraform-cloudflare/\">Same things used for terraform</a>) that on a commit, builds the site and then deploys it. </p><h2 id=\"docker\">Docker</h2><p>Now we have the pipeline running, we mentioned that it 'Builds the site' - This is managed as a docker container. This has to be the most simple docker contaier in the world...</p><pre><code>FROM nginx:alpine\nCOPY site/ /usr/share/nginx/html</code></pre><p>That's it! </p><p>This docker container is then pushed to <a href=\"https://cloud.google.com/container-registry/\">gcr.io</a> for a later stage!</p><p>We also use docker for building the site, as there is a mkdocs docker container. If you're new to the world of cicd, most things need to run as a container in order to be used on a cicd pipeline!</p><h2 id=\"terraform\">Terraform</h2><p>This is the 'later stage'<br>To be able to host this site as cheaply as possible, I've selected to run on <a href=\"https://cloud.google.com/container-registry/\">google cloud run</a>, which is a super pain free means to running simple containers in production, and scale automatically.</p><p>Terraform uses the git commit ID as the container ID, and variables for what to deploy. This is actually pulled from the Codefresh Pipeline</p><h2 id=\"cicd\">CICD</h2><p>Here's the coolest part I think!</p><p>The pipeline looks like below</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2021/12/image-5.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1628\" height=\"374\" srcset=\"__GHOST_URL__/content/images/size/w600/2021/12/image-5.png 600w, __GHOST_URL__/content/images/size/w1000/2021/12/image-5.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/12/image-5.png 1600w, __GHOST_URL__/content/images/2021/12/image-5.png 1628w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Mkdocs build, docker build and deploy are the coolest parts.</p><p>mkdocs build connects to the local volume codefresh presnets, builds the static files to a directory, then the docker build pulls these files over, build the container and automatically pushes to GCR! </p><p>Finally the deploy steps are as you'd expect. The first one deploys to terraform, then the second one copies the files to my webserver where you've probably visited at some point! </p><hr><h2 id=\"the-coolest-part\">The coolest part?</h2><p>Everything is managed as code.</p><p>This includes but not limited to:</p><ul><li>GCP API Enablement</li><li>GCP Service account creation</li><li>DNS Config</li><li>Site</li><li>Docker Containers</li><li>Pipeline setup</li></ul><p></p><hr><p>Closing notes:</p><p>The best way that I've seen to present your knowledge to recruiters as well as companies etc, is a site! I've got more comments about my site and my portfolio than my CV, as you're boxed in to this one page paradox where you're expected to <strong>only </strong>fill out a page, and miss things!</p><p>A good example of this, is the skills matrix on the home page, this shows anyone what I know, and how well!</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2021/12/image-6.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"544\" height=\"668\"></figure><p>This is also a really good project to learn cloudrun, terraform and CICD - And the best part is you have an actual end product that you can use, and present to people! If you want to have a crack at it, you can find a \"Homework\" assignment at the link below</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://github.com/bysd-project/portfolio-assignment\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">GitHub - bysd-project/portfolio-assignment</div><div class=\"kg-bookmark-description\">Contribute to bysd-project/portfolio-assignment development by creating an account on GitHub.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://github.githubassets.com/favicons/favicon.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">sd-project</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://opengraph.githubassets.com/8f004369117ca694b11fe0ba282659c57304c8e7c266273442ee7a8d6acf4f9a/bysd-project/portfolio-assignment\" alt=\"\"></div></a></figure><hr><p>So I've added the ability to subscribe to my site... I know no one will, but I thought I would try and let yall enjoy it? It will just be a link to the latest post! </p>","comment_id":"61b7e6d817e5727444dc7791","plaintext":"Being in IT is... fun\n\nAs with any job, you naturally move around and, if you're in IT, the second you open Linkedin to recruiters, 10,000 recruiters will blow your inbox up trying to get you. And boy, do they harass you! Wow.\n\nSee, I've had my fare share of interviews, and I know what I want (cough slack cough jira), and don't want - Windows.\n\nSo in order to speed up the process of recruiters talking to me, and also to have a place to showcase my work, I decided it was a cool idea to make a readme.md file almost, detailing what I do, past work and past projects.\n\nEnter: bradley.breadnet.co.uk\n\nNow this site looks pretty simple, and really, it is. But the magic is how it's come to exist.\n\n(recruiters get out your bingo sheet)\n\n\nMkdocs\n\nMkdocs is the platform on which the site is built, this is a project based documentation platform that renders md files in to a static site.\n\nI decided to go with this over something like Hugo was simple: I find Hugo overly complicated for what it is, and there is a lack of flexibility if you don't know a reasonable amount of WebDev, then it's just lost\n\nIt's SUPER simple to develop with, you just write a config file and then boom bow, run mkdocs serve and you can see the live site\n\n\nGit\n\nSeeing as all the files are .md, it only makes sense to store this all in Github.\n\nNow here's the catch. What we've done is build a Codefresh Pipeline (Same things used for terraform) that on a commit, builds the site and then deploys it.\n\n\nDocker\n\nNow we have the pipeline running, we mentioned that it 'Builds the site' - This is managed as a docker container. This has to be the most simple docker contaier in the world...\n\nFROM nginx:alpine\nCOPY site/ /usr/share/nginx/html\n\nThat's it!\n\nThis docker container is then pushed to gcr.io for a later stage!\n\nWe also use docker for building the site, as there is a mkdocs docker container. If you're new to the world of cicd, most things need to run as a container in order to be used on a cicd pipeline!\n\n\nTerraform\n\nThis is the 'later stage'\nTo be able to host this site as cheaply as possible, I've selected to run on google cloud run, which is a super pain free means to running simple containers in production, and scale automatically.\n\nTerraform uses the git commit ID as the container ID, and variables for what to deploy. This is actually pulled from the Codefresh Pipeline\n\n\nCICD\n\nHere's the coolest part I think!\n\nThe pipeline looks like below\n\nMkdocs build, docker build and deploy are the coolest parts.\n\nmkdocs build connects to the local volume codefresh presnets, builds the static files to a directory, then the docker build pulls these files over, build the container and automatically pushes to GCR!\n\nFinally the deploy steps are as you'd expect. The first one deploys to terraform, then the second one copies the files to my webserver where you've probably visited at some point!\n\n\nThe coolest part?\n\nEverything is managed as code.\n\nThis includes but not limited to:\n\n * GCP API Enablement\n * GCP Service account creation\n * DNS Config\n * Site\n * Docker Containers\n * Pipeline setup\n\n\n\nClosing notes:\n\nThe best way that I've seen to present your knowledge to recruiters as well as companies etc, is a site! I've got more comments about my site and my portfolio than my CV, as you're boxed in to this one page paradox where you're expected to only fill out a page, and miss things!\n\nA good example of this, is the skills matrix on the home page, this shows anyone what I know, and how well!\n\nThis is also a really good project to learn cloudrun, terraform and CICD - And the best part is you have an actual end product that you can use, and present to people! If you want to have a crack at it, you can find a \"Homework\" assignment at the link below\n\nGitHub - bysd-project/portfolio-assignmentContribute to bysd-project/portfolio-assignment development by creating an account on GitHub.GitHubsd-project\n\nSo I've added the ability to subscribe to my site... I know no one will, but I thought I would try and let yall enjoy it? It will just be a link to the latest post!","feature_image":"https://images.unsplash.com/photo-1586281380349-632531db7ed4?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwxMTc3M3wwfDF8c2VhcmNofDR8fGpvYnxlbnwwfHx8fDE2Mzk0NDM1NzI&ixlib=rb-1.2.1&q=80&w=2000","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2021-12-14T00:35:36.000Z","updated_at":"2022-01-24T13:03:29.000Z","published_at":"2021-12-14T01:01:31.000Z","custom_excerpt":"Scratching your head isn't the best way to show recruiters what you know... Portfolios are","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"6425f3bc192c0c150413d5a7","uuid":"26843d1b-607a-4a7b-97d8-8bbc61824aac","title":"Cloud-init, OVH and terraform","slug":"cloud-init-ovh-and-terraform","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"bookmark\",{\"url\":\"__GHOST_URL__/cloud-init-that-works/\",\"metadata\":{\"url\":\"__GHOST_URL__/cloud-init-that-works/\",\"title\":\"Cloud-init that works\",\"description\":\"Want to speed up the deployment of Linux servers on your Xen based server? Well I finally figured it out!\",\"author\":\"Bradley Stannard\",\"publisher\":\"breadNET\",\"thumbnail\":\"https://images.unsplash.com/photo-1451187580459-43490279c0fa?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=2000&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ\",\"icon\":\"https://breadnet.co.uk/favicon.png\"}}],[\"bookmark\",{\"type\":\"bookmark\",\"url\":\"https://learn.hashicorp.com/tutorials/terraform/cloud-init\",\"metadata\":{\"url\":\"https://learn.hashicorp.com/tutorials/terraform/cloud-init\",\"title\":\"Provision Infrastructure with Cloud-Init | Terraform - HashiCorp Learn\",\"description\":\"Deploy preconfigured infrastructure with Terraform using the Cloud-Init tool.\",\"author\":null,\"publisher\":\"HashiCorp Learn\",\"thumbnail\":\"https://www.datocms-assets.com/2885/1622161215-learn-card-2x.jpg\",\"icon\":\"https://learn.hashicorp.com/img/favicons/favicon-192x192.png\"}}],[\"code\",{\"code\":\"data \\\"template_file\\\" \\\"user_data\\\" {\\n  template = file(\\\"../scripts/add-ssh-web-app.yaml\\\")\\n}\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/03/image-1.png\",\"width\":1524,\"height\":274}],[\"code\",{\"code\":\"data \\\"cloudinit_config\\\" \\\"user_data\\\" {\\n  gzip = false\\n  base64_encode = false\\n  part {\\n    content_type = \\\"text/x-shellscript\\\"\\n    content = \\\"baz\\\"\\n    filename = file(\\\"./cloudinit.cfg\\\")\\n  }\\n}\"}],[\"code\",{\"code\":\"  user_data = file(\\\"./cloudinit.cfg\\\")\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/03/image-2.png\",\"width\":2692,\"height\":180}],[\"code\",{\"code\":\"#cloud-config\\n\\nruncmd:\\n - 'echo ============ Hello World ================'\"}],[\"hr\",{}],[\"code\",{\"code\":\"#cloud-config\\nruncmd:\\n  - 'echo ============ Hello World ================'\\nnetwork:\\n  ethernets:\\n    ens3:\\n      dhcp4: true\\n    ens4:\\n      dhcp4: true\\n  version: 2\\n\\n\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/03/image-3.png\",\"width\":2482,\"height\":222}],[\"code\",{\"code\":\"cloud-init -d init\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/03/image-4.png\",\"width\":1202,\"height\":158}],[\"code\",{\"code\":\"#cloud-config\\nwrite_files:\\n - path: /etc/cloud/cloud.cfg.d/99-custom-networking.cfg\\n        permissions: '0644'\\n        content: |\\n        network: {config: disabled}\\n\\n - path: /etc/netplan/my-new-config.yaml\\n   permissions: '0644'\\n   content: |\\n          network:\\n              version: 2\\n              ethernets:\\n                  ens3:\\n                      dhcp4: true\\n                  ens4:\\n                      dhcp4: true\\nruncmd:\\n        - rm /etc/netplan/50-cloud-init.yaml\\n        - netplan generate\\n        - netplan apply\\n        - echo \\\"=== hello === \\\"\\n\"}],[\"code\",{\"code\":\"#cloud-config\\nwrite_files:\\n  - encoding: b64\\n    content: bmV0d29yazoge2NvbmZpZzogZGlzYWJsZWR9Cg==\\n    owner: root:root\\n    path: /etc/cloud/cloud.cfg.d/99-custom-networking.cfg\\n    permissions: '0644'\\n\\n  - encoding: b64\\n    content: bmV0d29yazoKICBldGhlcm5ldHM6CiAgICBlbnMzOgogICAgICBkaGNwNDogdHJ1ZQogICAgZW5zNDoKICAgICAgZGhjcDQ6IHRydWUKICB2ZXJzaW9uOiAyCg==\\n    owner: root:root\\n    path: /etc/netplan/my-new-config.yaml\\n    permissions: '0644'\\n\\n\\nruncmd:\\n        - rm /etc/netplan/50-cloud-init.yaml\\n        - netplan generate\\n        - netplan apply\\n        - echo \\\"=== hello === \\\"\\nfinal_message: \\\"The system is finally up, after $UPTIME seconds\\\"\",\"language\":\"yaml\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/03/image-5.png\",\"width\":919,\"height\":441}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/03/image-6.png\",\"width\":2234,\"height\":118}],[\"code\",{\"code\":\"#cloud-config\\nwrite_files:\\n  - encoding: b64\\n    content: bmV0d29yazoge2NvbmZpZzogZGlzYWJsZWR9Cg==\\n    owner: root:root\\n    path: /etc/cloud/cloud.cfg.d/99-custom-networking.cfg\\n    permissions: '0644'\\n\\n  - encoding: b64\\n    content: bmV0d29yazoKICBldGhlcm5ldHM6CiAgICBlbnMzOgogICAgICBkaGNwNDogdHJ1ZQogICAgZW5zNDoKICAgICAgZGhjcDQ6IHRydWUKICB2ZXJzaW9uOiAyCg==\\n    owner: root:root\\n    path: /etc/netplan/my-new-config.yaml\\n    permissions: '0644'\\n\\n\\nruncmd:\\n        - rm /etc/netplan/50-cloud-init.yaml\\n        - netplan generate\\n        - netplan apply\\n        - echo \\\"=== hello === \\\"\\nfinal_message: \\\"The system is finally up, after $UPTIME seconds\\\"\"}],[\"code\",{\"code\":\"resource \\\"openstack_compute_instance_v2\\\" \\\"zero-access\\\" {\\n  name = \\\"zero-access\\\"\\n  flavor_name = \\\"s1-2\\\"\\n  key_pair = openstack_compute_keypair_v2.key.name\\n  image_name = \\\"Ubuntu 18.04\\\"\\n  security_groups = [ \\\"default\\\" ]\\n  user_data = file(\\\"./netplan.yml\\\")\\n  network {\\n    name = \\\"Ext-Net\\\"\\n  }\\n  network {\\n    name = openstack_networking_network_v2.vpc.name\\n\\n  }\"}],[\"hr\",{}]],\"markups\":[[\"code\"],[\"s\"]],\"sections\":[[1,\"p\",[[0,[],0,\"Yup, it's that time of the year again when I talk about some issues I've had and couldn't find a solution on google so I wrote about it. \"]]],[1,\"p\",[]],[1,\"h2\",[[0,[],0,\"What is the problem? \"]]],[1,\"p\",[[0,[],0,\"If you're never used OVH and terraform, you'll know that standing up the instance using terraform is quite easy. But adding a second network adapter and getting that to get DHCP... No. \"]]],[1,\"p\",[[0,[],0,\"I've previously written about using cloud-init, see below, on local infrastructure. \"]]],[10,0],[1,\"p\",[[0,[],0,\"But the difference here, is that the documentation is even worse!\"]]],[1,\"p\",[[0,[],0,\"I'm talking, like 3 different places saying something different.\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"If you're not here to read the entire blog, scroll to the bottom where I have the solution to getting cloud init to work on OVH\"]]],[1,\"h2\",[[0,[],0,\"Let's start\"]]],[1,\"p\",[[0,[],0,\"So seeing as we're using terraform, our first call of action is to look at their documentation for cloud init, which uses a field called \"],[0,[0],1,\"user_data\"]]],[10,1],[1,\"p\",[[0,[],0,\"The part that annoys me here, is they make you use a provider called \"],[0,[0],1,\"template_file\"],[0,[],0,\" which is actually deprecated...\"]]],[10,2],[1,\"p\",[[0,[],0,\"so if you get an error like below, you're welcome\"]]],[1,\"p\",[[0,[0],1,\"Provider registry.terraform.io/hashicorp/template v2.2.0 does not have a package available for your current platform, darwin_arm64.\"]]],[10,3],[1,\"p\",[[0,[],0,\"I tried to run terraform in a docker container forcing amd64 (as I am on an M1 mac) - No luck :(\"]]],[1,\"p\",[[0,[],0,\"The resolution for this one was to use a new provider, called \"],[0,[0],1,\"cloudinit_config\"],[0,[],0,\" which looks a little something like this\"]]],[10,4],[1,\"p\",[[0,[],0,\"But for the life of me, could not get this working. I was passing it to the instance rendered. \"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"I'd had enough and thought, well \"],[0,[1],1,\"fuck this noise\"],[0,[],0,\" we can probably pass the file straight in... Surely?\"]]],[10,5],[1,\"p\",[[0,[],0,\"This should work?\"]]],[1,\"p\",[[0,[],0,\"So I've tried it with the classic \"],[0,[0],1,\"hello world\"],[0,[],0,\" and it seemed to work.\"]]],[10,6],[1,\"p\",[[0,[],0,\"Below is what \"],[0,[0],1,\"./cloudinit.cfg\"],[0,[],0,\" looks like, if you're wondering\"]]],[10,7],[10,8],[1,\"h2\",[[0,[],0,\"Now on to the actual issue at hand - Network Interfaces\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"Because I am lazy I wont be doing a nifty \"],[0,[0],1,\"for\"],[0,[],0,\" loop, just going with a basic file that the instance calls. \"]]],[1,\"p\",[[0,[],0,\"Below is that file:\"]]],[10,9],[1,\"p\",[[0,[],0,\"I've saved this file as \"],[0,[0],1,\"netplan.cfg\"],[0,[],0,\" and kept the hello world, as this outputs to logs so hopefully, we can see if it actually applies or no!\"]]],[10,10],[1,\"p\",[[0,[],0,\"It ran hello world, but the network interface \"],[0,[0],1,\"ens4\"],[0,[],0,\" isnt enabled and getting DHCP \"]]],[1,\"p\",[[0,[],0,\":(\"]]],[1,\"p\",[[0,[],0,\"We can re-load our cloud init, whist logged in using\"]]],[10,11],[1,\"p\",[[0,[],0,\"Here's the error:\"]]],[10,12],[1,\"p\",[[0,[],0,\"Looks like we need to fix the file\"]]],[10,13],[1,\"p\",[[0,[],0,\"Becomes\"]]],[10,14],[1,\"p\",[[0,[],0,\"And would you look at that, it worked!\"]]],[10,15],[10,16],[1,\"p\",[]],[1,\"h2\",[[0,[],0,\"The solution\"]]],[3,\"ol\",[[[0,[],0,\"Base 64 encode both the new netplan file, and the disable auto-gen config file\"]],[[0,[],0,\"Use the above file\"]],[[0,[],0,\"Pull the file in under \"],[0,[0],1,\"user_data\"]]]],[1,\"p\",[[0,[0],1,\"netplan.yml\"]]],[10,17],[1,\"p\",[[0,[0],1,\"instance.tf\"]]],[10,18],[1,\"p\",[]],[10,19],[1,\"p\",[[0,[],0,\"This post is more of a brain dump than anything, hoping to help those with the issue I had:\"]]],[1,\"h2\",[[0,[],0,\"multiple interfaces on ovh not getting dhcp\"]]],[1,\"h2\",[[0,[],0,\"how to use cloud-init on ovh with terraform\"]]],[1,\"h2\",[[0,[],0,\"how to use cloud-init with openstace\"]]],[1,\"p\",[]],[1,\"p\",[]]],\"ghostVersion\":\"3.0\"}","lexical":null,"html":"<p>Yup, it's that time of the year again when I talk about some issues I've had and couldn't find a solution on google so I wrote about it. </p><p></p><h2 id=\"what-is-the-problem\">What is the problem? </h2><p>If you're never used OVH and terraform, you'll know that standing up the instance using terraform is quite easy. But adding a second network adapter and getting that to get DHCP... No. </p><p>I've previously written about using cloud-init, see below, on local infrastructure. </p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"__GHOST_URL__/cloud-init-that-works/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Cloud-init that works</div><div class=\"kg-bookmark-description\">Want to speed up the deployment of Linux servers on your Xen based server? Well I finally figured it out!</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://breadnet.co.uk/favicon.png\" alt=\"\"><span class=\"kg-bookmark-author\">breadNET</span><span class=\"kg-bookmark-publisher\">Bradley Stannard</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://images.unsplash.com/photo-1451187580459-43490279c0fa?ixlib&#x3D;rb-1.2.1&amp;q&#x3D;80&amp;fm&#x3D;jpg&amp;crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;w&#x3D;2000&amp;fit&#x3D;max&amp;ixid&#x3D;eyJhcHBfaWQiOjExNzczfQ\" alt=\"\"></div></a></figure><p>But the difference here, is that the documentation is even worse!</p><p>I'm talking, like 3 different places saying something different.</p><p></p><p>If you're not here to read the entire blog, scroll to the bottom where I have the solution to getting cloud init to work on OVH</p><h2 id=\"let-s-start\">Let's start</h2><p>So seeing as we're using terraform, our first call of action is to look at their documentation for cloud init, which uses a field called <code>user_data</code></p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://learn.hashicorp.com/tutorials/terraform/cloud-init\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Provision Infrastructure with Cloud-Init | Terraform - HashiCorp Learn</div><div class=\"kg-bookmark-description\">Deploy preconfigured infrastructure with Terraform using the Cloud-Init tool.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://learn.hashicorp.com/img/favicons/favicon-192x192.png\" alt=\"\"><span class=\"kg-bookmark-author\">HashiCorp Learn</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://www.datocms-assets.com/2885/1622161215-learn-card-2x.jpg\" alt=\"\"></div></a></figure><p>The part that annoys me here, is they make you use a provider called <code>template_file</code> which is actually deprecated...</p><pre><code>data \"template_file\" \"user_data\" {\n  template = file(\"../scripts/add-ssh-web-app.yaml\")\n}</code></pre><p>so if you get an error like below, you're welcome</p><p><code>Provider registry.terraform.io/hashicorp/template v2.2.0 does not have a package available for your current platform, darwin_arm64.</code></p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/03/image-1.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1524\" height=\"274\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/03/image-1.png 600w, __GHOST_URL__/content/images/size/w1000/2022/03/image-1.png 1000w, __GHOST_URL__/content/images/2022/03/image-1.png 1524w\" sizes=\"(min-width: 720px) 720px\"></figure><p>I tried to run terraform in a docker container forcing amd64 (as I am on an M1 mac) - No luck :(</p><p>The resolution for this one was to use a new provider, called <code>cloudinit_config</code> which looks a little something like this</p><pre><code>data \"cloudinit_config\" \"user_data\" {\n  gzip = false\n  base64_encode = false\n  part {\n    content_type = \"text/x-shellscript\"\n    content = \"baz\"\n    filename = file(\"./cloudinit.cfg\")\n  }\n}</code></pre><p>But for the life of me, could not get this working. I was passing it to the instance rendered. </p><p></p><p>I'd had enough and thought, well <s>fuck this noise</s> we can probably pass the file straight in... Surely?</p><pre><code>  user_data = file(\"./cloudinit.cfg\")</code></pre><p>This should work?</p><p>So I've tried it with the classic <code>hello world</code> and it seemed to work.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/03/image-2.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"134\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/03/image-2.png 600w, __GHOST_URL__/content/images/size/w1000/2022/03/image-2.png 1000w, __GHOST_URL__/content/images/size/w1600/2022/03/image-2.png 1600w, __GHOST_URL__/content/images/size/w2400/2022/03/image-2.png 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Below is what <code>./cloudinit.cfg</code> looks like, if you're wondering</p><pre><code>#cloud-config\n\nruncmd:\n - 'echo ============ Hello World ================'</code></pre><hr><h2 id=\"now-on-to-the-actual-issue-at-hand-network-interfaces\">Now on to the actual issue at hand - Network Interfaces</h2><p></p><p>Because I am lazy I wont be doing a nifty <code>for</code> loop, just going with a basic file that the instance calls. </p><p>Below is that file:</p><pre><code>#cloud-config\nruncmd:\n  - 'echo ============ Hello World ================'\nnetwork:\n  ethernets:\n    ens3:\n      dhcp4: true\n    ens4:\n      dhcp4: true\n  version: 2\n\n</code></pre><p>I've saved this file as <code>netplan.cfg</code> and kept the hello world, as this outputs to logs so hopefully, we can see if it actually applies or no!</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/03/image-3.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"179\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/03/image-3.png 600w, __GHOST_URL__/content/images/size/w1000/2022/03/image-3.png 1000w, __GHOST_URL__/content/images/size/w1600/2022/03/image-3.png 1600w, __GHOST_URL__/content/images/size/w2400/2022/03/image-3.png 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><p>It ran hello world, but the network interface <code>ens4</code> isnt enabled and getting DHCP </p><p>:(</p><p>We can re-load our cloud init, whist logged in using</p><pre><code>cloud-init -d init</code></pre><p>Here's the error:</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/03/image-4.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1202\" height=\"158\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/03/image-4.png 600w, __GHOST_URL__/content/images/size/w1000/2022/03/image-4.png 1000w, __GHOST_URL__/content/images/2022/03/image-4.png 1202w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Looks like we need to fix the file</p><pre><code>#cloud-config\nwrite_files:\n - path: /etc/cloud/cloud.cfg.d/99-custom-networking.cfg\n        permissions: '0644'\n        content: |\n        network: {config: disabled}\n\n - path: /etc/netplan/my-new-config.yaml\n   permissions: '0644'\n   content: |\n          network:\n              version: 2\n              ethernets:\n                  ens3:\n                      dhcp4: true\n                  ens4:\n                      dhcp4: true\nruncmd:\n        - rm /etc/netplan/50-cloud-init.yaml\n        - netplan generate\n        - netplan apply\n        - echo \"=== hello === \"\n</code></pre><p>Becomes</p><pre><code class=\"language-yaml\">#cloud-config\nwrite_files:\n  - encoding: b64\n    content: bmV0d29yazoge2NvbmZpZzogZGlzYWJsZWR9Cg==\n    owner: root:root\n    path: /etc/cloud/cloud.cfg.d/99-custom-networking.cfg\n    permissions: '0644'\n\n  - encoding: b64\n    content: bmV0d29yazoKICBldGhlcm5ldHM6CiAgICBlbnMzOgogICAgICBkaGNwNDogdHJ1ZQogICAgZW5zNDoKICAgICAgZGhjcDQ6IHRydWUKICB2ZXJzaW9uOiAyCg==\n    owner: root:root\n    path: /etc/netplan/my-new-config.yaml\n    permissions: '0644'\n\n\nruncmd:\n        - rm /etc/netplan/50-cloud-init.yaml\n        - netplan generate\n        - netplan apply\n        - echo \"=== hello === \"\nfinal_message: \"The system is finally up, after $UPTIME seconds\"</code></pre><p>And would you look at that, it worked!</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/03/image-5.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"919\" height=\"441\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/03/image-5.png 600w, __GHOST_URL__/content/images/2022/03/image-5.png 919w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/03/image-6.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"106\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/03/image-6.png 600w, __GHOST_URL__/content/images/size/w1000/2022/03/image-6.png 1000w, __GHOST_URL__/content/images/size/w1600/2022/03/image-6.png 1600w, __GHOST_URL__/content/images/2022/03/image-6.png 2234w\" sizes=\"(min-width: 720px) 720px\"></figure><p></p><h2 id=\"the-solution\">The solution</h2><ol><li>Base 64 encode both the new netplan file, and the disable auto-gen config file</li><li>Use the above file</li><li>Pull the file in under <code>user_data</code></li></ol><p><code>netplan.yml</code></p><pre><code>#cloud-config\nwrite_files:\n  - encoding: b64\n    content: bmV0d29yazoge2NvbmZpZzogZGlzYWJsZWR9Cg==\n    owner: root:root\n    path: /etc/cloud/cloud.cfg.d/99-custom-networking.cfg\n    permissions: '0644'\n\n  - encoding: b64\n    content: bmV0d29yazoKICBldGhlcm5ldHM6CiAgICBlbnMzOgogICAgICBkaGNwNDogdHJ1ZQogICAgZW5zNDoKICAgICAgZGhjcDQ6IHRydWUKICB2ZXJzaW9uOiAyCg==\n    owner: root:root\n    path: /etc/netplan/my-new-config.yaml\n    permissions: '0644'\n\n\nruncmd:\n        - rm /etc/netplan/50-cloud-init.yaml\n        - netplan generate\n        - netplan apply\n        - echo \"=== hello === \"\nfinal_message: \"The system is finally up, after $UPTIME seconds\"</code></pre><p><code>instance.tf</code></p><pre><code>resource \"openstack_compute_instance_v2\" \"zero-access\" {\n  name = \"zero-access\"\n  flavor_name = \"s1-2\"\n  key_pair = openstack_compute_keypair_v2.key.name\n  image_name = \"Ubuntu 18.04\"\n  security_groups = [ \"default\" ]\n  user_data = file(\"./netplan.yml\")\n  network {\n    name = \"Ext-Net\"\n  }\n  network {\n    name = openstack_networking_network_v2.vpc.name\n\n  }</code></pre><p></p><hr><p>This post is more of a brain dump than anything, hoping to help those with the issue I had:</p><h2 id=\"multiple-interfaces-on-ovh-not-getting-dhcp\">multiple interfaces on ovh not getting dhcp</h2><h2 id=\"how-to-use-cloud-init-on-ovh-with-terraform\">how to use cloud-init on ovh with terraform</h2><h2 id=\"how-to-use-cloud-init-with-openstace\">how to use cloud-init with openstace</h2><p></p>","comment_id":"621fe88317e5727444dc786b","plaintext":"Yup, it's that time of the year again when I talk about some issues I've had and couldn't find a solution on google so I wrote about it.\n\n\n\n\nWhat is the problem?\n\nIf you're never used OVH and terraform, you'll know that standing up the instance using terraform is quite easy. But adding a second network adapter and getting that to get DHCP... No.\n\nI've previously written about using cloud-init, see below, on local infrastructure.\n\nCloud-init that worksWant to speed up the deployment of Linux servers on your Xen based server? Well I finally figured it out!breadNETBradley Stannard\n\nBut the difference here, is that the documentation is even worse!\n\nI'm talking, like 3 different places saying something different.\n\n\n\nIf you're not here to read the entire blog, scroll to the bottom where I have the solution to getting cloud init to work on OVH\n\n\nLet's start\n\nSo seeing as we're using terraform, our first call of action is to look at their documentation for cloud init, which uses a field called user_data\n\nProvision Infrastructure with Cloud-Init | Terraform - HashiCorp LearnDeploy preconfigured infrastructure with Terraform using the Cloud-Init tool.HashiCorp Learn\n\nThe part that annoys me here, is they make you use a provider called template_file which is actually deprecated...\n\ndata \"template_file\" \"user_data\" {\n  template = file(\"../scripts/add-ssh-web-app.yaml\")\n}\n\nso if you get an error like below, you're welcome\n\nProvider registry.terraform.io/hashicorp/template v2.2.0 does not have a package available for your current platform, darwin_arm64.\n\nI tried to run terraform in a docker container forcing amd64 (as I am on an M1 mac) - No luck :(\n\nThe resolution for this one was to use a new provider, called cloudinit_config which looks a little something like this\n\ndata \"cloudinit_config\" \"user_data\" {\n  gzip = false\n  base64_encode = false\n  part {\n    content_type = \"text/x-shellscript\"\n    content = \"baz\"\n    filename = file(\"./cloudinit.cfg\")\n  }\n}\n\nBut for the life of me, could not get this working. I was passing it to the instance rendered.\n\n\n\nI'd had enough and thought, well fuck this noise we can probably pass the file straight in... Surely?\n\n  user_data = file(\"./cloudinit.cfg\")\n\nThis should work?\n\nSo I've tried it with the classic hello world and it seemed to work.\n\nBelow is what ./cloudinit.cfg looks like, if you're wondering\n\n#cloud-config\n\nruncmd:\n - 'echo ============ Hello World ================'\n\n\nNow on to the actual issue at hand - Network Interfaces\n\n\n\nBecause I am lazy I wont be doing a nifty for loop, just going with a basic file that the instance calls.\n\nBelow is that file:\n\n#cloud-config\nruncmd:\n  - 'echo ============ Hello World ================'\nnetwork:\n  ethernets:\n    ens3:\n      dhcp4: true\n    ens4:\n      dhcp4: true\n  version: 2\n\n\n\nI've saved this file as netplan.cfg and kept the hello world, as this outputs to logs so hopefully, we can see if it actually applies or no!\n\nIt ran hello world, but the network interface ens4 isnt enabled and getting DHCP\n\n:(\n\nWe can re-load our cloud init, whist logged in using\n\ncloud-init -d init\n\nHere's the error:\n\nLooks like we need to fix the file\n\n#cloud-config\nwrite_files:\n - path: /etc/cloud/cloud.cfg.d/99-custom-networking.cfg\n        permissions: '0644'\n        content: |\n        network: {config: disabled}\n\n - path: /etc/netplan/my-new-config.yaml\n   permissions: '0644'\n   content: |\n          network:\n              version: 2\n              ethernets:\n                  ens3:\n                      dhcp4: true\n                  ens4:\n                      dhcp4: true\nruncmd:\n        - rm /etc/netplan/50-cloud-init.yaml\n        - netplan generate\n        - netplan apply\n        - echo \"=== hello === \"\n\n\nBecomes\n\n#cloud-config\nwrite_files:\n  - encoding: b64\n    content: bmV0d29yazoge2NvbmZpZzogZGlzYWJsZWR9Cg==\n    owner: root:root\n    path: /etc/cloud/cloud.cfg.d/99-custom-networking.cfg\n    permissions: '0644'\n\n  - encoding: b64\n    content: bmV0d29yazoKICBldGhlcm5ldHM6CiAgICBlbnMzOgogICAgICBkaGNwNDogdHJ1ZQogICAgZW5zNDoKICAgICAgZGhjcDQ6IHRydWUKICB2ZXJzaW9uOiAyCg==\n    owner: root:root\n    path: /etc/netplan/my-new-config.yaml\n    permissions: '0644'\n\n\nruncmd:\n        - rm /etc/netplan/50-cloud-init.yaml\n        - netplan generate\n        - netplan apply\n        - echo \"=== hello === \"\nfinal_message: \"The system is finally up, after $UPTIME seconds\"\n\nAnd would you look at that, it worked!\n\n\n\n\nThe solution\n\n 1. Base 64 encode both the new netplan file, and the disable auto-gen config file\n 2. Use the above file\n 3. Pull the file in under user_data\n\nnetplan.yml\n\n#cloud-config\nwrite_files:\n  - encoding: b64\n    content: bmV0d29yazoge2NvbmZpZzogZGlzYWJsZWR9Cg==\n    owner: root:root\n    path: /etc/cloud/cloud.cfg.d/99-custom-networking.cfg\n    permissions: '0644'\n\n  - encoding: b64\n    content: bmV0d29yazoKICBldGhlcm5ldHM6CiAgICBlbnMzOgogICAgICBkaGNwNDogdHJ1ZQogICAgZW5zNDoKICAgICAgZGhjcDQ6IHRydWUKICB2ZXJzaW9uOiAyCg==\n    owner: root:root\n    path: /etc/netplan/my-new-config.yaml\n    permissions: '0644'\n\n\nruncmd:\n        - rm /etc/netplan/50-cloud-init.yaml\n        - netplan generate\n        - netplan apply\n        - echo \"=== hello === \"\nfinal_message: \"The system is finally up, after $UPTIME seconds\"\n\ninstance.tf\n\nresource \"openstack_compute_instance_v2\" \"zero-access\" {\n  name = \"zero-access\"\n  flavor_name = \"s1-2\"\n  key_pair = openstack_compute_keypair_v2.key.name\n  image_name = \"Ubuntu 18.04\"\n  security_groups = [ \"default\" ]\n  user_data = file(\"./netplan.yml\")\n  network {\n    name = \"Ext-Net\"\n  }\n  network {\n    name = openstack_networking_network_v2.vpc.name\n\n  }\n\n\n\nThis post is more of a brain dump than anything, hoping to help those with the issue I had:\n\n\nmultiple interfaces on ovh not getting dhcp\n\n\nhow to use cloud-init on ovh with terraform\n\n\nhow to use cloud-init with openstace\n\n","feature_image":"https://images.unsplash.com/photo-1620712943543-bcc4688e7485?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwxMTc3M3wwfDF8c2VhcmNofDZ8fGF1dG9tYXRpb258ZW58MHx8fHwxNjQ2MjYzNDE4&ixlib=rb-1.2.1&q=80&w=2000","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2022-03-02T21:58:27.000Z","updated_at":"2022-03-02T23:24:15.000Z","published_at":"2022-03-02T23:24:15.000Z","custom_excerpt":"I'll be honest, this one was a struggle but we did it!","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"6425f3bc192c0c150413d5a8","uuid":"33a032e4-dd4d-405c-9794-b3e764f52a42","title":"How I got to where I am now","slug":"how-i-got-to-where-i-am-now","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}]],\"cards\":[[\"hr\",{}],[\"bookmark\",{\"url\":\"https://www.old.breadnet.co.uk\",\"metadata\":{\"url\":\"https://www.old.breadnet.co.uk\",\"title\":\"breadNET\",\"description\":\"Welcome, you’ve made it over the ~waves~ of the internet This is my site. I know, not amazing, but we all started somewhere. I do have plans to move to wordpress and self host it, but that’s something I need to learn! A little about me:\\nI left school in 2018 and worked in a warehouse during the\",\"author\":null,\"publisher\":null,\"thumbnail\":\"https://lh3.googleusercontent.com/QoRn23_8QSccZlg2WdyHibri3I2ifm3mtG0AY3H2eKk1lWmiyDM33abtpiWb36OHcCBdkQ=w16383\",\"icon\":\"https://lh6.googleusercontent.com/jQdmbZGVu_OD7OjVQ0g4x09bPOXZ9cl94uxBzg2It5A73KR0OzFsjUpsOnUvHT0ahry7cX66o-0MRdQOGNB0pVonag-3oL0\"}}],[\"code\",{\"code\":\"<marquee behavior=\\\"scroll\\\" direction=\\\"left\\\">\\n    Here is some scrolling text... right to left!\\n</marquee>\\n\",\"language\":\"html\"}],[\"html\",{\"html\":\"<marquee behavior=\\\"scroll\\\" direction=\\\"left\\\">Here is some scrolling text... right to left!</marquee>\\n\"}],[\"bookmark\",{\"url\":\"__GHOST_URL__/cloud-init-ovh-and-terraform/\",\"metadata\":{\"url\":\"__GHOST_URL__/cloud-init-ovh-and-terraform/\",\"title\":\"Cloud-init, OVH and terraform\",\"description\":\"I’ll be honest, this one was a struggle but we did it!\",\"author\":\"Bradley Stannard\",\"publisher\":\"breadNET\",\"thumbnail\":\"https://images.unsplash.com/photo-1620712943543-bcc4688e7485?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwxMTc3M3wwfDF8c2VhcmNofDZ8fGF1dG9tYXRpb258ZW58MHx8fHwxNjQ2MjYzNDE4&ixlib=rb-1.2.1&q=80&w=2000\",\"icon\":\"https://breadnet.co.uk/favicon.png\"}}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/03/ikea.jpg\",\"width\":3024,\"height\":4032}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/03/desk.jpg\",\"width\":3024,\"height\":4032}],[\"hr\",{}]],\"markups\":[[\"em\"],[\"strong\"],[\"sup\"],[\"code\"]],\"sections\":[[1,\"p\",[[0,[],0,\"Getting a job in IT can be easy, but also not. In today's installment of blog post, we're going to do a deep dive in to my career, recommendations I would make to younger me and things I would avoid doing. \"]]],[1,\"p\",[[0,[],0,\"Let's start\"]]],[1,\"p\",[[0,[],0,\"This is structured as an interview with my self, but in reality it's me typing this at my desk on a Wednesday night. Go figure\"]]],[10,0],[1,\"h2\",[[0,[],0,\"Where I am now\"]]],[1,\"p\",[[0,[],0,\"So currently I am at a Pet care company, doing Devops. From what I can gather, I've been brought on as someone who's full time job is to know the cloud, and advise on things related to the cloud. \"]]],[1,\"p\",[[0,[],0,\"If you'd told me at the start of my IT career I'd be the sole Devops engineer for GCP at a national company, I'd have laughed. \"]]],[1,\"p\",[[0,[],0,\"But I'll be honest, imposter syndrome is a right kicker. I'm putting this down to the fact it's a new role, but there's days I sit there looking at terraform or some google product I've not seen going \\\"wtf, where do I even start?\\\" (if you scroll down to \"],[0,[0],1,\"Things that have saved me\"],[0,[],0,\", I talk about how to deal with this)\"]]],[1,\"p\",[[0,[],0,\"I mainly spend most of my days looking at our current infrastructure, seeing where we can make improvement's, as well as helping team members with their infrastructure queries. \"]]],[1,\"p\",[[0,[],0,\"Soon, my self, my manager and another colleague will split into sole GCP practice - So I'm working on a lot of documentation around architecture decisions, and processes. \"]]],[1,\"h2\",[[0,[],0,\"Where I started\"]]],[1,\"p\",[[0,[],0,\"I started on the help desk - Most people do to be honest with you. It's almost worrying when someone \"],[0,[1],1,\"hasn't \"],[0,[],0,\"started at the help desk. It really humbles you being treated like shit day in day out by people who are angry about the stupid computer not connecting to the VPN because vpn = virtual private which means you \"],[0,[1],1,\"don't\"],[0,[],0,\" have to connect to wifi as he network is \"],[0,[1],1,\"virtual \"],[0,[],0,\"and \"],[0,[1],1,\"private\"],[0,[],0,\". Real call btw\"]]],[1,\"p\",[[0,[],0,\"I mainly dealing with level 1 helpdesk tickets, so your password resets, creating users, building machines. \"]]],[1,\"p\",[[0,[],0,\"I got on to this automation stuff, so using powershell to install software like VNC server, enable RDP, install chrome and so on. \"]]],[1,\"p\",[[0,[],0,\"But if you want the \\\"\"],[0,[0],1,\"where did your tech interest start\\\" \"],[0,[],0,\"then this is going back to India. If you're not a frequent reader of my blog (firstly shame on you) - You won't know I lived in India for a few years. The school I went to was what I would call \\\"A digital school\\\" where everything was on computers. \"]]],[1,\"p\",[[0,[],0,\"Me being me, I would get bored easily so would spend time playing with stuff like VM's (docker had only \"],[0,[1],1,\"just\"],[0,[],0,\" been \"],[0,[2],1,\"(20 March 2013)\"],[0,[],0,\" released, and I was still getting my head around linux), web servers and linux. I think my first website was just the generic \\\"Hello world\\\" followed by probably a meme or a photo of a duck? \"]]],[1,\"p\",[[0,[],0,\"I want to say it went downhill from here? We moved back to the UK and after leaving school I got a job working in a warehouse, thus had income to purchase servers. \"]]],[1,\"p\",[[0,[],0,\"I built my lab and eventually created the OG breadnet site (below)\"]]],[10,1],[1,\"p\",[[0,[],0,\"I keep this \"],[0,[1],1,\"horrific \"],[0,[],0,\"site around as it got my my first job in IT, and without it I don't know where I would be today!\"]]],[1,\"h3\",[[0,[],0,\"But what point did you transition to the cloud?\"]]],[1,\"p\",[[0,[],0,\"Ha yeah, forgot about that!\"]]],[1,\"p\",[[0,[],0,\"So I was at a company based from Texas, and I joined as a Jr. Sys Admin, and they put me on a cloud project to see what would happen and I was decent from what I understand. \"]]],[1,\"p\",[[0,[],0,\"Eventually I went cloud full time and yeah - I never planned to be here! I was originally going for my CCNA to work in a NOC at a Datacenter. Strange isn't it! \"]]],[1,\"h2\",[[0,[],0,\"My career track I'm on\"]]],[1,\"p\",[[0,[],0,\"I'm hoping to be senior principal cloud architect? I'm not sure if I made that title up but that's what I'm aiming for!\"]]],[1,\"h3\",[[0,[],0,\"Why that title? Why not a manager?\"]]],[1,\"p\",[[0,[],0,\"Yeah good question actually. I don't want to manage people. I'm an engineer and we're awkward af. Could you imagine me having to speak to one of my direct reports about BO issues? \"]]],[1,\"p\",[[0,[],0,\"I want to stay in a role where I am solving issues day to day, and also helping the team to unblock their issues, or other stuff. Not sure yet!\"]]],[1,\"p\",[]],[1,\"h2\",[[0,[],0,\"Where I plan to end up\"]]],[1,\"p\",[[0,[],0,\"Like previously mentioned, I want to be senior principal \"],[0,[0],1,\"I forget the rest of the title. \"],[0,[],0,\"I don't want to lose out on solving issues and writing terraform. It's one of the few joys in my life\"]]],[1,\"h2\",[[0,[],0,\"How to actually get in to IT\"]]],[1,\"p\",[[0,[],0,\"I'm not sure if it's by chance of my Dad's role. But I can remember being interested in what he was doing. He was a programmer and now I honestly don't know what he does. \"]]],[1,\"p\",[[0,[],0,\"One of my earliest memories is on an XP laptop my dads friend gave me, with my Dad creating scrolling text in HTML. I think the code was something like the below:\"]]],[10,2],[10,3],[1,\"p\",[]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"From there I kind of progressed and wanted more technical knowledge. So it's fair to say that I've been interested in technology from a young age. I was initially thinking of going in to Mechanical engineering at college, but that was full - with the aims of keeping IT as a side hobby. \"]]],[1,\"h2\",[[0,[],0,\"Do you need a degree?\"]]],[1,\"p\",[[0,[],0,\"Depends. If you want one - I won't stop you!\"]]],[1,\"p\",[[0,[],0,\"I personally don't have anything higher than a B-TEC in IT systems. \"]]],[1,\"p\",[[0,[],0,\"If you're going in to something like ML and AI, I think a degree is beneficial, but if you're going in to cloud engineering or something along those lines - Nah g\"]]],[1,\"h2\",[[0,[],0,\"Certifications I suggest\"]]],[1,\"p\",[[0,[],0,\"Depends - Again.\"]]],[1,\"p\",[[0,[],0,\"If you're going in to cloud:\"]]],[3,\"ul\",[[[0,[],0,\"Linux Admin\"]],[[0,[],0,\"Google associate\"]],[[0,[],0,\"AWS associate\"]]]],[1,\"p\",[[0,[],0,\"If you're going in to the MS environment, idk man. I'm Google and AWS cloud.\"]]],[1,\"p\",[]],[1,\"h2\",[[0,[],0,\"Things that saved me\"]]],[1,\"p\",[]],[1,\"h3\",[[0,[],0,\"Small Chunks\"]]],[1,\"p\",[[0,[],0,\"Not every problem is as big as it seems.\"]]],[1,\"p\",[[0,[],0,\"Take my most recent publish \"]]],[10,4],[1,\"p\",[[0,[],0,\"At the surface, it seems like an impossible feat - No one on google or reddit got it working well. \"]]],[1,\"p\",[[0,[],0,\"When we look at what the actual issue is, and how we can resolve it, all it was; was editing a single file under \"],[0,[3],1,\"/etc/netplan/50*\"],[0,[],0,\" and running \"],[0,[3],1,\"netplan apply\"]]],[1,\"p\",[[0,[],0,\"The next steps were to then work out how to automate it. \"]]],[1,\"p\",[[0,[],0,\"We start with what we know, which is the above, and what we don't know - which is what platform to use.\"]]],[1,\"p\",[[0,[],0,\"We can use something like ansible, bash scripting, cloud-init, startup scripts or even manual ssh.\"]]],[1,\"p\",[[0,[],0,\"Next is to device on the tool, and the pro's and cons of each. \"]]],[1,\"p\",[[0,[],0,\"Done. You now have a small easy to solve problem.\"]]],[1,\"p\",[]],[1,\"h3\",[[0,[],0,\"Google is your friend\"]]],[1,\"p\",[[0,[],0,\"Just google the error. 90% of the time the first result will help.\"],[1,[],0,0],[0,[],0,\"Page 2 is for dire situations. \"]]],[1,\"p\",[]],[1,\"h3\",[[0,[],0,\"Don't make assumptions\"]]],[1,\"p\",[[0,[],0,\"This is something I learnt \"],[0,[1],1,\"very quickly.\"]]],[1,\"p\",[[0,[],0,\"If you assume that your colleague locked out a user, and it's on you to check and something goes wrong... Guess who's to blame! \"]]],[1,\"p\",[[0,[1],1,\"always double check!\"]]],[1,\"p\",[]],[1,\"h3\",[[0,[],0,\"7 P's\"]]],[1,\"p\",[[0,[0],1,\"Proper Planning and Preparation Prevents Piss Poor Performance\"]]],[1,\"p\",[[0,[],0,\"Plan everything you do. This doesn't need to be a 12 step plan on how to unlock the door. It can be as simple as:\"]]],[3,\"ol\",[[[0,[],0,\"Log in to server\"]],[[0,[],0,\"Check \"],[0,[3],1,\"/etc/passwd\"]],[[0,[],0,\"Record if user is there\"]],[[0,[],0,\"Shutdown server\"]]]],[1,\"p\",[[0,[],0,\"At least this way you know what you need to do, and now you can work out \\\"Hmm, how do I access that file, do I need root privileges?\\\" \"]]],[1,\"p\",[]],[1,\"h3\",[[0,[],0,\"Write documentation for the Junior's \"]]],[1,\"p\",[[0,[],0,\"I don't literally mean, write it for them personally.\"]]],[1,\"p\",[[0,[],0,\"We were all juniors at some point, and all suffered with imposter syndrome. \"]]],[1,\"p\",[[0,[],0,\"I can \"],[0,[1],1,\"very\"],[0,[],0,\" clearly remember sitting at the help desk reading some documentation feeling like an eejit because I didn't understand the different abbreviations etc etc. \"]]],[1,\"p\",[[0,[],0,\"It's important that anyone with a minimum technical understanding should be able to look at your \"],[0,[3],1,\"readme.md\"],[0,[],0,\" and be able to understand what's going on. \"]]],[1,\"p\",[[0,[],0,\"The same goes for your tickets. Make them super clear and include more detail than you think. The plan is that the junior can understand it and work out next steps\"]]],[1,\"h3\",[[0,[],0,\"Use a note pad for your daily to-do's \"]]],[1,\"p\",[[0,[],0,\"Yes - We're on the cutting edge of technology, but a paper notepad works well.\"]]],[1,\"p\",[[0,[],0,\"Personally, I use the little pieces of paper that you get from Ikea. I write my daily to-do on it, use it to draw diagrams etc.\"]]],[10,5],[1,\"p\",[]],[1,\"h3\",[[0,[],0,\"Keep your work area clear\"]]],[1,\"p\",[[0,[],0,\"Now I don't mean like clinically clean. Nothing on the desk at all.\"]]],[1,\"p\",[[0,[1],1,\"I mean\"],[0,[],0,\", what's on the desk, make it neat and tidy. Keep a little on the desk as possible.\"]]],[10,6],[1,\"p\",[[0,[],0,\"As you can see, there is stuff on the top left, but it's in a line; and what is on the desk is critical to my operations. \"]]],[3,\"ol\",[[[0,[],0,\"Water \"]],[[0,[],0,\"Candle\"]],[[0,[],0,\"Moose (Most important)\"]],[[0,[],0,\"Headphone stand\"]],[[0,[],0,\"Note paper\"]]]],[10,7],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"I hope this has given you a little hand in getting started with Cloud or your tech career. \"]]],[1,\"p\",[[0,[],0,\"Don't give up. Keep looking, if you need help you're welcome to contact me. \"]]],[1,\"p\",[[0,[],0,\"If you want to contact me, locate the \"],[0,[1,3],1,\"x-message-me\"],[0,[],1,\" \"],[0,[],0,\"header on the site's response - google how to find site response headers :) \"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"All the best,\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"Bradley\"]]]],\"ghostVersion\":\"3.0\"}","lexical":null,"html":"<p>Getting a job in IT can be easy, but also not. In today's installment of blog post, we're going to do a deep dive in to my career, recommendations I would make to younger me and things I would avoid doing. </p><p>Let's start</p><p>This is structured as an interview with my self, but in reality it's me typing this at my desk on a Wednesday night. Go figure</p><hr><h2 id=\"where-i-am-now\">Where I am now</h2><p>So currently I am at a Pet care company, doing Devops. From what I can gather, I've been brought on as someone who's full time job is to know the cloud, and advise on things related to the cloud. </p><p>If you'd told me at the start of my IT career I'd be the sole Devops engineer for GCP at a national company, I'd have laughed. </p><p>But I'll be honest, imposter syndrome is a right kicker. I'm putting this down to the fact it's a new role, but there's days I sit there looking at terraform or some google product I've not seen going \"wtf, where do I even start?\" (if you scroll down to <em>Things that have saved me</em>, I talk about how to deal with this)</p><p>I mainly spend most of my days looking at our current infrastructure, seeing where we can make improvement's, as well as helping team members with their infrastructure queries. </p><p>Soon, my self, my manager and another colleague will split into sole GCP practice - So I'm working on a lot of documentation around architecture decisions, and processes. </p><h2 id=\"where-i-started\">Where I started</h2><p>I started on the help desk - Most people do to be honest with you. It's almost worrying when someone <strong>hasn't </strong>started at the help desk. It really humbles you being treated like shit day in day out by people who are angry about the stupid computer not connecting to the VPN because vpn = virtual private which means you <strong>don't</strong> have to connect to wifi as he network is <strong>virtual </strong>and <strong>private</strong>. Real call btw</p><p>I mainly dealing with level 1 helpdesk tickets, so your password resets, creating users, building machines. </p><p>I got on to this automation stuff, so using powershell to install software like VNC server, enable RDP, install chrome and so on. </p><p>But if you want the \"<em>where did your tech interest start\" </em>then this is going back to India. If you're not a frequent reader of my blog (firstly shame on you) - You won't know I lived in India for a few years. The school I went to was what I would call \"A digital school\" where everything was on computers. </p><p>Me being me, I would get bored easily so would spend time playing with stuff like VM's (docker had only <strong>just</strong> been <sup>(20 March 2013)</sup> released, and I was still getting my head around linux), web servers and linux. I think my first website was just the generic \"Hello world\" followed by probably a meme or a photo of a duck? </p><p>I want to say it went downhill from here? We moved back to the UK and after leaving school I got a job working in a warehouse, thus had income to purchase servers. </p><p>I built my lab and eventually created the OG breadnet site (below)</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://www.old.breadnet.co.uk\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">breadNET</div><div class=\"kg-bookmark-description\">Welcome, you’ve made it over the ~waves~ of the internet This is my site. I know, not amazing, but we all started somewhere. I do have plans to move to wordpress and self host it, but that’s something I need to learn! A little about me:I left school in 2018 and worked in a warehouse during the</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://lh6.googleusercontent.com/jQdmbZGVu_OD7OjVQ0g4x09bPOXZ9cl94uxBzg2It5A73KR0OzFsjUpsOnUvHT0ahry7cX66o-0MRdQOGNB0pVonag-3oL0\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://lh3.googleusercontent.com/QoRn23_8QSccZlg2WdyHibri3I2ifm3mtG0AY3H2eKk1lWmiyDM33abtpiWb36OHcCBdkQ&#x3D;w16383\" alt=\"\"></div></a></figure><p>I keep this <strong>horrific </strong>site around as it got my my first job in IT, and without it I don't know where I would be today!</p><h3 id=\"but-what-point-did-you-transition-to-the-cloud\">But what point did you transition to the cloud?</h3><p>Ha yeah, forgot about that!</p><p>So I was at a company based from Texas, and I joined as a Jr. Sys Admin, and they put me on a cloud project to see what would happen and I was decent from what I understand. </p><p>Eventually I went cloud full time and yeah - I never planned to be here! I was originally going for my CCNA to work in a NOC at a Datacenter. Strange isn't it! </p><h2 id=\"my-career-track-i-m-on\">My career track I'm on</h2><p>I'm hoping to be senior principal cloud architect? I'm not sure if I made that title up but that's what I'm aiming for!</p><h3 id=\"why-that-title-why-not-a-manager\">Why that title? Why not a manager?</h3><p>Yeah good question actually. I don't want to manage people. I'm an engineer and we're awkward af. Could you imagine me having to speak to one of my direct reports about BO issues? </p><p>I want to stay in a role where I am solving issues day to day, and also helping the team to unblock their issues, or other stuff. Not sure yet!</p><p></p><h2 id=\"where-i-plan-to-end-up\">Where I plan to end up</h2><p>Like previously mentioned, I want to be senior principal <em>I forget the rest of the title. </em>I don't want to lose out on solving issues and writing terraform. It's one of the few joys in my life</p><h2 id=\"how-to-actually-get-in-to-it\">How to actually get in to IT</h2><p>I'm not sure if it's by chance of my Dad's role. But I can remember being interested in what he was doing. He was a programmer and now I honestly don't know what he does. </p><p>One of my earliest memories is on an XP laptop my dads friend gave me, with my Dad creating scrolling text in HTML. I think the code was something like the below:</p><pre><code class=\"language-html\">&lt;marquee behavior=\"scroll\" direction=\"left\"&gt;\n    Here is some scrolling text... right to left!\n&lt;/marquee&gt;\n</code></pre><!--kg-card-begin: html--><marquee behavior=\"scroll\" direction=\"left\">Here is some scrolling text... right to left!</marquee>\n<!--kg-card-end: html--><p></p><p></p><p>From there I kind of progressed and wanted more technical knowledge. So it's fair to say that I've been interested in technology from a young age. I was initially thinking of going in to Mechanical engineering at college, but that was full - with the aims of keeping IT as a side hobby. </p><h2 id=\"do-you-need-a-degree\">Do you need a degree?</h2><p>Depends. If you want one - I won't stop you!</p><p>I personally don't have anything higher than a B-TEC in IT systems. </p><p>If you're going in to something like ML and AI, I think a degree is beneficial, but if you're going in to cloud engineering or something along those lines - Nah g</p><h2 id=\"certifications-i-suggest\">Certifications I suggest</h2><p>Depends - Again.</p><p>If you're going in to cloud:</p><ul><li>Linux Admin</li><li>Google associate</li><li>AWS associate</li></ul><p>If you're going in to the MS environment, idk man. I'm Google and AWS cloud.</p><p></p><h2 id=\"things-that-saved-me\">Things that saved me</h2><p></p><h3 id=\"small-chunks\">Small Chunks</h3><p>Not every problem is as big as it seems.</p><p>Take my most recent publish </p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"__GHOST_URL__/cloud-init-ovh-and-terraform/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Cloud-init, OVH and terraform</div><div class=\"kg-bookmark-description\">I’ll be honest, this one was a struggle but we did it!</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://breadnet.co.uk/favicon.png\" alt=\"\"><span class=\"kg-bookmark-author\">breadNET</span><span class=\"kg-bookmark-publisher\">Bradley Stannard</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://images.unsplash.com/photo-1620712943543-bcc4688e7485?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;MnwxMTc3M3wwfDF8c2VhcmNofDZ8fGF1dG9tYXRpb258ZW58MHx8fHwxNjQ2MjYzNDE4&amp;ixlib&#x3D;rb-1.2.1&amp;q&#x3D;80&amp;w&#x3D;2000\" alt=\"\"></div></a></figure><p>At the surface, it seems like an impossible feat - No one on google or reddit got it working well. </p><p>When we look at what the actual issue is, and how we can resolve it, all it was; was editing a single file under <code>/etc/netplan/50*</code> and running <code>netplan apply</code></p><p>The next steps were to then work out how to automate it. </p><p>We start with what we know, which is the above, and what we don't know - which is what platform to use.</p><p>We can use something like ansible, bash scripting, cloud-init, startup scripts or even manual ssh.</p><p>Next is to device on the tool, and the pro's and cons of each. </p><p>Done. You now have a small easy to solve problem.</p><p></p><h3 id=\"google-is-your-friend\">Google is your friend</h3><p>Just google the error. 90% of the time the first result will help.<br>Page 2 is for dire situations. </p><p></p><h3 id=\"don-t-make-assumptions\">Don't make assumptions</h3><p>This is something I learnt <strong>very quickly.</strong></p><p>If you assume that your colleague locked out a user, and it's on you to check and something goes wrong... Guess who's to blame! </p><p><strong>always double check!</strong></p><p></p><h3 id=\"7-p-s\">7 P's</h3><p><em>Proper Planning and Preparation Prevents Piss Poor Performance</em></p><p>Plan everything you do. This doesn't need to be a 12 step plan on how to unlock the door. It can be as simple as:</p><ol><li>Log in to server</li><li>Check <code>/etc/passwd</code></li><li>Record if user is there</li><li>Shutdown server</li></ol><p>At least this way you know what you need to do, and now you can work out \"Hmm, how do I access that file, do I need root privileges?\" </p><p></p><h3 id=\"write-documentation-for-the-junior-s\">Write documentation for the Junior's </h3><p>I don't literally mean, write it for them personally.</p><p>We were all juniors at some point, and all suffered with imposter syndrome. </p><p>I can <strong>very</strong> clearly remember sitting at the help desk reading some documentation feeling like an eejit because I didn't understand the different abbreviations etc etc. </p><p>It's important that anyone with a minimum technical understanding should be able to look at your <code>readme.md</code> and be able to understand what's going on. </p><p>The same goes for your tickets. Make them super clear and include more detail than you think. The plan is that the junior can understand it and work out next steps</p><h3 id=\"use-a-note-pad-for-your-daily-to-do-s\">Use a note pad for your daily to-do's </h3><p>Yes - We're on the cutting edge of technology, but a paper notepad works well.</p><p>Personally, I use the little pieces of paper that you get from Ikea. I write my daily to-do on it, use it to draw diagrams etc.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/03/ikea.jpg\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"2667\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/03/ikea.jpg 600w, __GHOST_URL__/content/images/size/w1000/2022/03/ikea.jpg 1000w, __GHOST_URL__/content/images/size/w1600/2022/03/ikea.jpg 1600w, __GHOST_URL__/content/images/size/w2400/2022/03/ikea.jpg 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><p></p><h3 id=\"keep-your-work-area-clear\">Keep your work area clear</h3><p>Now I don't mean like clinically clean. Nothing on the desk at all.</p><p><strong>I mean</strong>, what's on the desk, make it neat and tidy. Keep a little on the desk as possible.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/03/desk.jpg\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"2667\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/03/desk.jpg 600w, __GHOST_URL__/content/images/size/w1000/2022/03/desk.jpg 1000w, __GHOST_URL__/content/images/size/w1600/2022/03/desk.jpg 1600w, __GHOST_URL__/content/images/size/w2400/2022/03/desk.jpg 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><p>As you can see, there is stuff on the top left, but it's in a line; and what is on the desk is critical to my operations. </p><ol><li>Water </li><li>Candle</li><li>Moose (Most important)</li><li>Headphone stand</li><li>Note paper</li></ol><hr><p></p><p>I hope this has given you a little hand in getting started with Cloud or your tech career. </p><p>Don't give up. Keep looking, if you need help you're welcome to contact me. </p><p>If you want to contact me, locate the <strong><code>x-message-me</code> </strong>header on the site's response - google how to find site response headers :) </p><p></p><p>All the best,</p><p></p><p>Bradley</p>","comment_id":"621ffcba17e5727444dc78f4","plaintext":"Getting a job in IT can be easy, but also not. In today's installment of blog post, we're going to do a deep dive in to my career, recommendations I would make to younger me and things I would avoid doing.\n\nLet's start\n\nThis is structured as an interview with my self, but in reality it's me typing this at my desk on a Wednesday night. Go figure\n\n\nWhere I am now\n\nSo currently I am at a Pet care company, doing Devops. From what I can gather, I've been brought on as someone who's full time job is to know the cloud, and advise on things related to the cloud.\n\nIf you'd told me at the start of my IT career I'd be the sole Devops engineer for GCP at a national company, I'd have laughed.\n\nBut I'll be honest, imposter syndrome is a right kicker. I'm putting this down to the fact it's a new role, but there's days I sit there looking at terraform or some google product I've not seen going \"wtf, where do I even start?\" (if you scroll down to Things that have saved me, I talk about how to deal with this)\n\nI mainly spend most of my days looking at our current infrastructure, seeing where we can make improvement's, as well as helping team members with their infrastructure queries.\n\nSoon, my self, my manager and another colleague will split into sole GCP practice - So I'm working on a lot of documentation around architecture decisions, and processes.\n\n\nWhere I started\n\nI started on the help desk - Most people do to be honest with you. It's almost worrying when someone hasn't started at the help desk. It really humbles you being treated like shit day in day out by people who are angry about the stupid computer not connecting to the VPN because vpn = virtual private which means you don't have to connect to wifi as he network is virtual and private. Real call btw\n\nI mainly dealing with level 1 helpdesk tickets, so your password resets, creating users, building machines.\n\nI got on to this automation stuff, so using powershell to install software like VNC server, enable RDP, install chrome and so on.\n\nBut if you want the \"where did your tech interest start\" then this is going back to India. If you're not a frequent reader of my blog (firstly shame on you) - You won't know I lived in India for a few years. The school I went to was what I would call \"A digital school\" where everything was on computers.\n\nMe being me, I would get bored easily so would spend time playing with stuff like VM's (docker had only just been (20 March 2013) released, and I was still getting my head around linux), web servers and linux. I think my first website was just the generic \"Hello world\" followed by probably a meme or a photo of a duck?\n\nI want to say it went downhill from here? We moved back to the UK and after leaving school I got a job working in a warehouse, thus had income to purchase servers.\n\nI built my lab and eventually created the OG breadnet site (below)\n\nbreadNETWelcome, you’ve made it over the ~waves~ of the internet This is my site. I know, not amazing, but we all started somewhere. I do have plans to move to wordpress and self host it, but that’s something I need to learn! A little about me:I left school in 2018 and worked in a warehouse during the\n\nI keep this horrific site around as it got my my first job in IT, and without it I don't know where I would be today!\n\n\nBut what point did you transition to the cloud?\n\nHa yeah, forgot about that!\n\nSo I was at a company based from Texas, and I joined as a Jr. Sys Admin, and they put me on a cloud project to see what would happen and I was decent from what I understand.\n\nEventually I went cloud full time and yeah - I never planned to be here! I was originally going for my CCNA to work in a NOC at a Datacenter. Strange isn't it!\n\n\nMy career track I'm on\n\nI'm hoping to be senior principal cloud architect? I'm not sure if I made that title up but that's what I'm aiming for!\n\n\nWhy that title? Why not a manager?\n\nYeah good question actually. I don't want to manage people. I'm an engineer and we're awkward af. Could you imagine me having to speak to one of my direct reports about BO issues?\n\nI want to stay in a role where I am solving issues day to day, and also helping the team to unblock their issues, or other stuff. Not sure yet!\n\n\n\n\nWhere I plan to end up\n\nLike previously mentioned, I want to be senior principal I forget the rest of the title. I don't want to lose out on solving issues and writing terraform. It's one of the few joys in my life\n\n\nHow to actually get in to IT\n\nI'm not sure if it's by chance of my Dad's role. But I can remember being interested in what he was doing. He was a programmer and now I honestly don't know what he does.\n\nOne of my earliest memories is on an XP laptop my dads friend gave me, with my Dad creating scrolling text in HTML. I think the code was something like the below:\n\n<marquee behavior=\"scroll\" direction=\"left\">\n    Here is some scrolling text... right to left!\n</marquee>\n\n\nHere is some scrolling text... right to left!\n\n\n\n\n\n\nFrom there I kind of progressed and wanted more technical knowledge. So it's fair to say that I've been interested in technology from a young age. I was initially thinking of going in to Mechanical engineering at college, but that was full - with the aims of keeping IT as a side hobby.\n\n\nDo you need a degree?\n\nDepends. If you want one - I won't stop you!\n\nI personally don't have anything higher than a B-TEC in IT systems.\n\nIf you're going in to something like ML and AI, I think a degree is beneficial, but if you're going in to cloud engineering or something along those lines - Nah g\n\n\nCertifications I suggest\n\nDepends - Again.\n\nIf you're going in to cloud:\n\n * Linux Admin\n * Google associate\n * AWS associate\n\nIf you're going in to the MS environment, idk man. I'm Google and AWS cloud.\n\n\n\n\nThings that saved me\n\n\n\n\nSmall Chunks\n\nNot every problem is as big as it seems.\n\nTake my most recent publish\n\nCloud-init, OVH and terraformI’ll be honest, this one was a struggle but we did it!breadNETBradley Stannard\n\nAt the surface, it seems like an impossible feat - No one on google or reddit got it working well.\n\nWhen we look at what the actual issue is, and how we can resolve it, all it was; was editing a single file under /etc/netplan/50* and running netplan apply\n\nThe next steps were to then work out how to automate it.\n\nWe start with what we know, which is the above, and what we don't know - which is what platform to use.\n\nWe can use something like ansible, bash scripting, cloud-init, startup scripts or even manual ssh.\n\nNext is to device on the tool, and the pro's and cons of each.\n\nDone. You now have a small easy to solve problem.\n\n\n\n\nGoogle is your friend\n\nJust google the error. 90% of the time the first result will help.\nPage 2 is for dire situations.\n\n\n\n\nDon't make assumptions\n\nThis is something I learnt very quickly.\n\nIf you assume that your colleague locked out a user, and it's on you to check and something goes wrong... Guess who's to blame!\n\nalways double check!\n\n\n\n\n7 P's\n\nProper Planning and Preparation Prevents Piss Poor Performance\n\nPlan everything you do. This doesn't need to be a 12 step plan on how to unlock the door. It can be as simple as:\n\n 1. Log in to server\n 2. Check /etc/passwd\n 3. Record if user is there\n 4. Shutdown server\n\nAt least this way you know what you need to do, and now you can work out \"Hmm, how do I access that file, do I need root privileges?\"\n\n\n\n\nWrite documentation for the Junior's\n\nI don't literally mean, write it for them personally.\n\nWe were all juniors at some point, and all suffered with imposter syndrome.\n\nI can very clearly remember sitting at the help desk reading some documentation feeling like an eejit because I didn't understand the different abbreviations etc etc.\n\nIt's important that anyone with a minimum technical understanding should be able to look at your readme.md and be able to understand what's going on.\n\nThe same goes for your tickets. Make them super clear and include more detail than you think. The plan is that the junior can understand it and work out next steps\n\n\nUse a note pad for your daily to-do's\n\nYes - We're on the cutting edge of technology, but a paper notepad works well.\n\nPersonally, I use the little pieces of paper that you get from Ikea. I write my daily to-do on it, use it to draw diagrams etc.\n\n\n\n\nKeep your work area clear\n\nNow I don't mean like clinically clean. Nothing on the desk at all.\n\nI mean, what's on the desk, make it neat and tidy. Keep a little on the desk as possible.\n\nAs you can see, there is stuff on the top left, but it's in a line; and what is on the desk is critical to my operations.\n\n 1. Water\n 2. Candle\n 3. Moose (Most important)\n 4. Headphone stand\n 5. Note paper\n\n\n\nI hope this has given you a little hand in getting started with Cloud or your tech career.\n\nDon't give up. Keep looking, if you need help you're welcome to contact me.\n\nIf you want to contact me, locate the x-message-me header on the site's response - google how to find site response headers :)\n\n\n\nAll the best,\n\n\n\nBradley","feature_image":"__GHOST_URL__/content/images/2022/03/IMG_5789.JPG","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2022-03-02T23:24:42.000Z","updated_at":"2022-03-03T00:39:30.000Z","published_at":"2022-03-03T00:38:32.000Z","custom_excerpt":"Interview with Bradley of Breadnet on how he got to where he is now, and advice he would give if he started again.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"6425f3bc192c0c150413d5a9","uuid":"abf020ff-75a7-4959-a31a-5e7b5ad7db0c","title":"Resolving the 'sts:GetCallerIdentity' error","slug":"wasabi-x-terraform","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/03/image-8.png\",\"width\":1568,\"height\":1348}],[\"bookmark\",{\"url\":\"https://www.terraform.io/language/settings/backends\",\"metadata\":{\"url\":\"https://www.terraform.io/language/settings/backends\",\"title\":\"Backend Overview - Configuration Language | Terraform by HashiCorp\",\"description\":\"A backend defines where Terraform stores its state. Learn about how backends work.\",\"author\":null,\"publisher\":\"Terraform by HashiCorp\",\"thumbnail\":\"https://www.terraform.io/img/og-image.png\",\"icon\":\"https://www.terraform.io/favicon.ico\"}}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/03/image-7.png\",\"width\":1627,\"height\":229}],[\"code\",{\"code\":\"Error: error configuring S3 Backend: error validating provider credentials: error calling sts:GetCallerIdentity: InvalidClientTokenId: The security token included in the request is invalid.\\n       status code: 403, request id:\"}],[\"code\",{\"code\":\"terraform {\\n\\n    backend \\\"s3\\\" {\\n    access_key = \\\"<WASABI-ACCESS-KEY>\\\"\\n    secret_key = \\\"<WASABI-SECRECT-KEY>\\\"\\n    endpoint = \\\"https://s3.uk-1.wasabisys.com\\\"\\n    region = \\\"us-east-1\\\"\\n    bucket = \\\"bradleys-terraform-state\\\"\\n    key = \\\"ovh/terraform.tfstate\\\"\\n  }\\n}\"}],[\"code\",{\"code\":\"skip_credentials_validation = true\"}],[\"code\",{\"code\":\"  backend \\\"s3\\\" {\\n    endpoint = \\\"https://s3.uk-1.wasabisys.com\\\"\\n    region = \\\"eu-west-1\\\"\\n    bucket = \\\"bradleys-terraform-state\\\"\\n    key = \\\"ovh/terraform.tfstate\\\"\\n    skip_credentials_validation = true\\n  }\"}],[\"hr\",{}],[\"hr\",{}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/03/a40a1329fd5e6b07239fb0c82be4ecb8.jpeg\",\"width\":782,\"height\":382,\"alt\":\"Man on a beach, sand falling between their fingers with the word \\\"terraform\\\" on the top centre, and \\\"state bucket\\\" at the bottom centre\",\"caption\":\"Meme\"}]],\"markups\":[[\"u\"],[\"strong\"],[\"em\"],[\"code\"],[\"a\",[\"href\",\"__GHOST_URL__/how-i-got-to-where-i-am-now/\"]],[\"a\",[\"href\",\"https://twitter.com/QuinnyPig\"]],[\"a\",[\"href\",\"https://wasabi-support.zendesk.com/hc/en-us/articles/360003362071-How-do-I-use-Terraform-with-Wasabi-\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I'll be honest, other than the 90 day delete policy (which annoys me to no end) wasabi is \"],[0,[0,1],1,\"dank\"],[0,[],0,\" \"],[0,[1],2,\"af\"],[0,[],0,\". \"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"Didn't expect me to start a blog post with that, did you? (Looking at you wasabi employees 👀) \"]]],[1,\"h2\",[[0,[],0,\"The problem at hand\"]]],[1,\"p\",[[0,[],0,\"I'm in the middle of a digital transformation, which means re-defining how I do things. \"]]],[1,\"p\",[[0,[],0,\"One of those is how I deal with terraform state. \"]]],[1,\"p\",[[0,[],0,\"Those of you who don't \"],[0,[2],1,\"terraform on the regular, \"],[0,[],0,\"let me explain the issue.\"]]],[1,\"p\",[[0,[],0,\"Terraform keeps track of what it's created, and what needs to be created or deleted next using a file called \"],[0,[3],1,\"terraform.tfstate\"],[0,[],0,\" - which for the most part is a \"],[0,[3],1,\"json\"],[0,[],0,\" file, see below.\"]]],[10,0],[1,\"p\",[[0,[],0,\"Now, when we're scaling our infra over multiple repos, modules etc, and having more people work on them, we get to this stage where we need a better solution on storing (and also versioning) state files. \"]]],[1,\"p\",[[0,[],0,\"Terraform has a few solutions you can use, see below\"]]],[10,1],[1,\"p\",[[0,[],0,\"I've decided to use s3 over others as:\"]]],[3,\"ol\",[[[0,[],0,\"GCS is expensive\"]],[[0,[],0,\"AWS S3 is expensive\"]],[[0,[],0,\"I already use Wasabi\"]],[[0,[],0,\"Terraform cloud is cool, but annoys me for some reason. \"]],[[0,[],0,\"The moose on my desk told me to (\"],[0,[4],1,\"See this post to learn more\"],[0,[],0,\")\"]]]],[1,\"p\",[]],[1,\"h2\",[[0,[],0,\"What we can use wasabi for\"]]],[1,\"p\",[[0,[],0,\"So as it stands, wasabi is AWS S3 compliant - But it's not AWS. \"]]],[1,\"p\",[[0,[],0,\"Without sounding like \"],[0,[5],1,\"Corey Quinn\"],[0,[],0,\" here, this is a stupid naming conventions. \"]]],[1,\"p\",[[0,[],0,\"Effectively what it means is you can use the AWS cli to connect to the endpoint that Wasabi offer, or use any other S3 cli to connect. It has the same IAM rules and stuff attached. \"]]],[1,\"p\",[[0,[],0,\"If I was to re-do it I would make S3 the standard, then each provider just has their offering.\"]]],[1,\"p\",[[0,[],0,\"I digress. \"]]],[1,\"p\",[[0,[],0,\"We are going to use wasabi to store our precious \"],[0,[3],1,\"terraform.tfstate\"],[0,[],0,\" file, with versioning enabled on the bucket.\"]]],[1,\"p\",[[0,[],0,\"This is important as without versioning it's hard to see what has changed for the environment, as well as later down the line using stuff like Open Policy Agent to enforce certain policies against the infrastructure. \"]]],[1,\"h2\",[[0,[],0,\"The error I had\"]]],[10,2],[10,3],[1,\"p\",[[0,[],0,\"What does this error mean?\"]]],[1,\"p\",[[0,[],0,\"I'll be honest, I have no clue. I was following the guide that Wasabi put out, \"],[0,[6],1,\"here\"],[0,[],0,\" - but it just would not work.\"]]],[1,\"p\",[[0,[],0,\"Here's what I had:\"]]],[10,4],[1,\"p\",[[0,[],0,\"It just would not work. No one else on the internet seems to have fixed this.\"]]],[1,\"h3\",[[0,[],0,\"Except me :) \"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"Here's what the file was missing - Call it the secret sauce:\"]]],[10,5],[1,\"p\",[[0,[],0,\"So our block should look something like this\"]]],[10,6],[1,\"p\",[[0,[],0,\"This seems to have done it!\"]]],[10,7],[1,\"p\",[[0,[],0,\"In my next post, we will be looking in to how to set up Wasabi to be a backend for Terraform, as well as the IAM policy required! \"]]],[10,8],[1,\"h2\",[[0,[],0,\"A meme\"]]],[10,9],[1,\"p\",[]]],\"ghostVersion\":\"3.0\"}","lexical":null,"html":"<p>I'll be honest, other than the 90 day delete policy (which annoys me to no end) wasabi is <u><strong>dank</strong> <strong>af</strong></u>. </p><p></p><p>Didn't expect me to start a blog post with that, did you? (Looking at you wasabi employees 👀) </p><h2 id=\"the-problem-at-hand\">The problem at hand</h2><p>I'm in the middle of a digital transformation, which means re-defining how I do things. </p><p>One of those is how I deal with terraform state. </p><p>Those of you who don't <em>terraform on the regular, </em>let me explain the issue.</p><p>Terraform keeps track of what it's created, and what needs to be created or deleted next using a file called <code>terraform.tfstate</code> - which for the most part is a <code>json</code> file, see below.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/03/image-8.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1568\" height=\"1348\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/03/image-8.png 600w, __GHOST_URL__/content/images/size/w1000/2022/03/image-8.png 1000w, __GHOST_URL__/content/images/2022/03/image-8.png 1568w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Now, when we're scaling our infra over multiple repos, modules etc, and having more people work on them, we get to this stage where we need a better solution on storing (and also versioning) state files. </p><p>Terraform has a few solutions you can use, see below</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://www.terraform.io/language/settings/backends\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Backend Overview - Configuration Language | Terraform by HashiCorp</div><div class=\"kg-bookmark-description\">A backend defines where Terraform stores its state. Learn about how backends work.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://www.terraform.io/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">Terraform by HashiCorp</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://www.terraform.io/img/og-image.png\" alt=\"\"></div></a></figure><p>I've decided to use s3 over others as:</p><ol><li>GCS is expensive</li><li>AWS S3 is expensive</li><li>I already use Wasabi</li><li>Terraform cloud is cool, but annoys me for some reason. </li><li>The moose on my desk told me to (<a href=\"__GHOST_URL__/how-i-got-to-where-i-am-now/\">See this post to learn more</a>)</li></ol><p></p><h2 id=\"what-we-can-use-wasabi-for\">What we can use wasabi for</h2><p>So as it stands, wasabi is AWS S3 compliant - But it's not AWS. </p><p>Without sounding like <a href=\"https://twitter.com/QuinnyPig\">Corey Quinn</a> here, this is a stupid naming conventions. </p><p>Effectively what it means is you can use the AWS cli to connect to the endpoint that Wasabi offer, or use any other S3 cli to connect. It has the same IAM rules and stuff attached. </p><p>If I was to re-do it I would make S3 the standard, then each provider just has their offering.</p><p>I digress. </p><p>We are going to use wasabi to store our precious <code>terraform.tfstate</code> file, with versioning enabled on the bucket.</p><p>This is important as without versioning it's hard to see what has changed for the environment, as well as later down the line using stuff like Open Policy Agent to enforce certain policies against the infrastructure. </p><h2 id=\"the-error-i-had\">The error I had</h2><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/03/image-7.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1627\" height=\"229\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/03/image-7.png 600w, __GHOST_URL__/content/images/size/w1000/2022/03/image-7.png 1000w, __GHOST_URL__/content/images/size/w1600/2022/03/image-7.png 1600w, __GHOST_URL__/content/images/2022/03/image-7.png 1627w\" sizes=\"(min-width: 720px) 720px\"></figure><pre><code>Error: error configuring S3 Backend: error validating provider credentials: error calling sts:GetCallerIdentity: InvalidClientTokenId: The security token included in the request is invalid.\n       status code: 403, request id:</code></pre><p>What does this error mean?</p><p>I'll be honest, I have no clue. I was following the guide that Wasabi put out, <a href=\"https://wasabi-support.zendesk.com/hc/en-us/articles/360003362071-How-do-I-use-Terraform-with-Wasabi-\">here</a> - but it just would not work.</p><p>Here's what I had:</p><pre><code>terraform {\n\n    backend \"s3\" {\n    access_key = \"&lt;WASABI-ACCESS-KEY&gt;\"\n    secret_key = \"&lt;WASABI-SECRECT-KEY&gt;\"\n    endpoint = \"https://s3.uk-1.wasabisys.com\"\n    region = \"us-east-1\"\n    bucket = \"bradleys-terraform-state\"\n    key = \"ovh/terraform.tfstate\"\n  }\n}</code></pre><p>It just would not work. No one else on the internet seems to have fixed this.</p><h3 id=\"except-me-\">Except me :) </h3><p></p><p>Here's what the file was missing - Call it the secret sauce:</p><pre><code>skip_credentials_validation = true</code></pre><p>So our block should look something like this</p><pre><code>  backend \"s3\" {\n    endpoint = \"https://s3.uk-1.wasabisys.com\"\n    region = \"eu-west-1\"\n    bucket = \"bradleys-terraform-state\"\n    key = \"ovh/terraform.tfstate\"\n    skip_credentials_validation = true\n  }</code></pre><p>This seems to have done it!</p><hr><p>In my next post, we will be looking in to how to set up Wasabi to be a backend for Terraform, as well as the IAM policy required! </p><hr><h2 id=\"a-meme\">A meme</h2><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2022/03/a40a1329fd5e6b07239fb0c82be4ecb8.jpeg\" class=\"kg-image\" alt=\"Man on a beach, sand falling between their fingers with the word &quot;terraform&quot; on the top centre, and &quot;state bucket&quot; at the bottom centre\" loading=\"lazy\" width=\"782\" height=\"382\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/03/a40a1329fd5e6b07239fb0c82be4ecb8.jpeg 600w, __GHOST_URL__/content/images/2022/03/a40a1329fd5e6b07239fb0c82be4ecb8.jpeg 782w\" sizes=\"(min-width: 720px) 720px\"><figcaption>Meme</figcaption></figure>","comment_id":"62211b9917e5727444dc79b4","plaintext":"I'll be honest, other than the 90 day delete policy (which annoys me to no end) wasabi is dank af.\n\n\n\nDidn't expect me to start a blog post with that, did you? (Looking at you wasabi employees 👀)\n\n\nThe problem at hand\n\nI'm in the middle of a digital transformation, which means re-defining how I do things.\n\nOne of those is how I deal with terraform state.\n\nThose of you who don't terraform on the regular, let me explain the issue.\n\nTerraform keeps track of what it's created, and what needs to be created or deleted next using a file called terraform.tfstate - which for the most part is a json file, see below.\n\nNow, when we're scaling our infra over multiple repos, modules etc, and having more people work on them, we get to this stage where we need a better solution on storing (and also versioning) state files.\n\nTerraform has a few solutions you can use, see below\n\nBackend Overview - Configuration Language | Terraform by HashiCorpA backend defines where Terraform stores its state. Learn about how backends work.Terraform by HashiCorp\n\nI've decided to use s3 over others as:\n\n 1. GCS is expensive\n 2. AWS S3 is expensive\n 3. I already use Wasabi\n 4. Terraform cloud is cool, but annoys me for some reason.\n 5. The moose on my desk told me to (See this post to learn more)\n\n\n\n\nWhat we can use wasabi for\n\nSo as it stands, wasabi is AWS S3 compliant - But it's not AWS.\n\nWithout sounding like Corey Quinn here, this is a stupid naming conventions.\n\nEffectively what it means is you can use the AWS cli to connect to the endpoint that Wasabi offer, or use any other S3 cli to connect. It has the same IAM rules and stuff attached.\n\nIf I was to re-do it I would make S3 the standard, then each provider just has their offering.\n\nI digress.\n\nWe are going to use wasabi to store our precious terraform.tfstate file, with versioning enabled on the bucket.\n\nThis is important as without versioning it's hard to see what has changed for the environment, as well as later down the line using stuff like Open Policy Agent to enforce certain policies against the infrastructure.\n\n\nThe error I had\n\nError: error configuring S3 Backend: error validating provider credentials: error calling sts:GetCallerIdentity: InvalidClientTokenId: The security token included in the request is invalid.\n       status code: 403, request id:\n\nWhat does this error mean?\n\nI'll be honest, I have no clue. I was following the guide that Wasabi put out, here - but it just would not work.\n\nHere's what I had:\n\nterraform {\n\n    backend \"s3\" {\n    access_key = \"<WASABI-ACCESS-KEY>\"\n    secret_key = \"<WASABI-SECRECT-KEY>\"\n    endpoint = \"https://s3.uk-1.wasabisys.com\"\n    region = \"us-east-1\"\n    bucket = \"bradleys-terraform-state\"\n    key = \"ovh/terraform.tfstate\"\n  }\n}\n\nIt just would not work. No one else on the internet seems to have fixed this.\n\n\nExcept me :)\n\n\n\nHere's what the file was missing - Call it the secret sauce:\n\nskip_credentials_validation = true\n\nSo our block should look something like this\n\n  backend \"s3\" {\n    endpoint = \"https://s3.uk-1.wasabisys.com\"\n    region = \"eu-west-1\"\n    bucket = \"bradleys-terraform-state\"\n    key = \"ovh/terraform.tfstate\"\n    skip_credentials_validation = true\n  }\n\nThis seems to have done it!\n\nIn my next post, we will be looking in to how to set up Wasabi to be a backend for Terraform, as well as the IAM policy required!\n\n\nA meme","feature_image":"https://images.unsplash.com/photo-1609558931189-d1bc0aa03dbb?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fHdhc2FiaXxlbnwwfHx8fDE2NDYzMzY5OTE&ixlib=rb-1.2.1&q=80&w=2000","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2022-03-03T19:48:41.000Z","updated_at":"2022-03-03T20:17:24.000Z","published_at":"2022-03-03T20:17:24.000Z","custom_excerpt":"Using Wasabi Hot Cloud Storage as a back end for Terraform","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"6425f3bc192c0c150413d5aa","uuid":"a08c85e7-7330-44bc-8a8f-c8d4fc19ce4a","title":"Scamming back the scammers","slug":"scamming-back-the-scammers","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/04/IMG_6100.jpg\",\"width\":1170,\"height\":2532}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/04/shipondesktop.jpg\",\"width\":3386,\"height\":1975}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/04/IMG_6104.jpg\",\"width\":1170,\"height\":2532}],[\"code\",{\"code\":\"'form_type': 'cc', 'ccname': ‘Tess Tickle’, 'ccnum': ‘123’, 'ccexp': ’12/25’, 'cccvv': ‘123’\"}],[\"code\",{\"code\":\"form_type : Name of the form\\nccname : Name on Credit Card number\\nccnum : Credit Card number\\nccexp : Expiry of the credit card\\ncccvv : CVV of the Credit card\"}],[\"hr\",{}],[\"code\",{\"code\":\"requests\\nurllib.parse\"}],[\"code\",{\"code\":\"from faker import Faker <code block>\"}],[\"code\",{\"code\":\"from faker import Faker\\nfake = Faker()\\n\\nname = fake.name()\\ncnum = fake.credit_card_number()\\ncexp = fake.credit_card_expire()\\ncvv = fake.credit_card_security_code()\"}],[\"code\",{\"code\":\"for s in range(100):\\ncookie = (\\\"PHPSESSID=\\\" + sessionrandom)\\nurl = f'https://local-shipstatus-gb.com/Finish.php?session={urlrandom}'\\nname = fake.name()\\ncnum = fake.credit_card_number()\\ncexp = fake.credit_card_expire()\\ncvv = fake.credit_card_security_code()\\n# f = { 'username' : name, 'password' : cnum, 'client_id' : cexp, 'client_secret' : cvv}\\nf = {'form_type': 'cc', 'ccname': name, 'ccnum': cnum, 'ccexp': cexp, 'cccvv': cvv}\\nurllib.parse.urlencode(f)\\nsafe_string = urllib.parse.urlencode(f)\\n\\npayload = safe_string\\nheaders = {\\n'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\\n'content-type': 'application/x-www-form-urlencoded',\\n'origin': 'https://local-shipstatus-gb.com',\\n#  'cookie':'PHPSESSID=134ddeae9b576bc8daa0eb412ced6945',\\n'cookie': cookie,\\n'content-length': '72',\\n'accept-language': 'en-GB,en;q=0.9',\\n'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1 Safari/605.1.15',\\n# 'user-agent': agent,\\n'referer': url,\\n'accept-encoding': 'gzip, deflate, br'\\n}\\n\\nresponse = requests.request(\\\"POST\\\", url, headers=headers, data=payload)\\n\\nif response.status_code == 200:\\nprint(\\\"ok\\\")\\nelif response.status_code != 200:\\nprint(response.status_code)\"}],[\"bookmark\",{\"url\":\"https://github.com/userbradley/fake-postoffice-spammer\",\"metadata\":{\"url\":\"https://github.com/userbradley/fake-postoffice-spammer\",\"title\":\"GitHub - userbradley/fake-postoffice-spammer\",\"description\":\"Contribute to userbradley/fake-postoffice-spammer development by creating an account on GitHub.\",\"author\":\"userbradley\",\"publisher\":\"GitHub\",\"thumbnail\":\"https://opengraph.githubassets.com/0f1b4fa6e1bd7a3e0794c27bb6b334458e4993c864543cd9bd7ae4c414d6435f/userbradley/fake-postoffice-spammer\",\"icon\":\"https://github.githubassets.com/favicons/favicon.svg\"}}]],\"markups\":[[\"a\",[\"href\",\"http://local-shipstatus-gb.com\"]]],\"sections\":[[1,\"p\",[]],[1,\"p\",[[0,[],0,\"Disclaimer: I do not accept any responsibility for your actions, what you do or decide not to, is up to you. This blog post serves as a “Here’s what I did” opposed to a “Here’s how to do it”\"],[1,[],0,0]]],[1,\"p\",[[0,[],0,\"Without wasting more time, let’s get to it.\"],[1,[],0,1]]],[1,\"p\",[[0,[],0,\"Recently I got one of the texts every citizen of the UK has got at some point in their life - A parcel has attempted to be delivered - Pay for it!\"]]],[1,\"p\",[[1,[],0,2]]],[10,0],[1,\"p\",[[0,[],0,\"Now here’s where they went wrong.\"]]],[1,\"p\",[[0,[],0,\"I have too much free time and unlimited internet.\"]]],[1,\"p\",[[1,[],0,3]]],[1,\"p\",[[0,[],0,\"I tried to browse to their site (\"],[0,[0],1,\"local-shipstatus-gb.com\"],[0,[],0,\") but they’re doing something clever where if you try and come to it form anything other than a mobile phone - you get a 404\"]]],[10,1],[1,\"p\",[[1,[],0,4]]],[1,\"p\",[[0,[],0,\"No matter what I did, I was unable to figure out a way around this on a browser.\"],[1,[],0,5]]],[1,\"p\",[[0,[],0,\"As you can see from the below page, you can see that it looks somewhat like a post office site, and requires some input\"]]],[1,\"p\",[[1,[],0,6]]],[10,2],[1,\"p\",[[1,[],0,7]]],[1,\"p\",[[0,[],0,\"Now stupidly I forgot to take a screen capture of this, but here’s what I did:\"]]],[1,\"p\",[[1,[],0,8]]],[3,\"ol\",[[[0,[],0,\"Installed mitmproxy to my Mac and started `mitmweb`\"]],[[0,[],0,\"Installed the Certs on my phone\"]],[[0,[],0,\"Set the phone to use the IP address of the Mac for a Proxy\"]],[[0,[],0,\"Browsed to the page and filled it out with Fake Data.\"]],[[0,[],0,\"Saved the capture\"]]]],[1,\"p\",[[1,[],0,9]]],[1,\"p\",[[0,[],0,\"From there I found the calls it was making to the web server\"]]],[1,\"p\",[[1,[],0,10]]],[10,3],[1,\"p\",[[1,[],0,11]]],[1,\"p\",[[0,[],0,\"Now what’s cool (not for them I guess?) is that their application doesn’t seem to be validating the numbers on the application - They are being captured then validated.\"]]],[1,\"p\",[[1,[],0,12]]],[1,\"p\",[[0,[],0,\"We simply need to write a python script that spams their server with the below fields: \"]]],[10,4],[10,5],[1,\"p\",[[0,[],0,\"We have to URL encode this in python, so spaces become %20, the normal stuff.\"]]],[1,\"p\",[[0,[],0,\"I used: \"]]],[10,6],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"Here’s our next issue… How do we make fake data to spam these chumps with?\"]]],[1,\"p\",[[0,[],0,\"Here comes Faker!\"]]],[10,7],[1,\"p\",[[0,[],0,\"We need to fake: Name, credit card, expiration and CVV - All of these faker can do\"]]],[10,8],[1,\"p\",[[0,[],0,\"Now all we need to do is use the user-agent from our phone, and then spam the living crap out of these low life scammers\"]]],[1,\"p\",[[1,[],0,13]]],[1,\"p\",[[0,[],0,\"Enter our for loop:\"]]],[10,9],[1,\"p\",[[1,[],0,14]]],[1,\"p\",[[0,[],0,\"If you want to read a full write up, and see how you could (not saying you should) launch an attach and their vulnerabilities:\"]]],[10,10],[1,\"p\",[]]],\"ghostVersion\":\"3.0\"}","lexical":null,"html":"<p></p><p>Disclaimer: I do not accept any responsibility for your actions, what you do or decide not to, is up to you. This blog post serves as a “Here’s what I did” opposed to a “Here’s how to do it”<br></p><p>Without wasting more time, let’s get to it.<br></p><p>Recently I got one of the texts every citizen of the UK has got at some point in their life - A parcel has attempted to be delivered - Pay for it!</p><p><br></p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/04/IMG_6100.jpg\" class=\"kg-image\" alt loading=\"lazy\" width=\"1170\" height=\"2532\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/04/IMG_6100.jpg 600w, __GHOST_URL__/content/images/size/w1000/2022/04/IMG_6100.jpg 1000w, __GHOST_URL__/content/images/2022/04/IMG_6100.jpg 1170w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Now here’s where they went wrong.</p><p>I have too much free time and unlimited internet.</p><p><br></p><p>I tried to browse to their site (<a href=\"http://local-shipstatus-gb.com\">local-shipstatus-gb.com</a>) but they’re doing something clever where if you try and come to it form anything other than a mobile phone - you get a 404</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/04/shipondesktop.jpg\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"1167\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/04/shipondesktop.jpg 600w, __GHOST_URL__/content/images/size/w1000/2022/04/shipondesktop.jpg 1000w, __GHOST_URL__/content/images/size/w1600/2022/04/shipondesktop.jpg 1600w, __GHOST_URL__/content/images/size/w2400/2022/04/shipondesktop.jpg 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><p><br></p><p>No matter what I did, I was unable to figure out a way around this on a browser.<br></p><p>As you can see from the below page, you can see that it looks somewhat like a post office site, and requires some input</p><p><br></p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/04/IMG_6104.jpg\" class=\"kg-image\" alt loading=\"lazy\" width=\"1170\" height=\"2532\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/04/IMG_6104.jpg 600w, __GHOST_URL__/content/images/size/w1000/2022/04/IMG_6104.jpg 1000w, __GHOST_URL__/content/images/2022/04/IMG_6104.jpg 1170w\" sizes=\"(min-width: 720px) 720px\"></figure><p><br></p><p>Now stupidly I forgot to take a screen capture of this, but here’s what I did:</p><p><br></p><ol><li>Installed mitmproxy to my Mac and started `mitmweb`</li><li>Installed the Certs on my phone</li><li>Set the phone to use the IP address of the Mac for a Proxy</li><li>Browsed to the page and filled it out with Fake Data.</li><li>Saved the capture</li></ol><p><br></p><p>From there I found the calls it was making to the web server</p><p><br></p><pre><code>'form_type': 'cc', 'ccname': ‘Tess Tickle’, 'ccnum': ‘123’, 'ccexp': ’12/25’, 'cccvv': ‘123’</code></pre><p><br></p><p>Now what’s cool (not for them I guess?) is that their application doesn’t seem to be validating the numbers on the application - They are being captured then validated.</p><p><br></p><p>We simply need to write a python script that spams their server with the below fields: </p><pre><code>form_type : Name of the form\nccname : Name on Credit Card number\nccnum : Credit Card number\nccexp : Expiry of the credit card\ncccvv : CVV of the Credit card</code></pre><hr><p>We have to URL encode this in python, so spaces become %20, the normal stuff.</p><p>I used: </p><pre><code>requests\nurllib.parse</code></pre><p></p><p>Here’s our next issue… How do we make fake data to spam these chumps with?</p><p>Here comes Faker!</p><pre><code>from faker import Faker &lt;code block&gt;</code></pre><p>We need to fake: Name, credit card, expiration and CVV - All of these faker can do</p><pre><code>from faker import Faker\nfake = Faker()\n\nname = fake.name()\ncnum = fake.credit_card_number()\ncexp = fake.credit_card_expire()\ncvv = fake.credit_card_security_code()</code></pre><p>Now all we need to do is use the user-agent from our phone, and then spam the living crap out of these low life scammers</p><p><br></p><p>Enter our for loop:</p><pre><code>for s in range(100):\ncookie = (\"PHPSESSID=\" + sessionrandom)\nurl = f'https://local-shipstatus-gb.com/Finish.php?session={urlrandom}'\nname = fake.name()\ncnum = fake.credit_card_number()\ncexp = fake.credit_card_expire()\ncvv = fake.credit_card_security_code()\n# f = { 'username' : name, 'password' : cnum, 'client_id' : cexp, 'client_secret' : cvv}\nf = {'form_type': 'cc', 'ccname': name, 'ccnum': cnum, 'ccexp': cexp, 'cccvv': cvv}\nurllib.parse.urlencode(f)\nsafe_string = urllib.parse.urlencode(f)\n\npayload = safe_string\nheaders = {\n'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n'content-type': 'application/x-www-form-urlencoded',\n'origin': 'https://local-shipstatus-gb.com',\n#  'cookie':'PHPSESSID=134ddeae9b576bc8daa0eb412ced6945',\n'cookie': cookie,\n'content-length': '72',\n'accept-language': 'en-GB,en;q=0.9',\n'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1 Safari/605.1.15',\n# 'user-agent': agent,\n'referer': url,\n'accept-encoding': 'gzip, deflate, br'\n}\n\nresponse = requests.request(\"POST\", url, headers=headers, data=payload)\n\nif response.status_code == 200:\nprint(\"ok\")\nelif response.status_code != 200:\nprint(response.status_code)</code></pre><p><br></p><p>If you want to read a full write up, and see how you could (not saying you should) launch an attach and their vulnerabilities:</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://github.com/userbradley/fake-postoffice-spammer\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">GitHub - userbradley/fake-postoffice-spammer</div><div class=\"kg-bookmark-description\">Contribute to userbradley/fake-postoffice-spammer development by creating an account on GitHub.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://github.githubassets.com/favicons/favicon.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">userbradley</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://opengraph.githubassets.com/0f1b4fa6e1bd7a3e0794c27bb6b334458e4993c864543cd9bd7ae4c414d6435f/userbradley/fake-postoffice-spammer\" alt=\"\"></div></a></figure>","comment_id":"625859de17e5727444dc7a36","plaintext":"Disclaimer: I do not accept any responsibility for your actions, what you do or decide not to, is up to you. This blog post serves as a “Here’s what I did” opposed to a “Here’s how to do it”\n\n\nWithout wasting more time, let’s get to it.\n\n\nRecently I got one of the texts every citizen of the UK has got at some point in their life - A parcel has attempted to be delivered - Pay for it!\n\n\n\n\nNow here’s where they went wrong.\n\nI have too much free time and unlimited internet.\n\n\n\n\nI tried to browse to their site (local-shipstatus-gb.com) but they’re doing something clever where if you try and come to it form anything other than a mobile phone - you get a 404\n\n\n\n\nNo matter what I did, I was unable to figure out a way around this on a browser.\n\n\nAs you can see from the below page, you can see that it looks somewhat like a post office site, and requires some input\n\n\n\n\n\n\n\nNow stupidly I forgot to take a screen capture of this, but here’s what I did:\n\n\n\n\n 1. Installed mitmproxy to my Mac and started `mitmweb`\n 2. Installed the Certs on my phone\n 3. Set the phone to use the IP address of the Mac for a Proxy\n 4. Browsed to the page and filled it out with Fake Data.\n 5. Saved the capture\n\n\n\n\nFrom there I found the calls it was making to the web server\n\n\n\n\n'form_type': 'cc', 'ccname': ‘Tess Tickle’, 'ccnum': ‘123’, 'ccexp': ’12/25’, 'cccvv': ‘123’\n\n\n\n\nNow what’s cool (not for them I guess?) is that their application doesn’t seem to be validating the numbers on the application - They are being captured then validated.\n\n\n\n\nWe simply need to write a python script that spams their server with the below fields:\n\nform_type : Name of the form\nccname : Name on Credit Card number\nccnum : Credit Card number\nccexp : Expiry of the credit card\ncccvv : CVV of the Credit card\n\nWe have to URL encode this in python, so spaces become %20, the normal stuff.\n\nI used:\n\nrequests\nurllib.parse\n\n\n\nHere’s our next issue… How do we make fake data to spam these chumps with?\n\nHere comes Faker!\n\nfrom faker import Faker <code block>\n\nWe need to fake: Name, credit card, expiration and CVV - All of these faker can do\n\nfrom faker import Faker\nfake = Faker()\n\nname = fake.name()\ncnum = fake.credit_card_number()\ncexp = fake.credit_card_expire()\ncvv = fake.credit_card_security_code()\n\nNow all we need to do is use the user-agent from our phone, and then spam the living crap out of these low life scammers\n\n\n\n\nEnter our for loop:\n\nfor s in range(100):\ncookie = (\"PHPSESSID=\" + sessionrandom)\nurl = f'https://local-shipstatus-gb.com/Finish.php?session={urlrandom}'\nname = fake.name()\ncnum = fake.credit_card_number()\ncexp = fake.credit_card_expire()\ncvv = fake.credit_card_security_code()\n# f = { 'username' : name, 'password' : cnum, 'client_id' : cexp, 'client_secret' : cvv}\nf = {'form_type': 'cc', 'ccname': name, 'ccnum': cnum, 'ccexp': cexp, 'cccvv': cvv}\nurllib.parse.urlencode(f)\nsafe_string = urllib.parse.urlencode(f)\n\npayload = safe_string\nheaders = {\n'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n'content-type': 'application/x-www-form-urlencoded',\n'origin': 'https://local-shipstatus-gb.com',\n#  'cookie':'PHPSESSID=134ddeae9b576bc8daa0eb412ced6945',\n'cookie': cookie,\n'content-length': '72',\n'accept-language': 'en-GB,en;q=0.9',\n'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1 Safari/605.1.15',\n# 'user-agent': agent,\n'referer': url,\n'accept-encoding': 'gzip, deflate, br'\n}\n\nresponse = requests.request(\"POST\", url, headers=headers, data=payload)\n\nif response.status_code == 200:\nprint(\"ok\")\nelif response.status_code != 200:\nprint(response.status_code)\n\n\n\n\nIf you want to read a full write up, and see how you could (not saying you should) launch an attach and their vulnerabilities:\n\nGitHub - userbradley/fake-postoffice-spammerContribute to userbradley/fake-postoffice-spammer development by creating an account on GitHub.GitHubuserbradley","feature_image":"https://images.unsplash.com/photo-1562813733-b31f71025d54?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwxMTc3M3wwfDF8c2VhcmNofDZ8fGhhY2tlcnxlbnwwfHx8fDE2NDk5NTc1MjI&ixlib=rb-1.2.1&q=80&w=2000","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2022-04-14T17:29:02.000Z","updated_at":"2022-04-14T17:38:00.000Z","published_at":"2022-04-14T17:38:00.000Z","custom_excerpt":"Recently got a scammy text, but now their database will bleed","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"6425f3bc192c0c150413d5ab","uuid":"f77ca678-880d-443c-b9b1-574ae2dbf20f","title":"Grocy with zebra","slug":"grocy-with-zebra","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}]],\"cards\":[[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/04/image.png\",\"width\":3456,\"height\":2050}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/04/image-1.png\",\"width\":3456,\"height\":1976}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/04/image-2.png\",\"width\":2226,\"height\":382,\"caption\":\"I've got no veg stock as tesco were sold out\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/04/image-3.png\",\"width\":3454,\"height\":1984}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/04/image-4.png\",\"width\":2066,\"height\":1572}],[\"code\",{\"code\":\"entity: products\\nName: tescolocation\\ncaption: Tesco Location\\nType: text (Single line)\\n\\nentity: products\\nname:vegan\\ncaption: Is vegan\\ntype: Checkbox\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/04/image-5.png\",\"width\":1486,\"height\":1258}],[\"markdown\",{\"markdown\":\"| Shop | Date |\\n| --- | --- |\"}],[\"hr\",{}],[\"code\",{\"code\":\"DefaultUserSetting('product_presets_default_due_days', -1);\\nDefaultUserSetting('scan_mode_consume_enabled', true); \\nDefaultUserSetting('scan_mode_purchase_enabled', true);\\nSetting('BASE_URL', 'https://grocy.breadoven'); //fixed the api thinking it was on localhost due to reverse proxy\"}],[\"hr\",{}],[\"hr\",{}]],\"markups\":[[\"code\"],[\"strong\"],[\"a\",[\"href\",\"https://www.bbcgoodfood.com/recipes/meatball-tomato-soup\"]],[\"em\"],[\"a\",[\"href\",\"https://m.apkpure.com/null-input-method/com.apedroid.hwkeyboardhelperfree\"]]],\"sections\":[[1,\"p\",[]],[1,\"blockquote\",[[0,[],0,\"Right off the bat, this is written more of a brain dump, to help those who also use grocy with a zebra scanner\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"So those of you who don't know me personally, I've moved to my own apartment. \"]]],[1,\"p\",[[0,[],0,\"Whilst yes, this is pretty cool, at the same time I now have an entire kitchen to myself. Which poses the issue of having more cupboards than I know what to do with.\"]]],[1,\"p\",[[0,[],0,\"I've recently started using a self hosted app called \"],[0,[0],1,\"Grocy\"],[0,[],0,\" which effectively, is a warehouse management system for your kitchen, with some added functionality. \"]]],[1,\"p\",[[0,[],0,\"The concept of Grocy is simple, how ever getting started takes \"],[0,[1],1,\"a lot\"],[0,[],0,\" of work. \"]]],[1,\"p\",[[0,[],0,\"If you're deploying Grocy for the first time, I suggest you start with adding the quantity units, locations, groups and if you're feeling spicy, the shelf number in the shop. (I give my values later on)\"]]],[1,\"p\",[[0,[],0,\"Regardless, what grocy has to offer is the ability to know when you got food, how long it has in your kitchen, when it's past it's sell by date, and then the best feature is automatically generating a shopping list. \"]]],[1,\"p\",[[0,[],0,\"Some screenshots of Grocy\"]]],[10,0],[1,\"p\",[[0,[],0,\"Here we can see that my Beef meatballs are set to be out of their \"],[0,[0],1,\"Best before date\"],[0,[],0,\" in 5 days, where as my white bread has 3 more days to go. \"]]],[10,1],[1,\"p\",[[0,[],0,\"Here is my weeks meal plan, showing that I don't have all the ingredients in stock to make my \"],[0,[2],1,\"Meatball Tomato soup\"],[0,[],0,\" (Recipe linked) \"]]],[10,2],[1,\"p\",[[0,[],0,\"Finally there's a Calendar section which shows food due, as well as chores \"]]],[10,3],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"The cool part here is we're able to assign barcodes to items (Both a grocy generated one, as well as the code from the shop, so a UPC)\"]]],[10,4],[1,\"p\",[[0,[],0,\"This is important to have, as it allows easy stock taking and purchasing, with my Zebra barcode scanner. \"]]],[1,\"p\",[[0,[],0,\"I'll share my values that I'm using for each section, in hope that it enables you to speed up deploying grocy:\"]]],[1,\"h3\",[[0,[],0,\"Quantity Units:\"]]],[3,\"ul\",[[[0,[],0,\"Bag\"]],[[0,[],0,\"Blister Pack\"]],[[0,[],0,\"Bottle\"]],[[0,[],0,\"Box\"]],[[0,[],0,\"Bunch\"]],[[0,[],0,\"Pack\"]],[[0,[],0,\"Pattie\"]],[[0,[],0,\"Piece\"]],[[0,[],0,\"Pill\"]],[[0,[],0,\"Sachet\"]],[[0,[],0,\"Slice\"]],[[0,[],0,\"Tin\"]]]],[1,\"h3\",[[0,[],0,\"Product Groups\"]]],[3,\"ul\",[[[0,[],0,\"Alcohol\"]],[[0,[],0,\"Breads\"]],[[0,[],0,\"Chips (I'm BRI'ISH)\"]],[[0,[],0,\"Crisps \"]],[[0,[],0,\"Fresh Produce\"]],[[0,[],0,\"Medical\"]],[[0,[],0,\"Pasta\"]],[[0,[],0,\"Rice\"]],[[0,[],0,\"Sauces\"]],[[0,[],0,\"Spices\"]],[[0,[],0,\"Sweets\"]],[[0,[],0,\"Tinned\"]]]],[1,\"h3\",[[0,[],0,\"Custom Fields (User fields)\"]]],[10,5],[1,\"p\",[[0,[],0,\"I'm using the tesco shelf location as the Tesco where I live is like no other, I can't find things to save my life. As I shop I scan the location code of the product as well as the barcode, which allows me to build shopping lists around items that are close by.\"]]],[1,\"p\",[]],[1,\"h2\",[[0,[],0,\"Roll on the zebra scanner\"]]],[1,\"p\",[[0,[],0,\"Like I said, I have a zebra TC56DJ (Most shops use these) which I recently purchased. \"]]],[1,\"p\",[[0,[],0,\"The serotonin this thing produces when it beeps is just 🤌\"]]],[10,6],[1,\"p\",[[0,[],0,\"For shopping I have my personal phone open with the Grocy shopping list, I mark an item as done then scan it in on the Zebra.\"]]],[1,\"p\",[[0,[],0,\"I would \"],[0,[1],1,\"love\"],[0,[],0,\" it if Grocy automatically removed items from the shopping list once there's been a purchase of that item, that's on the list... Oh well\"]]],[1,\"p\",[[0,[],0,\"I will update the below each time I get stopped with my scanner and someone assumes I work there\"]]],[10,7],[1,\"p\",[]],[10,8],[1,\"h3\",[[0,[],0,\"Specifics to allow the Barcode scanner to work well with Grocy\"]]],[1,\"p\",[[0,[],0,\"Under the app \"],[0,[0],1,\"DataWedge\"],[0,[],0,\" , make a new profile by clicking the 3 dots at the top right.\"]]],[1,\"p\",[[0,[],0,\"Once the new profile has loaded, click \"],[0,[0],1,\"Associated apps\"]]],[1,\"p\",[[0,[],0,\"Here select \"],[0,[0],1,\"com.android.chrome\"]]],[1,\"p\",[[0,[],0,\"On the menu after, select the \"],[0,[0],1,\"*\"],[0,[],0,\" - this means that any time you scan a barcode in Chrome, it will use the profile you've created. \"]]],[1,\"p\",[[0,[],0,\"By default the TC56 supports scanning the following barcodes:\"]]],[3,\"ul\",[[[0,[],0,\"QR\"]],[[0,[],0,\"Data Matrix\"]],[[0,[],0,\"PDF417\"]],[[0,[],0,\"Grid Matric\"]],[[0,[],0,\"Aztec code\"]],[[0,[],0,\"All 1D barcodes\"]],[[0,[],0,\"Royal Mail 2D barcode\"]],[[0,[],0,\"The weird barcodes you get on post\"]]]],[1,\"p\",[[0,[],0,\"This is by no means an exhaustive list of compatibility, it's a list of barcodes I could scan in my Apartment\"]]],[1,\"h2\",[[0,[],0,\"Config changes made\"]]],[1,\"p\",[[0,[],0,\"You can configure the scanner to send the enter key after each barcode, which is what I was using for about 3 hours before it \"],[0,[1],1,\"really\"],[0,[],0,\" messed things up.\"]]],[1,\"p\",[[0,[],0,\"I've disabled this and now gone with the below grocy config options\"]]],[10,9],[1,\"p\",[[0,[],0,\"By setting the default scan mode to true, the scanner doesn't always send an enter and then exit the form when you still need to add more data \"]]],[1,\"h2\",[[0,[],0,\"Limits to the scanner\"]]],[1,\"p\",[]],[1,\"h3\",[[0,[],0,\"Minimum barcode \"]]],[1,\"p\",[[0,[],0,\"Something I found quite funny was the \"],[0,[1],1,\"minimum barcode \"],[0,[],0,\"you need for a read. I currently have this at 1.10mm from about 20cm away\"]]],[1,\"h3\",[[0,[],0,\"Distance\"]]],[1,\"p\",[[0,[],0,\"Max distance for a 46.68mm x 10.28mm barcode is about 57cm with no shadows\"],[1,[],0,0],[0,[],0,\"Max distance for a 46.68mm x 10.28mm barcode is about 46cm with no light\"]]],[1,\"p\",[[0,[],0,\"Where as a 2d barcode measuring roughly 59.88 x 59.88 was able to read from a distance of 1.16\"],[0,[1],1,\"m!\"]]],[1,\"p\",[[0,[],0,\"Without sounding like a sales rep, this is not bad for a device released around 2017 \"]]],[10,10],[1,\"h2\",[[0,[],0,\"Issues faced\"]]],[1,\"p\",[[0,[],0,\"What would self hosted be without issues google won't help you with\"]]],[1,\"p\",[[0,[],0,\"Sometimes barcode would not scan in apps \"]]],[1,\"p\",[[0,[],0,\"Make new dwd profile and attach it to \"],[0,[0],1,\"com.android.chrome.*\"]]],[1,\"h3\",[[0,[],0,\"Enter key needing to be set to fill data\"]]],[1,\"p\",[[0,[],0,\"Set the config.php and disable send enter on barcode read \"],[0,[3],1,\"(See code block under changes made)\"]]],[1,\"h3\",[[0,[],0,\"Keyboard keeps popping up\"]]],[1,\"p\",[[0,[],0,\"The app to stop a keyboard popping up isn't on the play store, so download at your own risk\"]]],[1,\"p\",[[0,[],0,\"Once downloaded enable the keyboard in settings, and select it in chrome or grocy app.\"]]],[1,\"p\",[[0,[4],1,\"https://m.apkpure.com/null-input-method/com.apedroid.hwkeyboardhelperfree\"]]],[10,11],[1,\"p\",[[0,[],0,\"Overall I am quite excited to have this, I plan on trying to build my own UI to enable faster workflows, but I'm not a developer so who knows :)\"]]],[1,\"p\",[]]],\"ghostVersion\":\"3.0\"}","lexical":null,"html":"<p></p><blockquote>Right off the bat, this is written more of a brain dump, to help those who also use grocy with a zebra scanner</blockquote><p></p><p>So those of you who don't know me personally, I've moved to my own apartment. </p><p>Whilst yes, this is pretty cool, at the same time I now have an entire kitchen to myself. Which poses the issue of having more cupboards than I know what to do with.</p><p>I've recently started using a self hosted app called <code>Grocy</code> which effectively, is a warehouse management system for your kitchen, with some added functionality. </p><p>The concept of Grocy is simple, how ever getting started takes <strong>a lot</strong> of work. </p><p>If you're deploying Grocy for the first time, I suggest you start with adding the quantity units, locations, groups and if you're feeling spicy, the shelf number in the shop. (I give my values later on)</p><p>Regardless, what grocy has to offer is the ability to know when you got food, how long it has in your kitchen, when it's past it's sell by date, and then the best feature is automatically generating a shopping list. </p><p>Some screenshots of Grocy</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/04/image.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"1186\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/04/image.png 600w, __GHOST_URL__/content/images/size/w1000/2022/04/image.png 1000w, __GHOST_URL__/content/images/size/w1600/2022/04/image.png 1600w, __GHOST_URL__/content/images/size/w2400/2022/04/image.png 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Here we can see that my Beef meatballs are set to be out of their <code>Best before date</code> in 5 days, where as my white bread has 3 more days to go. </p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/04/image-1.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"1144\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/04/image-1.png 600w, __GHOST_URL__/content/images/size/w1000/2022/04/image-1.png 1000w, __GHOST_URL__/content/images/size/w1600/2022/04/image-1.png 1600w, __GHOST_URL__/content/images/size/w2400/2022/04/image-1.png 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Here is my weeks meal plan, showing that I don't have all the ingredients in stock to make my <a href=\"https://www.bbcgoodfood.com/recipes/meatball-tomato-soup\">Meatball Tomato soup</a> (Recipe linked) </p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2022/04/image-2.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"343\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/04/image-2.png 600w, __GHOST_URL__/content/images/size/w1000/2022/04/image-2.png 1000w, __GHOST_URL__/content/images/size/w1600/2022/04/image-2.png 1600w, __GHOST_URL__/content/images/2022/04/image-2.png 2226w\" sizes=\"(min-width: 720px) 720px\"><figcaption>I've got no veg stock as tesco were sold out</figcaption></figure><p>Finally there's a Calendar section which shows food due, as well as chores </p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/04/image-3.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"1149\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/04/image-3.png 600w, __GHOST_URL__/content/images/size/w1000/2022/04/image-3.png 1000w, __GHOST_URL__/content/images/size/w1600/2022/04/image-3.png 1600w, __GHOST_URL__/content/images/size/w2400/2022/04/image-3.png 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><p></p><p>The cool part here is we're able to assign barcodes to items (Both a grocy generated one, as well as the code from the shop, so a UPC)</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/04/image-4.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"1522\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/04/image-4.png 600w, __GHOST_URL__/content/images/size/w1000/2022/04/image-4.png 1000w, __GHOST_URL__/content/images/size/w1600/2022/04/image-4.png 1600w, __GHOST_URL__/content/images/2022/04/image-4.png 2066w\" sizes=\"(min-width: 720px) 720px\"></figure><p>This is important to have, as it allows easy stock taking and purchasing, with my Zebra barcode scanner. </p><p>I'll share my values that I'm using for each section, in hope that it enables you to speed up deploying grocy:</p><h3 id=\"quantity-units-\">Quantity Units:</h3><ul><li>Bag</li><li>Blister Pack</li><li>Bottle</li><li>Box</li><li>Bunch</li><li>Pack</li><li>Pattie</li><li>Piece</li><li>Pill</li><li>Sachet</li><li>Slice</li><li>Tin</li></ul><h3 id=\"product-groups\">Product Groups</h3><ul><li>Alcohol</li><li>Breads</li><li>Chips (I'm BRI'ISH)</li><li>Crisps </li><li>Fresh Produce</li><li>Medical</li><li>Pasta</li><li>Rice</li><li>Sauces</li><li>Spices</li><li>Sweets</li><li>Tinned</li></ul><h3 id=\"custom-fields-user-fields-\">Custom Fields (User fields)</h3><pre><code>entity: products\nName: tescolocation\ncaption: Tesco Location\nType: text (Single line)\n\nentity: products\nname:vegan\ncaption: Is vegan\ntype: Checkbox</code></pre><p>I'm using the tesco shelf location as the Tesco where I live is like no other, I can't find things to save my life. As I shop I scan the location code of the product as well as the barcode, which allows me to build shopping lists around items that are close by.</p><p></p><h2 id=\"roll-on-the-zebra-scanner\">Roll on the zebra scanner</h2><p>Like I said, I have a zebra TC56DJ (Most shops use these) which I recently purchased. </p><p>The serotonin this thing produces when it beeps is just 🤌</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/04/image-5.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1486\" height=\"1258\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/04/image-5.png 600w, __GHOST_URL__/content/images/size/w1000/2022/04/image-5.png 1000w, __GHOST_URL__/content/images/2022/04/image-5.png 1486w\" sizes=\"(min-width: 720px) 720px\"></figure><p>For shopping I have my personal phone open with the Grocy shopping list, I mark an item as done then scan it in on the Zebra.</p><p>I would <strong>love</strong> it if Grocy automatically removed items from the shopping list once there's been a purchase of that item, that's on the list... Oh well</p><p>I will update the below each time I get stopped with my scanner and someone assumes I work there</p><!--kg-card-begin: markdown--><table>\n<thead>\n<tr>\n<th>Shop</th>\n<th>Date</th>\n</tr>\n</thead>\n</table>\n<!--kg-card-end: markdown--><p></p><hr><h3 id=\"specifics-to-allow-the-barcode-scanner-to-work-well-with-grocy\">Specifics to allow the Barcode scanner to work well with Grocy</h3><p>Under the app <code>DataWedge</code> , make a new profile by clicking the 3 dots at the top right.</p><p>Once the new profile has loaded, click <code>Associated apps</code></p><p>Here select <code>com.android.chrome</code></p><p>On the menu after, select the <code>*</code> - this means that any time you scan a barcode in Chrome, it will use the profile you've created. </p><p>By default the TC56 supports scanning the following barcodes:</p><ul><li>QR</li><li>Data Matrix</li><li>PDF417</li><li>Grid Matric</li><li>Aztec code</li><li>All 1D barcodes</li><li>Royal Mail 2D barcode</li><li>The weird barcodes you get on post</li></ul><p>This is by no means an exhaustive list of compatibility, it's a list of barcodes I could scan in my Apartment</p><h2 id=\"config-changes-made\">Config changes made</h2><p>You can configure the scanner to send the enter key after each barcode, which is what I was using for about 3 hours before it <strong>really</strong> messed things up.</p><p>I've disabled this and now gone with the below grocy config options</p><pre><code>DefaultUserSetting('product_presets_default_due_days', -1);\nDefaultUserSetting('scan_mode_consume_enabled', true); \nDefaultUserSetting('scan_mode_purchase_enabled', true);\nSetting('BASE_URL', 'https://grocy.breadoven'); //fixed the api thinking it was on localhost due to reverse proxy</code></pre><p>By setting the default scan mode to true, the scanner doesn't always send an enter and then exit the form when you still need to add more data </p><h2 id=\"limits-to-the-scanner\">Limits to the scanner</h2><p></p><h3 id=\"minimum-barcode\">Minimum barcode </h3><p>Something I found quite funny was the <strong>minimum barcode </strong>you need for a read. I currently have this at 1.10mm from about 20cm away</p><h3 id=\"distance\">Distance</h3><p>Max distance for a 46.68mm x 10.28mm barcode is about 57cm with no shadows<br>Max distance for a 46.68mm x 10.28mm barcode is about 46cm with no light</p><p>Where as a 2d barcode measuring roughly 59.88 x 59.88 was able to read from a distance of 1.16<strong>m!</strong></p><p>Without sounding like a sales rep, this is not bad for a device released around 2017 </p><hr><h2 id=\"issues-faced\">Issues faced</h2><p>What would self hosted be without issues google won't help you with</p><p>Sometimes barcode would not scan in apps </p><p>Make new dwd profile and attach it to <code>com.android.chrome.*</code></p><h3 id=\"enter-key-needing-to-be-set-to-fill-data\">Enter key needing to be set to fill data</h3><p>Set the config.php and disable send enter on barcode read <em>(See code block under changes made)</em></p><h3 id=\"keyboard-keeps-popping-up\">Keyboard keeps popping up</h3><p>The app to stop a keyboard popping up isn't on the play store, so download at your own risk</p><p>Once downloaded enable the keyboard in settings, and select it in chrome or grocy app.</p><p><a href=\"https://m.apkpure.com/null-input-method/com.apedroid.hwkeyboardhelperfree\">https://m.apkpure.com/null-input-method/com.apedroid.hwkeyboardhelperfree</a></p><hr><p>Overall I am quite excited to have this, I plan on trying to build my own UI to enable faster workflows, but I'm not a developer so who knows :)</p>","comment_id":"626944be17e5727444dc7a76","plaintext":"Right off the bat, this is written more of a brain dump, to help those who also use grocy with a zebra scanner\n\n\n\nSo those of you who don't know me personally, I've moved to my own apartment.\n\nWhilst yes, this is pretty cool, at the same time I now have an entire kitchen to myself. Which poses the issue of having more cupboards than I know what to do with.\n\nI've recently started using a self hosted app called Grocy which effectively, is a warehouse management system for your kitchen, with some added functionality.\n\nThe concept of Grocy is simple, how ever getting started takes a lot of work.\n\nIf you're deploying Grocy for the first time, I suggest you start with adding the quantity units, locations, groups and if you're feeling spicy, the shelf number in the shop. (I give my values later on)\n\nRegardless, what grocy has to offer is the ability to know when you got food, how long it has in your kitchen, when it's past it's sell by date, and then the best feature is automatically generating a shopping list.\n\nSome screenshots of Grocy\n\nHere we can see that my Beef meatballs are set to be out of their Best before date in 5 days, where as my white bread has 3 more days to go.\n\nHere is my weeks meal plan, showing that I don't have all the ingredients in stock to make my Meatball Tomato soup (Recipe linked)\n\nFinally there's a Calendar section which shows food due, as well as chores\n\n\n\nThe cool part here is we're able to assign barcodes to items (Both a grocy generated one, as well as the code from the shop, so a UPC)\n\nThis is important to have, as it allows easy stock taking and purchasing, with my Zebra barcode scanner.\n\nI'll share my values that I'm using for each section, in hope that it enables you to speed up deploying grocy:\n\n\nQuantity Units:\n\n * Bag\n * Blister Pack\n * Bottle\n * Box\n * Bunch\n * Pack\n * Pattie\n * Piece\n * Pill\n * Sachet\n * Slice\n * Tin\n\n\nProduct Groups\n\n * Alcohol\n * Breads\n * Chips (I'm BRI'ISH)\n * Crisps\n * Fresh Produce\n * Medical\n * Pasta\n * Rice\n * Sauces\n * Spices\n * Sweets\n * Tinned\n\n\nCustom Fields (User fields)\n\nentity: products\nName: tescolocation\ncaption: Tesco Location\nType: text (Single line)\n\nentity: products\nname:vegan\ncaption: Is vegan\ntype: Checkbox\n\nI'm using the tesco shelf location as the Tesco where I live is like no other, I can't find things to save my life. As I shop I scan the location code of the product as well as the barcode, which allows me to build shopping lists around items that are close by.\n\n\n\n\nRoll on the zebra scanner\n\nLike I said, I have a zebra TC56DJ (Most shops use these) which I recently purchased.\n\nThe serotonin this thing produces when it beeps is just 🤌\n\nFor shopping I have my personal phone open with the Grocy shopping list, I mark an item as done then scan it in on the Zebra.\n\nI would love it if Grocy automatically removed items from the shopping list once there's been a purchase of that item, that's on the list... Oh well\n\nI will update the below each time I get stopped with my scanner and someone assumes I work there\n\n\n\n\nShop\nDate\n\n\n\n\n\n\n\n\nSpecifics to allow the Barcode scanner to work well with Grocy\n\nUnder the app DataWedge , make a new profile by clicking the 3 dots at the top right.\n\nOnce the new profile has loaded, click Associated apps\n\nHere select com.android.chrome\n\nOn the menu after, select the * - this means that any time you scan a barcode in Chrome, it will use the profile you've created.\n\nBy default the TC56 supports scanning the following barcodes:\n\n * QR\n * Data Matrix\n * PDF417\n * Grid Matric\n * Aztec code\n * All 1D barcodes\n * Royal Mail 2D barcode\n * The weird barcodes you get on post\n\nThis is by no means an exhaustive list of compatibility, it's a list of barcodes I could scan in my Apartment\n\n\nConfig changes made\n\nYou can configure the scanner to send the enter key after each barcode, which is what I was using for about 3 hours before it really messed things up.\n\nI've disabled this and now gone with the below grocy config options\n\nDefaultUserSetting('product_presets_default_due_days', -1);\nDefaultUserSetting('scan_mode_consume_enabled', true); \nDefaultUserSetting('scan_mode_purchase_enabled', true);\nSetting('BASE_URL', 'https://grocy.breadoven'); //fixed the api thinking it was on localhost due to reverse proxy\n\nBy setting the default scan mode to true, the scanner doesn't always send an enter and then exit the form when you still need to add more data\n\n\nLimits to the scanner\n\n\n\n\nMinimum barcode\n\nSomething I found quite funny was the minimum barcode you need for a read. I currently have this at 1.10mm from about 20cm away\n\n\nDistance\n\nMax distance for a 46.68mm x 10.28mm barcode is about 57cm with no shadows\nMax distance for a 46.68mm x 10.28mm barcode is about 46cm with no light\n\nWhere as a 2d barcode measuring roughly 59.88 x 59.88 was able to read from a distance of 1.16m!\n\nWithout sounding like a sales rep, this is not bad for a device released around 2017\n\n\nIssues faced\n\nWhat would self hosted be without issues google won't help you with\n\nSometimes barcode would not scan in apps\n\nMake new dwd profile and attach it to com.android.chrome.*\n\n\nEnter key needing to be set to fill data\n\nSet the config.php and disable send enter on barcode read (See code block under changes made)\n\n\nKeyboard keeps popping up\n\nThe app to stop a keyboard popping up isn't on the play store, so download at your own risk\n\nOnce downloaded enable the keyboard in settings, and select it in chrome or grocy app.\n\nhttps://m.apkpure.com/null-input-method/com.apedroid.hwkeyboardhelperfree\n\nOverall I am quite excited to have this, I plan on trying to build my own UI to enable faster workflows, but I'm not a developer so who knows :)","feature_image":"https://images.unsplash.com/photo-1616286071878-2c75b106cece?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fHRlc2NvfGVufDB8fHx8MTY1MTA2Njk1NQ&ixlib=rb-1.2.1&q=80&w=2000","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2022-04-27T13:27:26.000Z","updated_at":"2022-04-28T00:40:06.000Z","published_at":"2022-04-27T15:57:48.000Z","custom_excerpt":"Find out how I reduced food waste by using a zebra barcode scanner","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"6425f3bc192c0c150413d5ac","uuid":"e6c3e5fd-f57d-449b-9696-34ada8ea020c","title":"How BeReal works","slug":"how-be-real-works","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}]],\"cards\":[[\"hr\",{}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/08/image.png\",\"width\":1024,\"height\":326,\"caption\":\"BeReal Notification\",\"alt\":\"The BeReal notification: An iOS notificaiton bubble with the words \\\"Time to be real\\\" surrounded by two warning symbols. Below the  warning is the words \\\"2 min left to capture a BeReal and see what your friends are up to !\\\"\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/08/BeReal_example.png\",\"width\":1500,\"height\":2000,\"caption\":\"BeReal, where I was on the roof of a building\"}],[\"hr\",{}],[\"hr\",{}],[\"hr\",{}],[\"code\",{\"code\":\"➜ gsutil ls gs://storage.bere.al/Photos/                                 \\n\\nAccessDeniedException: 403 <>k does not have storage.objects.list access to the Google Cloud Storage bucket.\"}],[\"code\",{\"code\":\"{\\n    \\\"data\\\": {\\n        \\\"photoURL\\\": \\\"Photos/<me>/profile/<me>-1655905537-profile-picture.jpg\\\",\\n        \\\"topic\\\": \\\"<me>\\\",\\n        \\\"userName\\\": \\\"<me>\\\"\\n    }\\n}\"}],[\"code\",{\"code\":\"{\\n    \\\"result\\\":\\\"projects/alexisbarreyatbereal/messages/7517087177659076139\\\"\\n}\"}],[\"code\",{\"code\":\"{\\n    \\\"backCamera\\\": {\\n        \\\"bucket\\\": \\\"storage.bere.al\\\",\\n        \\\"height\\\": 2000,\\n        \\\"path\\\": \\\"Photos/<me>/bereal/7c44d6e8-086b-4a18-b8b4d3785f58cda8-1660122851.jpg\\\",\\n        \\\"width\\\": 1500\\n    },\\n    \\\"frontCamera\\\": {\\n        \\\"bucket\\\": \\\"storage.bere.al\\\",\\n        \\\"height\\\": 2000,\\n        \\\"path\\\": \\\"Photos/<me>/bereal/7c44d6e8-086b-4a18-b8b4d3785f58cda8-1660122851-secondary.jpg\\\",\\n        \\\"width\\\": 1500\\n    },\\n    \\\"isLate\\\": true,\\n    \\\"isPublic\\\": false,\\n    \\\"location\\\": {\\n        \\\"latitude\\\": <>>,\\n        \\\"longitude\\\": <>>\\n    },\\n    \\\"retakeCounter\\\": 4,\\n    \\\"takenAt\\\": \\\"2022-08-10T09:14:11Z\\\"\\n}\"}],[\"code\",{\"code\":\"\\\"location\\\": {\\n        \\\"latitude\\\": <>>,\\n        \\\"longitude\\\": <>>\\n },\"}],[\"code\",{\"code\":\"Photos/<me>/bereal/7c44d6e8-086b-4a18-b8b4-d3785f58cda8-1660122851.jpg\",\"language\":\"text\",\"caption\":\"Image URL\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/08/image-1.png\",\"width\":763,\"height\":622}],[\"code\",{\"code\":\"https://us-central1-alexisbarreyat-bereal.cloudfunctions.net\"}],[\"hr\",{}],[\"bookmark\",{\"url\":\"https://github.com/userbradley/beReal\",\"metadata\":{\"url\":\"https://github.com/userbradley/BeReal\",\"title\":\"GitHub - userbradley/BeReal: How does BeReal work (Under the hood)\",\"description\":\"How does BeReal work (Under the hood). Contribute to userbradley/BeReal development by creating an account on GitHub.\",\"author\":\"userbradley\",\"publisher\":\"GitHub\",\"thumbnail\":\"https://opengraph.githubassets.com/008e444a696cd7cde3b7e5b5219f89fafbcd509bee2c1277c4172ed25bc277d3/userbradley/BeReal\",\"icon\":\"https://github.githubassets.com/favicons/favicon.svg\"}}],[\"hr\",{}]],\"markups\":[[\"em\"],[\"a\",[\"href\",\"https://fr.linkedin.com/in/alexisbarreyat\"]],[\"strong\"],[\"a\",[\"href\",\"https://amplitude.com\"]],[\"code\"],[\"a\",[\"href\",\"https://shomil.me/bereal/#:~:text=Now%20one%20might,people%E2%80%99s%20homes%3F%20Workplaces%3F\"]],[\"a\",[\"href\",\"https://us-central1-alexisbarreyat-bereal.cloudfunctions.net/getUserNames\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"And no, this not a \"],[0,[0],1,\"how to,\"],[0,[],0,\" on how to use the app. But a deep dive on the infrastructure behind BeReal, and the engineering decisions that were taken. \"]]],[10,0],[1,\"p\",[[0,[],0,\"Firstly, what is BeReal?\"]]],[1,\"p\",[[0,[],0,\"BeReal is an app designed by a French Developer called \"],[0,[1],1,\"Alexis Barreyat\"],[0,[],0,\", which aims to break down the highly edited social media that we have come to know and hate. \"]]],[1,\"p\",[[0,[],0,\"You get a notification once a day, like the below, where you are urged to whip out your phone, and snap a pic.\"]]],[10,1],[1,\"p\",[[0,[],0,\"The application makes use of both the front facing and back facing camera to give \"],[0,[0],1,\"real view\"],[0,[],0,\" in to what you're up to.\"]]],[1,\"p\",[[0,[],0,\"See the example below:\"]]],[10,2],[1,\"p\",[[0,[],0,\"Enough about the app, let's take a look at how it works, under the hood.\"]]],[10,3],[1,\"h2\",[[0,[],0,\"The Legal stuff\"]]],[1,\"p\",[[0,[],0,\"I want to make this very clear, this does not go against the terms of service, as nothing malicious was intended, I did not send requests to the application, merely peeked in to the network traffic the application was sending.\"]]],[1,\"p\",[[0,[],0,\"Below is the part of the ToS' that could apply to this post, how ever for legal reasons, I must specify that there was no tampering as this is network traffic one can see when running an SSL proxy on their network. \"]]],[1,\"blockquote\",[[0,[],0,\"Tamper or attempt to tamper with the proper working of the Application, interfere with access to the Application or circumvent any measures we may use to block or restrict access to the Services;\"]]],[1,\"p\",[[0,[],0,\"Everything I say below is purely speculative, I do not work for, nor ever have worked for the BeReal Company/ Corporation. \"]]],[1,\"p\",[[0,[],0,\"For any legal issues, please contact legal@\"]]],[10,4],[1,\"h2\",[[0,[],0,\"The cloud Provider\"]]],[1,\"p\",[[0,[],0,\"From what I can see, BeReal makes extensive use of the below cloud providers\"]]],[1,\"h3\",[[0,[],0,\"DataDog\"]]],[1,\"p\",[[0,[],0,\"DataDog is being used for in-app metrics, as well as logging and something called RUM (Real user monitoring) - Which looks in to how the application is used by the user\"]]],[1,\"h3\",[[0,[],0,\"OneSignal\"]]],[1,\"p\",[[0,[],0,\"One\"],[0,[2],1,\"S\"],[0,[],0,\"ignal is being used for the the push notifications that users all over the world receive. I'm not sure about how Time Zones work, so don't ask me.\"]]],[1,\"h3\",[[0,[],0,\"Amplitude\"]]],[1,\"p\",[[0,[],0,\"I'm still not 100% sure on this one, but from their \"],[0,[3],1,\"website\"],[0,[],0,\" - They seem to be a user journey tracking tool, so how you signed up, and also surfacing better metrics. \"]]],[1,\"h3\",[[0,[],0,\"Google Cloud\"]]],[1,\"p\",[[0,[],0,\"BeReal makes \"],[0,[2],1,\"heavy\"],[0,[],0,\" use of the google cloud \"],[0,[2],1,\"global architecture\"],[0,[],0,\". \"],[1,[],0,0],[0,[],0,\"A good example of this is they use serverless where ever possible, as well as managed services. \"]]],[10,5],[1,\"h2\",[[0,[],0,\"The API's\"]]],[1,\"p\",[[0,[],0,\"The application is pretty bare bones, but the way it makes the API calls is... interesting. \"]]],[1,\"blockquote\",[[0,[],0,\"This blog post has taken over 2 months to write, and since starting they have implemented Certificate Pinning, preventing the Network wide proxy from intercepting the app traffic. \"]]],[1,\"p\",[[0,[],0,\"BeReal uses a GCS bucket called \"],[0,[4],1,\"storage.bere.al\"],[0,[],0,\" running on GCS. This bucket is behind a Global Load balancer on Google cloud. \"]]],[1,\"p\",[[0,[],0,\"We can try and run a gsutil ls gs://storage.bere.al but will get an IAM error\"]]],[10,6],[1,\"p\",[[0,[],0,\"So it seems the DevOps team are on top of the Bucket IAM permissions. \"]]],[1,\"p\",[[0,[],0,\"When a user takes a photo, (this is after the notification goes off), BeReal app sends a Post to \"],[0,[4],0,\"/\"],[0,[2],2,\"sendCaptureInProgressPush\"],[0,[2],1,\" \"],[0,[],0,\"which from what I can tell, publishes a message on Pubsub:\"]]],[10,7],[1,\"p\",[[0,[],0,\"And the response being\"]]],[10,8],[1,\"p\",[[0,[],0,\"This let's BeReal know that there will be an upload for the user. \"]]],[1,\"p\",[[0,[],0,\"When the user has finally taken the photo, it calls to \"],[0,[4],1,\"content/post\"],[0,[],0,\" API, and sends the below:\"]]],[10,9],[1,\"p\",[[0,[],0,\"Something of note here, is the way that BeReal stores images in a way that I can't just crawl all the Images, and download them all.\"]]],[1,\"p\",[[0,[],0,\"Something of concern here is the Precise location of a post, down to a 1M accuracy. \"]]],[10,10],[1,\"p\",[[0,[],0,\"You're wondering what we can do with this data? \"]]],[1,\"p\",[[0,[],0,\"We can scrape it for all our friends, and plot it out in Grafana or Tableau and work out where they spend a lot of time, as well as predict places of work, partners houses etc.\"]]],[1,\"p\",[[0,[5],1,\"Someone else who reverse engineered the app did something similar\"]]],[1,\"h3\",[[0,[],0,\"Let's break down the Image URL\"]]],[10,11],[1,\"p\",[[0,[4],1,\"Photos\"],[0,[],0,\" - This is the subdirectory in the Bucket that the images are stored in (This is down to how GCS and Load balancers interact with each other) \"]]],[1,\"p\",[[0,[4],1,\"<me>\"],[0,[],0,\" - This is going to be the Users Unique ID, AlphaNumeric string\"]]],[1,\"p\",[[0,[4],1,\"bereal\"],[0,[],0,\" - Not too sure why this is in here, I assume this may have something to do with where videos are also stored? \"]]],[1,\"p\",[[0,[4],1,\"7c44d6e8-086b-4a18-b8b4-d3785f58cda8\"],[0,[],0,\" - I'm not too sure about this, I would \"],[0,[0],1,\"assume \"],[0,[],0,\" that it's going to be a unique ID for that day and user to upload the image with \"]]],[1,\"p\",[[0,[4],1,\"1660122851\"],[0,[],0,\" - Linux Epoch time the image was finalized\"]]],[10,12],[1,\"p\",[]],[1,\"h3\",[[0,[],0,\"Perhaps, the most interesting API\"]]],[1,\"p\",[[0,[],0,\"One of the API's that stuck out the most was the \"],[0,[4],1,\"cloudfunctions\"],[0,[],0,\" one\"]]],[10,13],[1,\"p\",[[0,[],0,\"This is an application that runs on Google App Engine (GAE) which does some cool things:\"]]],[3,\"ul\",[[[0,[],0,\"Convert Id's to Names\"]],[[0,[],0,\"Convert Phone numbers to users\"]],[[0,[],0,\"Publishes a message to Pub/Sub \"]],[[0,[],0,\"[Deprecated] Suggest friends you may know\"]]]],[1,\"p\",[[0,[],0,\"Now this is useful especially from a data exfiltration standpoint as you are able to convert a butt load of Phone numbers to Users, and do some Osint. \"]]],[1,\"h2\",[[0,[],0,\"Public Feeds\"]]],[1,\"p\",[[0,[],0,\"The public feeds works a different way compared to your actual friends feeds.\"]]],[3,\"ol\",[[[0,[],0,\"You navigate to the feeds page\"]],[[0,[],0,\"It loads the feeds page\"]],[[0,[],0,\"It makes subsequent calls to the \"],[0,[6,4],2,\"https://us-central1-alexisbarreyat-bereal.cloudfunctions.net/getUserNames\"],[0,[],0,\" API to convert \"],[0,[4],1,\"uid\"],[0,[],0,\"'s to Usernames\"]],[[0,[],0,\"Loads the images from the storage bucket\"]]]],[10,14],[1,\"h2\",[[0,[],0,\"Closing Notes\"]]],[1,\"p\",[[0,[],0,\"There are alot more API endpoints I have not found yet, as well as don't want to mention from fear of lawyers coming at me. \"]]],[1,\"p\",[[0,[],0,\"But I have made a list of API endpoints, as well as responses etc available on my Github. \"]]],[10,15],[1,\"p\",[[0,[],0,\"BeReal seems to be cool application, and their intentions are good, but I am somewhat suspicious of what they do with the data, as I mean, they have daily coordinates of where you are and can build patterns around it.\"]]],[10,16],[1,\"h2\",[[0,[],0,\"About the Author \"]]],[1,\"p\",[[0,[],0,\"Bradley is the DevOps engineer for the biggest Pet care company in the UK, specializing in GCP as well as finding solutions for all problems. \"]]],[1,\"p\",[[0,[],0,\"His blog posts go from How to's to Infrastructure deep dives as well as reverse engineering. \"]]]],\"ghostVersion\":\"3.0\"}","lexical":null,"html":"<p>And no, this not a <em>how to,</em> on how to use the app. But a deep dive on the infrastructure behind BeReal, and the engineering decisions that were taken. </p><hr><p>Firstly, what is BeReal?</p><p>BeReal is an app designed by a French Developer called <a href=\"https://fr.linkedin.com/in/alexisbarreyat\">Alexis Barreyat</a>, which aims to break down the highly edited social media that we have come to know and hate. </p><p>You get a notification once a day, like the below, where you are urged to whip out your phone, and snap a pic.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2022/08/image.png\" class=\"kg-image\" alt=\"The BeReal notification: An iOS notificaiton bubble with the words &quot;Time to be real&quot; surrounded by two warning symbols. Below the  warning is the words &quot;2 min left to capture a BeReal and see what your friends are up to !&quot;\" loading=\"lazy\" width=\"1024\" height=\"326\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/08/image.png 600w, __GHOST_URL__/content/images/size/w1000/2022/08/image.png 1000w, __GHOST_URL__/content/images/2022/08/image.png 1024w\" sizes=\"(min-width: 720px) 720px\"><figcaption>BeReal Notification</figcaption></figure><p>The application makes use of both the front facing and back facing camera to give <em>real view</em> in to what you're up to.</p><p>See the example below:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2022/08/BeReal_example.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1500\" height=\"2000\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/08/BeReal_example.png 600w, __GHOST_URL__/content/images/size/w1000/2022/08/BeReal_example.png 1000w, __GHOST_URL__/content/images/2022/08/BeReal_example.png 1500w\" sizes=\"(min-width: 720px) 720px\"><figcaption>BeReal, where I was on the roof of a building</figcaption></figure><p>Enough about the app, let's take a look at how it works, under the hood.</p><hr><h2 id=\"the-legal-stuff\">The Legal stuff</h2><p>I want to make this very clear, this does not go against the terms of service, as nothing malicious was intended, I did not send requests to the application, merely peeked in to the network traffic the application was sending.</p><p>Below is the part of the ToS' that could apply to this post, how ever for legal reasons, I must specify that there was no tampering as this is network traffic one can see when running an SSL proxy on their network. </p><blockquote>Tamper or attempt to tamper with the proper working of the Application, interfere with access to the Application or circumvent any measures we may use to block or restrict access to the Services;</blockquote><p>Everything I say below is purely speculative, I do not work for, nor ever have worked for the BeReal Company/ Corporation. </p><p>For any legal issues, please contact legal@</p><hr><h2 id=\"the-cloud-provider\">The cloud Provider</h2><p>From what I can see, BeReal makes extensive use of the below cloud providers</p><h3 id=\"datadog\">DataDog</h3><p>DataDog is being used for in-app metrics, as well as logging and something called RUM (Real user monitoring) - Which looks in to how the application is used by the user</p><h3 id=\"onesignal\">OneSignal</h3><p>One<strong>S</strong>ignal is being used for the the push notifications that users all over the world receive. I'm not sure about how Time Zones work, so don't ask me.</p><h3 id=\"amplitude\">Amplitude</h3><p>I'm still not 100% sure on this one, but from their <a href=\"https://amplitude.com\">website</a> - They seem to be a user journey tracking tool, so how you signed up, and also surfacing better metrics. </p><h3 id=\"google-cloud\">Google Cloud</h3><p>BeReal makes <strong>heavy</strong> use of the google cloud <strong>global architecture</strong>. <br>A good example of this is they use serverless where ever possible, as well as managed services. </p><hr><h2 id=\"the-api-s\">The API's</h2><p>The application is pretty bare bones, but the way it makes the API calls is... interesting. </p><blockquote>This blog post has taken over 2 months to write, and since starting they have implemented Certificate Pinning, preventing the Network wide proxy from intercepting the app traffic. </blockquote><p>BeReal uses a GCS bucket called <code>storage.bere.al</code> running on GCS. This bucket is behind a Global Load balancer on Google cloud. </p><p>We can try and run a gsutil ls gs://storage.bere.al but will get an IAM error</p><pre><code>➜ gsutil ls gs://storage.bere.al/Photos/                                 \n\nAccessDeniedException: 403 &lt;&gt;k does not have storage.objects.list access to the Google Cloud Storage bucket.</code></pre><p>So it seems the DevOps team are on top of the Bucket IAM permissions. </p><p>When a user takes a photo, (this is after the notification goes off), BeReal app sends a Post to <code>/<strong>sendCaptureInProgressPush</strong></code><strong> </strong>which from what I can tell, publishes a message on Pubsub:</p><pre><code>{\n    \"data\": {\n        \"photoURL\": \"Photos/&lt;me&gt;/profile/&lt;me&gt;-1655905537-profile-picture.jpg\",\n        \"topic\": \"&lt;me&gt;\",\n        \"userName\": \"&lt;me&gt;\"\n    }\n}</code></pre><p>And the response being</p><pre><code>{\n    \"result\":\"projects/alexisbarreyatbereal/messages/7517087177659076139\"\n}</code></pre><p>This let's BeReal know that there will be an upload for the user. </p><p>When the user has finally taken the photo, it calls to <code>content/post</code> API, and sends the below:</p><pre><code>{\n    \"backCamera\": {\n        \"bucket\": \"storage.bere.al\",\n        \"height\": 2000,\n        \"path\": \"Photos/&lt;me&gt;/bereal/7c44d6e8-086b-4a18-b8b4d3785f58cda8-1660122851.jpg\",\n        \"width\": 1500\n    },\n    \"frontCamera\": {\n        \"bucket\": \"storage.bere.al\",\n        \"height\": 2000,\n        \"path\": \"Photos/&lt;me&gt;/bereal/7c44d6e8-086b-4a18-b8b4d3785f58cda8-1660122851-secondary.jpg\",\n        \"width\": 1500\n    },\n    \"isLate\": true,\n    \"isPublic\": false,\n    \"location\": {\n        \"latitude\": &lt;&gt;&gt;,\n        \"longitude\": &lt;&gt;&gt;\n    },\n    \"retakeCounter\": 4,\n    \"takenAt\": \"2022-08-10T09:14:11Z\"\n}</code></pre><p>Something of note here, is the way that BeReal stores images in a way that I can't just crawl all the Images, and download them all.</p><p>Something of concern here is the Precise location of a post, down to a 1M accuracy. </p><pre><code>\"location\": {\n        \"latitude\": &lt;&gt;&gt;,\n        \"longitude\": &lt;&gt;&gt;\n },</code></pre><p>You're wondering what we can do with this data? </p><p>We can scrape it for all our friends, and plot it out in Grafana or Tableau and work out where they spend a lot of time, as well as predict places of work, partners houses etc.</p><p><a href=\"https://shomil.me/bereal/#:~:text=Now%20one%20might,people%E2%80%99s%20homes%3F%20Workplaces%3F\">Someone else who reverse engineered the app did something similar</a></p><h3 id=\"let-s-break-down-the-image-url\">Let's break down the Image URL</h3><figure class=\"kg-card kg-code-card\"><pre><code class=\"language-text\">Photos/&lt;me&gt;/bereal/7c44d6e8-086b-4a18-b8b4-d3785f58cda8-1660122851.jpg</code></pre><figcaption>Image URL</figcaption></figure><p><code>Photos</code> - This is the subdirectory in the Bucket that the images are stored in (This is down to how GCS and Load balancers interact with each other) </p><p><code>&lt;me&gt;</code> - This is going to be the Users Unique ID, AlphaNumeric string</p><p><code>bereal</code> - Not too sure why this is in here, I assume this may have something to do with where videos are also stored? </p><p><code>7c44d6e8-086b-4a18-b8b4-d3785f58cda8</code> - I'm not too sure about this, I would <em>assume </em> that it's going to be a unique ID for that day and user to upload the image with </p><p><code>1660122851</code> - Linux Epoch time the image was finalized</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/08/image-1.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"763\" height=\"622\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/08/image-1.png 600w, __GHOST_URL__/content/images/2022/08/image-1.png 763w\" sizes=\"(min-width: 720px) 720px\"></figure><p></p><h3 id=\"perhaps-the-most-interesting-api\">Perhaps, the most interesting API</h3><p>One of the API's that stuck out the most was the <code>cloudfunctions</code> one</p><pre><code>https://us-central1-alexisbarreyat-bereal.cloudfunctions.net</code></pre><p>This is an application that runs on Google App Engine (GAE) which does some cool things:</p><ul><li>Convert Id's to Names</li><li>Convert Phone numbers to users</li><li>Publishes a message to Pub/Sub </li><li>[Deprecated] Suggest friends you may know</li></ul><p>Now this is useful especially from a data exfiltration standpoint as you are able to convert a butt load of Phone numbers to Users, and do some Osint. </p><h2 id=\"public-feeds\">Public Feeds</h2><p>The public feeds works a different way compared to your actual friends feeds.</p><ol><li>You navigate to the feeds page</li><li>It loads the feeds page</li><li>It makes subsequent calls to the <a href=\"https://us-central1-alexisbarreyat-bereal.cloudfunctions.net/getUserNames\"><code>https://us-central1-alexisbarreyat-bereal.cloudfunctions.net/getUserNames</code></a> API to convert <code>uid</code>'s to Usernames</li><li>Loads the images from the storage bucket</li></ol><hr><h2 id=\"closing-notes\">Closing Notes</h2><p>There are alot more API endpoints I have not found yet, as well as don't want to mention from fear of lawyers coming at me. </p><p>But I have made a list of API endpoints, as well as responses etc available on my Github. </p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://github.com/userbradley/beReal\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">GitHub - userbradley/BeReal: How does BeReal work (Under the hood)</div><div class=\"kg-bookmark-description\">How does BeReal work (Under the hood). Contribute to userbradley/BeReal development by creating an account on GitHub.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://github.githubassets.com/favicons/favicon.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">userbradley</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://opengraph.githubassets.com/008e444a696cd7cde3b7e5b5219f89fafbcd509bee2c1277c4172ed25bc277d3/userbradley/BeReal\" alt=\"\"></div></a></figure><p>BeReal seems to be cool application, and their intentions are good, but I am somewhat suspicious of what they do with the data, as I mean, they have daily coordinates of where you are and can build patterns around it.</p><hr><h2 id=\"about-the-author\">About the Author </h2><p>Bradley is the DevOps engineer for the biggest Pet care company in the UK, specializing in GCP as well as finding solutions for all problems. </p><p>His blog posts go from How to's to Infrastructure deep dives as well as reverse engineering. </p>","comment_id":"63069fdd17e5727444dc7b74","plaintext":"And no, this not a how to, on how to use the app. But a deep dive on the infrastructure behind BeReal, and the engineering decisions that were taken.\n\nFirstly, what is BeReal?\n\nBeReal is an app designed by a French Developer called Alexis Barreyat, which aims to break down the highly edited social media that we have come to know and hate.\n\nYou get a notification once a day, like the below, where you are urged to whip out your phone, and snap a pic.\n\nThe application makes use of both the front facing and back facing camera to give real view in to what you're up to.\n\nSee the example below:\n\nEnough about the app, let's take a look at how it works, under the hood.\n\n\nThe Legal stuff\n\nI want to make this very clear, this does not go against the terms of service, as nothing malicious was intended, I did not send requests to the application, merely peeked in to the network traffic the application was sending.\n\nBelow is the part of the ToS' that could apply to this post, how ever for legal reasons, I must specify that there was no tampering as this is network traffic one can see when running an SSL proxy on their network.\n\nTamper or attempt to tamper with the proper working of the Application, interfere with access to the Application or circumvent any measures we may use to block or restrict access to the Services;\n\nEverything I say below is purely speculative, I do not work for, nor ever have worked for the BeReal Company/ Corporation.\n\nFor any legal issues, please contact legal@\n\n\nThe cloud Provider\n\nFrom what I can see, BeReal makes extensive use of the below cloud providers\n\n\nDataDog\n\nDataDog is being used for in-app metrics, as well as logging and something called RUM (Real user monitoring) - Which looks in to how the application is used by the user\n\n\nOneSignal\n\nOneSignal is being used for the the push notifications that users all over the world receive. I'm not sure about how Time Zones work, so don't ask me.\n\n\nAmplitude\n\nI'm still not 100% sure on this one, but from their website - They seem to be a user journey tracking tool, so how you signed up, and also surfacing better metrics.\n\n\nGoogle Cloud\n\nBeReal makes heavy use of the google cloud global architecture.\nA good example of this is they use serverless where ever possible, as well as managed services.\n\n\nThe API's\n\nThe application is pretty bare bones, but the way it makes the API calls is... interesting.\n\nThis blog post has taken over 2 months to write, and since starting they have implemented Certificate Pinning, preventing the Network wide proxy from intercepting the app traffic.\n\nBeReal uses a GCS bucket called storage.bere.al running on GCS. This bucket is behind a Global Load balancer on Google cloud.\n\nWe can try and run a gsutil ls gs://storage.bere.al but will get an IAM error\n\n➜ gsutil ls gs://storage.bere.al/Photos/                                 \n\nAccessDeniedException: 403 <>k does not have storage.objects.list access to the Google Cloud Storage bucket.\n\nSo it seems the DevOps team are on top of the Bucket IAM permissions.\n\nWhen a user takes a photo, (this is after the notification goes off), BeReal app sends a Post to /sendCaptureInProgressPush which from what I can tell, publishes a message on Pubsub:\n\n{\n    \"data\": {\n        \"photoURL\": \"Photos/<me>/profile/<me>-1655905537-profile-picture.jpg\",\n        \"topic\": \"<me>\",\n        \"userName\": \"<me>\"\n    }\n}\n\nAnd the response being\n\n{\n    \"result\":\"projects/alexisbarreyatbereal/messages/7517087177659076139\"\n}\n\nThis let's BeReal know that there will be an upload for the user.\n\nWhen the user has finally taken the photo, it calls to content/post API, and sends the below:\n\n{\n    \"backCamera\": {\n        \"bucket\": \"storage.bere.al\",\n        \"height\": 2000,\n        \"path\": \"Photos/<me>/bereal/7c44d6e8-086b-4a18-b8b4d3785f58cda8-1660122851.jpg\",\n        \"width\": 1500\n    },\n    \"frontCamera\": {\n        \"bucket\": \"storage.bere.al\",\n        \"height\": 2000,\n        \"path\": \"Photos/<me>/bereal/7c44d6e8-086b-4a18-b8b4d3785f58cda8-1660122851-secondary.jpg\",\n        \"width\": 1500\n    },\n    \"isLate\": true,\n    \"isPublic\": false,\n    \"location\": {\n        \"latitude\": <>>,\n        \"longitude\": <>>\n    },\n    \"retakeCounter\": 4,\n    \"takenAt\": \"2022-08-10T09:14:11Z\"\n}\n\nSomething of note here, is the way that BeReal stores images in a way that I can't just crawl all the Images, and download them all.\n\nSomething of concern here is the Precise location of a post, down to a 1M accuracy.\n\n\"location\": {\n        \"latitude\": <>>,\n        \"longitude\": <>>\n },\n\nYou're wondering what we can do with this data?\n\nWe can scrape it for all our friends, and plot it out in Grafana or Tableau and work out where they spend a lot of time, as well as predict places of work, partners houses etc.\n\nSomeone else who reverse engineered the app did something similar\n\n\nLet's break down the Image URL\n\nPhotos/<me>/bereal/7c44d6e8-086b-4a18-b8b4-d3785f58cda8-1660122851.jpg\n\nPhotos - This is the subdirectory in the Bucket that the images are stored in (This is down to how GCS and Load balancers interact with each other)\n\n<me> - This is going to be the Users Unique ID, AlphaNumeric string\n\nbereal - Not too sure why this is in here, I assume this may have something to do with where videos are also stored?\n\n7c44d6e8-086b-4a18-b8b4-d3785f58cda8 - I'm not too sure about this, I would assume that it's going to be a unique ID for that day and user to upload the image with\n\n1660122851 - Linux Epoch time the image was finalized\n\n\n\n\nPerhaps, the most interesting API\n\nOne of the API's that stuck out the most was the cloudfunctions one\n\nhttps://us-central1-alexisbarreyat-bereal.cloudfunctions.net\n\nThis is an application that runs on Google App Engine (GAE) which does some cool things:\n\n * Convert Id's to Names\n * Convert Phone numbers to users\n * Publishes a message to Pub/Sub\n * [Deprecated] Suggest friends you may know\n\nNow this is useful especially from a data exfiltration standpoint as you are able to convert a butt load of Phone numbers to Users, and do some Osint.\n\n\nPublic Feeds\n\nThe public feeds works a different way compared to your actual friends feeds.\n\n 1. You navigate to the feeds page\n 2. It loads the feeds page\n 3. It makes subsequent calls to the https://us-central1-alexisbarreyat-bereal.cloudfunctions.net/getUserNames API to convert uid's to Usernames\n 4. Loads the images from the storage bucket\n\n\nClosing Notes\n\nThere are alot more API endpoints I have not found yet, as well as don't want to mention from fear of lawyers coming at me.\n\nBut I have made a list of API endpoints, as well as responses etc available on my Github.\n\nGitHub - userbradley/BeReal: How does BeReal work (Under the hood)How does BeReal work (Under the hood). Contribute to userbradley/BeReal development by creating an account on GitHub.GitHubuserbradley\n\nBeReal seems to be cool application, and their intentions are good, but I am somewhat suspicious of what they do with the data, as I mean, they have daily coordinates of where you are and can build patterns around it.\n\n\nAbout the Author\n\nBradley is the DevOps engineer for the biggest Pet care company in the UK, specializing in GCP as well as finding solutions for all problems.\n\nHis blog posts go from How to's to Infrastructure deep dives as well as reverse engineering.","feature_image":"__GHOST_URL__/content/images/2022/08/DSCF7592-Large-1.jpeg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2022-08-24T22:02:05.000Z","updated_at":"2022-08-25T00:33:47.000Z","published_at":"2022-08-18T10:00:00.000Z","custom_excerpt":"Today we take a deep dive on how the BeReal app works, behind the Scenes","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"6425f3bc192c0c150413d5ad","uuid":"42f758f0-2472-4da3-9362-cd2803c6eb6e","title":"Migrating off of Bookstack","slug":"migrating-off-bookstack","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}]],\"cards\":[[\"hr\",{}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/09/image.png\",\"width\":941,\"height\":712}],[\"bookmark\",{\"type\":\"bookmark\",\"url\":\"https://www.bookstackapp.com\",\"metadata\":{\"url\":\"https://www.bookstackapp.com\",\"title\":\"BookStack\",\"description\":\"BookStack is a simple, open-source, self-hosted, easy-to-use platform for organising and storing information.\",\"author\":null,\"publisher\":\"BookStack\",\"thumbnail\":\"https://www.bookstackapp.com\",\"icon\":\"https://www.bookstackapp.com/images/favicon-196x196.png\"}}],[\"code\",{\"code\":\"MariaDB [bookstack]> select count(slug) as \\\"current pages\\\" from pages;\\n+---------------+\\n| current pages |\\n+---------------+\\n|           125 |\\n+---------------+\\n1 row in set (0.001 sec)\"}],[\"bookmark\",{\"url\":\"https://documentation.breadnet.co.uk\",\"metadata\":{\"url\":\"https://documentation.breadnet.co.uk\",\"title\":\"Welcome - breadNET Documentation\",\"description\":\"breadNET Public Documentation\",\"author\":null,\"publisher\":\"logo\",\"thumbnail\":\"https://documentation.breadnet.co.uk/assets/pipeline.png\",\"icon\":\"__GHOST_URL__/favicon.ico\"}}],[\"hr\",{}],[\"bookmark\",{\"url\":\"https://www.mkdocs.org\",\"metadata\":{\"url\":\"https://www.mkdocs.org\",\"title\":\"MkDocs\",\"description\":\"Project documentation with Markdown.\",\"author\":\"MkDocs Team\",\"publisher\":\"MkDocs\",\"thumbnail\":null,\"icon\":\"https://www.mkdocs.org/img/favicon.ico\"}}],[\"bookmark\",{\"url\":\"https://squidfunk.github.io/mkdocs-material/\",\"metadata\":{\"url\":\"https://squidfunk.github.io/mkdocs-material/\",\"title\":\"Material for MkDocs\",\"description\":\"Write your documentation in Markdown and create a professional static site in minutes – searchable, customizable, for all devices\",\"author\":\"Martin Donath\",\"publisher\":null,\"thumbnail\":\"https://squidfunk.github.io/mkdocs-material/assets/images/social/index.png\",\"icon\":\"https://squidfunk.github.io/mkdocs-material/assets/favicon.png\"}}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/09/image-1.png\",\"width\":612,\"height\":526}],[\"code\",{\"code\":\"nav:\\n  - Home: index.md\\n  - KB Articles:\\n      - Disk management:\\n        - Expanding a file system: kb/disk-management/expanding-a-filesystem.md\\n        - Formatting drive and Auto mount: kb/disk-management/formatting-drive-automount.md\\n        - Mount a new drive: kb/disk-management/mount-a-new-drive.md\\n        - \\\"GPT PMBR Size Mismatch will be corrected by w(rite)\\\" : kb/disk-management/gpt-pmbr-size-mismatch-will-be-corrected-by-write.md\\n      - Minio:\\n          - OLD : kb/minio/old.md\\n          - Connecting to minio over s3fs: kb/minio/minio-over-s3fs.md\\n          - creating users and assigning policies: kb/minio/s3-policies.md\\n      - PHP:\\n          - Install PHP: kb/php/install-php.md\\n          - Wordpress permissions: kb/php/wordpress-permissions.md\\n      - Docker:\\n          - Installing Docker: kb/docker/installing-docker.md\\n          - Basics of docker: kb/docker/basics-of-docker.md\\n          - Installing jellyfin: kb/docker/installing-jellyfin.md\\n          - \\\"Docker: Intro and notes\\\": kb/docker/docker-intro-and-notes.md\\n          - Exporting and importing: kb/docker/exporting-and-importing.md\\n          - Docker Architecture: kb/docker/docker-architecture.md\\n          - Bulk retag: kb/docker/bulk-retag.md\",\"language\":\"yaml\"}],[\"code\",{\"code\":\"theme:\\n  name: material\",\"language\":\"yaml\"}],[\"hr\",{}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/09/image-2.png\",\"width\":2525,\"height\":1345,\"caption\":\"Dev (dev-documentation.breadnet.co.uk)\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/09/image-3.png\",\"width\":2525,\"height\":1345,\"caption\":\"Production (documentation.breadnet.co.uk)\"}],[\"code\",{\"code\":\"site_name: !ENV [env,\\\"breadNET Documentation\\\"]\"}],[\"hr\",{}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/09/image-4.png\",\"width\":2195,\"height\":413}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/09/image-5.png\",\"width\":1225,\"height\":808}],[\"hr\",{}],[\"bookmark\",{\"url\":\"https://bradley.breadnet.co.uk\",\"metadata\":{\"url\":\"https://bradley.breadnet.co.uk\",\"title\":\"About me - Portfolio\",\"description\":null,\"author\":null,\"publisher\":\"logo\",\"thumbnail\":\"https://img.shields.io/badge/GoogleCloud-%234285F4.svg?style=for-the-badge&logo=google-cloud&logoColor=white\",\"icon\":\"__GHOST_URL__/favicon.ico\"}}],[\"bookmark\",{\"url\":\"https://documentation.breadnet.co.uk\",\"metadata\":{\"url\":\"https://documentation.breadnet.co.uk\",\"title\":\"Welcome - breadNET Documentation\",\"description\":\"breadNET Public Documentation\",\"author\":null,\"publisher\":\"logo\",\"thumbnail\":\"https://documentation.breadnet.co.uk/assets/pipeline.png\",\"icon\":\"__GHOST_URL__/favicon.ico\"}}]],\"markups\":[[\"code\"],[\"a\",[\"href\",\"https://www.bookstackapp.com/blog/10k-stars-and-a-look-back-at-first-sharing/\"]],[\"em\"]],\"sections\":[[1,\"p\",[[0,[],0,\"Organizing documentation is hard.\"]]],[1,\"p\",[[0,[],0,\"I'll start the post off with that.\"]]],[10,0],[1,\"p\",[[0,[],0,\"Since I started my home lab back in 2018, I knew I needed a place to write down important information like the server name, where it's located and all the other stuff that goes along with that - Example below:\"]]],[10,1],[1,\"p\",[[0,[],0,\"For a long time, that was bookstack. One of my favourite pieces of open source software. \"]]],[1,\"p\",[[0,[],0,\"For those of you who aren't familiar with BookStack:\"]]],[3,\"ul\",[[[0,[],0,\"Self Hosted Wiki\"]],[[0,[0],1,\"wysiwyg\"],[0,[],0,\" and \"],[0,[0],1,\"markdown\"],[0,[],0,\" support\"]],[[0,[],0,\"Supports LDAP, AD and other integrations\"]],[[0,[],0,\"Built in PHP\"]],[[0,[],0,\"Used MariaDB as a database to store content\"]],[[0,[],0,\"Full API\"]],[[0,[1],1,\"Recently hit 10k Stars\"]]]],[1,\"p\",[[0,[],0,\"For more information on BookStack, you can visit the site:\"]]],[10,2],[1,\"p\",[[0,[],0,\"At the time of writing, I currently have 125 pages active:\"]]],[10,3],[1,\"p\",[[0,[],0,\"I will add, that using BookStack has been amazing for my documentation writing skills. So much so, I have free-lanced simply being on calls with engineers, taking their \"],[0,[2],1,\"engineer\"],[0,[],0,\" speak, and turning it into documents that the average Joe can understand.  \"]]],[1,\"h2\",[[0,[],0,\"I speak so highly of BookStack... Why migrate?\"]]],[1,\"p\",[[0,[],0,\"It's come to the point where the overhead of running BookStack is more than I need at the moment.\"]]],[1,\"p\",[[0,[],0,\"The way I see it is I will be making more specialized sites, and lots of them, opposed to clustering them all in to one site. \"]]],[1,\"p\",[[0,[],0,\"A good example of this would be the new site, as well a new undertaking I have started:\"]]],[10,4],[1,\"p\",[[0,[],0,\"The top one is my new home for documentation (Which I will touch on in a bit)\"],[1,[],0,0],[0,[],0,\"Where as the second on is a highly specialized site, that serves one purpose: To explain Kubernetes manifests.\"]]],[1,\"h2\",[[0,[],0,\"The reason for migrating\"]]],[1,\"p\",[[0,[],0,\"As I mentioned, I have started building more specialized sites, but never answered why.\"]]],[1,\"p\",[[0,[],0,\"My job's have changed so much since I started. I went from Junior network engineer to sole DevOps engineer rolling out GKE to a national retailer in less time than it takes to get a \"],[0,[0],1,\"Veterinary Medicine BVM&S\"],[0,[],0,\" Degree (5 years) and naturally so has the type of stuff I am doing in my free time.\"]]],[1,\"p\",[[0,[],0,\"I've done from configuring cisco switches, to writing code to deploy a Google Kubernetes cluster with a Highly available app in 15 minutes from start to finish.\"]]],[1,\"p\",[[0,[],0,\"Enough chit-chat:\"]]],[3,\"ol\",[[[0,[],0,\"Documentation software does not need to be so complex\"]],[[0,[],0,\"Less overhead (Does not need a database)\"]],[[0,[],0,\"Lower barrier to entry (Just know markdown)\"]],[[0,[],0,\"Documentation is markdown files\"]],[[0,[],0,\"Source control (Easy roll-back)\"]],[[0,[],0,\"No security issues\"]]]],[10,5],[1,\"h2\",[[0,[],0,\"Platform of choice\"]]],[1,\"p\",[[0,[],0,\"I don't know if I would call it a \"],[0,[2],1,\"platform\"],[0,[],0,\" necisarrly, but I chose to go with; mkdocs, with Material Theme.\"]]],[10,6],[10,7],[1,\"p\",[[0,[],0,\"The way mkocs works, is:\"]]],[3,\"ol\",[[[0,[],0,\"You write your site in markdown documents\"]]]],[10,8],[1,\"p\",[[0,[],0,\"2. You tell mkdocs how the site navigation should look like\"]]],[10,9],[1,\"p\",[[0,[],0,\"3. You tell mkdocs what theme to use (This is comparable to a theme in wordpress, or ghost)\"]]],[10,10],[1,\"p\",[[0,[],0,\"4. You build and then host it on an nginx server, or Apache.\"]]],[1,\"p\",[[0,[],0,\"The nice part about mkdocs is it's simple, if you can write markdown, you can build a good documentation site. \"]]],[10,11],[1,\"h1\",[[0,[],0,\"Pipelines\"]]],[1,\"p\",[[0,[],0,\"Seeing as the site is just being built on demand, we can make a code pipeline to deploy on certain actions.\"]]],[1,\"p\",[[0,[],0,\"I have mine deploy on a push to \"],[0,[0],1,\"dev\"],[0,[],0,\" branch which goes to a development site (dev-documentation.breadnet.co.uk) and then once you open a PR and merge to master, it deploys the site to production.\"]]],[1,\"p\",[[0,[],0,\"As you can see, there is a stark difference between Dev:\"]]],[10,12],[1,\"p\",[[0,[],0,\"And Production\"]]],[10,13],[1,\"p\",[[0,[],0,\"This is achieved in mkdocs' flexible nature to use Environment variables on build\"]]],[10,14],[1,\"p\",[[0,[],0,\"The above sets the site name, so what you see at the top left.\"]]],[1,\"p\",[[0,[],0,\"What happens here is on build, mkdocs checks what the value for \"],[0,[0],1,\"env\"],[0,[],0,\" is, and if it's \"],[0,[0],1,\"null\"],[0,[],0,\" (so not set), it will default to \"],[0,[0],1,\"breadNET Documentation\"]]],[1,\"p\",[[0,[],0,\"It's the same for the color and anything else I want to set.\"]]],[10,15],[1,\"h1\",[[0,[],0,\"The Code pipeline\"]]],[1,\"p\",[[0,[],0,\"At the time of writing, this is how the pipeline looks\"]]],[10,16],[3,\"ol\",[[[0,[],0,\"git clone\"]],[[0,[],0,\"If on dev, set vars\"]],[[0,[],0,\"If on dev, set Robots.txt\"]],[[0,[],0,\"Build site\"]],[[0,[],0,\"If on dev, copy to dev server\"]],[[0,[],0,\"If on master, copy to prod server.\"]]]],[1,\"p\",[[0,[],0,\"The nice thing is the site is built on each commit, so you can see what will be going in to production.\"]]],[10,17],[10,18],[1,\"h2\",[[0,[],0,\"Closing notes\"]]],[1,\"p\",[[0,[],0,\"I will miss BookStack. It's been one of the best pieces of software that I have used. It's opened my eyes to what open source has to offer, as well as teaching me ansible!\"]]],[1,\"p\",[[0,[],0,\"I don't know if I will run BookStack again -  However it's my go-to application for testing things. So at some point I will release a helm chart for BookStack as I learn helm more. \"]]],[1,\"p\",[[0,[],0,\"As always, a link to all the \"],[0,[2],1,\"\\\"micro sites\\\"\"],[0,[],0,\" I run\"]]],[10,19],[10,20],[1,\"p\",[]]],\"ghostVersion\":\"3.0\"}","lexical":null,"html":"<p>Organizing documentation is hard.</p><p>I'll start the post off with that.</p><hr><p>Since I started my home lab back in 2018, I knew I needed a place to write down important information like the server name, where it's located and all the other stuff that goes along with that - Example below:</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/09/image.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"941\" height=\"712\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/09/image.png 600w, __GHOST_URL__/content/images/2022/09/image.png 941w\" sizes=\"(min-width: 720px) 720px\"></figure><p>For a long time, that was bookstack. One of my favourite pieces of open source software. </p><p>For those of you who aren't familiar with BookStack:</p><ul><li>Self Hosted Wiki</li><li><code>wysiwyg</code> and <code>markdown</code> support</li><li>Supports LDAP, AD and other integrations</li><li>Built in PHP</li><li>Used MariaDB as a database to store content</li><li>Full API</li><li><a href=\"https://www.bookstackapp.com/blog/10k-stars-and-a-look-back-at-first-sharing/\">Recently hit 10k Stars</a></li></ul><p>For more information on BookStack, you can visit the site:</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://www.bookstackapp.com\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">BookStack</div><div class=\"kg-bookmark-description\">BookStack is a simple, open-source, self-hosted, easy-to-use platform for organising and storing information.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://www.bookstackapp.com/images/favicon-196x196.png\" alt=\"\"><span class=\"kg-bookmark-author\">BookStack</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://www.bookstackapp.com\" alt=\"\"></div></a></figure><p>At the time of writing, I currently have 125 pages active:</p><pre><code>MariaDB [bookstack]&gt; select count(slug) as \"current pages\" from pages;\n+---------------+\n| current pages |\n+---------------+\n|           125 |\n+---------------+\n1 row in set (0.001 sec)</code></pre><p>I will add, that using BookStack has been amazing for my documentation writing skills. So much so, I have free-lanced simply being on calls with engineers, taking their <em>engineer</em> speak, and turning it into documents that the average Joe can understand.  </p><h2 id=\"i-speak-so-highly-of-bookstack-why-migrate\">I speak so highly of BookStack... Why migrate?</h2><p>It's come to the point where the overhead of running BookStack is more than I need at the moment.</p><p>The way I see it is I will be making more specialized sites, and lots of them, opposed to clustering them all in to one site. </p><p>A good example of this would be the new site, as well a new undertaking I have started:</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://documentation.breadnet.co.uk\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Welcome - breadNET Documentation</div><div class=\"kg-bookmark-description\">breadNET Public Documentation</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"__GHOST_URL__/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">logo</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://documentation.breadnet.co.uk/assets/pipeline.png\" alt=\"\"></div></a></figure><p>The top one is my new home for documentation (Which I will touch on in a bit)<br>Where as the second on is a highly specialized site, that serves one purpose: To explain Kubernetes manifests.</p><h2 id=\"the-reason-for-migrating\">The reason for migrating</h2><p>As I mentioned, I have started building more specialized sites, but never answered why.</p><p>My job's have changed so much since I started. I went from Junior network engineer to sole DevOps engineer rolling out GKE to a national retailer in less time than it takes to get a <code>Veterinary Medicine BVM&amp;S</code> Degree (5 years) and naturally so has the type of stuff I am doing in my free time.</p><p>I've done from configuring cisco switches, to writing code to deploy a Google Kubernetes cluster with a Highly available app in 15 minutes from start to finish.</p><p>Enough chit-chat:</p><ol><li>Documentation software does not need to be so complex</li><li>Less overhead (Does not need a database)</li><li>Lower barrier to entry (Just know markdown)</li><li>Documentation is markdown files</li><li>Source control (Easy roll-back)</li><li>No security issues</li></ol><hr><h2 id=\"platform-of-choice\">Platform of choice</h2><p>I don't know if I would call it a <em>platform</em> necisarrly, but I chose to go with; mkdocs, with Material Theme.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://www.mkdocs.org\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">MkDocs</div><div class=\"kg-bookmark-description\">Project documentation with Markdown.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://www.mkdocs.org/img/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">MkDocs</span><span class=\"kg-bookmark-publisher\">MkDocs Team</span></div></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://squidfunk.github.io/mkdocs-material/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Material for MkDocs</div><div class=\"kg-bookmark-description\">Write your documentation in Markdown and create a professional static site in minutes – searchable, customizable, for all devices</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://squidfunk.github.io/mkdocs-material/assets/favicon.png\" alt=\"\"><span class=\"kg-bookmark-publisher\">Martin Donath</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://squidfunk.github.io/mkdocs-material/assets/images/social/index.png\" alt=\"\"></div></a></figure><p>The way mkocs works, is:</p><ol><li>You write your site in markdown documents</li></ol><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/09/image-1.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"612\" height=\"526\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/09/image-1.png 600w, __GHOST_URL__/content/images/2022/09/image-1.png 612w\"></figure><p>2. You tell mkdocs how the site navigation should look like</p><pre><code class=\"language-yaml\">nav:\n  - Home: index.md\n  - KB Articles:\n      - Disk management:\n        - Expanding a file system: kb/disk-management/expanding-a-filesystem.md\n        - Formatting drive and Auto mount: kb/disk-management/formatting-drive-automount.md\n        - Mount a new drive: kb/disk-management/mount-a-new-drive.md\n        - \"GPT PMBR Size Mismatch will be corrected by w(rite)\" : kb/disk-management/gpt-pmbr-size-mismatch-will-be-corrected-by-write.md\n      - Minio:\n          - OLD : kb/minio/old.md\n          - Connecting to minio over s3fs: kb/minio/minio-over-s3fs.md\n          - creating users and assigning policies: kb/minio/s3-policies.md\n      - PHP:\n          - Install PHP: kb/php/install-php.md\n          - Wordpress permissions: kb/php/wordpress-permissions.md\n      - Docker:\n          - Installing Docker: kb/docker/installing-docker.md\n          - Basics of docker: kb/docker/basics-of-docker.md\n          - Installing jellyfin: kb/docker/installing-jellyfin.md\n          - \"Docker: Intro and notes\": kb/docker/docker-intro-and-notes.md\n          - Exporting and importing: kb/docker/exporting-and-importing.md\n          - Docker Architecture: kb/docker/docker-architecture.md\n          - Bulk retag: kb/docker/bulk-retag.md</code></pre><p>3. You tell mkdocs what theme to use (This is comparable to a theme in wordpress, or ghost)</p><pre><code class=\"language-yaml\">theme:\n  name: material</code></pre><p>4. You build and then host it on an nginx server, or Apache.</p><p>The nice part about mkdocs is it's simple, if you can write markdown, you can build a good documentation site. </p><hr><h1 id=\"pipelines\">Pipelines</h1><p>Seeing as the site is just being built on demand, we can make a code pipeline to deploy on certain actions.</p><p>I have mine deploy on a push to <code>dev</code> branch which goes to a development site (dev-documentation.breadnet.co.uk) and then once you open a PR and merge to master, it deploys the site to production.</p><p>As you can see, there is a stark difference between Dev:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2022/09/image-2.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"1065\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/09/image-2.png 600w, __GHOST_URL__/content/images/size/w1000/2022/09/image-2.png 1000w, __GHOST_URL__/content/images/size/w1600/2022/09/image-2.png 1600w, __GHOST_URL__/content/images/size/w2400/2022/09/image-2.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption>Dev (dev-documentation.breadnet.co.uk)</figcaption></figure><p>And Production</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2022/09/image-3.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"1065\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/09/image-3.png 600w, __GHOST_URL__/content/images/size/w1000/2022/09/image-3.png 1000w, __GHOST_URL__/content/images/size/w1600/2022/09/image-3.png 1600w, __GHOST_URL__/content/images/size/w2400/2022/09/image-3.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption>Production (documentation.breadnet.co.uk)</figcaption></figure><p>This is achieved in mkdocs' flexible nature to use Environment variables on build</p><pre><code>site_name: !ENV [env,\"breadNET Documentation\"]</code></pre><p>The above sets the site name, so what you see at the top left.</p><p>What happens here is on build, mkdocs checks what the value for <code>env</code> is, and if it's <code>null</code> (so not set), it will default to <code>breadNET Documentation</code></p><p>It's the same for the color and anything else I want to set.</p><hr><h1 id=\"the-code-pipeline\">The Code pipeline</h1><p>At the time of writing, this is how the pipeline looks</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/09/image-4.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"376\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/09/image-4.png 600w, __GHOST_URL__/content/images/size/w1000/2022/09/image-4.png 1000w, __GHOST_URL__/content/images/size/w1600/2022/09/image-4.png 1600w, __GHOST_URL__/content/images/2022/09/image-4.png 2195w\" sizes=\"(min-width: 720px) 720px\"></figure><ol><li>git clone</li><li>If on dev, set vars</li><li>If on dev, set Robots.txt</li><li>Build site</li><li>If on dev, copy to dev server</li><li>If on master, copy to prod server.</li></ol><p>The nice thing is the site is built on each commit, so you can see what will be going in to production.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/09/image-5.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1225\" height=\"808\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/09/image-5.png 600w, __GHOST_URL__/content/images/size/w1000/2022/09/image-5.png 1000w, __GHOST_URL__/content/images/2022/09/image-5.png 1225w\" sizes=\"(min-width: 720px) 720px\"></figure><hr><h2 id=\"closing-notes\">Closing notes</h2><p>I will miss BookStack. It's been one of the best pieces of software that I have used. It's opened my eyes to what open source has to offer, as well as teaching me ansible!</p><p>I don't know if I will run BookStack again -  However it's my go-to application for testing things. So at some point I will release a helm chart for BookStack as I learn helm more. </p><p>As always, a link to all the <em>\"micro sites\"</em> I run</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://bradley.breadnet.co.uk\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">About me - Portfolio</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"__GHOST_URL__/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">logo</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://img.shields.io/badge/GoogleCloud-%234285F4.svg?style&#x3D;for-the-badge&amp;logo&#x3D;google-cloud&amp;logoColor&#x3D;white\" alt=\"\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://documentation.breadnet.co.uk\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Welcome - breadNET Documentation</div><div class=\"kg-bookmark-description\">breadNET Public Documentation</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"__GHOST_URL__/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">logo</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://documentation.breadnet.co.uk/assets/pipeline.png\" alt=\"\"></div></a></figure>","comment_id":"6335d8bf17e5727444dc7c44","plaintext":"Organizing documentation is hard.\n\nI'll start the post off with that.\n\nSince I started my home lab back in 2018, I knew I needed a place to write down important information like the server name, where it's located and all the other stuff that goes along with that - Example below:\n\nFor a long time, that was bookstack. One of my favourite pieces of open source software.\n\nFor those of you who aren't familiar with BookStack:\n\n * Self Hosted Wiki\n * wysiwyg and markdown support\n * Supports LDAP, AD and other integrations\n * Built in PHP\n * Used MariaDB as a database to store content\n * Full API\n * Recently hit 10k Stars\n\nFor more information on BookStack, you can visit the site:\n\nBookStackBookStack is a simple, open-source, self-hosted, easy-to-use platform for organising and storing information.BookStack\n\nAt the time of writing, I currently have 125 pages active:\n\nMariaDB [bookstack]> select count(slug) as \"current pages\" from pages;\n+---------------+\n| current pages |\n+---------------+\n|           125 |\n+---------------+\n1 row in set (0.001 sec)\n\nI will add, that using BookStack has been amazing for my documentation writing skills. So much so, I have free-lanced simply being on calls with engineers, taking their engineer speak, and turning it into documents that the average Joe can understand.  \n\n\nI speak so highly of BookStack... Why migrate?\n\nIt's come to the point where the overhead of running BookStack is more than I need at the moment.\n\nThe way I see it is I will be making more specialized sites, and lots of them, opposed to clustering them all in to one site.\n\nA good example of this would be the new site, as well a new undertaking I have started:\n\nWelcome - breadNET DocumentationbreadNET Public Documentationlogo\n\nThe top one is my new home for documentation (Which I will touch on in a bit)\nWhere as the second on is a highly specialized site, that serves one purpose: To explain Kubernetes manifests.\n\n\nThe reason for migrating\n\nAs I mentioned, I have started building more specialized sites, but never answered why.\n\nMy job's have changed so much since I started. I went from Junior network engineer to sole DevOps engineer rolling out GKE to a national retailer in less time than it takes to get a Veterinary Medicine BVM&S Degree (5 years) and naturally so has the type of stuff I am doing in my free time.\n\nI've done from configuring cisco switches, to writing code to deploy a Google Kubernetes cluster with a Highly available app in 15 minutes from start to finish.\n\nEnough chit-chat:\n\n 1. Documentation software does not need to be so complex\n 2. Less overhead (Does not need a database)\n 3. Lower barrier to entry (Just know markdown)\n 4. Documentation is markdown files\n 5. Source control (Easy roll-back)\n 6. No security issues\n\n\nPlatform of choice\n\nI don't know if I would call it a platform necisarrly, but I chose to go with; mkdocs, with Material Theme.\n\nMkDocsProject documentation with Markdown.MkDocsMkDocs TeamMaterial for MkDocsWrite your documentation in Markdown and create a professional static site in minutes – searchable, customizable, for all devicesMartin Donath\n\nThe way mkocs works, is:\n\n 1. You write your site in markdown documents\n\n2. You tell mkdocs how the site navigation should look like\n\nnav:\n  - Home: index.md\n  - KB Articles:\n      - Disk management:\n        - Expanding a file system: kb/disk-management/expanding-a-filesystem.md\n        - Formatting drive and Auto mount: kb/disk-management/formatting-drive-automount.md\n        - Mount a new drive: kb/disk-management/mount-a-new-drive.md\n        - \"GPT PMBR Size Mismatch will be corrected by w(rite)\" : kb/disk-management/gpt-pmbr-size-mismatch-will-be-corrected-by-write.md\n      - Minio:\n          - OLD : kb/minio/old.md\n          - Connecting to minio over s3fs: kb/minio/minio-over-s3fs.md\n          - creating users and assigning policies: kb/minio/s3-policies.md\n      - PHP:\n          - Install PHP: kb/php/install-php.md\n          - Wordpress permissions: kb/php/wordpress-permissions.md\n      - Docker:\n          - Installing Docker: kb/docker/installing-docker.md\n          - Basics of docker: kb/docker/basics-of-docker.md\n          - Installing jellyfin: kb/docker/installing-jellyfin.md\n          - \"Docker: Intro and notes\": kb/docker/docker-intro-and-notes.md\n          - Exporting and importing: kb/docker/exporting-and-importing.md\n          - Docker Architecture: kb/docker/docker-architecture.md\n          - Bulk retag: kb/docker/bulk-retag.md\n\n3. You tell mkdocs what theme to use (This is comparable to a theme in wordpress, or ghost)\n\ntheme:\n  name: material\n\n4. You build and then host it on an nginx server, or Apache.\n\nThe nice part about mkdocs is it's simple, if you can write markdown, you can build a good documentation site.\n\n\nPipelines\n\nSeeing as the site is just being built on demand, we can make a code pipeline to deploy on certain actions.\n\nI have mine deploy on a push to dev branch which goes to a development site (dev-documentation.breadnet.co.uk) and then once you open a PR and merge to master, it deploys the site to production.\n\nAs you can see, there is a stark difference between Dev:\n\nAnd Production\n\nThis is achieved in mkdocs' flexible nature to use Environment variables on build\n\nsite_name: !ENV [env,\"breadNET Documentation\"]\n\nThe above sets the site name, so what you see at the top left.\n\nWhat happens here is on build, mkdocs checks what the value for env is, and if it's null (so not set), it will default to breadNET Documentation\n\nIt's the same for the color and anything else I want to set.\n\n\nThe Code pipeline\n\nAt the time of writing, this is how the pipeline looks\n\n 1. git clone\n 2. If on dev, set vars\n 3. If on dev, set Robots.txt\n 4. Build site\n 5. If on dev, copy to dev server\n 6. If on master, copy to prod server.\n\nThe nice thing is the site is built on each commit, so you can see what will be going in to production.\n\n\nClosing notes\n\nI will miss BookStack. It's been one of the best pieces of software that I have used. It's opened my eyes to what open source has to offer, as well as teaching me ansible!\n\nI don't know if I will run BookStack again -  However it's my go-to application for testing things. So at some point I will release a helm chart for BookStack as I learn helm more.\n\nAs always, a link to all the \"micro sites\" I run\n\nAbout me - PortfoliologoWelcome - breadNET DocumentationbreadNET Public Documentationlogo","feature_image":"https://images.unsplash.com/photo-1457694587812-e8bf29a43845?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwxMTc3M3wwfDF8c2VhcmNofDEyfHxkb2N1bWVudHxlbnwwfHx8fDE2NjQ0NzM2OTM&ixlib=rb-1.2.1&q=80&w=2000","featured":1,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2022-09-29T17:41:19.000Z","updated_at":"2023-11-24T14:29:47.000Z","published_at":"2022-09-30T00:14:48.000Z","custom_excerpt":"Saying good bye to BookStack 👋 (for now)","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"6425f3bc192c0c150413d5ae","uuid":"afb3b185-09ac-466e-ad7e-fd7a6d844c07","title":"Your Spotify: 2022","slug":"your-spotify-2022","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"markdown\",{\"markdown\":\"![](https://user-images.githubusercontent.com/17204739/154752226-c2215a51-e20e-4ade-ac63-42c5abb25240.png)\"}],[\"code\",{\"code\":\"➜ docker -v\\n➜ docker-compose -v \\n➜ git -v  \\n\\n\"}],[\"code\",{\"code\":\"➜ docker -v                                   \\nDocker version 20.10.17-rd, build c2e4e01\\n\\n➜ docker-compose -v   \\nDocker Compose version v2.11.1\\n\\n➜ git -v  \\ngit version 2.37.0 (Apple Git-136)\\n\"}],[\"hr\",{}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/10/image.png\",\"width\":3367,\"height\":1976}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/10/image-1.png\",\"width\":1002,\"height\":274}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/10/image-2.png\",\"width\":1004,\"height\":1204,\"cardWidth\":\"\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/10/image-3.png\",\"width\":3386,\"height\":4956}],[\"markdown\",{\"markdown\":\"| Field Name | Value |\\n| --- | --- |\\n| Redirect URIs | `http://localhost:3000/oauth/spotify/callback`  `http://localhost:8080/oauth/spotify/callback` |\\n\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/10/image-4.png\",\"width\":1198,\"height\":1550}],[\"code\",{\"code\":\"git clone git@github.com:Yooooomi/your_spotify.git\"}],[\"code\",{\"code\":\"      - SPOTIFY_PUBLIC=__your_spotify_client_id__\\n      - SPOTIFY_SECRET=__your_spotify_secret__\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/10/image-5.png\",\"width\":1256,\"height\":752,\"caption\":\"These dont work, dont even try\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/10/image-6.png\",\"width\":1342,\"height\":1416}],[\"code\",{\"code\":\"docker-compose up -d\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/10/image-7.png\",\"width\":3456,\"height\":2234}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/10/image-8.png\",\"width\":1200,\"height\":1688}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/10/image-9.png\",\"width\":3386,\"height\":5252}]],\"markups\":[[\"code\"],[\"a\",[\"href\",\"https://www.docker.com\"]],[\"a\",[\"href\",\"https://docs.docker.com/engine/install/\"]],[\"a\",[\"href\",\"https://docs.docker.com/compose/install/\"]],[\"em\"],[\"a\",[\"href\",\"https://warehouse-camo.ingress.cmh1.psfhosted.org/2e5d6f776534fb7359fe412019743be199f86e6a/68747470733a2f2f692e696d6775722e636f6d2f426a4b5445386d2e6a706567\"]],[\"a\",[\"href\",\"https://www.mulesoft.com/resources/api/what-is-an-api\"]],[\"a\",[\"href\",\"https://developer.spotify.com/dashboard/applications\"]],[\"a\",[\"href\",\"https://git-scm.com/book/en/v2/Getting-Started-Installing-Git\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I wrote a post in June of 2020 about installing this super cool application called \"],[0,[0],1,\"your spotify\"],[0,[],0,\" \"]]],[1,\"p\",[[0,[],0,\"If you didn't join us in 2020, let me give you the rundown on what \"],[0,[0],1,\"your_spotify\"],[0,[],0,\" is\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"It's an application that runs on \"],[0,[1],1,\"Docker containers\"],[0,[],0,\" that poll Spotify's API to get information about your current listening, and then build dashboards around them. See below photos\"]]],[10,0],[1,\"h2\",[[0,[],0,\"How to install and use\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"You will need the below to be installed\"]]],[3,\"ol\",[[[0,[2],1,\"Docker\"]],[[0,[3],1,\"docker-compose\"]],[[0,[],0,\"git\"]]]],[1,\"p\",[[0,[],0,\"Once these are installed, check they are working\"]]],[1,\"p\",[[0,[],0,\"Run the below in the command line:\"]]],[10,1],[1,\"p\",[[0,[],0,\"You should see a similar output to the below\"]]],[10,2],[1,\"p\",[[0,[4],0,\"Your versions will be different, do not \"],[0,[5],2,\"panik\"]]],[10,3],[1,\"p\",[[0,[],0,\"As this is an application that connects to an API, it will need to be able to Authenticate to the API.\"]]],[1,\"p\",[[0,[4],0,\"If you want to learn more about an API, go \"],[0,[6],2,\"here\"]]],[1,\"h2\",[[0,[],0,\"Spotify part\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"Navigate to \"],[0,[7],1,\"https://developer.spotify.com/dashboard/applications\"]]],[1,\"p\",[[0,[],0,\"You will need to login with your Spotify account\"]]],[10,4],[1,\"p\",[[0,[],0,\"Once logged in, click on \"],[0,[0],1,\"create app\"],[0,[],0,\" on the right hand side\"]]],[10,5],[1,\"p\",[[0,[],0,\"You can name this anything you want, but it's reccomended you call this something like \"],[0,[0],1,\"Your spotify\"],[0,[],0,\" or \"],[0,[0],1,\"your_spotify\"],[0,[],0,\" so you know what it does if you ever have issues\"]]],[1,\"p\",[[0,[],0,\"Ensure that you read the T&C's and ensure you're not signing away your first born by accident\"]]],[10,6],[1,\"p\",[[0,[],0,\"You should now be greeted by a page similar to the below\"]]],[10,7],[1,\"p\",[[0,[],0,\"Click Edit and paste the below values in to their respective fields.\"]]],[10,8],[1,\"p\",[[0,[],0,\"Should look like the below\"]]],[10,9],[1,\"p\",[]],[1,\"h2\",[[0,[],0,\"The Docker part\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"Now that the spotify section is done, we are able to create the actual application\"]]],[1,\"p\",[[0,[],0,\"On your computer or server, git clone the GitHub repo.\"]]],[1,\"blockquote\",[[0,[],0,\"Something to note, if you are on windows, you may have to install git and the git cli. \"],[0,[8],1,\"Please refer to the official documentation\"]]],[1,\"p\",[[0,[],0,\"Open the terminal (Or command line on windows)\"]]],[10,10],[1,\"p\",[[0,[],0,\"This will download the files to a folder called \"],[0,[0],1,\"your_spotify\"],[0,[],0,\" where ever you ran the command.\"]]],[1,\"p\",[[0,[],0,\"Open the file called \"],[0,[0],1,\"docker-compose.yml\"]]],[1,\"p\",[[0,[],0,\"I suggest you do this in an IDE like Intelli-j or vscode (Or vim if you're hardcore)\"]]],[1,\"p\",[[0,[],0,\"Edit the lines below\"]]],[10,11],[1,\"p\",[[0,[],0,\"You will need to replace these values from the ones on spotify.\"]]],[1,\"p\",[[0,[],0,\"Navigate back to the Dev Portal, select the application we created, and on the left hand side, locate the credentials.\"]]],[1,\"p\",[[0,[],0,\"They look like the below\"]]],[10,12],[1,\"p\",[[0,[],0,\"Once edited the lines should look like the below\"]]],[10,13],[1,\"p\",[[0,[],0,\"You can now run the docker compose command\"]]],[10,14],[1,\"p\",[[0,[],0,\"the \"],[0,[0],1,\"-d\"],[0,[],0,\" runs this in the background if you were wondering \"]]],[1,\"p\",[[0,[],0,\"You can now navigate to \"],[0,[0],1,\"localhost:3000\"],[0,[],0,\" and are greeted with a login page\"]]],[10,15],[10,16],[1,\"p\",[[0,[],0,\"Once logged in, you need to authorise your account to access the app. \"]]],[1,\"p\",[[0,[],0,\"Once that's done, you're all sorted! \"]]],[1,\"p\",[[0,[],0,\"Listen to some spotify and boom, now the page \"],[0,[4],1,\"should\"],[0,[],0,\" look something like the below\"]]],[10,17],[1,\"p\",[]]],\"ghostVersion\":\"3.0\"}","lexical":null,"html":"<p>I wrote a post in June of 2020 about installing this super cool application called <code>your spotify</code> </p><p>If you didn't join us in 2020, let me give you the rundown on what <code>your_spotify</code> is</p><p></p><p>It's an application that runs on <a href=\"https://www.docker.com\">Docker containers</a> that poll Spotify's API to get information about your current listening, and then build dashboards around them. See below photos</p><!--kg-card-begin: markdown--><p><img src=\"https://user-images.githubusercontent.com/17204739/154752226-c2215a51-e20e-4ade-ac63-42c5abb25240.png\" alt=\"\" loading=\"lazy\"></p>\n<!--kg-card-end: markdown--><h2 id=\"how-to-install-and-use\">How to install and use</h2><p></p><p>You will need the below to be installed</p><ol><li><a href=\"https://docs.docker.com/engine/install/\">Docker</a></li><li><a href=\"https://docs.docker.com/compose/install/\">docker-compose</a></li><li>git</li></ol><p>Once these are installed, check they are working</p><p>Run the below in the command line:</p><pre><code>➜ docker -v\n➜ docker-compose -v \n➜ git -v  \n\n</code></pre><p>You should see a similar output to the below</p><pre><code>➜ docker -v                                   \nDocker version 20.10.17-rd, build c2e4e01\n\n➜ docker-compose -v   \nDocker Compose version v2.11.1\n\n➜ git -v  \ngit version 2.37.0 (Apple Git-136)\n</code></pre><p><em>Your versions will be different, do not <a href=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/2e5d6f776534fb7359fe412019743be199f86e6a/68747470733a2f2f692e696d6775722e636f6d2f426a4b5445386d2e6a706567\">panik</a></em></p><hr><p>As this is an application that connects to an API, it will need to be able to Authenticate to the API.</p><p><em>If you want to learn more about an API, go <a href=\"https://www.mulesoft.com/resources/api/what-is-an-api\">here</a></em></p><h2 id=\"spotify-part\">Spotify part</h2><p></p><p>Navigate to <a href=\"https://developer.spotify.com/dashboard/applications\">https://developer.spotify.com/dashboard/applications</a></p><p>You will need to login with your Spotify account</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/10/image.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"1174\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/10/image.png 600w, __GHOST_URL__/content/images/size/w1000/2022/10/image.png 1000w, __GHOST_URL__/content/images/size/w1600/2022/10/image.png 1600w, __GHOST_URL__/content/images/size/w2400/2022/10/image.png 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Once logged in, click on <code>create app</code> on the right hand side</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/10/image-1.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1002\" height=\"274\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/10/image-1.png 600w, __GHOST_URL__/content/images/size/w1000/2022/10/image-1.png 1000w, __GHOST_URL__/content/images/2022/10/image-1.png 1002w\" sizes=\"(min-width: 720px) 720px\"></figure><p>You can name this anything you want, but it's reccomended you call this something like <code>Your spotify</code> or <code>your_spotify</code> so you know what it does if you ever have issues</p><p>Ensure that you read the T&amp;C's and ensure you're not signing away your first born by accident</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/10/image-2.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1004\" height=\"1204\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/10/image-2.png 600w, __GHOST_URL__/content/images/size/w1000/2022/10/image-2.png 1000w, __GHOST_URL__/content/images/2022/10/image-2.png 1004w\" sizes=\"(min-width: 720px) 720px\"></figure><p>You should now be greeted by a page similar to the below</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/10/image-3.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"2927\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/10/image-3.png 600w, __GHOST_URL__/content/images/size/w1000/2022/10/image-3.png 1000w, __GHOST_URL__/content/images/size/w1600/2022/10/image-3.png 1600w, __GHOST_URL__/content/images/size/w2400/2022/10/image-3.png 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Click Edit and paste the below values in to their respective fields.</p><!--kg-card-begin: markdown--><table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Value</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Redirect URIs</td>\n<td><code>http://localhost:3000/oauth/spotify/callback</code>  <code>http://localhost:8080/oauth/spotify/callback</code></td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: markdown--><p>Should look like the below</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/10/image-4.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1198\" height=\"1550\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/10/image-4.png 600w, __GHOST_URL__/content/images/size/w1000/2022/10/image-4.png 1000w, __GHOST_URL__/content/images/2022/10/image-4.png 1198w\" sizes=\"(min-width: 720px) 720px\"></figure><p></p><h2 id=\"the-docker-part\">The Docker part</h2><p></p><p>Now that the spotify section is done, we are able to create the actual application</p><p>On your computer or server, git clone the GitHub repo.</p><blockquote>Something to note, if you are on windows, you may have to install git and the git cli. <a href=\"https://git-scm.com/book/en/v2/Getting-Started-Installing-Git\">Please refer to the official documentation</a></blockquote><p>Open the terminal (Or command line on windows)</p><pre><code>git clone git@github.com:Yooooomi/your_spotify.git</code></pre><p>This will download the files to a folder called <code>your_spotify</code> where ever you ran the command.</p><p>Open the file called <code>docker-compose.yml</code></p><p>I suggest you do this in an IDE like Intelli-j or vscode (Or vim if you're hardcore)</p><p>Edit the lines below</p><pre><code>      - SPOTIFY_PUBLIC=__your_spotify_client_id__\n      - SPOTIFY_SECRET=__your_spotify_secret__</code></pre><p>You will need to replace these values from the ones on spotify.</p><p>Navigate back to the Dev Portal, select the application we created, and on the left hand side, locate the credentials.</p><p>They look like the below</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2022/10/image-5.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1256\" height=\"752\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/10/image-5.png 600w, __GHOST_URL__/content/images/size/w1000/2022/10/image-5.png 1000w, __GHOST_URL__/content/images/2022/10/image-5.png 1256w\" sizes=\"(min-width: 720px) 720px\"><figcaption>These dont work, dont even try</figcaption></figure><p>Once edited the lines should look like the below</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/10/image-6.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1342\" height=\"1416\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/10/image-6.png 600w, __GHOST_URL__/content/images/size/w1000/2022/10/image-6.png 1000w, __GHOST_URL__/content/images/2022/10/image-6.png 1342w\" sizes=\"(min-width: 720px) 720px\"></figure><p>You can now run the docker compose command</p><pre><code>docker-compose up -d</code></pre><p>the <code>-d</code> runs this in the background if you were wondering </p><p>You can now navigate to <code>localhost:3000</code> and are greeted with a login page</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/10/image-7.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"1293\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/10/image-7.png 600w, __GHOST_URL__/content/images/size/w1000/2022/10/image-7.png 1000w, __GHOST_URL__/content/images/size/w1600/2022/10/image-7.png 1600w, __GHOST_URL__/content/images/size/w2400/2022/10/image-7.png 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/10/image-8.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1200\" height=\"1688\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/10/image-8.png 600w, __GHOST_URL__/content/images/size/w1000/2022/10/image-8.png 1000w, __GHOST_URL__/content/images/2022/10/image-8.png 1200w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Once logged in, you need to authorise your account to access the app. </p><p>Once that's done, you're all sorted! </p><p>Listen to some spotify and boom, now the page <em>should</em> look something like the below</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/10/image-9.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"3102\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/10/image-9.png 600w, __GHOST_URL__/content/images/size/w1000/2022/10/image-9.png 1000w, __GHOST_URL__/content/images/size/w1600/2022/10/image-9.png 1600w, __GHOST_URL__/content/images/size/w2400/2022/10/image-9.png 2400w\" sizes=\"(min-width: 720px) 720px\"></figure>","comment_id":"6355628017e5727444dc7cf5","plaintext":"I wrote a post in June of 2020 about installing this super cool application called your spotify\n\nIf you didn't join us in 2020, let me give you the rundown on what your_spotify is\n\n\n\nIt's an application that runs on Docker containers that poll Spotify's API to get information about your current listening, and then build dashboards around them. See below photos\n\n\n\n\n\nHow to install and use\n\n\n\nYou will need the below to be installed\n\n 1. Docker\n 2. docker-compose\n 3. git\n\nOnce these are installed, check they are working\n\nRun the below in the command line:\n\n➜ docker -v\n➜ docker-compose -v \n➜ git -v  \n\n\n\nYou should see a similar output to the below\n\n➜ docker -v                                   \nDocker version 20.10.17-rd, build c2e4e01\n\n➜ docker-compose -v   \nDocker Compose version v2.11.1\n\n➜ git -v  \ngit version 2.37.0 (Apple Git-136)\n\n\nYour versions will be different, do not panik\n\nAs this is an application that connects to an API, it will need to be able to Authenticate to the API.\n\nIf you want to learn more about an API, go here\n\n\nSpotify part\n\n\n\nNavigate to https://developer.spotify.com/dashboard/applications\n\nYou will need to login with your Spotify account\n\nOnce logged in, click on create app on the right hand side\n\nYou can name this anything you want, but it's reccomended you call this something like Your spotify or your_spotify so you know what it does if you ever have issues\n\nEnsure that you read the T&C's and ensure you're not signing away your first born by accident\n\nYou should now be greeted by a page similar to the below\n\nClick Edit and paste the below values in to their respective fields.\n\n\n\n\nField Name\nValue\n\n\n\n\nRedirect URIs\nhttp://localhost:3000/oauth/spotify/callback http://localhost:8080/oauth/spotify/callback\n\n\n\n\n\nShould look like the below\n\n\n\n\nThe Docker part\n\n\n\nNow that the spotify section is done, we are able to create the actual application\n\nOn your computer or server, git clone the GitHub repo.\n\nSomething to note, if you are on windows, you may have to install git and the git cli. Please refer to the official documentation\n\nOpen the terminal (Or command line on windows)\n\ngit clone git@github.com:Yooooomi/your_spotify.git\n\nThis will download the files to a folder called your_spotify where ever you ran the command.\n\nOpen the file called docker-compose.yml\n\nI suggest you do this in an IDE like Intelli-j or vscode (Or vim if you're hardcore)\n\nEdit the lines below\n\n      - SPOTIFY_PUBLIC=__your_spotify_client_id__\n      - SPOTIFY_SECRET=__your_spotify_secret__\n\nYou will need to replace these values from the ones on spotify.\n\nNavigate back to the Dev Portal, select the application we created, and on the left hand side, locate the credentials.\n\nThey look like the below\n\nOnce edited the lines should look like the below\n\nYou can now run the docker compose command\n\ndocker-compose up -d\n\nthe -d runs this in the background if you were wondering\n\nYou can now navigate to localhost:3000 and are greeted with a login page\n\nOnce logged in, you need to authorise your account to access the app.\n\nOnce that's done, you're all sorted!\n\nListen to some spotify and boom, now the page should look something like the below","feature_image":"https://images.unsplash.com/photo-1527150122806-f682d2fd8b09?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwxMTc3M3wwfDF8c2VhcmNofDV8fHNwb3RpZnl8ZW58MHx8fHwxNjY2NTQxODA1&ixlib=rb-4.0.3&q=80&w=2000","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2022-10-23T15:49:20.000Z","updated_at":"2022-10-23T16:17:48.000Z","published_at":"2022-10-23T16:17:48.000Z","custom_excerpt":"Installing your_spotify in 2022 has never been easier. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"6425f3bc192c0c150413d5af","uuid":"f4abb12d-42e4-41a8-82c5-7595a8184f11","title":"Fly.io is cool","slug":"fly-io-is-cool","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"bookmark\",{\"type\":\"bookmark\",\"url\":\"__GHOST_URL__/migrating-off-bookstack/\",\"metadata\":{\"url\":\"__GHOST_URL__/migrating-off-bookstack/\",\"title\":\"Migrating from BookStack to Mkdocs\",\"description\":\"Saying good bye to BookStack 👋 (for now)\",\"author\":\"Bradley Stannard\",\"publisher\":\"breadNET\",\"thumbnail\":\"https://images.unsplash.com/photo-1457694587812-e8bf29a43845?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwxMTc3M3wwfDF8c2VhcmNofDEyfHxkb2N1bWVudHxlbnwwfHx8fDE2NjQ0NzM2OTM&ixlib=rb-1.2.1&q=80&w=2000\",\"icon\":\"https://breadnet.co.uk/favicon.png\"}}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/11/image.png\",\"width\":1518,\"height\":788}],[\"bookmark\",{\"type\":\"bookmark\",\"url\":\"https://documentation.breadnet.co.uk/kb/nginx/nginxservice-failed-because-the-control-process-exited/\",\"metadata\":{\"url\":\"https://documentation.breadnet.co.uk/kb/nginx/nginxservice-failed-because-the-control-process-exited/\",\"title\":\"nginx Failed because the Control process exited - breadNET Documentation\",\"description\":\"breadNET Documentation\",\"author\":null,\"publisher\":\"logo\",\"thumbnail\":\"https://trace-agent.breadnet.co.uk/matomo.php?idsite=17&rec=1\",\"icon\":\"https://breadnet.co.uk/favicon.ico\"}}],[\"bookmark\",{\"type\":\"bookmark\",\"url\":\"https://fly.io?utm_source=breadnet_co_uk&utm_campaign=fly-io-is-coo\",\"metadata\":{\"url\":\"https://fly.io?utm_source=breadnet_co_uk&utm_campaign=fly-io-is-coo\",\"title\":\"Deploy app servers close to your users · Fly\",\"description\":null,\"author\":null,\"publisher\":null,\"thumbnail\":\"https://fly.io/phx/ui/images/livebeats-4fa2c8aa83838b17b5190c9058107baa.png?vsn=d\",\"icon\":\"https://fly.io/phx/ui/images/favicon/apple-touch-icon-3e4c9ce127b5cd6f5516638d4bbf1dd5.png?vsn=d\"}}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/11/image-1.png\",\"width\":2898,\"height\":1188}],[\"bookmark\",{\"type\":\"bookmark\",\"url\":\"https://documentation.breadnet.co.uk/cloud/fly/fly-docker-auth/\",\"metadata\":{\"url\":\"https://documentation.breadnet.co.uk/cloud/fly/fly-docker-auth/\",\"title\":\"Authenticate to Fly docker Registry - breadNET Documentation\",\"description\":\"breadNET Documentation\",\"author\":null,\"publisher\":\"logo\",\"thumbnail\":\"https://trace-agent.breadnet.co.uk/matomo.php?idsite=17&rec=1\",\"icon\":\"https://breadnet.co.uk/favicon.ico\"}}],[\"bookmark\",{\"type\":\"bookmark\",\"url\":\"https://documentation.breadnet.co.uk/cloud/fly/fly-regions/#solution_1:~:text=config%20%3Cenv%3E.toml-,Not%20set,-You%20will%20see\",\"metadata\":{\"url\":\"https://documentation.breadnet.co.uk/cloud/fly/fly-regions/\",\"title\":\"Fly regions and scaling - breadNET Documentation\",\"description\":\"breadNET Documentation\",\"author\":null,\"publisher\":\"logo\",\"thumbnail\":\"https://trace-agent.breadnet.co.uk/matomo.php?idsite=17&rec=1\",\"icon\":\"https://breadnet.co.uk/favicon.ico\"}}],[\"hr\",{}],[\"hr\",{}]],\"markups\":[[\"em\"],[\"strong\"],[\"a\",[\"href\",\"https://firecracker-microvm.github.io\"]],[\"a\",[\"href\",\"buildpacks.io/\"]],[\"a\",[\"href\",\"https://documentation.breadnet.co.uk/cloud/fly/fly-regions/\"]],[\"a\",[\"href\",\"https://www.equinix.co.uk/data-centers/europe-colocation/united-kingdom-colocation/london-data-centers/ld5\"]],[\"a\",[\"href\",\"https://hub.docker.com/r/userbradley/documentation\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"This is not a sponsored post, but I wont say no to being sponsored in the future! \"]]],[1,\"p\",[[0,[],0,\"Since last month I have migrated completely off Bookstack and now 100% on mkdocs. You can read more about that below\"]]],[10,0],[1,\"p\",[[0,[],0,\"The TL;DR of that is:\"]]],[3,\"ul\",[[[0,[],0,\"Easier management\"]],[[0,[],0,\"Better caching\"]],[[0,[],0,\"More secure\"]],[[0,[],0,\" Better Uptime\"]]]],[1,\"p\",[[0,[],0,\"Now I've added the last one in, as it's what we're going to talk about today\"]]],[1,\"p\",[]],[1,\"h1\",[[0,[],0,\"The internet when I die\"]]],[1,\"p\",[[0,[],0,\"I am by no means a god, but when I eventually croak over and pass on, my credit cards, computers and other assorted belongings will probably be disposed of. \"]]],[1,\"p\",[[0,[],0,\"With this, goes all my Knowledge on Cloud, Linux and all the other assorted things I learnt in my life time. \"]]],[1,\"p\",[[0,[],0,\"That's a huge great waste!\"]]],[1,\"p\",[[0,[],0,\"I've spent many hours optimizing this site so that the Opensource community and those who are looking for things, can find it! There's nothing worse than clicking a bookmark and then it comes back with the all too familiar \"],[0,[0],1,\"This site cant be reached\"]]],[10,1],[1,\"h1\",[[0,[],0,\"What's your point\"]]],[1,\"p\",[[0,[],0,\"The abstract point, is that running my documentation site on a server that constantly needs to be poked to work, and systematically restarting nginx because the control process exited, is a joke.\"]]],[10,2],[1,\"p\",[[0,[],0,\"At work I support applications that the Business rely on, so why can't I do the same thing here?\"]]],[1,\"p\",[[0,[],0,\"This is where Fly.io comes in... \"]]],[10,3],[1,\"h1\",[[0,[],0,\"What is fly.io\"]]],[1,\"p\",[[0,[],0,\"Fly pitch them selves as:\"]]],[1,\"blockquote\",[[0,[1],1,\"Purpose Built cloud\"],[1,[],0,0],[1,[],0,1],[0,[],0,\"We run physical servers in cities close to your users. As close to the metal as you can get without paying shipping.\"]]],[1,\"p\",[[0,[],0,\"And boy do they mean it! \"]]],[1,\"p\",[[0,[],0,\"They run on servers (duh) but then on top of that, they run both:\"]]],[3,\"ul\",[[[0,[],0,\"Nomad; for the containerized workloads\"]],[[0,[2],1,\"Firecracker MicroVm's\"],[0,[],0,\" for the Compute workloads you want to run\"]]]],[1,\"p\",[[0,[],0,\"They lower the barrier to entry by using \"],[0,[3],1,\"BuildPacks\"],[0,[],0,\" to get your code up and running in a matter of minutes.\"]]],[1,\"p\",[[0,[],0,\"They allow you to have 3 free applications, in as many \"],[0,[4],1,\"regions\"],[0,[],0,\" as you want!\"]]],[1,\"p\",[[0,[],0,\"You get given a Fly domain to use, but you can just as easily point your existing DNS to the application and call it a day!\"]]],[1,\"h1\",[[0,[],0,\"Why are you telling me about this\"]]],[1,\"p\",[[0,[],0,\"Basically; I have migrated my mkdocs site from running on my Ubuntu server in London (\"],[0,[5],1,\"actually in Slough I just found out\"],[0,[],0,\") to being in the UK as well as Texas! \"]]],[1,\"p\",[[0,[],0,\"I am able to get latency in the USA down from around 2 \"],[0,[1],1,\"seconds\"],[0,[],0,\" for the page to paint, to around \"],[0,[1],1,\"400ms! \"],[0,[],0,\"(This is also using Cloudflare's caching, but still)\"]]],[1,\"p\",[[0,[],0,\"The deployment process is a lot easier too, as we are building the site as a docker image, uploading it to their Private registry and then turning it in to a Micro VM! \"]]],[1,\"p\",[[0,[],0,\"Like I mentioned last time, I am using a Codefresh Pipeline to easily deploy the site\"]]],[10,4],[1,\"p\",[[0,[],0,\"So a break down of the pipeline\"]]],[3,\"ol\",[[[0,[],0,\"Clone the repo\"]],[[0,[],0,\"a. Set the API Token ENV var b. Set robots.txt on dev site if branch is dev\"]],[[0,[],0,\"Build a docker image and push it to Dockerhub\"]],[[0,[],0,\"Build a docker image and push it to Fly.io's registry\"]],[[0,[],0,\"Run the flyctl command line to update to the latest version of the container\"]]]],[1,\"p\",[[0,[],0,\"All the Public images can be pulled from: \"],[0,[6],1,\"userbradley/documentation\"],[0,[],0,\" \"]]],[1,\"p\",[[0,[],0,\"I've been able to speed up the time it takes for the site to build and then become live from around 1:30 to 55s.\"]]],[1,\"h2\",[[0,[],0,\"Issues I had to overcome\"]]],[1,\"p\",[[0,[],0,\"As with anything, there's always issues.\"]]],[1,\"p\",[[0,[],0,\"I found that there is not a huge user base on Fly.io, so you really need to read the documentation!\"]]],[1,\"p\",[[0,[],0,\"One of them was how to Authenticate to their Docker registry to push my images, as they don't really tell you how to do it out of using their command line tool\"]]],[10,5],[1,\"p\",[[0,[],0,\"The other one was a known bug around scaling. \"]]],[10,6],[10,7],[1,\"p\",[]],[1,\"h1\",[[0,[],0,\"Wrapping up\"]]],[1,\"p\",[[0,[],0,\"If you are looking at running a static site, I highly recommend Fly.io. It took me around 40 minutes to get everything up and running, and serving traffic.\"]]],[1,\"p\",[[0,[],0,\"If you get stuck on anything, please reach out to me! \"]]],[10,8],[1,\"blockquote\",[[0,[],0,\"Disclaimer, this blog post is not sponsored, but if you want to sponsor it, I wont say no!\"],[1,[],0,2],[1,[],0,3],[0,[],0,\"That being said, everything here is my own opinion. \"]]]],\"ghostVersion\":\"3.0\"}","lexical":null,"html":"<p>This is not a sponsored post, but I wont say no to being sponsored in the future! </p><p>Since last month I have migrated completely off Bookstack and now 100% on mkdocs. You can read more about that below</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"__GHOST_URL__/migrating-off-bookstack/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Migrating from BookStack to Mkdocs</div><div class=\"kg-bookmark-description\">Saying good bye to BookStack 👋 (for now)</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://breadnet.co.uk/favicon.png\" alt=\"\"><span class=\"kg-bookmark-author\">breadNET</span><span class=\"kg-bookmark-publisher\">Bradley Stannard</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://images.unsplash.com/photo-1457694587812-e8bf29a43845?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;MnwxMTc3M3wwfDF8c2VhcmNofDEyfHxkb2N1bWVudHxlbnwwfHx8fDE2NjQ0NzM2OTM&amp;ixlib&#x3D;rb-1.2.1&amp;q&#x3D;80&amp;w&#x3D;2000\" alt=\"\"></div></a></figure><p>The TL;DR of that is:</p><ul><li>Easier management</li><li>Better caching</li><li>More secure</li><li> Better Uptime</li></ul><p>Now I've added the last one in, as it's what we're going to talk about today</p><p></p><h1 id=\"the-internet-when-i-die\">The internet when I die</h1><p>I am by no means a god, but when I eventually croak over and pass on, my credit cards, computers and other assorted belongings will probably be disposed of. </p><p>With this, goes all my Knowledge on Cloud, Linux and all the other assorted things I learnt in my life time. </p><p>That's a huge great waste!</p><p>I've spent many hours optimizing this site so that the Opensource community and those who are looking for things, can find it! There's nothing worse than clicking a bookmark and then it comes back with the all too familiar <em>This site cant be reached</em></p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/11/image.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1518\" height=\"788\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/11/image.png 600w, __GHOST_URL__/content/images/size/w1000/2022/11/image.png 1000w, __GHOST_URL__/content/images/2022/11/image.png 1518w\" sizes=\"(min-width: 720px) 720px\"></figure><h1 id=\"what-s-your-point\">What's your point</h1><p>The abstract point, is that running my documentation site on a server that constantly needs to be poked to work, and systematically restarting nginx because the control process exited, is a joke.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://documentation.breadnet.co.uk/kb/nginx/nginxservice-failed-because-the-control-process-exited/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">nginx Failed because the Control process exited - breadNET Documentation</div><div class=\"kg-bookmark-description\">breadNET Documentation</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://breadnet.co.uk/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">logo</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://trace-agent.breadnet.co.uk/matomo.php?idsite&#x3D;17&amp;rec&#x3D;1\" alt=\"\"></div></a></figure><p>At work I support applications that the Business rely on, so why can't I do the same thing here?</p><p>This is where Fly.io comes in... </p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://fly.io?utm_source&#x3D;breadnet_co_uk&amp;utm_campaign&#x3D;fly-io-is-coo\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Deploy app servers close to your users · Fly</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://fly.io/phx/ui/images/favicon/apple-touch-icon-3e4c9ce127b5cd6f5516638d4bbf1dd5.png?vsn&#x3D;d\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://fly.io/phx/ui/images/livebeats-4fa2c8aa83838b17b5190c9058107baa.png?vsn&#x3D;d\" alt=\"\"></div></a></figure><h1 id=\"what-is-fly-io\">What is fly.io</h1><p>Fly pitch them selves as:</p><blockquote><strong>Purpose Built cloud</strong><br><br>We run physical servers in cities close to your users. As close to the metal as you can get without paying shipping.</blockquote><p>And boy do they mean it! </p><p>They run on servers (duh) but then on top of that, they run both:</p><ul><li>Nomad; for the containerized workloads</li><li><a href=\"https://firecracker-microvm.github.io\">Firecracker MicroVm's</a> for the Compute workloads you want to run</li></ul><p>They lower the barrier to entry by using <a href=\"buildpacks.io/\">BuildPacks</a> to get your code up and running in a matter of minutes.</p><p>They allow you to have 3 free applications, in as many <a href=\"https://documentation.breadnet.co.uk/cloud/fly/fly-regions/\">regions</a> as you want!</p><p>You get given a Fly domain to use, but you can just as easily point your existing DNS to the application and call it a day!</p><h1 id=\"why-are-you-telling-me-about-this\">Why are you telling me about this</h1><p>Basically; I have migrated my mkdocs site from running on my Ubuntu server in London (<a href=\"https://www.equinix.co.uk/data-centers/europe-colocation/united-kingdom-colocation/london-data-centers/ld5\">actually in Slough I just found out</a>) to being in the UK as well as Texas! </p><p>I am able to get latency in the USA down from around 2 <strong>seconds</strong> for the page to paint, to around <strong>400ms! </strong>(This is also using Cloudflare's caching, but still)</p><p>The deployment process is a lot easier too, as we are building the site as a docker image, uploading it to their Private registry and then turning it in to a Micro VM! </p><p>Like I mentioned last time, I am using a Codefresh Pipeline to easily deploy the site</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2022/11/image-1.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"820\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/11/image-1.png 600w, __GHOST_URL__/content/images/size/w1000/2022/11/image-1.png 1000w, __GHOST_URL__/content/images/size/w1600/2022/11/image-1.png 1600w, __GHOST_URL__/content/images/size/w2400/2022/11/image-1.png 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><p>So a break down of the pipeline</p><ol><li>Clone the repo</li><li>a. Set the API Token ENV var b. Set robots.txt on dev site if branch is dev</li><li>Build a docker image and push it to Dockerhub</li><li>Build a docker image and push it to Fly.io's registry</li><li>Run the flyctl command line to update to the latest version of the container</li></ol><p>All the Public images can be pulled from: <a href=\"https://hub.docker.com/r/userbradley/documentation\">userbradley/documentation</a> </p><p>I've been able to speed up the time it takes for the site to build and then become live from around 1:30 to 55s.</p><h2 id=\"issues-i-had-to-overcome\">Issues I had to overcome</h2><p>As with anything, there's always issues.</p><p>I found that there is not a huge user base on Fly.io, so you really need to read the documentation!</p><p>One of them was how to Authenticate to their Docker registry to push my images, as they don't really tell you how to do it out of using their command line tool</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://documentation.breadnet.co.uk/cloud/fly/fly-docker-auth/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Authenticate to Fly docker Registry - breadNET Documentation</div><div class=\"kg-bookmark-description\">breadNET Documentation</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://breadnet.co.uk/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">logo</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://trace-agent.breadnet.co.uk/matomo.php?idsite&#x3D;17&amp;rec&#x3D;1\" alt=\"\"></div></a></figure><p>The other one was a known bug around scaling. </p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://documentation.breadnet.co.uk/cloud/fly/fly-regions/#solution_1:~:text&#x3D;config%20%3Cenv%3E.toml-,Not%20set,-You%20will%20see\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Fly regions and scaling - breadNET Documentation</div><div class=\"kg-bookmark-description\">breadNET Documentation</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://breadnet.co.uk/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">logo</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://trace-agent.breadnet.co.uk/matomo.php?idsite&#x3D;17&amp;rec&#x3D;1\" alt=\"\"></div></a></figure><hr><p></p><h1 id=\"wrapping-up\">Wrapping up</h1><p>If you are looking at running a static site, I highly recommend Fly.io. It took me around 40 minutes to get everything up and running, and serving traffic.</p><p>If you get stuck on anything, please reach out to me! </p><hr><blockquote>Disclaimer, this blog post is not sponsored, but if you want to sponsor it, I wont say no!<br><br>That being said, everything here is my own opinion. </blockquote>","comment_id":"637fa54717e5727444dc7d7b","plaintext":"This is not a sponsored post, but I wont say no to being sponsored in the future!\n\nSince last month I have migrated completely off Bookstack and now 100% on mkdocs. You can read more about that below\n\nMigrating from BookStack to MkdocsSaying good bye to BookStack 👋 (for now)breadNETBradley Stannard\n\nThe TL;DR of that is:\n\n * Easier management\n * Better caching\n * More secure\n * Better Uptime\n\nNow I've added the last one in, as it's what we're going to talk about today\n\n\n\n\nThe internet when I die\n\nI am by no means a god, but when I eventually croak over and pass on, my credit cards, computers and other assorted belongings will probably be disposed of.\n\nWith this, goes all my Knowledge on Cloud, Linux and all the other assorted things I learnt in my life time.\n\nThat's a huge great waste!\n\nI've spent many hours optimizing this site so that the Opensource community and those who are looking for things, can find it! There's nothing worse than clicking a bookmark and then it comes back with the all too familiar This site cant be reached\n\n\nWhat's your point\n\nThe abstract point, is that running my documentation site on a server that constantly needs to be poked to work, and systematically restarting nginx because the control process exited, is a joke.\n\nnginx Failed because the Control process exited - breadNET DocumentationbreadNET Documentationlogo\n\nAt work I support applications that the Business rely on, so why can't I do the same thing here?\n\nThis is where Fly.io comes in...\n\nDeploy app servers close to your users · Fly\n\n\nWhat is fly.io\n\nFly pitch them selves as:\n\nPurpose Built cloud\n\nWe run physical servers in cities close to your users. As close to the metal as you can get without paying shipping.\n\nAnd boy do they mean it!\n\nThey run on servers (duh) but then on top of that, they run both:\n\n * Nomad; for the containerized workloads\n * Firecracker MicroVm's for the Compute workloads you want to run\n\nThey lower the barrier to entry by using BuildPacks to get your code up and running in a matter of minutes.\n\nThey allow you to have 3 free applications, in as many regions as you want!\n\nYou get given a Fly domain to use, but you can just as easily point your existing DNS to the application and call it a day!\n\n\nWhy are you telling me about this\n\nBasically; I have migrated my mkdocs site from running on my Ubuntu server in London (actually in Slough I just found out) to being in the UK as well as Texas!\n\nI am able to get latency in the USA down from around 2 seconds for the page to paint, to around 400ms! (This is also using Cloudflare's caching, but still)\n\nThe deployment process is a lot easier too, as we are building the site as a docker image, uploading it to their Private registry and then turning it in to a Micro VM!\n\nLike I mentioned last time, I am using a Codefresh Pipeline to easily deploy the site\n\nSo a break down of the pipeline\n\n 1. Clone the repo\n 2. a. Set the API Token ENV var b. Set robots.txt on dev site if branch is dev\n 3. Build a docker image and push it to Dockerhub\n 4. Build a docker image and push it to Fly.io's registry\n 5. Run the flyctl command line to update to the latest version of the container\n\nAll the Public images can be pulled from: userbradley/documentation\n\nI've been able to speed up the time it takes for the site to build and then become live from around 1:30 to 55s.\n\n\nIssues I had to overcome\n\nAs with anything, there's always issues.\n\nI found that there is not a huge user base on Fly.io, so you really need to read the documentation!\n\nOne of them was how to Authenticate to their Docker registry to push my images, as they don't really tell you how to do it out of using their command line tool\n\nAuthenticate to Fly docker Registry - breadNET DocumentationbreadNET Documentationlogo\n\nThe other one was a known bug around scaling.\n\nFly regions and scaling - breadNET DocumentationbreadNET Documentationlogo\n\n\n\n\nWrapping up\n\nIf you are looking at running a static site, I highly recommend Fly.io. It took me around 40 minutes to get everything up and running, and serving traffic.\n\nIf you get stuck on anything, please reach out to me!\n\nDisclaimer, this blog post is not sponsored, but if you want to sponsor it, I wont say no!\n\nThat being said, everything here is my own opinion.","feature_image":"https://images.unsplash.com/photo-1549740425-5e9ed4d8cd34?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwxMTc3M3wwfDF8c2VhcmNofDV8fGhvdCUyMGFpciUyMGJhbGxvb258ZW58MHx8fHwxNjY5MzA5ODA2&ixlib=rb-4.0.3&q=80&w=2000","featured":1,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2022-11-24T17:09:27.000Z","updated_at":"2022-11-24T17:52:23.000Z","published_at":"2022-11-24T17:52:23.000Z","custom_excerpt":"Looking at an easier way to host static sites? Fly.io has you covered! ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"6425f3bc192c0c150413d5b0","uuid":"a78bcb50-b252-462f-a99a-4555be3dec5c","title":"Years in review","slug":"years-in-review","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/12/CA5BF65E-F4CB-44A9-A33F-435E86F76F3A.jpeg\",\"width\":3024,\"height\":4032,\"caption\":\"The server rack before tearing it all down\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/12/B2D8FA89-C4DB-4761-BE74-824CA30D94F8_1_105_c.jpeg\",\"width\":1024,\"height\":768,\"caption\":\"The 3 servers stacked&nbsp;\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/12/3C0521C3-4D06-4F51-8AE5-75546B0C21AE_1_105_c-1.jpeg\",\"width\":1024,\"height\":768,\"caption\":\"The servers in the back of my car going to my friend who purchased them from me\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/12/053DD6DD-E538-4C12-AF24-4D947CC55A78_1_105_c.jpeg\",\"width\":665,\"height\":1182}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/12/C74A9311-6295-444A-A3FC-F167C3E6D1C7_1_105_c.jpeg\",\"width\":665,\"height\":1182}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2020/06/https_lh3.googleusercontent.com-K_G-DEwtoaIXff2z51OZfIAAAAAAAAqLgEki2cnIG_PscK3oExlNvzneVTb5rVjoeACLkCGAYYCws0IMG_2407.JPG\"}]],\"markups\":[[\"strong\"],[\"a\",[\"href\",\"https://bookstack.breadnet.co.uk\"]],[\"code\"],[\"a\",[\"href\",\"https://www.sumologic.com/blog/snowflake-configurations-and-devops-automation/#:~:text=In%20DevOps,%20a%20snowflake%20is,the%20snowflake%20system%20by%20hand.\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"This page captures all the \\\"x year in review\\\" pages I originally had!\"]]],[1,\"h1\",[[0,[],0,\"2022 : Year in review\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"2022 was the year of the cloud migration. I went through all the services I required and then migrated them.\"]]],[1,\"p\",[[0,[],0,\"Sadly because it's all cloud infrastructure, I cant show you anything. \"]]],[1,\"p\",[[0,[],0,\"I moved out in 2021, so this year was mainly spent sitting on a VPN connection trying to copy data form my parents garage (where the breadNET existed) to cambridge.\"]]],[1,\"p\",[[0,[],0,\"Funnily, I mentioned that I wanted to work on more ansible. This never happened. I don't need it as most of my work is now docker related. Crazy that I once thought \\\"Docker will never take off\\\"\"]]],[1,\"h1\",[[0,[0,0],1,\"20\"],[0,[],0,\"21\"],[0,[0],2,\" : Year in review\"]]],[1,\"p\",[[0,[],0,\"This has been a good year for the Homelab, but sadly towards the end of the year, we've sold it all off :( \"]]],[1,\"p\",[[0,[],0,\"I was able to get Kubernetes running on my lab! Was quite painful to get this working, but dude, this is cool!\"]]],[1,\"p\",[[0,[],0,\"I've been bugging some devs who are making a terraform provider for XOA, so I can define my lab as code! I think maybe next would be moving my lab to Openstack? (spoiler, this never happened)\"]]],[1,\"p\",[[0,[],0,\"I moved out of my parents place to Cambridge, UK.\"]]],[1,\"p\",[[0,[],0,\"I was able to migrate the crucial services such as passbolt and bookstack off the physical servers to the cloud. \"]]],[1,\"p\",[[0,[],0,\"I migrated my email server to Office 365 after getting annoyed with Attachments not working properly.\"]]],[10,0],[10,1],[10,2],[1,\"p\",[[0,[],0,\"If you're wondering what happened to the rack?\"]]],[1,\"p\",[[0,[],0,\"Someone from Scotland has purchased it to use in their workshop to build POC off-grid power. \"]]],[1,\"p\",[[0,[],0,\"I've not included many photos to respect the privacy of the Purchaser, as well as confidential details on the build. But just know it's cool!\"]]],[10,3],[10,4],[1,\"p\",[[0,[],0,\"This rack has been kicking around since 2003 I think... Not 100% sure, but it's definitely served me and all those before and after me well. \"]]],[1,\"p\",[[0,[],0,\"Live on breadRACK!\"]]],[1,\"h3\",[[0,[],0,\"Is this the end for breadNET? - No\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"breadNET will continue to produce badly written blogs and produce shabby documentation on \"],[0,[1],1,\"Bookstack\"],[0,[],0,\" as well as some sketchy python scripts.\"]]],[1,\"p\",[[0,[],0,\"As time and my career progresses, I will start writing more about Google cloud and Amazon Web services, as well as a lot more \"],[0,[2],1,\"x as code\"],[0,[],0,\" projects!\"]]],[1,\"p\",[[0,[],0,\"I plan on rebuilding a lot of services I run to be more cloud native, as well as docker based and finally get rid of the \"],[0,[3],1,\"Snowflake server\"],[0,[],0,\" that runs this site. There are so many customizations on it, that I'm not even sure if it will survive a reboot :| \"]]],[1,\"p\",[[0,[],0,\"I am starting to look at getting my own place soon, which means I am responsible for the power bills, which means metrics I can monitor and graph - HAAS.IO will probably be featured here a fair bit!\"]]],[1,\"p\",[[0,[],0,\"Until December, 2022... Thanks for reading!\"]]],[1,\"p\",[]],[1,\"h1\",[[0,[0,0],1,\"20\"],[0,[],0,\"20\"],[0,[0],2,\" : Year in review\"]]],[1,\"p\",[[0,[],0,\"2020 has been an amazing year especially with the new job, I've been exposed to alot more tech. All my servers now use Ansible for automation, AWX handles the patching and installing base things like Zabbix etc. I'm thinking of bursting some workloads to the cloud as the time moves on as I will be moving out in 2021 so won't be able to take my servers with me!\"]]],[1,\"p\",[[0,[],0,\"I focused on backups a lot this year and got everything backed up to Wasabi S3, then wrote a blog post about their strange billing. \"]]],[1,\"h1\",[[0,[0,0],2,\"2019 : Year in review\"]]],[1,\"p\",[[0,[],0,\"Where to begin? The homelab has come quite some way. I've made breakthroughs, as well as wanting to break through something. I can tell you now, it's been a royal pain in the neck, but it's grown. I've now got a total of around 20 vm's running, purchased some drives off someone on reddit. 10g for backups and now have external backups going to wasabi who are doing me dirty with deleted storage. \"]]],[1,\"p\",[[0,[],0,\"I've done some custom paint jobs on the dell bezel on the front of my servers... because ye\"]]],[10,5],[1,\"p\",[[0,[],0,\"It's not much, but it's coming along!\"]]],[1,\"h1\",[]],[1,\"h1\",[]],[1,\"p\",[]]],\"ghostVersion\":\"3.0\"}","lexical":null,"html":"<p>This page captures all the \"x year in review\" pages I originally had!</p><h1 id=\"2022-year-in-review\">2022 : Year in review</h1><p></p><p>2022 was the year of the cloud migration. I went through all the services I required and then migrated them.</p><p>Sadly because it's all cloud infrastructure, I cant show you anything. </p><p>I moved out in 2021, so this year was mainly spent sitting on a VPN connection trying to copy data form my parents garage (where the breadNET existed) to cambridge.</p><p>Funnily, I mentioned that I wanted to work on more ansible. This never happened. I don't need it as most of my work is now docker related. Crazy that I once thought \"Docker will never take off\"</p><h1 id=\"2021-year-in-review\"><strong><strong>20</strong>21<strong> : Year in review</strong></strong></h1><p>This has been a good year for the Homelab, but sadly towards the end of the year, we've sold it all off :( </p><p>I was able to get Kubernetes running on my lab! Was quite painful to get this working, but dude, this is cool!</p><p>I've been bugging some devs who are making a terraform provider for XOA, so I can define my lab as code! I think maybe next would be moving my lab to Openstack? (spoiler, this never happened)</p><p>I moved out of my parents place to Cambridge, UK.</p><p>I was able to migrate the crucial services such as passbolt and bookstack off the physical servers to the cloud. </p><p>I migrated my email server to Office 365 after getting annoyed with Attachments not working properly.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2021/12/CA5BF65E-F4CB-44A9-A33F-435E86F76F3A.jpeg\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"2667\" srcset=\"__GHOST_URL__/content/images/size/w600/2021/12/CA5BF65E-F4CB-44A9-A33F-435E86F76F3A.jpeg 600w, __GHOST_URL__/content/images/size/w1000/2021/12/CA5BF65E-F4CB-44A9-A33F-435E86F76F3A.jpeg 1000w, __GHOST_URL__/content/images/size/w1600/2021/12/CA5BF65E-F4CB-44A9-A33F-435E86F76F3A.jpeg 1600w, __GHOST_URL__/content/images/size/w2400/2021/12/CA5BF65E-F4CB-44A9-A33F-435E86F76F3A.jpeg 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption>The server rack before tearing it all down</figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2021/12/B2D8FA89-C4DB-4761-BE74-824CA30D94F8_1_105_c.jpeg\" class=\"kg-image\" alt loading=\"lazy\" width=\"1024\" height=\"768\" srcset=\"__GHOST_URL__/content/images/size/w600/2021/12/B2D8FA89-C4DB-4761-BE74-824CA30D94F8_1_105_c.jpeg 600w, __GHOST_URL__/content/images/size/w1000/2021/12/B2D8FA89-C4DB-4761-BE74-824CA30D94F8_1_105_c.jpeg 1000w, __GHOST_URL__/content/images/2021/12/B2D8FA89-C4DB-4761-BE74-824CA30D94F8_1_105_c.jpeg 1024w\" sizes=\"(min-width: 720px) 720px\"><figcaption>The 3 servers stacked&nbsp;</figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2021/12/3C0521C3-4D06-4F51-8AE5-75546B0C21AE_1_105_c-1.jpeg\" class=\"kg-image\" alt loading=\"lazy\" width=\"1024\" height=\"768\" srcset=\"__GHOST_URL__/content/images/size/w600/2021/12/3C0521C3-4D06-4F51-8AE5-75546B0C21AE_1_105_c-1.jpeg 600w, __GHOST_URL__/content/images/size/w1000/2021/12/3C0521C3-4D06-4F51-8AE5-75546B0C21AE_1_105_c-1.jpeg 1000w, __GHOST_URL__/content/images/2021/12/3C0521C3-4D06-4F51-8AE5-75546B0C21AE_1_105_c-1.jpeg 1024w\" sizes=\"(min-width: 720px) 720px\"><figcaption>The servers in the back of my car going to my friend who purchased them from me</figcaption></figure><p>If you're wondering what happened to the rack?</p><p>Someone from Scotland has purchased it to use in their workshop to build POC off-grid power. </p><p>I've not included many photos to respect the privacy of the Purchaser, as well as confidential details on the build. But just know it's cool!</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2021/12/053DD6DD-E538-4C12-AF24-4D947CC55A78_1_105_c.jpeg\" class=\"kg-image\" alt loading=\"lazy\" width=\"665\" height=\"1182\" srcset=\"__GHOST_URL__/content/images/size/w600/2021/12/053DD6DD-E538-4C12-AF24-4D947CC55A78_1_105_c.jpeg 600w, __GHOST_URL__/content/images/2021/12/053DD6DD-E538-4C12-AF24-4D947CC55A78_1_105_c.jpeg 665w\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2021/12/C74A9311-6295-444A-A3FC-F167C3E6D1C7_1_105_c.jpeg\" class=\"kg-image\" alt loading=\"lazy\" width=\"665\" height=\"1182\" srcset=\"__GHOST_URL__/content/images/size/w600/2021/12/C74A9311-6295-444A-A3FC-F167C3E6D1C7_1_105_c.jpeg 600w, __GHOST_URL__/content/images/2021/12/C74A9311-6295-444A-A3FC-F167C3E6D1C7_1_105_c.jpeg 665w\"></figure><p>This rack has been kicking around since 2003 I think... Not 100% sure, but it's definitely served me and all those before and after me well. </p><p>Live on breadRACK!</p><h3 id=\"is-this-the-end-for-breadnet-no\">Is this the end for breadNET? - No</h3><p></p><p>breadNET will continue to produce badly written blogs and produce shabby documentation on <a href=\"https://bookstack.breadnet.co.uk\">Bookstack</a> as well as some sketchy python scripts.</p><p>As time and my career progresses, I will start writing more about Google cloud and Amazon Web services, as well as a lot more <code>x as code</code> projects!</p><p>I plan on rebuilding a lot of services I run to be more cloud native, as well as docker based and finally get rid of the <a href=\"https://www.sumologic.com/blog/snowflake-configurations-and-devops-automation/#:~:text=In%20DevOps,%20a%20snowflake%20is,the%20snowflake%20system%20by%20hand.\">Snowflake server</a> that runs this site. There are so many customizations on it, that I'm not even sure if it will survive a reboot :| </p><p>I am starting to look at getting my own place soon, which means I am responsible for the power bills, which means metrics I can monitor and graph - HAAS.IO will probably be featured here a fair bit!</p><p>Until December, 2022... Thanks for reading!</p><p></p><h1 id=\"2020-year-in-review\"><strong><strong>20</strong>20<strong> : Year in review</strong></strong></h1><p>2020 has been an amazing year especially with the new job, I've been exposed to alot more tech. All my servers now use Ansible for automation, AWX handles the patching and installing base things like Zabbix etc. I'm thinking of bursting some workloads to the cloud as the time moves on as I will be moving out in 2021 so won't be able to take my servers with me!</p><p>I focused on backups a lot this year and got everything backed up to Wasabi S3, then wrote a blog post about their strange billing. </p><h1 id=\"2019-year-in-review\"><strong><strong>2019 : Year in review</strong></strong></h1><p>Where to begin? The homelab has come quite some way. I've made breakthroughs, as well as wanting to break through something. I can tell you now, it's been a royal pain in the neck, but it's grown. I've now got a total of around 20 vm's running, purchased some drives off someone on reddit. 10g for backups and now have external backups going to wasabi who are doing me dirty with deleted storage. </p><p>I've done some custom paint jobs on the dell bezel on the front of my servers... because ye</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2020/06/https_lh3.googleusercontent.com-K_G-DEwtoaIXff2z51OZfIAAAAAAAAqLgEki2cnIG_PscK3oExlNvzneVTb5rVjoeACLkCGAYYCws0IMG_2407.JPG\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>It's not much, but it's coming along!</p><h1></h1><h1></h1>","comment_id":"637fb42b17e5727444dc7dfc","plaintext":"This page captures all the \"x year in review\" pages I originally had!\n\n\n2022 : Year in review\n\n\n\n2022 was the year of the cloud migration. I went through all the services I required and then migrated them.\n\nSadly because it's all cloud infrastructure, I cant show you anything.\n\nI moved out in 2021, so this year was mainly spent sitting on a VPN connection trying to copy data form my parents garage (where the breadNET existed) to cambridge.\n\nFunnily, I mentioned that I wanted to work on more ansible. This never happened. I don't need it as most of my work is now docker related. Crazy that I once thought \"Docker will never take off\"\n\n\n2021 : Year in review\n\nThis has been a good year for the Homelab, but sadly towards the end of the year, we've sold it all off :(\n\nI was able to get Kubernetes running on my lab! Was quite painful to get this working, but dude, this is cool!\n\nI've been bugging some devs who are making a terraform provider for XOA, so I can define my lab as code! I think maybe next would be moving my lab to Openstack? (spoiler, this never happened)\n\nI moved out of my parents place to Cambridge, UK.\n\nI was able to migrate the crucial services such as passbolt and bookstack off the physical servers to the cloud.\n\nI migrated my email server to Office 365 after getting annoyed with Attachments not working properly.\n\nIf you're wondering what happened to the rack?\n\nSomeone from Scotland has purchased it to use in their workshop to build POC off-grid power.\n\nI've not included many photos to respect the privacy of the Purchaser, as well as confidential details on the build. But just know it's cool!\n\nThis rack has been kicking around since 2003 I think... Not 100% sure, but it's definitely served me and all those before and after me well.\n\nLive on breadRACK!\n\n\nIs this the end for breadNET? - No\n\n\n\nbreadNET will continue to produce badly written blogs and produce shabby documentation on Bookstack as well as some sketchy python scripts.\n\nAs time and my career progresses, I will start writing more about Google cloud and Amazon Web services, as well as a lot more x as code projects!\n\nI plan on rebuilding a lot of services I run to be more cloud native, as well as docker based and finally get rid of the Snowflake server that runs this site. There are so many customizations on it, that I'm not even sure if it will survive a reboot :|\n\nI am starting to look at getting my own place soon, which means I am responsible for the power bills, which means metrics I can monitor and graph - HAAS.IO will probably be featured here a fair bit!\n\nUntil December, 2022... Thanks for reading!\n\n\n\n\n2020 : Year in review\n\n2020 has been an amazing year especially with the new job, I've been exposed to alot more tech. All my servers now use Ansible for automation, AWX handles the patching and installing base things like Zabbix etc. I'm thinking of bursting some workloads to the cloud as the time moves on as I will be moving out in 2021 so won't be able to take my servers with me!\n\nI focused on backups a lot this year and got everything backed up to Wasabi S3, then wrote a blog post about their strange billing.\n\n\n2019 : Year in review\n\nWhere to begin? The homelab has come quite some way. I've made breakthroughs, as well as wanting to break through something. I can tell you now, it's been a royal pain in the neck, but it's grown. I've now got a total of around 20 vm's running, purchased some drives off someone on reddit. 10g for backups and now have external backups going to wasabi who are doing me dirty with deleted storage.\n\nI've done some custom paint jobs on the dell bezel on the front of my servers... because ye\n\nIt's not much, but it's coming along!\n\n\n\n\n\n","feature_image":"https://images.unsplash.com/photo-1647714510425-638166b75440?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwxMTc3M3wwfDF8c2VhcmNofDZ8fHdva2luZ2hhbXxlbnwwfHx8fDE2NjkzMTM3NDM&ixlib=rb-4.0.3&q=80&w=2000","featured":0,"type":"page","status":"draft","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2022-11-24T18:12:59.000Z","updated_at":"2025-04-02T12:31:32.000Z","published_at":"2022-11-24T18:16:39.000Z","custom_excerpt":"Catch all page for \"Year in review\"","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"6425f3bc192c0c150413d5b1","uuid":"44625e7c-354a-4d76-940d-d29616af52cd","title":"Free stuff to start your business with","slug":"free-stuff-to-start-your-business-with","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"bookmark\",{\"type\":\"bookmark\",\"url\":\"__GHOST_URL__/what-it-takes-to-run-breadnet/\",\"metadata\":{\"url\":\"__GHOST_URL__/what-it-takes-to-run-breadnet/\",\"title\":\"What’s behind breadNET\",\"description\":\"Let’s take a peak behind the scenes at breadNET and see what it takes to keep this well(ish) oiled machine chugging\",\"author\":\"Bradley Stannard\",\"publisher\":\"breadNET\",\"thumbnail\":\"https://images.unsplash.com/photo-1616551569669-b60598758c4f?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwxMTc3M3wwfDF8c2VhcmNofDZ8fHRlYXIlMjBkb3dufGVufDB8fHx8MTYxODAyNDM2NQ&ixlib=rb-1.2.1&q=80&w=2000\",\"icon\":\"https://breadnet.co.uk/favicon.png\"}}],[\"hr\",{}],[\"bookmark\",{\"type\":\"bookmark\",\"url\":\"https://www.cloudflare.com/en-gb/products/registrar/\",\"metadata\":{\"url\":\"https://www.cloudflare.com/products/registrar/\",\"title\":\"Cloudflare Registrar | New Domain Registration\",\"description\":\"Cloudflare Registrar securely registers your domain names. We offer domain names at cost, with no fees or markups. Register new domains or transfer existing domains to Cloudflare Registrar.\",\"author\":null,\"publisher\":\"Cloudflare\",\"thumbnail\":\"https://www.cloudflare.com/static/149f58294a9d0b11de8c0ae35f9590b4/facebook-link-image.png\",\"icon\":\"https://www.cloudflare.com/favicon.ico\"}}],[\"hr\",{}],[\"bookmark\",{\"type\":\"bookmark\",\"url\":\"https://github.com/pricing\",\"metadata\":{\"url\":\"https://github.com/pricing\",\"title\":\"Pricing · Plans for every developer\",\"description\":\"Whether you’re starting an open source project or choosing new tools for your team, we’ve got you covered.\",\"author\":null,\"publisher\":\"GitHub\",\"thumbnail\":\"https://github.githubassets.com/images/modules/site/social-cards/pricing.png\",\"icon\":\"https://github.githubassets.com/favicons/favicon.svg\"}}],[\"hr\",{}],[\"bookmark\",{\"url\":\"https://codefresh.io\",\"metadata\":{\"url\":\"https://codefresh.io\",\"title\":\"Codefresh | The #1 Argo and GitOps CI/CD Solution\",\"description\":\"Codefresh is the #1 software delivery platform that brings the best of Argo and GitOps into a single cohesive, scalable, and secure platform.\",\"author\":null,\"publisher\":\"Codefresh\",\"thumbnail\":\"https://codefresh.io/wp-content/uploads/2022/08/Open_Graph_Homepage.png\",\"icon\":\"https://codefresh.io/wp-content/uploads/2022/07/cropped-favicon_codefresh_2_512x512-192x192.png\"}}],[\"hr\",{}],[\"hr\",{}],[\"bookmark\",{\"type\":\"bookmark\",\"url\":\"https://documentation.breadnet.co.uk\",\"metadata\":{\"url\":\"https://documentation.breadnet.co.uk\",\"title\":\"Welcome\",\"description\":\"breadNET Documentation\",\"author\":null,\"publisher\":\"logo\",\"thumbnail\":\"https://documentation.breadnet.co.uk/assets/images/social/index.png\",\"icon\":\"https://documentation.breadnet.co.uk/favicon.ico\"}}],[\"bookmark\",{\"url\":\"https://fly.io\",\"metadata\":{\"url\":\"https://fly.io\",\"title\":\"Deploy app servers close to your users · Fly\",\"description\":null,\"author\":null,\"publisher\":null,\"thumbnail\":\"https://fly.io/phx/ui/images/livebeats-4fa2c8aa83838b17b5190c9058107baa.png?vsn=d\",\"icon\":\"https://fly.io/phx/ui/images/favicon/apple-touch-icon-3e4c9ce127b5cd6f5516638d4bbf1dd5.png?vsn=d\"}}],[\"hr\",{}],[\"bookmark\",{\"type\":\"bookmark\",\"url\":\"https://www.digitalocean.com/pricing/droplets\",\"metadata\":{\"url\":\"https://www.digitalocean.com/pricing/droplets\",\"title\":\"Droplet Pricing | DigitalOcean\",\"description\":\"Helping millions of developers easily build, test, manage, and scale applications of any size – faster than ever before.\",\"author\":null,\"publisher\":\"DigitalOcean\",\"thumbnail\":\"https://www.digitalocean.com/_next/static/media/social-share-default.e8530e9e.jpeg\",\"icon\":\"https://www.digitalocean.com/_next/static/media/android-chrome-512x512.5f2e6221.png\"}}],[\"bookmark\",{\"type\":\"bookmark\",\"url\":\"https://cloud.google.com/free/docs/free-cloud-features\",\"metadata\":{\"url\":\"https://cloud.google.com/free/docs/free-cloud-features\",\"title\":\"Free cloud features and trial offer | Google Cloud Free Program\",\"description\":\"Discover the free cloud features that come with the Google Cloud trial offer and more information on how to upgrade your account.\",\"author\":null,\"publisher\":\"Google Cloud\",\"thumbnail\":\"https://cloud.google.com/_static/cloud/images/social-icon-google-cloud-1200-630.png\",\"icon\":\"https://www.gstatic.com/devrel-devsite/prod/vdbc400b97a86c8815ab6ee057e8dc91626aee8cf89b10f7d89037e5a33539f53/cloud/images/favicons/onecloud/super_cloud.png\"}}],[\"markdown\",{\"markdown\":\"* Decice what your business is and what it's compute needs are\\n* Try and use Cloud native systems\\n    * This is basically Docker, use docker\\n* Look in to running everything on Serverless\\n    * Cloud Run\\n    * Cloud Functions\\n\\n* Try avoid compute as much as you can\\n* Look at using other services like [Firebase](https://firebase.google.com) for Mobile applications\"}]],\"markups\":[[\"a\",[\"href\",\"https://www.huffingtonpost.co.uk/entry/brexit-cost-uk-government-revealed_uk_63a2d473e4b0a13a950ba3b5\"]],[\"a\",[\"href\",\"https://en.wikipedia.org/wiki/2021%E2%80%93present_United_Kingdom_cost_of_living_crisis\"]],[\"em\"],[\"a\",[\"href\",\"https://seekingalpha.com/article/4437171-cloudflare-is-much-more-scalable-than-you-think\"]],[\"a\",[\"href\",\"github.com\"]],[\"a\",[\"href\",\"codefresh.io/\"]],[\"code\"],[\"a\",[\"href\",\"https://hub.docker.com/r/userbradley/documentation\"]],[\"a\",[\"href\",\"https://firecracker-microvm.github.io\"]],[\"a\",[\"href\",\"https://documentation.breadnet.co.uk/cloud/fly/fly-docker-auth/?mtm_campaign=free-stuff-to-start-your-business-with\"]],[\"a\",[\"href\",\"https://m.do.co/c/77be3c3aa96c\"]],[\"a\",[\"href\",\"https://www.youtube.com/watch?v=N6lYcXjd4pg\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"No, this isn't me trying to shill you an online course on how to start your business.\"]]],[1,\"p\",[[0,[],0,\"In fact, I am doing the opposite here! I am writing a little list of the free technology (and some slightly paid) that I use to run my tech stack.\"]]],[1,\"p\",[[0,[],0,\"This is somewhat of an extension of my previous blog post \"]]],[10,0],[1,\"p\",[[0,[],0,\"Where I talked about what it takes to run breadNET, how much it costs etc. \"]]],[1,\"p\",[[0,[],0,\"I've since been able to cut my costs down a fair bit, and as the country I reside in is in the middle of a \"],[0,[0],1,\"governmental failure \"],[0,[],0,\"as well as a \"],[0,[1],1,\"Cost of living crisis\"],[0,[],0,\". \"]]],[1,\"p\",[[0,[],0,\"Politics aside, lets look at the tech I have found\"]]],[1,\"p\",[[0,[],0,\"It's broken down in to 2 parts:\"]]],[3,\"ul\",[[[0,[],0,\"Pretty much free\"]],[[0,[],0,\"Somewhat expensive\"]]]],[1,\"p\",[[0,[2],1,\"Pretty much free\"],[0,[],0,\" is services that cost under $15 a year totally, provided you are happy with their free services. \"]]],[1,\"p\",[[0,[2],1,\"Somewhat expensive\"],[0,[],0,\" is the services that have a monthly billing cycle, and can get pretty expensive if not done well. \"]]],[1,\"p\",[[0,[],0,\"As the list goes down, it moves from hosting your domain, to code, cicd, storing artifacts and finally hosting a website. \"]]],[10,1],[1,\"h2\",[[0,[],0,\"Pretty much free\"]]],[1,\"p\",[]],[1,\"h3\",[[0,[],0,\"Cloudflare\"]]],[1,\"p\",[[0,[],0,\"Cloudflare is a CDN, DNS service and recently a domain registrar. \"]]],[1,\"p\",[[0,[],0,\"As with all of these services, you \"],[0,[2],1,\"can \"],[0,[],0,\"pay, but I have been using it for YEARS with no worries. \"]]],[1,\"p\",[[0,[],0,\"The nice thing about Cloudflare is if you buy your domain through them, they deal with the DNS for you automatically.\"]]],[1,\"p\",[[0,[],0,\"If you didn't purchase it through them you can transfer it to them, and you will see a huge reduction in cost.\"]]],[1,\"p\",[[0,[],0,\"My domain used to cost £13 a year, and now has gone to $9 a year. \"]]],[10,2],[1,\"p\",[[0,[],0,\"Cloudflare charge you what they get charged, \"],[0,[3],1,\"their business model isn't selling domains\"],[0,[],0,\", it's building a CDN network and everything around it.\"]]],[1,\"p\",[[0,[],0,\"Below is a list of services they offer\"]]],[3,\"ul\",[[[0,[],0,\"Domain Registration\"]],[[0,[],0,\"DNS management \"]],[[0,[],0,\"Zero Trust\"]],[[0,[],0,\"CDN\"]]]],[10,3],[1,\"h3\",[[0,[4],1,\"Github\"]]],[1,\"p\",[[0,[],0,\"At a high level, GitHub is a website and cloud-based service that helps developers store and manage their code, as well as track and control changes to their code.\"]]],[1,\"p\",[[0,[],0,\"Github is pretty much free, unless you're an absolute code machine and want to flex on your friends, there is no need to hand over payment.\"]]],[1,\"p\",[[0,[],0,\"You can create unlimited repositories, have access to 2000 CI/CD minutes a month (that's like an entire day of the job running btw) \"]]],[1,\"p\",[[0,[],0,\"Below is a list of services they offer:\"]]],[3,\"ul\",[[[0,[],0,\"Unlimited public/private repositories\"]],[[0,[],0,\"Automatic security and version updates\"]],[[0,[],0,\"500MB of Packages storage\"]],[[0,[],0,\"Creating Organizations and then, unlimited repos\"]]]],[10,4],[10,5],[1,\"h3\",[[0,[5],1,\"Codefresh\"]]],[1,\"p\",[[0,[],0,\"Codefresh is my chosen CI/CD provider because it's simple to use, each step is simply a docker image, and it's fast. \"]]],[1,\"p\",[[0,[],0,\"Codefresh's free tier is very generous and allows \"],[0,[2],1,\"1000 cloud credits \"],[0,[],0,\"(what ever that means) \"]]],[1,\"p\",[[0,[],0,\"It allows you to pretty much make as many pipelines as you wish, with as many  steps as you want.\"]]],[1,\"p\",[[0,[],0,\"I use it for building docker images, running terraform and deploying documentation sites... and hopefully in the future this site will be built too.\"]]],[1,\"p\",[[0,[],0,\"Below is a list of services they offer:\"]]],[3,\"ul\",[[[0,[],0,\"CI/CD Pipelines\"]],[[0,[],0,\"Private Helm store\"]],[[0,[],0,\"Private Docker repo\"]],[[0,[],0,\"Full stack testing (they call this composition) \"]]]],[10,6],[10,7],[1,\"h3\",[[0,[],0,\"Docker Hub\"]]],[1,\"p\",[[0,[],0,\"Moving on from Codefresh, where we build the images, we now need a place to store them.\"]]],[1,\"p\",[[0,[],0,\"Docker Hub is pretty much the go to for the average Joe who doesn't have a corporate credit card and unlimited money to use the likes of Google artifact registry. \"]]],[1,\"p\",[[0,[],0,\"Just to clear it up, Docker is the software that uses OCI compliant images, and Docker Inc is the company that made it. \"]]],[1,\"p\",[[0,[],0,\"Docker Hub is where we can store the images we build either Locally, or on Codefresh. \"]]],[1,\"p\",[[0,[],0,\"Docker hub allows 2 free \"],[0,[6],1,\"Private\"],[0,[],0,\" registries (no one can see them unless invited to) and then unlimited public images (\"],[0,[7],1,\"Example\"],[0,[],0,\")\"]]],[1,\"p\",[[0,[],0,\"Below is a list of services they offer:\"]]],[3,\"ul\",[[[0,[],0,\"Docker image storage\"]]]],[10,8],[1,\"h3\",[[0,[],0,\"Fly.io\"]]],[1,\"p\",[[0,[],0,\"I recently posted about this company on it's own. \"]]],[1,\"p\",[[0,[],0,\"Fly is awesome if you ask me. They take docker images, host them on either Nomad or turn them in to a \"],[0,[2],1,\"Micro-vm \"],[0,[],0,\"using \"],[0,[8],1,\"Firecracker\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"For free you can run 3 \"],[0,[2],1,\"applications \"],[0,[],0,\"anywhere in the world. \"]]],[1,\"p\",[[0,[],0,\"I make heavy use of this, like I mentioned previously I plan to re-write this site to be hosted on fly, further reducing my costs.\"]]],[1,\"p\",[[0,[],0,\"I host my documentation site on Fly and have had 0 down time since\"]]],[10,9],[1,\"p\",[[0,[],0,\"Below is a list of services they offer:\"]]],[3,\"ul\",[[[0,[],0,\"Hosted Docker (\"],[0,[9],1,\"Requires some fiddling\"],[0,[],0,\")\"]],[[0,[],0,\"Hosted Applications\"]],[[0,[],0,\"SQL Hosting\"]],[[0,[],0,\"3 free applications or\"]],[[0,[],0,\"2 free applications and a builder\"]],[[0,[],0,\"Free URL\"]],[[0,[],0,\"Free SSL Certificates\"]]]],[10,10],[10,11],[1,\"h2\",[[0,[],0,\"Somewhat expensive\"]]],[1,\"p\",[[0,[],0,\"This section focuses on platforms that you have to pay to use.\"]]],[1,\"p\",[[0,[],0,\"Note, that Digital Ocean is a referral link. You get $200 in credit over 60 days, I get a little kickback once you spend $25. You are welcome to not use it, but it helps me bring this content to you <3 \"]]],[1,\"h3\",[[0,[10],1,\"Digital Ocean\"]]],[1,\"p\",[[0,[10],1,\"Digital ocean\"],[0,[],0,\" is one of the \"],[0,[2],1,\"smaller \"],[0,[],0,\"cloud providers, they have a different target audience then the next mentioned Google cloud. \"]]],[1,\"p\",[[0,[],0,\"I've been using \"],[0,[10],1,\"Digital Ocean\"],[0,[],0,\" for around 4 years now, and never had any issues. \"]]],[1,\"p\",[[0,[],0,\"They offer everything from Kubernetes to S3 \"],[0,[2],1,\"compliant \"],[0,[],0,\"storage. \"]]],[1,\"p\",[[0,[],0,\"You can \"],[0,[2],1,\"purchase \"],[0,[],0,\"a \"],[0,[2],1,\"droplet \"],[0,[],0,\"from them starting at $4 a month, which is more than enough to run a static site on.\"]]],[10,12],[1,\"p\",[[0,[],0,\"Below is a list of services they offer:\"]]],[3,\"ul\",[[[0,[10],1,\"Apps\"]],[[0,[10],1,\"Droplets\"]],[[0,[10],1,\"Functions\"]],[[0,[10],1,\"Kubernetes\"]],[[0,[10],1,\"Volumes\"]],[[0,[10],1,\"Managed Databases\"]],[[0,[10],1,\"Container Registry\"]],[[0,[10],1,\"Compute images\"]],[[0,[10],1,\"VPC\"]]]],[1,\"p\",[]],[1,\"h3\",[[0,[],0,\"Google Cloud\"]]],[1,\"p\",[[0,[],0,\"I know you're reading this thinking... \\\"\"],[0,[11],1,\"Eh google isn't free\"],[0,[],0,\"\\\"  and you're \"],[0,[2],1,\"technically \"],[0,[],0,\"not wrong.\"]]],[1,\"p\",[[0,[],0,\"Google actually has a very generous Free tier \"]]],[10,13],[1,\"p\",[[0,[],0,\"This has offerings like free compute instances, Big query (analytics) \"]]],[1,\"p\",[[0,[],0,\"If you're planning to use Google clouds free tier, I have some recommendations for you:\"]]],[10,14],[1,\"p\",[[0,[],0,\"Also, shameless self plug, but I am a Google certified cloud architect, so like, hit me up if you need some help!\"]]],[1,\"p\",[]]],\"ghostVersion\":\"3.0\"}","lexical":null,"html":"<p>No, this isn't me trying to shill you an online course on how to start your business.</p><p>In fact, I am doing the opposite here! I am writing a little list of the free technology (and some slightly paid) that I use to run my tech stack.</p><p>This is somewhat of an extension of my previous blog post </p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"__GHOST_URL__/what-it-takes-to-run-breadnet/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">What’s behind breadNET</div><div class=\"kg-bookmark-description\">Let’s take a peak behind the scenes at breadNET and see what it takes to keep this well(ish) oiled machine chugging</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://breadnet.co.uk/favicon.png\" alt=\"\"><span class=\"kg-bookmark-author\">breadNET</span><span class=\"kg-bookmark-publisher\">Bradley Stannard</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://images.unsplash.com/photo-1616551569669-b60598758c4f?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;MnwxMTc3M3wwfDF8c2VhcmNofDZ8fHRlYXIlMjBkb3dufGVufDB8fHx8MTYxODAyNDM2NQ&amp;ixlib&#x3D;rb-1.2.1&amp;q&#x3D;80&amp;w&#x3D;2000\" alt=\"\"></div></a></figure><p>Where I talked about what it takes to run breadNET, how much it costs etc. </p><p>I've since been able to cut my costs down a fair bit, and as the country I reside in is in the middle of a <a href=\"https://www.huffingtonpost.co.uk/entry/brexit-cost-uk-government-revealed_uk_63a2d473e4b0a13a950ba3b5\">governmental failure </a>as well as a <a href=\"https://en.wikipedia.org/wiki/2021%E2%80%93present_United_Kingdom_cost_of_living_crisis\">Cost of living crisis</a>. </p><p>Politics aside, lets look at the tech I have found</p><p>It's broken down in to 2 parts:</p><ul><li>Pretty much free</li><li>Somewhat expensive</li></ul><p><em>Pretty much free</em> is services that cost under $15 a year totally, provided you are happy with their free services. </p><p><em>Somewhat expensive</em> is the services that have a monthly billing cycle, and can get pretty expensive if not done well. </p><p>As the list goes down, it moves from hosting your domain, to code, cicd, storing artifacts and finally hosting a website. </p><hr><h2 id=\"pretty-much-free\">Pretty much free</h2><p></p><h3 id=\"cloudflare\">Cloudflare</h3><p>Cloudflare is a CDN, DNS service and recently a domain registrar. </p><p>As with all of these services, you <em>can </em>pay, but I have been using it for YEARS with no worries. </p><p>The nice thing about Cloudflare is if you buy your domain through them, they deal with the DNS for you automatically.</p><p>If you didn't purchase it through them you can transfer it to them, and you will see a huge reduction in cost.</p><p>My domain used to cost £13 a year, and now has gone to $9 a year. </p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://www.cloudflare.com/en-gb/products/registrar/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Cloudflare Registrar | New Domain Registration</div><div class=\"kg-bookmark-description\">Cloudflare Registrar securely registers your domain names. We offer domain names at cost, with no fees or markups. Register new domains or transfer existing domains to Cloudflare Registrar.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://www.cloudflare.com/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">Cloudflare</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://www.cloudflare.com/static/149f58294a9d0b11de8c0ae35f9590b4/facebook-link-image.png\" alt=\"\"></div></a></figure><p>Cloudflare charge you what they get charged, <a href=\"https://seekingalpha.com/article/4437171-cloudflare-is-much-more-scalable-than-you-think\">their business model isn't selling domains</a>, it's building a CDN network and everything around it.</p><p>Below is a list of services they offer</p><ul><li>Domain Registration</li><li>DNS management </li><li>Zero Trust</li><li>CDN</li></ul><hr><h3 id=\"github\"><a href=\"github.com\">Github</a></h3><p>At a high level, GitHub is a website and cloud-based service that helps developers store and manage their code, as well as track and control changes to their code.</p><p>Github is pretty much free, unless you're an absolute code machine and want to flex on your friends, there is no need to hand over payment.</p><p>You can create unlimited repositories, have access to 2000 CI/CD minutes a month (that's like an entire day of the job running btw) </p><p>Below is a list of services they offer:</p><ul><li>Unlimited public/private repositories</li><li>Automatic security and version updates</li><li>500MB of Packages storage</li><li>Creating Organizations and then, unlimited repos</li></ul><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://github.com/pricing\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Pricing · Plans for every developer</div><div class=\"kg-bookmark-description\">Whether you’re starting an open source project or choosing new tools for your team, we’ve got you covered.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://github.githubassets.com/favicons/favicon.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://github.githubassets.com/images/modules/site/social-cards/pricing.png\" alt=\"\"></div></a></figure><hr><h3 id=\"codefresh\"><a href=\"codefresh.io/\">Codefresh</a></h3><p>Codefresh is my chosen CI/CD provider because it's simple to use, each step is simply a docker image, and it's fast. </p><p>Codefresh's free tier is very generous and allows <em>1000 cloud credits </em>(what ever that means) </p><p>It allows you to pretty much make as many pipelines as you wish, with as many  steps as you want.</p><p>I use it for building docker images, running terraform and deploying documentation sites... and hopefully in the future this site will be built too.</p><p>Below is a list of services they offer:</p><ul><li>CI/CD Pipelines</li><li>Private Helm store</li><li>Private Docker repo</li><li>Full stack testing (they call this composition) </li></ul><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://codefresh.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Codefresh | The #1 Argo and GitOps CI/CD Solution</div><div class=\"kg-bookmark-description\">Codefresh is the #1 software delivery platform that brings the best of Argo and GitOps into a single cohesive, scalable, and secure platform.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://codefresh.io/wp-content/uploads/2022/07/cropped-favicon_codefresh_2_512x512-192x192.png\" alt=\"\"><span class=\"kg-bookmark-author\">Codefresh</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://codefresh.io/wp-content/uploads/2022/08/Open_Graph_Homepage.png\" alt=\"\"></div></a></figure><hr><h3 id=\"docker-hub\">Docker Hub</h3><p>Moving on from Codefresh, where we build the images, we now need a place to store them.</p><p>Docker Hub is pretty much the go to for the average Joe who doesn't have a corporate credit card and unlimited money to use the likes of Google artifact registry. </p><p>Just to clear it up, Docker is the software that uses OCI compliant images, and Docker Inc is the company that made it. </p><p>Docker Hub is where we can store the images we build either Locally, or on Codefresh. </p><p>Docker hub allows 2 free <code>Private</code> registries (no one can see them unless invited to) and then unlimited public images (<a href=\"https://hub.docker.com/r/userbradley/documentation\">Example</a>)</p><p>Below is a list of services they offer:</p><ul><li>Docker image storage</li></ul><hr><h3 id=\"fly-io\">Fly.io</h3><p>I recently posted about this company on it's own. </p><p>Fly is awesome if you ask me. They take docker images, host them on either Nomad or turn them in to a <em>Micro-vm </em>using <a href=\"https://firecracker-microvm.github.io\">Firecracker</a>.</p><p>For free you can run 3 <em>applications </em>anywhere in the world. </p><p>I make heavy use of this, like I mentioned previously I plan to re-write this site to be hosted on fly, further reducing my costs.</p><p>I host my documentation site on Fly and have had 0 down time since</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://documentation.breadnet.co.uk\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Welcome</div><div class=\"kg-bookmark-description\">breadNET Documentation</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://documentation.breadnet.co.uk/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">logo</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://documentation.breadnet.co.uk/assets/images/social/index.png\" alt=\"\"></div></a></figure><p>Below is a list of services they offer:</p><ul><li>Hosted Docker (<a href=\"https://documentation.breadnet.co.uk/cloud/fly/fly-docker-auth/?mtm_campaign=free-stuff-to-start-your-business-with\">Requires some fiddling</a>)</li><li>Hosted Applications</li><li>SQL Hosting</li><li>3 free applications or</li><li>2 free applications and a builder</li><li>Free URL</li><li>Free SSL Certificates</li></ul><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://fly.io\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Deploy app servers close to your users · Fly</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://fly.io/phx/ui/images/favicon/apple-touch-icon-3e4c9ce127b5cd6f5516638d4bbf1dd5.png?vsn&#x3D;d\" alt=\"\"></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://fly.io/phx/ui/images/livebeats-4fa2c8aa83838b17b5190c9058107baa.png?vsn&#x3D;d\" alt=\"\"></div></a></figure><hr><h2 id=\"somewhat-expensive\">Somewhat expensive</h2><p>This section focuses on platforms that you have to pay to use.</p><p>Note, that Digital Ocean is a referral link. You get $200 in credit over 60 days, I get a little kickback once you spend $25. You are welcome to not use it, but it helps me bring this content to you &lt;3 </p><h3 id=\"digital-ocean\"><a href=\"https://m.do.co/c/77be3c3aa96c\">Digital Ocean</a></h3><p><a href=\"https://m.do.co/c/77be3c3aa96c\">Digital ocean</a> is one of the <em>smaller </em>cloud providers, they have a different target audience then the next mentioned Google cloud. </p><p>I've been using <a href=\"https://m.do.co/c/77be3c3aa96c\">Digital Ocean</a> for around 4 years now, and never had any issues. </p><p>They offer everything from Kubernetes to S3 <em>compliant </em>storage. </p><p>You can <em>purchase </em>a <em>droplet </em>from them starting at $4 a month, which is more than enough to run a static site on.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://www.digitalocean.com/pricing/droplets\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Droplet Pricing | DigitalOcean</div><div class=\"kg-bookmark-description\">Helping millions of developers easily build, test, manage, and scale applications of any size – faster than ever before.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://www.digitalocean.com/_next/static/media/android-chrome-512x512.5f2e6221.png\" alt=\"\"><span class=\"kg-bookmark-author\">DigitalOcean</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://www.digitalocean.com/_next/static/media/social-share-default.e8530e9e.jpeg\" alt=\"\"></div></a></figure><p>Below is a list of services they offer:</p><ul><li><a href=\"https://m.do.co/c/77be3c3aa96c\">Apps</a></li><li><a href=\"https://m.do.co/c/77be3c3aa96c\">Droplets</a></li><li><a href=\"https://m.do.co/c/77be3c3aa96c\">Functions</a></li><li><a href=\"https://m.do.co/c/77be3c3aa96c\">Kubernetes</a></li><li><a href=\"https://m.do.co/c/77be3c3aa96c\">Volumes</a></li><li><a href=\"https://m.do.co/c/77be3c3aa96c\">Managed Databases</a></li><li><a href=\"https://m.do.co/c/77be3c3aa96c\">Container Registry</a></li><li><a href=\"https://m.do.co/c/77be3c3aa96c\">Compute images</a></li><li><a href=\"https://m.do.co/c/77be3c3aa96c\">VPC</a></li></ul><p></p><h3 id=\"google-cloud\">Google Cloud</h3><p>I know you're reading this thinking... \"<a href=\"https://www.youtube.com/watch?v=N6lYcXjd4pg\">Eh google isn't free</a>\"  and you're <em>technically </em>not wrong.</p><p>Google actually has a very generous Free tier </p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://cloud.google.com/free/docs/free-cloud-features\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Free cloud features and trial offer | Google Cloud Free Program</div><div class=\"kg-bookmark-description\">Discover the free cloud features that come with the Google Cloud trial offer and more information on how to upgrade your account.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://www.gstatic.com/devrel-devsite/prod/vdbc400b97a86c8815ab6ee057e8dc91626aee8cf89b10f7d89037e5a33539f53/cloud/images/favicons/onecloud/super_cloud.png\" alt=\"\"><span class=\"kg-bookmark-author\">Google Cloud</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://cloud.google.com/_static/cloud/images/social-icon-google-cloud-1200-630.png\" alt=\"\"></div></a></figure><p>This has offerings like free compute instances, Big query (analytics) </p><p>If you're planning to use Google clouds free tier, I have some recommendations for you:</p><!--kg-card-begin: markdown--><ul>\n<li>\n<p>Decice what your business is and what it's compute needs are</p>\n</li>\n<li>\n<p>Try and use Cloud native systems</p>\n<ul>\n<li>This is basically Docker, use docker</li>\n</ul>\n</li>\n<li>\n<p>Look in to running everything on Serverless</p>\n<ul>\n<li>Cloud Run</li>\n<li>Cloud Functions</li>\n</ul>\n</li>\n<li>\n<p>Try avoid compute as much as you can</p>\n</li>\n<li>\n<p>Look at using other services like <a href=\"https://firebase.google.com\">Firebase</a> for Mobile applications</p>\n</li>\n</ul>\n<!--kg-card-end: markdown--><p>Also, shameless self plug, but I am a Google certified cloud architect, so like, hit me up if you need some help!</p>","comment_id":"63ae001717e5727444dc7e13","plaintext":"No, this isn't me trying to shill you an online course on how to start your business.\n\nIn fact, I am doing the opposite here! I am writing a little list of the free technology (and some slightly paid) that I use to run my tech stack.\n\nThis is somewhat of an extension of my previous blog post\n\nWhat’s behind breadNETLet’s take a peak behind the scenes at breadNET and see what it takes to keep this well(ish) oiled machine chuggingbreadNETBradley Stannard\n\nWhere I talked about what it takes to run breadNET, how much it costs etc.\n\nI've since been able to cut my costs down a fair bit, and as the country I reside in is in the middle of a governmental failure as well as a Cost of living crisis.\n\nPolitics aside, lets look at the tech I have found\n\nIt's broken down in to 2 parts:\n\n * Pretty much free\n * Somewhat expensive\n\nPretty much free is services that cost under $15 a year totally, provided you are happy with their free services.\n\nSomewhat expensive is the services that have a monthly billing cycle, and can get pretty expensive if not done well.\n\nAs the list goes down, it moves from hosting your domain, to code, cicd, storing artifacts and finally hosting a website.\n\n\nPretty much free\n\n\n\n\nCloudflare\n\nCloudflare is a CDN, DNS service and recently a domain registrar.\n\nAs with all of these services, you can pay, but I have been using it for YEARS with no worries.\n\nThe nice thing about Cloudflare is if you buy your domain through them, they deal with the DNS for you automatically.\n\nIf you didn't purchase it through them you can transfer it to them, and you will see a huge reduction in cost.\n\nMy domain used to cost £13 a year, and now has gone to $9 a year.\n\nCloudflare Registrar | New Domain RegistrationCloudflare Registrar securely registers your domain names. We offer domain names at cost, with no fees or markups. Register new domains or transfer existing domains to Cloudflare Registrar.Cloudflare\n\nCloudflare charge you what they get charged, their business model isn't selling domains, it's building a CDN network and everything around it.\n\nBelow is a list of services they offer\n\n * Domain Registration\n * DNS management\n * Zero Trust\n * CDN\n\n\nGithub\n\nAt a high level, GitHub is a website and cloud-based service that helps developers store and manage their code, as well as track and control changes to their code.\n\nGithub is pretty much free, unless you're an absolute code machine and want to flex on your friends, there is no need to hand over payment.\n\nYou can create unlimited repositories, have access to 2000 CI/CD minutes a month (that's like an entire day of the job running btw)\n\nBelow is a list of services they offer:\n\n * Unlimited public/private repositories\n * Automatic security and version updates\n * 500MB of Packages storage\n * Creating Organizations and then, unlimited repos\n\nPricing · Plans for every developerWhether you’re starting an open source project or choosing new tools for your team, we’ve got you covered.GitHub\n\n\nCodefresh\n\nCodefresh is my chosen CI/CD provider because it's simple to use, each step is simply a docker image, and it's fast.\n\nCodefresh's free tier is very generous and allows 1000 cloud credits (what ever that means)\n\nIt allows you to pretty much make as many pipelines as you wish, with as many  steps as you want.\n\nI use it for building docker images, running terraform and deploying documentation sites... and hopefully in the future this site will be built too.\n\nBelow is a list of services they offer:\n\n * CI/CD Pipelines\n * Private Helm store\n * Private Docker repo\n * Full stack testing (they call this composition)\n\nCodefresh | The #1 Argo and GitOps CI/CD SolutionCodefresh is the #1 software delivery platform that brings the best of Argo and GitOps into a single cohesive, scalable, and secure platform.Codefresh\n\n\nDocker Hub\n\nMoving on from Codefresh, where we build the images, we now need a place to store them.\n\nDocker Hub is pretty much the go to for the average Joe who doesn't have a corporate credit card and unlimited money to use the likes of Google artifact registry.\n\nJust to clear it up, Docker is the software that uses OCI compliant images, and Docker Inc is the company that made it.\n\nDocker Hub is where we can store the images we build either Locally, or on Codefresh.\n\nDocker hub allows 2 free Private registries (no one can see them unless invited to) and then unlimited public images (Example)\n\nBelow is a list of services they offer:\n\n * Docker image storage\n\n\nFly.io\n\nI recently posted about this company on it's own.\n\nFly is awesome if you ask me. They take docker images, host them on either Nomad or turn them in to a Micro-vm using Firecracker.\n\nFor free you can run 3 applications anywhere in the world.\n\nI make heavy use of this, like I mentioned previously I plan to re-write this site to be hosted on fly, further reducing my costs.\n\nI host my documentation site on Fly and have had 0 down time since\n\nWelcomebreadNET Documentationlogo\n\nBelow is a list of services they offer:\n\n * Hosted Docker (Requires some fiddling)\n * Hosted Applications\n * SQL Hosting\n * 3 free applications or\n * 2 free applications and a builder\n * Free URL\n * Free SSL Certificates\n\nDeploy app servers close to your users · Fly\n\n\nSomewhat expensive\n\nThis section focuses on platforms that you have to pay to use.\n\nNote, that Digital Ocean is a referral link. You get $200 in credit over 60 days, I get a little kickback once you spend $25. You are welcome to not use it, but it helps me bring this content to you <3\n\n\nDigital Ocean\n\nDigital ocean is one of the smaller cloud providers, they have a different target audience then the next mentioned Google cloud.\n\nI've been using Digital Ocean for around 4 years now, and never had any issues.\n\nThey offer everything from Kubernetes to S3 compliant storage.\n\nYou can purchase a droplet from them starting at $4 a month, which is more than enough to run a static site on.\n\nDroplet Pricing | DigitalOceanHelping millions of developers easily build, test, manage, and scale applications of any size – faster than ever before.DigitalOcean\n\nBelow is a list of services they offer:\n\n * Apps\n * Droplets\n * Functions\n * Kubernetes\n * Volumes\n * Managed Databases\n * Container Registry\n * Compute images\n * VPC\n\n\n\n\nGoogle Cloud\n\nI know you're reading this thinking... \"Eh google isn't free\"  and you're technically not wrong.\n\nGoogle actually has a very generous Free tier\n\nFree cloud features and trial offer | Google Cloud Free ProgramDiscover the free cloud features that come with the Google Cloud trial offer and more information on how to upgrade your account.Google Cloud\n\nThis has offerings like free compute instances, Big query (analytics)\n\nIf you're planning to use Google clouds free tier, I have some recommendations for you:\n\n * \n   \n   \n   Decice what your business is and what it's compute needs are\n   \n\n * \n   \n   \n   Try and use Cloud native systems\n   \n   \n   * This is basically Docker, use docker\n   \n * \n   \n   \n   Look in to running everything on Serverless\n   \n   \n   * Cloud Run\n   * Cloud Functions\n   \n * \n   \n   \n   Try avoid compute as much as you can\n   \n\n * \n   \n   \n   Look at using other services like Firebase for Mobile applications\n   \n\n\nAlso, shameless self plug, but I am a Google certified cloud architect, so like, hit me up if you need some help!","feature_image":"https://images.unsplash.com/photo-1514108225820-2b602873ac36?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwxMTc3M3wwfDF8c2VhcmNofDI5fHxtb25leXxlbnwwfHx8fDE2NzIzNDc3ODg&ixlib=rb-4.0.3&q=80&w=2000","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2022-12-29T21:01:11.000Z","updated_at":"2022-12-29T23:13:11.000Z","published_at":"2022-12-29T23:13:11.000Z","custom_excerpt":"A useful list of free products to get your business off the ground, and spend as little as you can on infrastructure","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"6425f3bc192c0c150413d5b2","uuid":"cf421e0d-6604-4c9b-a38e-48c568a13d15","title":"Terraform init on codefresh with private modules","slug":"terraform-init-with-modules-on-codefresh","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}]],\"cards\":[[\"code\",{\"code\":\"module \\\"consul\\\" {\\n  source = \\\"github.com/hashicorp/example\\\"\\n}\\n\"}],[\"code\",{\"code\":\"module \\\"documentation\\\" {\\n  source  = \\\"git::ssh://git@github.com/userbradley/terraform-modules-cloudflare-breadnet-<>.git\\\"\\n  name    = \\\"documentation\\\"\\n  proxied = \\\"true\\\"\\n  value   = \\\"<>\\\"\\n}\"}],[\"hr\",{}],[\"markdown\",{\"markdown\":\"* You have a dedicated Git/ Bitbucket account for your Codefresh account\\n    * We are calling mine `brobot`\\n* You are running on a *nix based system (I am on a mac)\\n\"}],[\"hr\",{}],[\"code\",{\"code\":\"  sshSetUp:\\n    image: alpine/git:2.36.3\\n    title: Setting up SSH\\n    stage: init\\n    commands:\\n      - mkdir -p /codefresh/volume/ssh\\n      - echo $ssh_key | base64 -d > /codefresh/volume/ssh/id_rsa\\n      - ssh-keyscan github.com > /codefresh/volume/ssh/known_hosts\\n      - chmod 600 /codefresh/volume/ssh/id_rsa\\n\"}],[\"markdown\",{\"markdown\":\"* image:\\n    * We use an alpine based image as it's small and use the git one as it has git built in\\n* title: Give it a name basically\\n* stage: what stage this runs in the pipeline. best to use init\\n* commands:\\n    * mkdir\\n        * Creates the `/codefresh/volume/ssh` directory if it doesnt already exist\\n    * echo `$ssh_key` \\n        * Echos the key, base64 decodes it and then writes it to the volume\\n    * ssh-keyscan\\n        * scans github.com:22 for the public keys it has on record (So we dont get an error about keys not found)\\n    * chmod\\n        * Sets the permissions on the key file\"}],[\"code\",{\"code\":\"  TerraformInit:\\n    image: hashicorp/terraform:light\\n    title: Terraform Init\\n    stage: init\\n    working_directory: \\\"${{clone}}/kubernetes\\\"\\n    commands:\\n      - terraform init\\n    volumes:\\n      - ./ssh:/root/.ssh\"}],[\"markdown\",{\"markdown\":\"* working_directory\\n    * Sets where the container is running the task. Comparible to going `cd /bradley`\\n* volumes\\n    * mounting `/codefresh/volume/ssh` to `/root/.ssh` on the container\"}],[\"hr\",{}],[\"bookmark\",{\"url\":\"https://documentation.breadnet.co.uk/?mtm_campaign=blogpost&mtm_kwd=terraform-init-codefresh\",\"metadata\":{\"url\":\"https://documentation.breadnet.co.uk\",\"title\":\"Welcome\",\"description\":\"breadNET Documentation\",\"author\":null,\"publisher\":\"logo\",\"thumbnail\":\"https://documentation.breadnet.co.uk/assets/images/social/index.png\",\"icon\":\"https://documentation.breadnet.co.uk/favicon.ico\"}}]],\"markups\":[[\"code\"],[\"strong\"],[\"a\",[\"href\",\"https://developer.hashicorp.com/terraform/language/modules/sources#github\"]],[\"a\",[\"href\",\"https://github.com/hashicorp/terraform/issues/21522#issuecomment-497963956\"]],[\"a\",[\"href\",\"https://docs.github.com/en/authentication/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account\"]],[\"a\",[\"href\",\"https://codefresh.io/docs/docs/yaml-examples/examples/shared-volumes-between-builds/\"]],[\"em\"]],\"sections\":[[1,\"p\",[[0,[],0,\"It's no secret that I love codefresh. I find it incredibly simple to use, and it just makes sense. \"]]],[1,\"p\",[[0,[],0,\"One thing that does not make sense is how to easily run \"],[0,[0],1,\"terraform init\"],[0,[],0,\" on codefresh with \"],[0,[1],1,\"private modules\"],[0,[],0,\" stored on Github (or BitBucket)\"]]],[1,\"h2\",[[0,[],0,\"The issue\"]]],[1,\"p\",[[0,[],0,\"So the issue we have with any module, is how we access it. If your module is public, then you can use the \"],[0,[2],1,\"GitHub\"],[0,[],0,\" URL like the below\"]]],[10,0],[1,\"p\",[[0,[],0,\"This makes the assumption that the module is public\"]]],[1,\"p\",[[0,[],0,\"When we have private modules, like for exmaple my DNS records one:\"]]],[10,1],[1,\"p\",[[0,[],0,\"The \"],[0,[0],1,\"source\"],[0,[],0,\" has changed to use ssh now. In order to run \"],[0,[0],1,\"terraform init\"],[0,[],0,\" on this, we're basically doing a \"],[0,[0],1,\"git clone\"],[0,[],0,\" (\"],[0,[3],1,\"as explained here\"],[0,[],0,\") on the repo's url.\"]]],[1,\"p\",[]],[10,2],[1,\"h2\",[[0,[],0,\"Some assumptions we are going to make\"]]],[1,\"p\",[[0,[],0,\"Annoyingly, before we get on to the solution, we need to make the below assumptions\"]]],[10,3],[1,\"h2\",[[0,[],0,\"Setting up SSH keys\"]]],[1,\"p\",[[0,[],0,\"We need to create the keys, and add them to the relevant systems that need them!\"]]],[1,\"h3\",[[0,[],0,\"Creating the keys \"]]],[1,\"p\",[[0,[],0,\"The first step is creating the SSH keys to use.\"]]],[3,\"ol\",[[[0,[],0,\"Open a terminal\"]],[[0,[],0,\"Run \"],[0,[0],1,\"ssh-keygen-t ed25519\"],[0,[],0,\" and give it a useful name like \"],[0,[0],1,\"robot\"]],[[0,[],0,\"Copy the keys to the clipboard\"]],[[0,[0],1,\"cat ~/.ssh/robot.pub | pbcopy\"]],[[0,[4],1,\"Login to Github and add SSH Keys \"]]]],[1,\"h3\",[[0,[],0,\"Create the secret on codefresh\"]]],[1,\"p\",[[0,[],0,\"Once we've got the public key uploaded to Github, we are able to add the private key to Codefresh.\"]]],[1,\"p\",[[0,[],0,\"Depending on your security posture, you can either add these variables at a Pipeline  level, or a Project. For ease, I recommend at Project level as it means less times you need to locate the ssh keys, and also... You can delete them from your compute. \"]]],[3,\"ol\",[[[0,[],0,\"Login to \"],[0,[0],1,\"g.codefresh.io\"]],[[0,[],0,\"Navigate to Projects\"]],[[0,[],0,\"Click on the project you want to add the keys to\"]],[[0,[],0,\"Click \"],[0,[0],1,\"Variables\"]],[[0,[],0,\"Copy the private key \"],[0,[0],1,\"cat ~/.robot | base64 | pbcopy\"],[0,[],0,\" \"]],[[0,[],0,\"Create a secret on codefresh called \"],[0,[0],1,\"ssh_key\"],[0,[],0,\" \"]],[[0,[],0,\"Paste the value in, and click encrypt\"],[1,[],0,0]]]],[1,\"blockquote\",[[0,[],0,\"Why are we base64 encoding it?\"]]],[1,\"p\",[[0,[],0,\"In order to get the file to not be formatted like garbage, we base64 the file so it keeps formatting, and means we can single line it when we put it in codefresh, as I don't think codefresh has a \"],[0,[0],1,\"--from-file=\"],[0,[],0,\" option like kubernetes\"]]],[10,4],[1,\"h2\",[[0,[],0,\"Creating the pipeline\"]]],[1,\"p\",[[0,[],0,\"Now that we have the SSH keys created, and the keys uploaded to both GitHub and Codefresh, now it's time to actually use them. \"]]],[1,\"p\",[[0,[],0,\"The tl;dr is:\"]]],[3,\"ul\",[[[0,[],0,\"Create a step to echo the file, base64 decode it to the shared volume\"]],[[0,[0],1,\"ssh-keyscan\"],[0,[],0,\" github and add it to shared volume\"]],[[0,[0],1,\"chmod\"],[0,[],0,\" it\"]],[[0,[],0,\"mount shared volume\"]]]],[1,\"h3\",[[0,[],0,\"SSH Key init step\"]]],[1,\"p\",[[0,[],0,\"This is the most importat step, as this is where we take the secret, and inject it in to the shared volume \"],[0,[5],1,\"(read more about that here)\"]]],[1,\"p\",[[0,[],0,\"Below is some \"],[0,[6],1,\"okay\"],[0,[],0,\" code that does the job\"]]],[10,5],[1,\"p\",[[0,[],0,\"A breif explination is below on what each line does\"]]],[10,6],[1,\"h3\",[[0,[],0,\"Consuming the keys in the init step\"]]],[1,\"p\",[[0,[],0,\"Now we have the keys on the shared volume, we need to consume it in the init step.\"]]],[1,\"p\",[[0,[],0,\"I orginally went about this by trying to copy the files from the shared volume each time, as \"],[0,[0],1,\"/root\"],[0,[],0,\" on the steps are ephemeral\"]]],[1,\"p\",[[0,[],0,\"I then trued to symlink the \"],[0,[0],1,\"/codefresh/volume/ssh/id_rsa\"],[0,[],0,\" to \"],[0,[0],1,\"/root/.ssh\"],[0,[],0,\" when I thougt \\\"wonder if we can mount volumes\\\"\"]]],[1,\"p\",[[0,[],0,\"You can!\"]]],[10,7],[1,\"p\",[[0,[],0,\"This follows the same format, but there are 2 important details here\"]]],[10,8],[1,\"p\",[[0,[],0,\"This then allows terraform to run the init step, puling the module from Github!\"]]],[10,9],[1,\"h2\",[[0,[],0,\"Wrap up\"]]],[1,\"p\",[[0,[],0,\"I hope this page was of some use to you.\"]]],[1,\"p\",[[0,[],0,\"I put all the documentation I write on my new documentation site, feel free to have a look around\"]]],[10,10],[1,\"p\",[[0,[],0,\"As always, if you struggle, you can reach out to me at \"],[0,[0],1,\"webmaster at breadnet dot co dot uk\"],[0,[],0,\" and I will do my best to help you where I can!\"]]]],\"ghostVersion\":\"3.0\"}","lexical":null,"html":"<p>It's no secret that I love codefresh. I find it incredibly simple to use, and it just makes sense. </p><p>One thing that does not make sense is how to easily run <code>terraform init</code> on codefresh with <strong>private modules</strong> stored on Github (or BitBucket)</p><h2 id=\"the-issue\">The issue</h2><p>So the issue we have with any module, is how we access it. If your module is public, then you can use the <a href=\"https://developer.hashicorp.com/terraform/language/modules/sources#github\">GitHub</a> URL like the below</p><pre><code>module \"consul\" {\n  source = \"github.com/hashicorp/example\"\n}\n</code></pre><p>This makes the assumption that the module is public</p><p>When we have private modules, like for exmaple my DNS records one:</p><pre><code>module \"documentation\" {\n  source  = \"git::ssh://git@github.com/userbradley/terraform-modules-cloudflare-breadnet-&lt;&gt;.git\"\n  name    = \"documentation\"\n  proxied = \"true\"\n  value   = \"&lt;&gt;\"\n}</code></pre><p>The <code>source</code> has changed to use ssh now. In order to run <code>terraform init</code> on this, we're basically doing a <code>git clone</code> (<a href=\"https://github.com/hashicorp/terraform/issues/21522#issuecomment-497963956\">as explained here</a>) on the repo's url.</p><p></p><hr><h2 id=\"some-assumptions-we-are-going-to-make\">Some assumptions we are going to make</h2><p>Annoyingly, before we get on to the solution, we need to make the below assumptions</p><!--kg-card-begin: markdown--><ul>\n<li>You have a dedicated Git/ Bitbucket account for your Codefresh account\n<ul>\n<li>We are calling mine <code>brobot</code></li>\n</ul>\n</li>\n<li>You are running on a *nix based system (I am on a mac)</li>\n</ul>\n<!--kg-card-end: markdown--><h2 id=\"setting-up-ssh-keys\">Setting up SSH keys</h2><p>We need to create the keys, and add them to the relevant systems that need them!</p><h3 id=\"creating-the-keys\">Creating the keys </h3><p>The first step is creating the SSH keys to use.</p><ol><li>Open a terminal</li><li>Run <code>ssh-keygen-t ed25519</code> and give it a useful name like <code>robot</code></li><li>Copy the keys to the clipboard</li><li><code>cat ~/.ssh/robot.pub | pbcopy</code></li><li><a href=\"https://docs.github.com/en/authentication/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account\">Login to Github and add SSH Keys </a></li></ol><h3 id=\"create-the-secret-on-codefresh\">Create the secret on codefresh</h3><p>Once we've got the public key uploaded to Github, we are able to add the private key to Codefresh.</p><p>Depending on your security posture, you can either add these variables at a Pipeline  level, or a Project. For ease, I recommend at Project level as it means less times you need to locate the ssh keys, and also... You can delete them from your compute. </p><ol><li>Login to <code>g.codefresh.io</code></li><li>Navigate to Projects</li><li>Click on the project you want to add the keys to</li><li>Click <code>Variables</code></li><li>Copy the private key <code>cat ~/.robot | base64 | pbcopy</code> </li><li>Create a secret on codefresh called <code>ssh_key</code> </li><li>Paste the value in, and click encrypt<br></li></ol><blockquote>Why are we base64 encoding it?</blockquote><p>In order to get the file to not be formatted like garbage, we base64 the file so it keeps formatting, and means we can single line it when we put it in codefresh, as I don't think codefresh has a <code>--from-file=</code> option like kubernetes</p><hr><h2 id=\"creating-the-pipeline\">Creating the pipeline</h2><p>Now that we have the SSH keys created, and the keys uploaded to both GitHub and Codefresh, now it's time to actually use them. </p><p>The tl;dr is:</p><ul><li>Create a step to echo the file, base64 decode it to the shared volume</li><li><code>ssh-keyscan</code> github and add it to shared volume</li><li><code>chmod</code> it</li><li>mount shared volume</li></ul><h3 id=\"ssh-key-init-step\">SSH Key init step</h3><p>This is the most importat step, as this is where we take the secret, and inject it in to the shared volume <a href=\"https://codefresh.io/docs/docs/yaml-examples/examples/shared-volumes-between-builds/\">(read more about that here)</a></p><p>Below is some <em>okay</em> code that does the job</p><pre><code>  sshSetUp:\n    image: alpine/git:2.36.3\n    title: Setting up SSH\n    stage: init\n    commands:\n      - mkdir -p /codefresh/volume/ssh\n      - echo $ssh_key | base64 -d &gt; /codefresh/volume/ssh/id_rsa\n      - ssh-keyscan github.com &gt; /codefresh/volume/ssh/known_hosts\n      - chmod 600 /codefresh/volume/ssh/id_rsa\n</code></pre><p>A breif explination is below on what each line does</p><!--kg-card-begin: markdown--><ul>\n<li>image:\n<ul>\n<li>We use an alpine based image as it's small and use the git one as it has git built in</li>\n</ul>\n</li>\n<li>title: Give it a name basically</li>\n<li>stage: what stage this runs in the pipeline. best to use init</li>\n<li>commands:\n<ul>\n<li>mkdir\n<ul>\n<li>Creates the <code>/codefresh/volume/ssh</code> directory if it doesnt already exist</li>\n</ul>\n</li>\n<li>echo <code>$ssh_key</code>\n<ul>\n<li>Echos the key, base64 decodes it and then writes it to the volume</li>\n</ul>\n</li>\n<li>ssh-keyscan\n<ul>\n<li>scans github.com:22 for the public keys it has on record (So we dont get an error about keys not found)</li>\n</ul>\n</li>\n<li>chmod\n<ul>\n<li>Sets the permissions on the key file</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<!--kg-card-end: markdown--><h3 id=\"consuming-the-keys-in-the-init-step\">Consuming the keys in the init step</h3><p>Now we have the keys on the shared volume, we need to consume it in the init step.</p><p>I orginally went about this by trying to copy the files from the shared volume each time, as <code>/root</code> on the steps are ephemeral</p><p>I then trued to symlink the <code>/codefresh/volume/ssh/id_rsa</code> to <code>/root/.ssh</code> when I thougt \"wonder if we can mount volumes\"</p><p>You can!</p><pre><code>  TerraformInit:\n    image: hashicorp/terraform:light\n    title: Terraform Init\n    stage: init\n    working_directory: \"${{clone}}/kubernetes\"\n    commands:\n      - terraform init\n    volumes:\n      - ./ssh:/root/.ssh</code></pre><p>This follows the same format, but there are 2 important details here</p><!--kg-card-begin: markdown--><ul>\n<li>working_directory\n<ul>\n<li>Sets where the container is running the task. Comparible to going <code>cd /bradley</code></li>\n</ul>\n</li>\n<li>volumes\n<ul>\n<li>mounting <code>/codefresh/volume/ssh</code> to <code>/root/.ssh</code> on the container</li>\n</ul>\n</li>\n</ul>\n<!--kg-card-end: markdown--><p>This then allows terraform to run the init step, puling the module from Github!</p><hr><h2 id=\"wrap-up\">Wrap up</h2><p>I hope this page was of some use to you.</p><p>I put all the documentation I write on my new documentation site, feel free to have a look around</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://documentation.breadnet.co.uk/?mtm_campaign&#x3D;blogpost&amp;mtm_kwd&#x3D;terraform-init-codefresh\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Welcome</div><div class=\"kg-bookmark-description\">breadNET Documentation</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://documentation.breadnet.co.uk/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">logo</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://documentation.breadnet.co.uk/assets/images/social/index.png\" alt=\"\"></div></a></figure><p>As always, if you struggle, you can reach out to me at <code>webmaster at breadnet dot co dot uk</code> and I will do my best to help you where I can!</p>","comment_id":"63e6c6dc17e5727444dc7f55","plaintext":"It's no secret that I love codefresh. I find it incredibly simple to use, and it just makes sense.\n\nOne thing that does not make sense is how to easily run terraform init on codefresh with private modules stored on Github (or BitBucket)\n\n\nThe issue\n\nSo the issue we have with any module, is how we access it. If your module is public, then you can use the GitHub URL like the below\n\nmodule \"consul\" {\n  source = \"github.com/hashicorp/example\"\n}\n\n\nThis makes the assumption that the module is public\n\nWhen we have private modules, like for exmaple my DNS records one:\n\nmodule \"documentation\" {\n  source  = \"git::ssh://git@github.com/userbradley/terraform-modules-cloudflare-breadnet-<>.git\"\n  name    = \"documentation\"\n  proxied = \"true\"\n  value   = \"<>\"\n}\n\nThe source has changed to use ssh now. In order to run terraform init on this, we're basically doing a git clone (as explained here) on the repo's url.\n\n\n\n\nSome assumptions we are going to make\n\nAnnoyingly, before we get on to the solution, we need to make the below assumptions\n\n * You have a dedicated Git/ Bitbucket account for your Codefresh account\n   \n   * We are calling mine brobot\n   \n * You are running on a *nix based system (I am on a mac)\n\n\n\nSetting up SSH keys\n\nWe need to create the keys, and add them to the relevant systems that need them!\n\n\nCreating the keys\n\nThe first step is creating the SSH keys to use.\n\n 1. Open a terminal\n 2. Run ssh-keygen-t ed25519 and give it a useful name like robot\n 3. Copy the keys to the clipboard\n 4. cat ~/.ssh/robot.pub | pbcopy\n 5. Login to Github and add SSH Keys\n\n\nCreate the secret on codefresh\n\nOnce we've got the public key uploaded to Github, we are able to add the private key to Codefresh.\n\nDepending on your security posture, you can either add these variables at a Pipeline  level, or a Project. For ease, I recommend at Project level as it means less times you need to locate the ssh keys, and also... You can delete them from your compute.\n\n 1. Login to g.codefresh.io\n 2. Navigate to Projects\n 3. Click on the project you want to add the keys to\n 4. Click Variables\n 5. Copy the private key cat ~/.robot | base64 | pbcopy\n 6. Create a secret on codefresh called ssh_key\n 7. Paste the value in, and click encrypt\n    \n\nWhy are we base64 encoding it?\n\nIn order to get the file to not be formatted like garbage, we base64 the file so it keeps formatting, and means we can single line it when we put it in codefresh, as I don't think codefresh has a --from-file= option like kubernetes\n\n\nCreating the pipeline\n\nNow that we have the SSH keys created, and the keys uploaded to both GitHub and Codefresh, now it's time to actually use them.\n\nThe tl;dr is:\n\n * Create a step to echo the file, base64 decode it to the shared volume\n * ssh-keyscan github and add it to shared volume\n * chmod it\n * mount shared volume\n\n\nSSH Key init step\n\nThis is the most importat step, as this is where we take the secret, and inject it in to the shared volume (read more about that here)\n\nBelow is some okay code that does the job\n\n  sshSetUp:\n    image: alpine/git:2.36.3\n    title: Setting up SSH\n    stage: init\n    commands:\n      - mkdir -p /codefresh/volume/ssh\n      - echo $ssh_key | base64 -d > /codefresh/volume/ssh/id_rsa\n      - ssh-keyscan github.com > /codefresh/volume/ssh/known_hosts\n      - chmod 600 /codefresh/volume/ssh/id_rsa\n\n\nA breif explination is below on what each line does\n\n * image:\n   \n   * We use an alpine based image as it's small and use the git one as it has git built in\n   \n * title: Give it a name basically\n * stage: what stage this runs in the pipeline. best to use init\n * commands:\n   \n   * mkdir\n     \n     * Creates the /codefresh/volume/ssh directory if it doesnt already exist\n     \n   * echo $ssh_key\n     \n     * Echos the key, base64 decodes it and then writes it to the volume\n     \n   * ssh-keyscan\n     \n     * scans github.com:22 for the public keys it has on record (So we dont get an error about keys not found)\n     \n   * chmod\n     \n     * Sets the permissions on the key file\n     \n   \n\n\n\nConsuming the keys in the init step\n\nNow we have the keys on the shared volume, we need to consume it in the init step.\n\nI orginally went about this by trying to copy the files from the shared volume each time, as /root on the steps are ephemeral\n\nI then trued to symlink the /codefresh/volume/ssh/id_rsa to /root/.ssh when I thougt \"wonder if we can mount volumes\"\n\nYou can!\n\n  TerraformInit:\n    image: hashicorp/terraform:light\n    title: Terraform Init\n    stage: init\n    working_directory: \"${{clone}}/kubernetes\"\n    commands:\n      - terraform init\n    volumes:\n      - ./ssh:/root/.ssh\n\nThis follows the same format, but there are 2 important details here\n\n * working_directory\n   \n   * Sets where the container is running the task. Comparible to going cd /bradley\n   \n * volumes\n   \n   * mounting /codefresh/volume/ssh to /root/.ssh on the container\n   \n\n\nThis then allows terraform to run the init step, puling the module from Github!\n\n\nWrap up\n\nI hope this page was of some use to you.\n\nI put all the documentation I write on my new documentation site, feel free to have a look around\n\nWelcomebreadNET Documentationlogo\n\nAs always, if you struggle, you can reach out to me at webmaster at breadnet dot co dot uk and I will do my best to help you where I can!","feature_image":"https://images.unsplash.com/photo-1618939404235-8747e5c37089?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwxMTc3M3wwfDF8c2VhcmNofDE2fHxtb2R1bGV8ZW58MHx8fHwxNjc2MDY4Njk4&ixlib=rb-4.0.3&q=80&w=2000","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2023-02-10T22:36:12.000Z","updated_at":"2023-02-10T23:53:59.000Z","published_at":"2023-02-10T23:53:59.000Z","custom_excerpt":"How to use Terraform on Codefresh with SSH keys","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"6425f3bc192c0c150413d5b3","uuid":"30be22ed-3a10-4694-a33c-450596f508a7","title":"Terraform init on GitHub actions with private modules","slug":"terraform-init-on-github-actions-with-private-modules","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"bookmark\",{\"url\":\"https://codefresh.io/pricing/\",\"metadata\":{\"url\":\"https://codefresh.io/pricing/\",\"title\":\"Pricing & Plans | Codefresh\",\"description\":\"Sign up for Codefresh today and get a 14-day trial of our Enterprise plan. Flexible pricing plans for every project and budget.\",\"author\":null,\"publisher\":\"Codefresh\",\"thumbnail\":\"https://codefresh.io/wp-content/uploads/2023/03/Open_Graph_Homepage.jpg\",\"icon\":\"https://codefresh.io/wp-content/uploads/2022/07/cropped-favicon_codefresh_2_512x512-192x192.png\"}}],[\"hr\",{}],[\"bookmark\",{\"type\":\"bookmark\",\"url\":\"https://developer.hashicorp.com/terraform/language/modules/sources#generic-git-repository\",\"metadata\":{\"url\":\"https://developer.hashicorp.com/terraform/language/modules/sources\",\"title\":\"Module Sources | Terraform | HashiCorp Developer\",\"description\":\"The source argument tells Terraform where to find child modules’s configurations in locations like GitHub, the Terraform Registry, Bitbucket, Git, Mercurial, S3, and GCS.\",\"author\":null,\"publisher\":\"Module Sources | Terraform | HashiCorp Developer\",\"thumbnail\":\"https://developer.hashicorp.com/og-image/terraform.jpg\",\"icon\":\"https://developer.hashicorp.com/icon.svg\"}}],[\"bookmark\",{\"type\":\"bookmark\",\"url\":\"https://maelvls.dev/gh-actions-with-tf-private-repo/\",\"metadata\":{\"url\":\"https://maelvls.dev/gh-actions-with-tf-private-repo/\",\"title\":\"Github Actions with a private Terraform module\",\"description\":\"Terraform makes it easy to manage infrastructure at scale; you might want to share code between modules, and that’s where it becomes tricky. In this post, I try to give some clues on how to use terraform across private Github repos.\",\"author\":\"Maël Valais\",\"publisher\":\"maelvls dev blog\",\"thumbnail\":\"https://maelvls.dev/gh-actions-with-tf-private-repo/cover-gh-actions-with-tf-private-repo.png\",\"icon\":\"https://maelvls.dev/favicon-32x32.png\"}}],[\"code\",{\"code\":\"Cloning into 'terraform-module-google-dns'...\\nremote: Repository not found.\\nfatal: repository 'https://github.com/<>/terraform-module-google-dns.git/' not found\\n\"}],[\"code\",{\"code\":\"ssh-keygen -t ed25519\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2023/03/image.png\",\"width\":1954,\"height\":232}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2023/03/image-1.png\",\"width\":660,\"height\":302}],[\"code\",{\"code\":\"cat githubactions-terraform.pub | pbcopy\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2023/03/image-2.png\",\"width\":1694,\"height\":182}],[\"code\",{\"code\":\"cat githubacionts-terraform | pbcopy\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2023/03/image-3.png\",\"width\":652,\"height\":512}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2023/03/image-4.png\",\"width\":1646,\"height\":156}],[\"bookmark\",{\"type\":\"bookmark\",\"url\":\"https://docs.github.com/en/actions/security-guides/encrypted-secrets\",\"metadata\":{\"url\":\"https://ghdocs-prod.azurewebsites.net/en/actions/security-guides/encrypted-secrets\",\"title\":\"Encrypted secrets - GitHub Docs\",\"description\":\"Encrypted secrets allow you to store sensitive information in your organization, repository, or repository environments.\",\"author\":null,\"publisher\":\"GitHub Docs\",\"thumbnail\":\"https://github.githubassets.com/images/modules/open_graph/github-logo.png\",\"icon\":\"https://docs.github.com/assets/cb-803/images/site/favicon.svg\"}}],[\"code\",{\"code\":\"name: DNS\\n\\non:\\n  push:\\n    branches:\\n      - main\\n\\njobs:\\n  terraform:\\n    runs-on: ubuntu-latest\\n\\n    steps:\\n      - name: Checkout code\\n        uses: actions/checkout@v3\\n\\n      - name: Install Terraform\\n        uses: hashicorp/setup-terraform@v2\\n        with:\\n          terraform_version: 1.3.9\\n\\n      - name: Terraform Init\\n        working-directory: ${{ env.DIR }}\\n        run: |\\n          eval `ssh-agent -s`\\n          ssh-add - <<< '${{ secrets.SSH_KEY_GITHUB_ACTIONS }}'\\n          terraform init\"}]],\"markups\":[[\"code\"],[\"a\",[\"href\",\"https://docs.github.com/en/actions/using-jobs/assigning-permissions-to-jobs\"]],[\"strong\"],[\"a\",[\"href\",\"__GHOST_URL__/terraform-init-with-modules-on-codefresh/\"]],[\"a\",[\"href\",\"https://github.com/hashicorp/terraform/issues/21522#issuecomment-497963956\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Last month I wrote a blog post about how to run \"],[0,[0],1,\"terraform init\"],[0,[],0,\" on codefresh. Sadly Codefresh are changing their pricing which makes it alot more expenive per developer. For this reason, we're migrating our pipelines at work to GitHub actions.\"]]],[10,0],[1,\"p\",[[0,[],0,\"tl;dr: It's going from $34 pcm for 10 devs on a small runner to ~$50+ per dev per month. \"]]],[10,1],[1,\"h1\",[[0,[],0,\"The issue we have\"]]],[1,\"p\",[[0,[],0,\"At work, all our terraform modules are store in Git Hub. Also, all our modules use \"],[0,[0],1,\"git::ssh://\"],[0,[],0,\" as the method of pulling them.\"]]],[10,2],[1,\"p\",[[0,[],0,\"I've tried to follow blog posts like the below, which did not work.\"]]],[1,\"p\",[[0,[],0,\"When I say this, I mean I could not for the life of me get it work, regardless of what \"],[0,[1],1,\"permissions\"],[0,[],0,\" I gave the pipeline.\"]]],[10,3],[1,\"p\",[[0,[],0,\"I was still getting the issue\"]]],[10,4],[1,\"p\",[[0,[],0,\"Despite the repo \"],[0,[2],1,\"very much being there\"]]],[1,\"h1\",[[0,[],0,\"Solution\"]]],[1,\"p\",[[0,[],0,\"The solution was pretty much the same as the \"],[0,[3],1,\"Codefresh\"],[0,[],0,\" solution: Install SSH Keys on the pipeline, so when terraform runs \"],[0,[4],1,\"git clone \"],[0,[],0,\"on the module, it has the keys.\"]]],[1,\"h2\",[[0,[],0,\"Create the keys\"]]],[1,\"p\",[[0,[],0,\"You need to create specific keys for the modules. I recommend creating ones called \"],[0,[0],1,\"githubactions-terraform\"]]],[10,5],[1,\"p\",[[0,[],0,\"When it asks what you want to name it, supply the full path to your \"],[0,[0],1,\".ssh\"],[0,[],0,\" directory, and append \"],[0,[0],1,\"githubactions-terraform\"]]],[1,\"p\",[[0,[],0,\"Now we have the keys created, we can upload them to the GitHub repo that the pipeline needs to pull them from\"]]],[1,\"h2\",[[0,[],0,\"Adding the keys to the relevant GitHub repos\"]]],[1,\"p\",[[0,[],0,\"Navigate to your repo of choice, in this example I will use my Codefresh IP module\"]]],[1,\"p\",[[0,[],0,\"Click on \"],[0,[0],1,\"Settings\"]]],[10,6],[1,\"p\",[[0,[],0,\"Then click on \"],[0,[0],1,\"Deploy Keys\"],[0,[],0,\" down the left hand side\"]]],[10,7],[1,\"p\",[[0,[],0,\"Open your terminal and copy the public key to the clipboard\"]]],[10,8],[1,\"p\",[[0,[],0,\"Click \"],[0,[0],1,\"Add Deploy Key\"],[0,[],0,\" and paste it in\"]]],[10,9],[1,\"p\",[[0,[],0,\"Next step is to add the \"],[0,[2],1,\"private\"],[0,[],0,\" SSH keys to the repo with the GitHub actions enaled\"]]],[1,\"h2\",[[0,[],0,\"SSH Keys on the actions repo\"]]],[1,\"p\",[[0,[],0,\"Open Terminal and copy the private keys to clipboard\"]]],[10,10],[1,\"p\",[[0,[],0,\"Navigate to the repo with the actions in, click settings, then click \"],[0,[0],1,\"Security > Actions\"]]],[10,11],[1,\"p\",[[0,[],0,\"Click \"],[0,[0],1,\"New repository secret\"]]],[10,12],[1,\"p\",[[0,[],0,\"Give it a name, for example I am calling this \"],[0,[0],1,\"SSH_KEY_GITHUB_ACTIONS\"]]],[1,\"p\",[[0,[],0,\"Paste the Private SSH key in.\"]]],[1,\"p\",[[0,[],0,\"Read the below page if you are worried about secrets being leaked\"]]],[10,13],[1,\"h2\",[[0,[],0,\"The GitHub actions part\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"So, we have the SSH keys setup, now we need to actually use them!\"]]],[1,\"p\",[[0,[],0,\"I have created the below GitHub actions that:\"]]],[3,\"ul\",[[[0,[],0,\"Installs terraform on the runner\"]],[[0,[],0,\"Adds the SSH keys\"]],[[0,[],0,\"Runs \"],[0,[0],1,\"terraform init\"]]]],[10,14],[1,\"p\",[[0,[],0,\"You are then free to continue with what ever other terraform steps you need to do in your pipeline.\"]]]],\"ghostVersion\":\"3.0\"}","lexical":null,"html":"<p>Last month I wrote a blog post about how to run <code>terraform init</code> on codefresh. Sadly Codefresh are changing their pricing which makes it alot more expenive per developer. For this reason, we're migrating our pipelines at work to GitHub actions.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://codefresh.io/pricing/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Pricing &amp; Plans | Codefresh</div><div class=\"kg-bookmark-description\">Sign up for Codefresh today and get a 14-day trial of our Enterprise plan. Flexible pricing plans for every project and budget.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://codefresh.io/wp-content/uploads/2022/07/cropped-favicon_codefresh_2_512x512-192x192.png\" alt=\"\"><span class=\"kg-bookmark-author\">Codefresh</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://codefresh.io/wp-content/uploads/2023/03/Open_Graph_Homepage.jpg\" alt=\"\"></div></a></figure><p>tl;dr: It's going from $34 pcm for 10 devs on a small runner to ~$50+ per dev per month. </p><hr><h1 id=\"the-issue-we-have\">The issue we have</h1><p>At work, all our terraform modules are store in Git Hub. Also, all our modules use <code>git::ssh://</code> as the method of pulling them.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://developer.hashicorp.com/terraform/language/modules/sources#generic-git-repository\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Module Sources | Terraform | HashiCorp Developer</div><div class=\"kg-bookmark-description\">The source argument tells Terraform where to find child modules’s configurations in locations like GitHub, the Terraform Registry, Bitbucket, Git, Mercurial, S3, and GCS.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://developer.hashicorp.com/icon.svg\" alt=\"\"><span class=\"kg-bookmark-author\">Module Sources | Terraform | HashiCorp Developer</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://developer.hashicorp.com/og-image/terraform.jpg\" alt=\"\"></div></a></figure><p>I've tried to follow blog posts like the below, which did not work.</p><p>When I say this, I mean I could not for the life of me get it work, regardless of what <a href=\"https://docs.github.com/en/actions/using-jobs/assigning-permissions-to-jobs\">permissions</a> I gave the pipeline.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://maelvls.dev/gh-actions-with-tf-private-repo/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Github Actions with a private Terraform module</div><div class=\"kg-bookmark-description\">Terraform makes it easy to manage infrastructure at scale; you might want to share code between modules, and that’s where it becomes tricky. In this post, I try to give some clues on how to use terraform across private Github repos.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://maelvls.dev/favicon-32x32.png\" alt=\"\"><span class=\"kg-bookmark-author\">maelvls dev blog</span><span class=\"kg-bookmark-publisher\">Maël Valais</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://maelvls.dev/gh-actions-with-tf-private-repo/cover-gh-actions-with-tf-private-repo.png\" alt=\"\"></div></a></figure><p>I was still getting the issue</p><pre><code>Cloning into 'terraform-module-google-dns'...\nremote: Repository not found.\nfatal: repository 'https://github.com/&lt;&gt;/terraform-module-google-dns.git/' not found\n</code></pre><p>Despite the repo <strong>very much being there</strong></p><h1 id=\"solution\">Solution</h1><p>The solution was pretty much the same as the <a href=\"__GHOST_URL__/terraform-init-with-modules-on-codefresh/\">Codefresh</a> solution: Install SSH Keys on the pipeline, so when terraform runs <a href=\"https://github.com/hashicorp/terraform/issues/21522#issuecomment-497963956\">git clone </a>on the module, it has the keys.</p><h2 id=\"create-the-keys\">Create the keys</h2><p>You need to create specific keys for the modules. I recommend creating ones called <code>githubactions-terraform</code></p><pre><code>ssh-keygen -t ed25519</code></pre><p>When it asks what you want to name it, supply the full path to your <code>.ssh</code> directory, and append <code>githubactions-terraform</code></p><p>Now we have the keys created, we can upload them to the GitHub repo that the pipeline needs to pull them from</p><h2 id=\"adding-the-keys-to-the-relevant-github-repos\">Adding the keys to the relevant GitHub repos</h2><p>Navigate to your repo of choice, in this example I will use my Codefresh IP module</p><p>Click on <code>Settings</code></p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2023/03/image.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1954\" height=\"232\" srcset=\"__GHOST_URL__/content/images/size/w600/2023/03/image.png 600w, __GHOST_URL__/content/images/size/w1000/2023/03/image.png 1000w, __GHOST_URL__/content/images/size/w1600/2023/03/image.png 1600w, __GHOST_URL__/content/images/2023/03/image.png 1954w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Then click on <code>Deploy Keys</code> down the left hand side</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2023/03/image-1.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"660\" height=\"302\" srcset=\"__GHOST_URL__/content/images/size/w600/2023/03/image-1.png 600w, __GHOST_URL__/content/images/2023/03/image-1.png 660w\"></figure><p>Open your terminal and copy the public key to the clipboard</p><pre><code>cat githubactions-terraform.pub | pbcopy</code></pre><p>Click <code>Add Deploy Key</code> and paste it in</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2023/03/image-2.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1694\" height=\"182\" srcset=\"__GHOST_URL__/content/images/size/w600/2023/03/image-2.png 600w, __GHOST_URL__/content/images/size/w1000/2023/03/image-2.png 1000w, __GHOST_URL__/content/images/size/w1600/2023/03/image-2.png 1600w, __GHOST_URL__/content/images/2023/03/image-2.png 1694w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Next step is to add the <strong>private</strong> SSH keys to the repo with the GitHub actions enaled</p><h2 id=\"ssh-keys-on-the-actions-repo\">SSH Keys on the actions repo</h2><p>Open Terminal and copy the private keys to clipboard</p><pre><code>cat githubacionts-terraform | pbcopy</code></pre><p>Navigate to the repo with the actions in, click settings, then click <code>Security &gt; Actions</code></p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2023/03/image-3.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"652\" height=\"512\" srcset=\"__GHOST_URL__/content/images/size/w600/2023/03/image-3.png 600w, __GHOST_URL__/content/images/2023/03/image-3.png 652w\"></figure><p>Click <code>New repository secret</code></p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2023/03/image-4.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1646\" height=\"156\" srcset=\"__GHOST_URL__/content/images/size/w600/2023/03/image-4.png 600w, __GHOST_URL__/content/images/size/w1000/2023/03/image-4.png 1000w, __GHOST_URL__/content/images/size/w1600/2023/03/image-4.png 1600w, __GHOST_URL__/content/images/2023/03/image-4.png 1646w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Give it a name, for example I am calling this <code>SSH_KEY_GITHUB_ACTIONS</code></p><p>Paste the Private SSH key in.</p><p>Read the below page if you are worried about secrets being leaked</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://docs.github.com/en/actions/security-guides/encrypted-secrets\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Encrypted secrets - GitHub Docs</div><div class=\"kg-bookmark-description\">Encrypted secrets allow you to store sensitive information in your organization, repository, or repository environments.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://docs.github.com/assets/cb-803/images/site/favicon.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub Docs</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://github.githubassets.com/images/modules/open_graph/github-logo.png\" alt=\"\"></div></a></figure><h2 id=\"the-github-actions-part\">The GitHub actions part</h2><p></p><p>So, we have the SSH keys setup, now we need to actually use them!</p><p>I have created the below GitHub actions that:</p><ul><li>Installs terraform on the runner</li><li>Adds the SSH keys</li><li>Runs <code>terraform init</code></li></ul><pre><code>name: DNS\n\non:\n  push:\n    branches:\n      - main\n\njobs:\n  terraform:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v3\n\n      - name: Install Terraform\n        uses: hashicorp/setup-terraform@v2\n        with:\n          terraform_version: 1.3.9\n\n      - name: Terraform Init\n        working-directory: ${{ env.DIR }}\n        run: |\n          eval `ssh-agent -s`\n          ssh-add - &lt;&lt;&lt; '${{ secrets.SSH_KEY_GITHUB_ACTIONS }}'\n          terraform init</code></pre><p>You are then free to continue with what ever other terraform steps you need to do in your pipeline.</p>","comment_id":"64050c4317e5727444dc7fed","plaintext":"Last month I wrote a blog post about how to run terraform init on codefresh. Sadly Codefresh are changing their pricing which makes it alot more expenive per developer. For this reason, we're migrating our pipelines at work to GitHub actions.\n\nPricing & Plans | CodefreshSign up for Codefresh today and get a 14-day trial of our Enterprise plan. Flexible pricing plans for every project and budget.Codefresh\n\ntl;dr: It's going from $34 pcm for 10 devs on a small runner to ~$50+ per dev per month.\n\n\nThe issue we have\n\nAt work, all our terraform modules are store in Git Hub. Also, all our modules use git::ssh:// as the method of pulling them.\n\nModule Sources | Terraform | HashiCorp DeveloperThe source argument tells Terraform where to find child modules’s configurations in locations like GitHub, the Terraform Registry, Bitbucket, Git, Mercurial, S3, and GCS.Module Sources | Terraform | HashiCorp Developer\n\nI've tried to follow blog posts like the below, which did not work.\n\nWhen I say this, I mean I could not for the life of me get it work, regardless of what permissions I gave the pipeline.\n\nGithub Actions with a private Terraform moduleTerraform makes it easy to manage infrastructure at scale; you might want to share code between modules, and that’s where it becomes tricky. In this post, I try to give some clues on how to use terraform across private Github repos.maelvls dev blogMaël Valais\n\nI was still getting the issue\n\nCloning into 'terraform-module-google-dns'...\nremote: Repository not found.\nfatal: repository 'https://github.com/<>/terraform-module-google-dns.git/' not found\n\n\nDespite the repo very much being there\n\n\nSolution\n\nThe solution was pretty much the same as the Codefresh solution: Install SSH Keys on the pipeline, so when terraform runs git clone on the module, it has the keys.\n\n\nCreate the keys\n\nYou need to create specific keys for the modules. I recommend creating ones called githubactions-terraform\n\nssh-keygen -t ed25519\n\nWhen it asks what you want to name it, supply the full path to your .ssh directory, and append githubactions-terraform\n\nNow we have the keys created, we can upload them to the GitHub repo that the pipeline needs to pull them from\n\n\nAdding the keys to the relevant GitHub repos\n\nNavigate to your repo of choice, in this example I will use my Codefresh IP module\n\nClick on Settings\n\nThen click on Deploy Keys down the left hand side\n\nOpen your terminal and copy the public key to the clipboard\n\ncat githubactions-terraform.pub | pbcopy\n\nClick Add Deploy Key and paste it in\n\nNext step is to add the private SSH keys to the repo with the GitHub actions enaled\n\n\nSSH Keys on the actions repo\n\nOpen Terminal and copy the private keys to clipboard\n\ncat githubacionts-terraform | pbcopy\n\nNavigate to the repo with the actions in, click settings, then click Security > Actions\n\nClick New repository secret\n\nGive it a name, for example I am calling this SSH_KEY_GITHUB_ACTIONS\n\nPaste the Private SSH key in.\n\nRead the below page if you are worried about secrets being leaked\n\nEncrypted secrets - GitHub DocsEncrypted secrets allow you to store sensitive information in your organization, repository, or repository environments.GitHub Docs\n\n\nThe GitHub actions part\n\n\n\nSo, we have the SSH keys setup, now we need to actually use them!\n\nI have created the below GitHub actions that:\n\n * Installs terraform on the runner\n * Adds the SSH keys\n * Runs terraform init\n\nname: DNS\n\non:\n  push:\n    branches:\n      - main\n\njobs:\n  terraform:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v3\n\n      - name: Install Terraform\n        uses: hashicorp/setup-terraform@v2\n        with:\n          terraform_version: 1.3.9\n\n      - name: Terraform Init\n        working-directory: ${{ env.DIR }}\n        run: |\n          eval `ssh-agent -s`\n          ssh-add - <<< '${{ secrets.SSH_KEY_GITHUB_ACTIONS }}'\n          terraform init\n\nYou are then free to continue with what ever other terraform steps you need to do in your pipeline.","feature_image":"https://images.unsplash.com/photo-1647166545674-ce28ce93bdca?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwxMTc3M3wwfDF8c2VhcmNofDE1fHxnaXRodWJ8ZW58MHx8fHwxNjc4MDU0NjM4&ixlib=rb-4.0.3&q=80&w=2000","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2023-03-05T21:40:19.000Z","updated_at":"2023-04-19T20:53:46.000Z","published_at":"2023-03-05T22:17:52.000Z","custom_excerpt":"How to use Terraform on GitHub Actions with SSH keys","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"6425f3bc192c0c150413d5b4","uuid":"907d0bb0-6933-4975-8084-dd40c3af1575","title":"Mount a GCS bucket to nginx","slug":"gcs-web-server-using-gcsfuse","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"bookmark\",{\"type\":\"bookmark\",\"url\":\"https://issuetracker.google.com/issues/114133245\",\"metadata\":{\"url\":\"https://issuetracker.google.com/issues/114133245\",\"title\":\"Google Issue Tracker\",\"description\":null,\"author\":null,\"publisher\":null,\"thumbnail\":null,\"icon\":\"https://www.gstatic.com/buganizer/img/v0/favicon.ico\"}}],[\"bookmark\",{\"type\":\"bookmark\",\"url\":\"https://github.com/userbradley/gcs-web-server\",\"metadata\":{\"url\":\"https://github.com/userbradley/gcs-web-server\",\"title\":\"GitHub - userbradley/gcs-web-server: Container that allows you to mount a GCS Bucket and serve it via nginx\",\"description\":\"Container that allows you to mount a GCS Bucket and serve it via nginx - GitHub - userbradley/gcs-web-server: Container that allows you to mount a GCS Bucket and serve it via nginx\",\"author\":\"userbradley\",\"publisher\":\"GitHub\",\"thumbnail\":\"https://opengraph.githubassets.com/549a2784b6270b8fcf62f865f6b6d01de2dfb93915f7d575c5f89e0d6e5440d4/userbradley/gcs-web-server\",\"icon\":\"https://github.githubassets.com/favicons/favicon.svg\"}}]],\"markups\":[[\"a\",[\"href\",\"https://cloud.google.com/load-balancing/docs/https/ext-load-balancer-backend-buckets\"]],[\"code\"]],\"sections\":[[1,\"p\",[[0,[],0,\"Recently I had to solve a problem where we needed to serve a web page from GCS.\"]]],[1,\"p\",[[0,[],0,\"You've probably come here from googling something like:\"]]],[3,\"ul\",[[[0,[],0,\"How to mount a GCS bucket to a docker container\"]],[[0,[],0,\"How to mount a GCS bucket to a pod\"]],[[0,[],0,\"How to serve a gcs bucket in nginx\"]]]],[1,\"p\",[[0,[],0,\"I know the Hard core Google cloud engineers among us are screaming \"]]],[1,\"blockquote\",[[0,[0],1,\"Why dont you use a load balancer with a GCS bucket as a backend?\"]]],[1,\"p\",[[0,[],0,\"This is why: You cant attach Cloud IAP to it. \"]]],[10,0],[1,\"h2\",[[0,[],0,\"The Solution\"]]],[1,\"p\",[[0,[],0,\"We have a number of GKE clusters I manage, so the easiest solution was below:\"]]],[3,\"ul\",[[[0,[],0,\"Mount buckets as file mounts in linux\"]],[[0,[],0,\"whack nginx in front of it\"]],[[0,[],0,\"create a load balancer and enable IAP\"]]]],[1,\"h2\",[[0,[],0,\"GitHub examples\"]]],[1,\"p\",[[0,[],0,\"I put together a repo for you, that has terraform and Kubernetes manifests in to get you up and running\"]]],[10,1],[1,\"p\",[[0,[],0,\"The idea behind this is that you are equipped with all the tools needed in a docker container to run the web server.\"]]],[1,\"p\",[[0,[],0,\"All you have to do is put the \"],[0,[1],1,\"html\"],[0,[],0,\" files you want to serve, in the GCS bucket anywhere, and then the container will serve them.\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"If you have any issues with this, please don't hesitate to get in contact with me, or open an issue on GitHub\"]]],[1,\"p\",[]]],\"ghostVersion\":\"3.0\"}","lexical":null,"html":"<p>Recently I had to solve a problem where we needed to serve a web page from GCS.</p><p>You've probably come here from googling something like:</p><ul><li>How to mount a GCS bucket to a docker container</li><li>How to mount a GCS bucket to a pod</li><li>How to serve a gcs bucket in nginx</li></ul><p>I know the Hard core Google cloud engineers among us are screaming </p><blockquote><a href=\"https://cloud.google.com/load-balancing/docs/https/ext-load-balancer-backend-buckets\">Why dont you use a load balancer with a GCS bucket as a backend?</a></blockquote><p>This is why: You cant attach Cloud IAP to it. </p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://issuetracker.google.com/issues/114133245\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Google Issue Tracker</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://www.gstatic.com/buganizer/img/v0/favicon.ico\" alt=\"\"></div></div></a></figure><h2 id=\"the-solution\">The Solution</h2><p>We have a number of GKE clusters I manage, so the easiest solution was below:</p><ul><li>Mount buckets as file mounts in linux</li><li>whack nginx in front of it</li><li>create a load balancer and enable IAP</li></ul><h2 id=\"github-examples\">GitHub examples</h2><p>I put together a repo for you, that has terraform and Kubernetes manifests in to get you up and running</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://github.com/userbradley/gcs-web-server\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">GitHub - userbradley/gcs-web-server: Container that allows you to mount a GCS Bucket and serve it via nginx</div><div class=\"kg-bookmark-description\">Container that allows you to mount a GCS Bucket and serve it via nginx - GitHub - userbradley/gcs-web-server: Container that allows you to mount a GCS Bucket and serve it via nginx</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://github.githubassets.com/favicons/favicon.svg\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">userbradley</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://opengraph.githubassets.com/549a2784b6270b8fcf62f865f6b6d01de2dfb93915f7d575c5f89e0d6e5440d4/userbradley/gcs-web-server\" alt=\"\"></div></a></figure><p>The idea behind this is that you are equipped with all the tools needed in a docker container to run the web server.</p><p>All you have to do is put the <code>html</code> files you want to serve, in the GCS bucket anywhere, and then the container will serve them.</p><p></p><p>If you have any issues with this, please don't hesitate to get in contact with me, or open an issue on GitHub</p>","comment_id":"6405153d17e5727444dc806d","plaintext":"Recently I had to solve a problem where we needed to serve a web page from GCS.\n\nYou've probably come here from googling something like:\n\n * How to mount a GCS bucket to a docker container\n * How to mount a GCS bucket to a pod\n * How to serve a gcs bucket in nginx\n\nI know the Hard core Google cloud engineers among us are screaming\n\nWhy dont you use a load balancer with a GCS bucket as a backend?\n\nThis is why: You cant attach Cloud IAP to it.\n\nGoogle Issue Tracker\n\n\nThe Solution\n\nWe have a number of GKE clusters I manage, so the easiest solution was below:\n\n * Mount buckets as file mounts in linux\n * whack nginx in front of it\n * create a load balancer and enable IAP\n\n\nGitHub examples\n\nI put together a repo for you, that has terraform and Kubernetes manifests in to get you up and running\n\nGitHub - userbradley/gcs-web-server: Container that allows you to mount a GCS Bucket and serve it via nginxContainer that allows you to mount a GCS Bucket and serve it via nginx - GitHub - userbradley/gcs-web-server: Container that allows you to mount a GCS Bucket and serve it via nginxGitHubuserbradley\n\nThe idea behind this is that you are equipped with all the tools needed in a docker container to run the web server.\n\nAll you have to do is put the html files you want to serve, in the GCS bucket anywhere, and then the container will serve them.\n\n\n\nIf you have any issues with this, please don't hesitate to get in contact with me, or open an issue on GitHub","feature_image":"https://images.unsplash.com/photo-1605164612754-da6d7c6ba723?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwxMTc3M3wwfDF8c2VhcmNofDJ8fGJ1Y2tldHxlbnwwfHx8fDE2NzgwNTU0Mzc&ixlib=rb-4.0.3&q=80&w=2000","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2023-03-05T22:18:37.000Z","updated_at":"2023-07-01T17:14:43.000Z","published_at":"2023-03-05T22:32:00.000Z","custom_excerpt":"Mount a GCS Bucket to a pod in kubernetes\n","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"6425f3bc192c0c150413d5b5","uuid":"6dd57625-bb20-4970-997a-8d714000bcbb","title":"Using Google Artifact Registry with k3s","slug":"using-google-artifact-registry-with-k3s","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}]],\"cards\":[[\"code\",{\"code\":\" ErrImagePull: rpc error: code = Unknown desc = failed to pull and unpack image \\\"europe-west2-docker.pkg.dev/breadnet-container-store/redacted/documentation-dev:0.0.1\\\": failed to resolve reference \\\"europe-west2-docker.pkg.dev/breadnet-container-store/redacted/documentation-dev:0.0.1\\\": failed to authorize: failed to fetch anonymous token: unexpected status: 403 Forbidden\\n\"}],[\"code\",{\"code\":\"{\\n  \\\"type\\\": \\\"service_account\\\",\\n  \\\"project_id\\\": \\\"redacted\\\",\\n  \\\"private_key_id\\\": \\\"redacted\\\",\\n  \\\"private_key\\\": \\\"-----BEGIN PRIVATE KEY-----\\\\nredacted\\\\n-----END         PRIVATE KEY-----\\\\n\\\",\\n  \\\"client_email\\\": \\\"k3s-container-puller@redacted.iam.gserviceaccount.com\\\",\\n  \\\"client_id\\\": \\\"redacted\\\",\\n  \\\"auth_uri\\\": \\\"https://accounts.google.com/o/oauth2/auth\\\",\\n  \\\"token_uri\\\": \\\"https://oauth2.googleapis.com/token\\\",\\n  \\\"auth_provider_x509_cert_url\\\": \\\"https://www.googleapis.com/oauth2/v1/certs\\\",\\n  \\\"client_x509_cert_url\\\": \\\"https://www.googleapis.com/robot/v1/metadata/x509/k3s-container-puller%40redacted.iam.gserviceaccount.com\\\"\\n}\"}],[\"code\",{\"code\":\"{ \\\"type\\\": \\\"service_account\\\", \\\"project_id\\\": \\\"redacted\\\",  \\\"private_key_id\\\": \\\"redacted\\\",  \\\"private_key\\\": \\\"-----BEGIN PRIVATE KEY-----\\\\nredacted\\\\n-----END PRIVATE KEY-----\\\\n\\\",  \\\"client_email\\\": \\\"k3s-container-puller@redacted.iam.gserviceaccount.com\\\",  \\\"client_id\\\": \\\"redacted\\\",  \\\"auth_uri\\\": \\\"https://accounts.google.com/o/oauth2/auth\\\",  \\\"token_uri\\\": \\\"https://oauth2.googleapis.com/token\\\",  \\\"auth_provider_x509_cert_url\\\": \\\"https://www.googleapis.com/oauth2/v1/certs\\\",  \\\"client_x509_cert_url\\\": \\\"https://www.googleapis.com/robot/v1/metadata/x509/k3s-container-puller%40redacted.iam.gserviceaccount.com\\\"}\",\"caption\":\"Ignore the formatting, if you copied and pasted this, it would be on one line\"}],[\"code\",{\"code\":\"# registries.yaml \\nmirrors:\\n  europe-west2-docker.pkg.dev:\\n    endpoint:\\n      - \\\"https://europe-west2-docker.pkg.dev\\\"\\nconfigs:\\n  europe-west2-docker.pkg.dev:\\n    auth:\\n      username: _json_key\\n      password: '{ \\\"type\\\": \\\"service_account\\\", \\\"project_id\\\": \\\"redacted\\\",  \\\"private_key_id\\\": \\\"redacted\\\",  \\\"private_key\\\": \\\"-----BEGIN PRIVATE KEY-----\\\\nredacted\\\\n-----END PRIVATE KEY-----\\\\n\\\",  \\\"client_email\\\": \\\"k3s-container-puller@redacted.iam.gserviceaccount.com\\\",  \\\"client_id\\\": \\\"redacted\\\",  \\\"auth_uri\\\": \\\"https://accounts.google.com/o/oauth2/auth\\\",  \\\"token_uri\\\": \\\"https://oauth2.googleapis.com/token\\\",  \\\"auth_provider_x509_cert_url\\\": \\\"https://www.googleapis.com/oauth2/v1/certs\\\",  \\\"client_x509_cert_url\\\": \\\"https://www.googleapis.com/robot/v1/metadata/x509/k3s-container-puller%40redacted.iam.gserviceaccount.com\\\"}'\",\"language\":\"yaml\"}],[\"code\",{\"code\":\"systemctl restart k3s\"}],[\"bookmark\",{\"type\":\"bookmark\",\"url\":\"https://documentation.breadnet.co.uk/kubernetes/k3s/k3s-private-registry-using-google-artifact-registry/?mtm_campaign=breadnet&mtm_kwd=private-reg\",\"metadata\":{\"url\":\"https://documentation.breadnet.co.uk/kubernetes/k3s/k3s-private-registry-using-google-artifact-registry/\",\"title\":\"K3s private registry using Google Artifact Registry - breadNET Documentation\",\"description\":\"breadNET Documentation\",\"author\":null,\"publisher\":\"logo\",\"thumbnail\":\"https://documentation.breadnet.co.uk/assets/images/social/kubernetes/k3s/k3s-private-registry-using-google-artifact-registry.png\",\"icon\":\"https://documentation.breadnet.co.uk/favicon.ico\"}}]],\"markups\":[[\"a\",[\"href\",\"https://cloud.google.com/artifact-registry\"]],[\"code\"],[\"em\"],[\"strong\"]],\"sections\":[[1,\"p\",[[0,[],0,\"It's been a while since we've done a post about truly self hosted stuff. This is in part due to me being a DevOps engineer. \"]]],[1,\"p\",[[0,[],0,\"That aside, today I aim to solve the problem of how to authenticate a k3s cluster to Google Artifact Registry\"]]],[1,\"h2\",[[0,[],0,\"Why we need to do this\"]]],[1,\"p\",[[0,[],0,\"Google has a really good Container registry, called \"],[0,[0],1,\"Artifact Registry\"],[0,[],0,\". I am hosting all of my images on the container registry as it allows better IAM control, as well as being easier to authenticate to when using GKE.\"]]],[1,\"p\",[[0,[],0,\"Why bother mentioning GKE? Well I spend about 90% of my day working on GKE, so it makes sense that I would use a Google Cloud service for this.\"]]],[1,\"h2\",[[0,[],0,\"What is the actual issue\"]]],[1,\"p\",[[0,[],0,\"Unless your GAR (Google Artifact Registry) repo is public, then you have to authenticate to it. \"]]],[1,\"p\",[[0,[],0,\"If we try and pull an image on our k3s cluster without authentication we get the below error\"]]],[10,0],[1,\"p\",[[0,[],0,\"This is basically saying, you've not told me who you are, I am not letting you in.\"]]],[1,\"p\",[[0,[],0,\"In order to resolve this, we need to authenticate k3s to Google Cloud.\"]]],[1,\"h2\",[[0,[],0,\"How to fix this\"]]],[1,\"p\",[[0,[],0,\"I wont go in to detail on the GCP cloud side, as if you've come across this I assume you know how to do the below but:\"]]],[3,\"ol\",[[[0,[],0,\"Create a Service account\"]],[[0,[],0,\"Export the Service account keys to your computer as a json file (hold on to this, we need it later!)\"]],[[0,[],0,\"Add that service account to the Google Artifact Registry using \"],[0,[1],1,\"Artifact Registry Reader\"]]]],[1,\"h3\",[[0,[],0,\"Formatting the Service account file correctly\"]]],[1,\"p\",[[0,[],0,\"In order to provision the \"],[0,[2],1,\"custom\"],[0,[],0,\" registry in k3s, we need to authenticate to it.\"]]],[1,\"p\",[[0,[],0,\"The json file needs to be \"],[0,[2],1,\"stacked\"],[0,[],0,\" or converted to a single line.\"]]],[1,\"p\",[[0,[],0,\"This can be done by going to the end of each line, and deleting the new line. Example is below\"]]],[10,1],[1,\"p\",[[0,[],0,\"Becomes \"]]],[10,2],[1,\"p\",[[0,[],0,\"We are then able to creat the \"],[0,[1],1,\"registries.yaml\"],[0,[],0,\" file\"]]],[10,3],[1,\"blockquote\",[[0,[],0,\"Note!\"],[1,[],0,0],[0,[],0,\"Ensure that you surround the value for \"],[0,[1],1,\"password\"],[0,[],0,\" with \"],[0,[1],1,\"'\"],[0,[],0,\" or it will brea\"]]],[1,\"p\",[[0,[],0,\"Name this \"],[0,[1],1,\"registries.yaml\"]]],[1,\"p\",[[0,[],0,\"Copy this file to \"],[0,[3],1,\"all workers\"],[0,[],0,\" (and servers if pods can be scheduled on them)\"]]],[1,\"p\",[[0,[],0,\"Put the file in \"],[0,[1],1,\"/etc/rancher/k3s\"]]],[1,\"p\",[[0,[],0,\"You will then need to restart \"],[0,[1],1,\"k3s\"]]],[10,4],[1,\"h1\",[[0,[],0,\"Further reading\"]]],[1,\"p\",[[0,[],0,\"This blog post was made off of a documentation page I wrote\"]]],[10,5],[1,\"p\",[]]],\"ghostVersion\":\"3.0\"}","lexical":null,"html":"<p>It's been a while since we've done a post about truly self hosted stuff. This is in part due to me being a DevOps engineer. </p><p>That aside, today I aim to solve the problem of how to authenticate a k3s cluster to Google Artifact Registry</p><h2 id=\"why-we-need-to-do-this\">Why we need to do this</h2><p>Google has a really good Container registry, called <a href=\"https://cloud.google.com/artifact-registry\">Artifact Registry</a>. I am hosting all of my images on the container registry as it allows better IAM control, as well as being easier to authenticate to when using GKE.</p><p>Why bother mentioning GKE? Well I spend about 90% of my day working on GKE, so it makes sense that I would use a Google Cloud service for this.</p><h2 id=\"what-is-the-actual-issue\">What is the actual issue</h2><p>Unless your GAR (Google Artifact Registry) repo is public, then you have to authenticate to it. </p><p>If we try and pull an image on our k3s cluster without authentication we get the below error</p><pre><code> ErrImagePull: rpc error: code = Unknown desc = failed to pull and unpack image \"europe-west2-docker.pkg.dev/breadnet-container-store/redacted/documentation-dev:0.0.1\": failed to resolve reference \"europe-west2-docker.pkg.dev/breadnet-container-store/redacted/documentation-dev:0.0.1\": failed to authorize: failed to fetch anonymous token: unexpected status: 403 Forbidden\n</code></pre><p>This is basically saying, you've not told me who you are, I am not letting you in.</p><p>In order to resolve this, we need to authenticate k3s to Google Cloud.</p><h2 id=\"how-to-fix-this\">How to fix this</h2><p>I wont go in to detail on the GCP cloud side, as if you've come across this I assume you know how to do the below but:</p><ol><li>Create a Service account</li><li>Export the Service account keys to your computer as a json file (hold on to this, we need it later!)</li><li>Add that service account to the Google Artifact Registry using <code>Artifact Registry Reader</code></li></ol><h3 id=\"formatting-the-service-account-file-correctly\">Formatting the Service account file correctly</h3><p>In order to provision the <em>custom</em> registry in k3s, we need to authenticate to it.</p><p>The json file needs to be <em>stacked</em> or converted to a single line.</p><p>This can be done by going to the end of each line, and deleting the new line. Example is below</p><pre><code>{\n  \"type\": \"service_account\",\n  \"project_id\": \"redacted\",\n  \"private_key_id\": \"redacted\",\n  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nredacted\\n-----END         PRIVATE KEY-----\\n\",\n  \"client_email\": \"k3s-container-puller@redacted.iam.gserviceaccount.com\",\n  \"client_id\": \"redacted\",\n  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/k3s-container-puller%40redacted.iam.gserviceaccount.com\"\n}</code></pre><p>Becomes </p><figure class=\"kg-card kg-code-card\"><pre><code>{ \"type\": \"service_account\", \"project_id\": \"redacted\",  \"private_key_id\": \"redacted\",  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nredacted\\n-----END PRIVATE KEY-----\\n\",  \"client_email\": \"k3s-container-puller@redacted.iam.gserviceaccount.com\",  \"client_id\": \"redacted\",  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",  \"token_uri\": \"https://oauth2.googleapis.com/token\",  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/k3s-container-puller%40redacted.iam.gserviceaccount.com\"}</code></pre><figcaption>Ignore the formatting, if you copied and pasted this, it would be on one line</figcaption></figure><p>We are then able to creat the <code>registries.yaml</code> file</p><pre><code class=\"language-yaml\"># registries.yaml \nmirrors:\n  europe-west2-docker.pkg.dev:\n    endpoint:\n      - \"https://europe-west2-docker.pkg.dev\"\nconfigs:\n  europe-west2-docker.pkg.dev:\n    auth:\n      username: _json_key\n      password: '{ \"type\": \"service_account\", \"project_id\": \"redacted\",  \"private_key_id\": \"redacted\",  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nredacted\\n-----END PRIVATE KEY-----\\n\",  \"client_email\": \"k3s-container-puller@redacted.iam.gserviceaccount.com\",  \"client_id\": \"redacted\",  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",  \"token_uri\": \"https://oauth2.googleapis.com/token\",  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/k3s-container-puller%40redacted.iam.gserviceaccount.com\"}'</code></pre><blockquote>Note!<br>Ensure that you surround the value for <code>password</code> with <code>'</code> or it will brea</blockquote><p>Name this <code>registries.yaml</code></p><p>Copy this file to <strong>all workers</strong> (and servers if pods can be scheduled on them)</p><p>Put the file in <code>/etc/rancher/k3s</code></p><p>You will then need to restart <code>k3s</code></p><pre><code>systemctl restart k3s</code></pre><h1 id=\"further-reading\">Further reading</h1><p>This blog post was made off of a documentation page I wrote</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://documentation.breadnet.co.uk/kubernetes/k3s/k3s-private-registry-using-google-artifact-registry/?mtm_campaign&#x3D;breadnet&amp;mtm_kwd&#x3D;private-reg\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">K3s private registry using Google Artifact Registry - breadNET Documentation</div><div class=\"kg-bookmark-description\">breadNET Documentation</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://documentation.breadnet.co.uk/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">logo</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://documentation.breadnet.co.uk/assets/images/social/kubernetes/k3s/k3s-private-registry-using-google-artifact-registry.png\" alt=\"\"></div></a></figure>","comment_id":"6411048017e5727444dc80a9","plaintext":"It's been a while since we've done a post about truly self hosted stuff. This is in part due to me being a DevOps engineer.\n\nThat aside, today I aim to solve the problem of how to authenticate a k3s cluster to Google Artifact Registry\n\n\nWhy we need to do this\n\nGoogle has a really good Container registry, called Artifact Registry. I am hosting all of my images on the container registry as it allows better IAM control, as well as being easier to authenticate to when using GKE.\n\nWhy bother mentioning GKE? Well I spend about 90% of my day working on GKE, so it makes sense that I would use a Google Cloud service for this.\n\n\nWhat is the actual issue\n\nUnless your GAR (Google Artifact Registry) repo is public, then you have to authenticate to it.\n\nIf we try and pull an image on our k3s cluster without authentication we get the below error\n\n ErrImagePull: rpc error: code = Unknown desc = failed to pull and unpack image \"europe-west2-docker.pkg.dev/breadnet-container-store/redacted/documentation-dev:0.0.1\": failed to resolve reference \"europe-west2-docker.pkg.dev/breadnet-container-store/redacted/documentation-dev:0.0.1\": failed to authorize: failed to fetch anonymous token: unexpected status: 403 Forbidden\n\n\nThis is basically saying, you've not told me who you are, I am not letting you in.\n\nIn order to resolve this, we need to authenticate k3s to Google Cloud.\n\n\nHow to fix this\n\nI wont go in to detail on the GCP cloud side, as if you've come across this I assume you know how to do the below but:\n\n 1. Create a Service account\n 2. Export the Service account keys to your computer as a json file (hold on to this, we need it later!)\n 3. Add that service account to the Google Artifact Registry using Artifact Registry Reader\n\n\nFormatting the Service account file correctly\n\nIn order to provision the custom registry in k3s, we need to authenticate to it.\n\nThe json file needs to be stacked or converted to a single line.\n\nThis can be done by going to the end of each line, and deleting the new line. Example is below\n\n{\n  \"type\": \"service_account\",\n  \"project_id\": \"redacted\",\n  \"private_key_id\": \"redacted\",\n  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nredacted\\n-----END         PRIVATE KEY-----\\n\",\n  \"client_email\": \"k3s-container-puller@redacted.iam.gserviceaccount.com\",\n  \"client_id\": \"redacted\",\n  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/k3s-container-puller%40redacted.iam.gserviceaccount.com\"\n}\n\nBecomes\n\n{ \"type\": \"service_account\", \"project_id\": \"redacted\",  \"private_key_id\": \"redacted\",  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nredacted\\n-----END PRIVATE KEY-----\\n\",  \"client_email\": \"k3s-container-puller@redacted.iam.gserviceaccount.com\",  \"client_id\": \"redacted\",  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",  \"token_uri\": \"https://oauth2.googleapis.com/token\",  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/k3s-container-puller%40redacted.iam.gserviceaccount.com\"}\n\nWe are then able to creat the registries.yaml file\n\n# registries.yaml \nmirrors:\n  europe-west2-docker.pkg.dev:\n    endpoint:\n      - \"https://europe-west2-docker.pkg.dev\"\nconfigs:\n  europe-west2-docker.pkg.dev:\n    auth:\n      username: _json_key\n      password: '{ \"type\": \"service_account\", \"project_id\": \"redacted\",  \"private_key_id\": \"redacted\",  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nredacted\\n-----END PRIVATE KEY-----\\n\",  \"client_email\": \"k3s-container-puller@redacted.iam.gserviceaccount.com\",  \"client_id\": \"redacted\",  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",  \"token_uri\": \"https://oauth2.googleapis.com/token\",  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/k3s-container-puller%40redacted.iam.gserviceaccount.com\"}'\n\nNote!\nEnsure that you surround the value for password with ' or it will brea\n\nName this registries.yaml\n\nCopy this file to all workers (and servers if pods can be scheduled on them)\n\nPut the file in /etc/rancher/k3s\n\nYou will then need to restart k3s\n\nsystemctl restart k3s\n\n\nFurther reading\n\nThis blog post was made off of a documentation page I wrote\n\nK3s private registry using Google Artifact Registry - breadNET DocumentationbreadNET Documentationlogo","feature_image":"https://images.unsplash.com/photo-1613690399151-65ea69478674?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwxMTc3M3wwfDF8c2VhcmNofDN8fHNoaXBwaW5nJTIwY29udGFpbmVyfGVufDB8fHx8MTY4MDIxMzY4NA&ixlib=rb-4.0.3&q=80&w=2000","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2023-03-14T23:34:24.000Z","updated_at":"2023-03-30T22:01:44.000Z","published_at":"2023-03-15T00:01:12.000Z","custom_excerpt":"Authenticate K3S to Google Artifact Registry","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"6427175a5da07d2d3382d211","uuid":"f8aa99b1-fc24-4cd8-87ff-21a92e210dce","title":"Migrating to the cloud: The end","slug":"moving-to-the-cloud-3","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"bookmark\",{\"version\":\"1.0\",\"type\":\"bookmark\",\"url\":\"https://breadnet.co.uk/moving-to-the-cloud-1/\",\"metadata\":{\"url\":\"https://breadnet.co.uk/moving-to-the-cloud-1/\",\"title\":\"breadNET Cloud Migration\",\"description\":\"How did I move all my servers to the cloud? Well - Ansible, automation and CI/CD!\",\"author\":\"Bradley Stannard\",\"publisher\":\"breadNET\",\"thumbnail\":\"https://images.unsplash.com/photo-1560182413-53772f3d7134?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=2000&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ\",\"icon\":\"https://breadnet.co.uk/favicon.png\"}}],[\"bookmark\",{\"version\":\"1.0\",\"type\":\"bookmark\",\"url\":\"https://breadnet.co.uk/moving-to-the-cloud-2/\",\"metadata\":{\"url\":\"https://breadnet.co.uk/moving-to-the-cloud-2/\",\"title\":\"Moving to the cloud: Infrastructure\",\"description\":\"Part 2 of moving to the cloud - Let’s talk about IaC\",\"author\":\"Bradley Stannard\",\"publisher\":\"breadNET\",\"thumbnail\":\"https://images.unsplash.com/photo-1555066931-4365d14bab8c?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwxMTc3M3wwfDF8c2VhcmNofDEzfHxjb2RlfGVufDB8fHx8MTYxNzUwMzcyMA&ixlib=rb-1.2.1&q=80&w=2000\",\"icon\":\"https://breadnet.co.uk/favicon.png\"}}],[\"bookmark\",{\"version\":\"1.0\",\"type\":\"bookmark\",\"url\":\"https://registry.terraform.io/providers/digitalocean/digitalocean/latest/docs\",\"metadata\":{\"url\":\"https://registry.terraform.io/providers/digitalocean/digitalocean/latest/docs\",\"title\":\"Terraform Registry\",\"description\":null,\"author\":null,\"publisher\":null,\"thumbnail\":null,\"icon\":\"https://registry.terraform.io/images/favicons/apple-touch-icon.png\"}}],[\"code\",{\"code\":\"# Server Config\\n# /etc/cloudflared/config.yml\\nlogDirectory: /var/log/cloudflared\\ntunnel: 1b70-4325-9bb5\\ncredentials-file: /home/<>/.cloudflared/1b70-4325-9bb51.json\\nno-autoupdate: true\\ningress:\\n  - hostname: ssh-<server>.breadinfra.net\\n    service: ssh://127.0.0.1:22\\n  - service: http_status:404\\n  \\n \"}],[\"code\",{\"code\":\"# Laptop Config\\n# ~/.ssh/config\\nhost unifi\\n hostname ssh-<>.breadinfra.net\\n user root\\n ProxyCommand /opt/homebrew/bin/cloudflared access ssh --hostname %h\"}],[\"bookmark\",{\"version\":\"1.0\",\"type\":\"bookmark\",\"url\":\"https://documentation.breadnet.co.uk\",\"metadata\":{\"url\":\"https://documentation.breadnet.co.uk/\",\"title\":\"Welcome\",\"description\":\"breadNET Documentation\",\"author\":null,\"publisher\":\"logo\",\"thumbnail\":\"https://documentation.breadnet.co.uk/assets/images/social/index.png\",\"icon\":\"https://documentation.breadnet.co.uk/favicon.ico\"}}],[\"bookmark\",{\"version\":\"1.0\",\"type\":\"bookmark\",\"url\":\"https://github.com/userbradley/documentation.breadnet.co.uk\",\"metadata\":{\"url\":\"https://github.com/userbradley/documentation.breadnet.co.uk\",\"title\":\"GitHub - userbradley/documentation.breadnet.co.uk: breadNET Documentation maintained by @userbradley\",\"description\":\"breadNET Documentation maintained by @userbradley. Contribute to userbradley/documentation.breadnet.co.uk development by creating an account on GitHub.\",\"author\":\"userbradley\",\"publisher\":\"GitHub\",\"thumbnail\":\"https://opengraph.githubassets.com/7bf95a4efb4f28145e423104c6a29be7324eaad2d2239a569a88da139b1cc82f/userbradley/documentation.breadnet.co.uk\",\"icon\":\"https://github.com/fluidicon.png\"}}],[\"callout\",{\"calloutEmoji\":\"💸\",\"calloutText\":\"I'm sorry, I have included Digital Ocean Refferal links, it helps keep the lights on. I don't run ads on this site, or track you across the web. I only track you across the site, but as soon as you leave I have no idea what you're up to. <br><br>Thank you for your understanding on this, you're welcome to buy me a coffee instead!\",\"backgroundColor\":\"grey\"}]],\"markups\":[[\"em\"],[\"a\",[\"href\",\"https://www.openstack.org?ref=breadnet.co.uk\"]],[\"a\",[\"href\",\"https://m.do.co/c/77be3c3aa96c\"]],[\"a\",[\"href\",\"https://en.wikipedia.org/wiki/Monorepo\"]],[\"a\",[\"href\",\"https://documentation.breadnet.co.uk/kb/cloudflared/cloudflared-on-ubuntu-for-ssh/?mtm_campaign=breadnetsite&mtm_kwd=migrating-to-cloud-the-end\"]],[\"a\",[\"href\",\"https://breadnet.co.uk/migrating-off-bookstack/\"]],[\"s\"],[\"code\"]],\"sections\":[[1,\"p\",[[0,[],0,\"This is the final post in my \"],[0,[0],1,\"Migrating to the cloud \"],[0,[],0,\"trio. Just like Orange is the new black, it can only get worse after the 3rd installment. \"]]],[10,0],[10,1],[1,\"p\",[[0,[],0,\"In the previous years, I wrote about migrating from servers in my parents garage to being on the cloud. \"]]],[1,\"p\",[[0,[],0,\"Since then, breadNET has changed a lot!\"]]],[1,\"p\",[[0,[],0,\"We've gone from being running 100% on premise, to 50/50, to 70/30 then back on premise and then back to the cloud. It's been quite a mish mash of moving around.\"]]],[1,\"p\",[[0,[],0,\"After about 3 years (almost to the date) I have 100% migrated to the cloud, and now have everything stored as code in git. \"]]],[1,\"p\",[[0,[],0,\"We're going to take a look at the issues I had during the migration, the data I've had to put on ice and the career changes that aided these migrations.\"]]],[1,\"h1\",[[0,[],0,\"First there was the IaC\"]]],[1,\"p\",[[0,[],0,\"Previously I migrated all my services to OVH, which was great for getting things set up and running until I was able to get a job that gave me disposable income. \"]]],[1,\"p\",[[0,[],0,\"I migrated as many applications as I could to Software as a service - with the most noticeable ones being:\"]]],[3,\"ul\",[[[0,[],0,\"Jira\"]],[[0,[],0,\"Git\"]]]],[1,\"p\",[[0,[],0,\"It was not economical for us to run a Jira instance (Requires around 4GB ram and 2 cores - Which costs more on the cloud) \"]]],[1,\"p\",[[0,[],0,\"I wanted everything as code, so terraform again. We previously used OVH, which under the hood is \"],[0,[1],1,\"Open Stack\"],[0,[],0,\". We can use the \"],[0,[2],1,\"Digital ocean\"],[0,[],0,\" terraform provider to control our account\"]]],[10,2],[1,\"p\",[[0,[],0,\"Since moving to \"],[0,[2],1,\"Digital Ocean\"],[0,[],0,\", I've found my site is a lot more reliable and latency is a lot lower. Only issue I've had is it's a little more pricy.\"]]],[1,\"h2\",[[0,[],0,\"GitOps\"]]],[1,\"p\",[[0,[],0,\"This is the best thing ever, this basically means using GitHub and CI to build and deploy infrastructure. \"]]],[1,\"p\",[[0,[],0,\"This very nicely moves us on to the next part of the migration. Git.\"]]],[1,\"p\",[[0,[],0,\"I've moved all my Git based \"],[0,[0],1,\"activities \"],[0,[],0,\"to GitHub. all in a single \"],[0,[3],1,\"Mono Repository\"],[0,[],0,\" meaning that all the code related to my infrastructure is in one place.\"]]],[1,\"h1\",[[0,[],0,\"Accessing servers\"]]],[1,\"p\",[[0,[],0,\"I previously wrote about using Pritunl Zero for accessing my servers and internal applications. This method works quite well if all your applications sit on-premise and you have LDAP (as well as the paid plan for Pritunl)\"]]],[1,\"p\",[[0,[],0,\"Cloudflare (The people I get my DNS from) also have a feature called Zero. It's pretty much a free zero trust platform that uses the cloudflare edge to terminate your traffic, then send it back out again close to your server.\"]]],[1,\"p\",[[0,[],0,\"I've got this installed on all my servers, meaning I can use my native SSH client to connect, and not having to overwrite my SSH client with the one built by Pritunl.\"]]],[1,\"p\",[[0,[],0,\"Below is an example code snippet that goes on the server, \"],[0,[4],1,\"once you've followed the Installing cloudflared on Ubuntu page\"],[0,[],0,\" \"]]],[10,3],[1,\"p\",[[0,[],0,\"Finally, you can then configure your laptop with the below which allows your client to connect over ssh to the server\"]]],[10,4],[1,\"h1\",[[0,[],0,\"Serverless\"]]],[1,\"p\",[[0,[],0,\"Serverless is a weird concept. Just means that you don't manage the server, and only deploy a docker container or just your code to it.\"]]],[1,\"p\",[[0,[],0,\"A great example of serverless is, my documentation site. I write markdown and then it gets rendered and hosted\"]]],[10,5],[1,\"p\",[[0,[],0,\"By migrating \"],[0,[5],1,\"Bookstack to Mkdocs\"],[0,[],0,\" I have been able to save $6 a month, by hosting the site on fly.io.\"]]],[1,\"p\",[[0,[],0,\"Not to keep going on, but a great example of why this is great is, it costs me absolutly nothing a month. The Git is free, the CICD is free as it's a public repo\"]]],[10,6],[1,\"h1\",[[0,[],0,\"Firewalls \"]]],[1,\"p\",[[0,[],0,\"Cloudflare have \"],[0,[6],1,\"Argo Tunnels\"],[0,[],0,\" \"],[0,[7],1,\"Cloudflare Tunnels\"],[0,[],0,\", which allows you to punch out from your device, to the Cloudflare edge opposed to port forwarding. This is great for security as you're no longer doing one of the two:\"]]],[3,\"ol\",[[[0,[],0,\"Port forwarding to all the Cloudflare IP ranges\"]],[[0,[],0,\"Port forwarding \"],[0,[7],1,\"tcp/80\"],[0,[],0,\" and \"],[0,[7],1,\"tcp/443\"],[0,[],0,\" to the entire internet\"]]]],[1,\"p\",[[0,[],0,\"All you have to do, if you have restrictive outbound rules, is allow \"],[0,[7],1,\"udp/7844\"],[0,[],0,\" and \"],[0,[7],1,\"tcp/7844\"],[0,[],0,\" \"]]],[1,\"h1\",[[0,[],0,\"Wrapping it up\"]]],[1,\"p\",[[0,[],0,\"I know this was not the best blog, but if you have noticed, the site has been migrated to a new faster server, and now uses Cloudflare Tunnels (Dog fooding) \"]]],[1,\"p\",[[0,[],0,\"I have plans this year to spend some more time working with Kubernetes and possibly running some applications on k3s using cloudflare tunnels.\"]]],[1,\"p\",[]],[1,\"p\",[]],[1,\"p\",[]],[1,\"p\",[]],[1,\"p\",[]],[1,\"p\",[]],[10,7],[1,\"p\",[]],[1,\"p\",[]],[1,\"p\",[]]],\"ghostVersion\":\"4.0\"}","lexical":null,"html":"<p>This is the final post in my <em>Migrating to the cloud </em>trio. Just like Orange is the new black, it can only get worse after the 3rd installment. </p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://breadnet.co.uk/moving-to-the-cloud-1/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">breadNET Cloud Migration</div><div class=\"kg-bookmark-description\">How did I move all my servers to the cloud? Well - Ansible, automation and CI/CD!</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://breadnet.co.uk/favicon.png\" alt=\"\"><span class=\"kg-bookmark-author\">breadNET</span><span class=\"kg-bookmark-publisher\">Bradley Stannard</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://images.unsplash.com/photo-1560182413-53772f3d7134?ixlib&#x3D;rb-1.2.1&amp;q&#x3D;80&amp;fm&#x3D;jpg&amp;crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;w&#x3D;2000&amp;fit&#x3D;max&amp;ixid&#x3D;eyJhcHBfaWQiOjExNzczfQ\" alt=\"\"></div></a></figure><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://breadnet.co.uk/moving-to-the-cloud-2/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Moving to the cloud: Infrastructure</div><div class=\"kg-bookmark-description\">Part 2 of moving to the cloud - Let’s talk about IaC</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://breadnet.co.uk/favicon.png\" alt=\"\"><span class=\"kg-bookmark-author\">breadNET</span><span class=\"kg-bookmark-publisher\">Bradley Stannard</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://images.unsplash.com/photo-1555066931-4365d14bab8c?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;MnwxMTc3M3wwfDF8c2VhcmNofDEzfHxjb2RlfGVufDB8fHx8MTYxNzUwMzcyMA&amp;ixlib&#x3D;rb-1.2.1&amp;q&#x3D;80&amp;w&#x3D;2000\" alt=\"\"></div></a></figure><p>In the previous years, I wrote about migrating from servers in my parents garage to being on the cloud. </p><p>Since then, breadNET has changed a lot!</p><p>We've gone from being running 100% on premise, to 50/50, to 70/30 then back on premise and then back to the cloud. It's been quite a mish mash of moving around.</p><p>After about 3 years (almost to the date) I have 100% migrated to the cloud, and now have everything stored as code in git. </p><p>We're going to take a look at the issues I had during the migration, the data I've had to put on ice and the career changes that aided these migrations.</p><h1 id=\"first-there-was-the-iac\">First there was the IaC</h1><p>Previously I migrated all my services to OVH, which was great for getting things set up and running until I was able to get a job that gave me disposable income. </p><p>I migrated as many applications as I could to Software as a service - with the most noticeable ones being:</p><ul><li>Jira</li><li>Git</li></ul><p>It was not economical for us to run a Jira instance (Requires around 4GB ram and 2 cores - Which costs more on the cloud) </p><p>I wanted everything as code, so terraform again. We previously used OVH, which under the hood is <a href=\"https://www.openstack.org?ref=breadnet.co.uk\">Open Stack</a>. We can use the <a href=\"https://m.do.co/c/77be3c3aa96c\">Digital ocean</a> terraform provider to control our account</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://registry.terraform.io/providers/digitalocean/digitalocean/latest/docs\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Terraform Registry</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://registry.terraform.io/images/favicons/apple-touch-icon.png\" alt=\"\"></div></div></a></figure><p>Since moving to <a href=\"https://m.do.co/c/77be3c3aa96c\">Digital Ocean</a>, I've found my site is a lot more reliable and latency is a lot lower. Only issue I've had is it's a little more pricy.</p><h2 id=\"gitops\">GitOps</h2><p>This is the best thing ever, this basically means using GitHub and CI to build and deploy infrastructure. </p><p>This very nicely moves us on to the next part of the migration. Git.</p><p>I've moved all my Git based <em>activities </em>to GitHub. all in a single <a href=\"https://en.wikipedia.org/wiki/Monorepo\">Mono Repository</a> meaning that all the code related to my infrastructure is in one place.</p><h1 id=\"accessing-servers\">Accessing servers</h1><p>I previously wrote about using Pritunl Zero for accessing my servers and internal applications. This method works quite well if all your applications sit on-premise and you have LDAP (as well as the paid plan for Pritunl)</p><p>Cloudflare (The people I get my DNS from) also have a feature called Zero. It's pretty much a free zero trust platform that uses the cloudflare edge to terminate your traffic, then send it back out again close to your server.</p><p>I've got this installed on all my servers, meaning I can use my native SSH client to connect, and not having to overwrite my SSH client with the one built by Pritunl.</p><p>Below is an example code snippet that goes on the server, <a href=\"https://documentation.breadnet.co.uk/kb/cloudflared/cloudflared-on-ubuntu-for-ssh/?mtm_campaign=breadnetsite&amp;mtm_kwd=migrating-to-cloud-the-end\">once you've followed the Installing cloudflared on Ubuntu page</a> </p><pre><code># Server Config\n# /etc/cloudflared/config.yml\nlogDirectory: /var/log/cloudflared\ntunnel: 1b70-4325-9bb5\ncredentials-file: /home/&lt;&gt;/.cloudflared/1b70-4325-9bb51.json\nno-autoupdate: true\ningress:\n  - hostname: ssh-&lt;server&gt;.breadinfra.net\n    service: ssh://127.0.0.1:22\n  - service: http_status:404\n  \n </code></pre><p>Finally, you can then configure your laptop with the below which allows your client to connect over ssh to the server</p><pre><code># Laptop Config\n# ~/.ssh/config\nhost unifi\n hostname ssh-&lt;&gt;.breadinfra.net\n user root\n ProxyCommand /opt/homebrew/bin/cloudflared access ssh --hostname %h</code></pre><h1 id=\"serverless\">Serverless</h1><p>Serverless is a weird concept. Just means that you don't manage the server, and only deploy a docker container or just your code to it.</p><p>A great example of serverless is, my documentation site. I write markdown and then it gets rendered and hosted</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://documentation.breadnet.co.uk\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Welcome</div><div class=\"kg-bookmark-description\">breadNET Documentation</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://documentation.breadnet.co.uk/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">logo</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://documentation.breadnet.co.uk/assets/images/social/index.png\" alt=\"\"></div></a></figure><p>By migrating <a href=\"https://breadnet.co.uk/migrating-off-bookstack/\">Bookstack to Mkdocs</a> I have been able to save $6 a month, by hosting the site on fly.io.</p><p>Not to keep going on, but a great example of why this is great is, it costs me absolutly nothing a month. The Git is free, the CICD is free as it's a public repo</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://github.com/userbradley/documentation.breadnet.co.uk\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">GitHub - userbradley/documentation.breadnet.co.uk: breadNET Documentation maintained by @userbradley</div><div class=\"kg-bookmark-description\">breadNET Documentation maintained by @userbradley. Contribute to userbradley/documentation.breadnet.co.uk development by creating an account on GitHub.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://github.com/fluidicon.png\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">userbradley</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://opengraph.githubassets.com/7bf95a4efb4f28145e423104c6a29be7324eaad2d2239a569a88da139b1cc82f/userbradley/documentation.breadnet.co.uk\" alt=\"\"></div></a></figure><h1 id=\"firewalls\">Firewalls </h1><p>Cloudflare have <s>Argo Tunnels</s> <code>Cloudflare Tunnels</code>, which allows you to punch out from your device, to the Cloudflare edge opposed to port forwarding. This is great for security as you're no longer doing one of the two:</p><ol><li>Port forwarding to all the Cloudflare IP ranges</li><li>Port forwarding <code>tcp/80</code> and <code>tcp/443</code> to the entire internet</li></ol><p>All you have to do, if you have restrictive outbound rules, is allow <code>udp/7844</code> and <code>tcp/7844</code> </p><h1 id=\"wrapping-it-up\">Wrapping it up</h1><p>I know this was not the best blog, but if you have noticed, the site has been migrated to a new faster server, and now uses Cloudflare Tunnels (Dog fooding) </p><p>I have plans this year to spend some more time working with Kubernetes and possibly running some applications on k3s using cloudflare tunnels.</p><p></p><p></p><p></p><p></p><p></p><p></p><div class=\"kg-card kg-callout-card kg-callout-card-grey\"><div class=\"kg-callout-emoji\">💸</div><div class=\"kg-callout-text\">I'm sorry, I have included Digital Ocean Refferal links, it helps keep the lights on. I don't run ads on this site, or track you across the web. I only track you across the site, but as soon as you leave I have no idea what you're up to. <br><br>Thank you for your understanding on this, you're welcome to buy me a coffee instead!</div></div><p></p><p></p>","comment_id":"6427175a5da07d2d3382d211","plaintext":"This is the final post in my Migrating to the cloud trio. Just like Orange is the new black, it can only get worse after the 3rd installment.\n\nbreadNET Cloud MigrationHow did I move all my servers to the cloud? Well - Ansible, automation and CI/CD!breadNETBradley StannardMoving to the cloud: InfrastructurePart 2 of moving to the cloud - Let’s talk about IaCbreadNETBradley Stannard\n\nIn the previous years, I wrote about migrating from servers in my parents garage to being on the cloud.\n\nSince then, breadNET has changed a lot!\n\nWe've gone from being running 100% on premise, to 50/50, to 70/30 then back on premise and then back to the cloud. It's been quite a mish mash of moving around.\n\nAfter about 3 years (almost to the date) I have 100% migrated to the cloud, and now have everything stored as code in git.\n\nWe're going to take a look at the issues I had during the migration, the data I've had to put on ice and the career changes that aided these migrations.\n\n\nFirst there was the IaC\n\nPreviously I migrated all my services to OVH, which was great for getting things set up and running until I was able to get a job that gave me disposable income.\n\nI migrated as many applications as I could to Software as a service - with the most noticeable ones being:\n\n * Jira\n * Git\n\nIt was not economical for us to run a Jira instance (Requires around 4GB ram and 2 cores - Which costs more on the cloud)\n\nI wanted everything as code, so terraform again. We previously used OVH, which under the hood is Open Stack. We can use the Digital ocean terraform provider to control our account\n\nTerraform Registry\n\nSince moving to Digital Ocean, I've found my site is a lot more reliable and latency is a lot lower. Only issue I've had is it's a little more pricy.\n\n\nGitOps\n\nThis is the best thing ever, this basically means using GitHub and CI to build and deploy infrastructure.\n\nThis very nicely moves us on to the next part of the migration. Git.\n\nI've moved all my Git based activities to GitHub. all in a single Mono Repository meaning that all the code related to my infrastructure is in one place.\n\n\nAccessing servers\n\nI previously wrote about using Pritunl Zero for accessing my servers and internal applications. This method works quite well if all your applications sit on-premise and you have LDAP (as well as the paid plan for Pritunl)\n\nCloudflare (The people I get my DNS from) also have a feature called Zero. It's pretty much a free zero trust platform that uses the cloudflare edge to terminate your traffic, then send it back out again close to your server.\n\nI've got this installed on all my servers, meaning I can use my native SSH client to connect, and not having to overwrite my SSH client with the one built by Pritunl.\n\nBelow is an example code snippet that goes on the server, once you've followed the Installing cloudflared on Ubuntu page\n\n# Server Config\n# /etc/cloudflared/config.yml\nlogDirectory: /var/log/cloudflared\ntunnel: 1b70-4325-9bb5\ncredentials-file: /home/<>/.cloudflared/1b70-4325-9bb51.json\nno-autoupdate: true\ningress:\n  - hostname: ssh-<server>.breadinfra.net\n    service: ssh://127.0.0.1:22\n  - service: http_status:404\n  \n \n\nFinally, you can then configure your laptop with the below which allows your client to connect over ssh to the server\n\n# Laptop Config\n# ~/.ssh/config\nhost unifi\n hostname ssh-<>.breadinfra.net\n user root\n ProxyCommand /opt/homebrew/bin/cloudflared access ssh --hostname %h\n\n\nServerless\n\nServerless is a weird concept. Just means that you don't manage the server, and only deploy a docker container or just your code to it.\n\nA great example of serverless is, my documentation site. I write markdown and then it gets rendered and hosted\n\nWelcomebreadNET Documentationlogo\n\nBy migrating Bookstack to Mkdocs I have been able to save $6 a month, by hosting the site on fly.io.\n\nNot to keep going on, but a great example of why this is great is, it costs me absolutly nothing a month. The Git is free, the CICD is free as it's a public repo\n\nGitHub - userbradley/documentation.breadnet.co.uk: breadNET Documentation maintained by @userbradleybreadNET Documentation maintained by @userbradley. Contribute to userbradley/documentation.breadnet.co.uk development by creating an account on GitHub.GitHubuserbradley\n\n\nFirewalls\n\nCloudflare have Argo Tunnels Cloudflare Tunnels, which allows you to punch out from your device, to the Cloudflare edge opposed to port forwarding. This is great for security as you're no longer doing one of the two:\n\n 1. Port forwarding to all the Cloudflare IP ranges\n 2. Port forwarding tcp/80 and tcp/443 to the entire internet\n\nAll you have to do, if you have restrictive outbound rules, is allow udp/7844 and tcp/7844\n\n\nWrapping it up\n\nI know this was not the best blog, but if you have noticed, the site has been migrated to a new faster server, and now uses Cloudflare Tunnels (Dog fooding)\n\nI have plans this year to spend some more time working with Kubernetes and possibly running some applications on k3s using cloudflare tunnels.\n\n\n\n\n\n\n\n\n\n\n\n\n\n💸I'm sorry, I have included Digital Ocean Refferal links, it helps keep the lights on. I don't run ads on this site, or track you across the web. I only track you across the site, but as soon as you leave I have no idea what you're up to.\n\nThank you for your understanding on this, you're welcome to buy me a coffee instead!\n\n\n\n","feature_image":"https://images.unsplash.com/photo-1610692507254-3bc16d2527ea?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwxMTc3M3wwfDF8c2VhcmNofDE0fHxjbG91ZHxlbnwwfHx8fDE2ODAyODM0ODc&ixlib=rb-4.0.3&q=80&w=2000","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2023-03-31T17:24:42.000Z","updated_at":"2023-03-31T18:22:31.000Z","published_at":"2023-03-31T17:25:35.000Z","custom_excerpt":"Final blog post about moving breadNET to the cloud","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"64405506c9cae386d9c53b55","uuid":"4da3d721-4318-481a-8221-5a5288a35552","title":"Docker it's over, moving to podman","slug":"docker-its-over-moving-to-podman","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"callout\",{\"calloutEmoji\":\"💡\",\"calloutText\":\"I will get on to installing podman and how to use it later on in this blog<br>we just need to talk a bit about docker and podman.\",\"backgroundColor\":\"blue\"}],[\"bookmark\",{\"version\":\"1.0\",\"type\":\"bookmark\",\"url\":\"https://dnsmichi.at/2022/03/15/docker-desktop-alternatives-macos-podman-nerdctl-rancher-desktop/?utm_source=breadnet-co-uk&utm_medium=Docker-its-over-moving-to-podman\",\"metadata\":{\"url\":\"https://dnsmichi.at/2022/03/15/docker-desktop-alternatives-macos-podman-nerdctl-rancher-desktop/\",\"title\":\"Docker Desktop alternatives on macOS: podman, nerdctl, Rancher Desktop\",\"description\":\"Docker changed its subscription model including Docker Desktop, thus generating\\nmore demand for alternatives. In this blog post, we look into podman, nerdctl,\\nand Rancher Desktop as Docker Desktop alternatives on macOS. The Docker Desktop subscription changes\\n[https://www.docker.com/blog/updating-…\",\"author\":\"Michael Friedrich\",\"publisher\":\"dnsmichi.at\",\"thumbnail\":\"https://dnsmichi.at/content/images/2022/03/AC6BA369-E613-464F-AEB7-CCA674DC61BE.jpeg\",\"icon\":\"https://dnsmichi.at/favicon.ico\"}}],[\"callout\",{\"calloutEmoji\":\"📬\",\"calloutText\":\"I am going to assume that you don't need me to explain what podman is and what docker is, if you are reading this post&nbsp;\",\"backgroundColor\":\"yellow\"}],[\"code\",{\"code\":\"brew install podman\\npodman machine init\\npodman machine start\"}],[\"code\",{\"code\":\"podman machine init --now --cpus=4 --memory=2046 -v $HOME:$HOME\"}],[\"bookmark\",{\"version\":\"1.0\",\"type\":\"bookmark\",\"url\":\"https://github.com/containers/podman-compose\",\"metadata\":{\"url\":\"https://github.com/containers/podman-compose\",\"title\":\"GitHub - containers/podman-compose: a script to run docker-compose.yml using podman\",\"description\":\"a script to run docker-compose.yml using podman. Contribute to containers/podman-compose development by creating an account on GitHub.\",\"author\":\"containers\",\"publisher\":\"GitHub\",\"thumbnail\":\"https://opengraph.githubassets.com/feddadaf0941e1f1c56e0d8857ead38a124850f7d0dd0f6668371a29e3c8c76a/containers/podman-compose\",\"icon\":\"https://github.com/fluidicon.png\"}}],[\"code\",{\"code\":\"brew install podman-compose\"}],[\"code\",{\"code\":\"alias \\\"docker\\\"=\\\"podman\\\"\\nalias \\\"docker-compose\\\"=\\\"podman-compose\\\"\\n\"}],[\"code\",{\"code\":\"source ~/.zshrc\"}],[\"code\",{\"code\":\"brew install podman-desktop\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2023/04/image.png\",\"width\":2100,\"height\":1402}],[\"code\",{\"code\":\"podman run -it alpine /bin/sh\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2023/04/image-1.png\",\"width\":1100,\"height\":628}],[\"bookmark\",{\"version\":\"1.0\",\"type\":\"bookmark\",\"url\":\"https://skaffold.dev\",\"metadata\":{\"url\":\"https://skaffold.dev/\",\"title\":\"Skaffold\",\"description\":\"Easy and Repeatable Container & Kubernetes Development\",\"author\":\"Warren Strange, Engineering Director, ForgeRock\",\"publisher\":\"Skaffold\",\"thumbnail\":\"https://skaffold.dev/featured-background.jpeg\",\"icon\":\"https://skaffold.dev/favicons/android-192x192.png\"}}],[\"bookmark\",{\"version\":\"1.0\",\"type\":\"bookmark\",\"url\":\"https://github.com/GoogleContainerTools/skaffold/issues/8430\",\"metadata\":{\"url\":\"https://github.com/GoogleContainerTools/skaffold/issues/8430\",\"title\":\"Feature Request - Support for podman builder · Issue #8430 · GoogleContainerTools/skaffold\",\"description\":\"It would be great to have podman in the builders list. I have also tried using podman-docker to fake docker commands to use podman in vain (as expected). ❯ skaffold build Generating tags... - azure…\",\"author\":\"GoogleContainerTools\",\"publisher\":\"GitHub\",\"thumbnail\":\"https://opengraph.githubassets.com/281d390ecbfd29e95c5012d81a0d1e791974ea590843d0674c4500f7aebde57a/GoogleContainerTools/skaffold/issues/8430\",\"icon\":\"https://github.com/fluidicon.png\"}}]],\"markups\":[[\"a\",[\"href\",\"https://boxboat.com/2018/12/07/docker-ce-vs-docker-ee/\"]],[\"a\",[\"href\",\"https://www.docker.com/pricing/october-2022-pricing-change-faq/\"]],[\"a\",[\"href\",\"https://www.bleepingcomputer.com/news/security/docker-hub-repositories-hide-over-1-650-malicious-containers/\"]],[\"a\",[\"href\",\"https://www.docker.com/pricing/october-2022-pricing-change-faq/#:~:text=On%20October%2027%2C%202022%2C%20Docker,)%2C%20per%20user%20per%20month\"]],[\"a\",[\"href\",\"https://www.infoworld.com/article/3630393/docker-desktop-is-no-longer-free-for-enterprise-users.html\"]],[\"a\",[\"href\",\"https://docs.docker.com/docker-hub/download-rate-limit\"]],[\"a\",[\"href\",\"https://www.docker.com/blog/no-longer-sunsetting-the-free-team-plan/\"]],[\"a\",[\"href\",\"https://survey.stackoverflow.co/2022/#section-most-popular-technologies-operating-system\"]],[\"em\"],[\"strong\"],[\"code\"],[\"a\",[\"href\",\"https://www.urbandictionary.com/define.php?term=dosh\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Docker is cool, don't get me wrong. They pretty much brought containerization to the masses. \"]]],[1,\"p\",[[0,[],0,\"So why the breakup letter? What pushed you over the edge\"]]],[3,\"ol\",[[[0,[0],1,\"2017: Licensing changes \"]],[[0,[1],1,\"Docker hub pricing changes\"]],[[0,[2],1,\"Malware hosted on docker\"]],[[0,[3],1,\"Price changes 2022\"]],[[0,[4],1,\"Docker Desktop pricing\"]],[[0,[5],1,\"Rate limit\"]],[[0,[6],1,\"Docker free teams being sunsetted (and then saying sorry for bad comms, and then actually saying sorry and no longer doing it)\"]]]],[1,\"p\",[[0,[],0,\"Sure, most of these above points are about money, but the docker desktop and docker hub ones annoyed me the most\"]]],[10,0],[1,\"p\",[[0,[],0,\"Specifically, on mac \"],[0,[7],1,\"(which has 32% market share on devs)\"],[0,[],0,\" you can only install docker using the desktop CLI. If you then use it for work, you need to pay.\"]]],[1,\"p\",[[0,[],0,\"Of course I am still yet to see Docker Inc take a company to court for not paying for licenses. \"]]],[1,\"h2\",[[0,[],0,\"So why podman? Why switch?\"]]],[1,\"p\",[[0,[],0,\"The real reason I switched was because I broke my docker install so much, that it was easier to switch to a new container runtime \"],[0,[8],1,\"thing\"],[0,[],0,\" on my computer, than it was to try fix it.\"]]],[1,\"p\",[[0,[],0,\"Having a search about, I came across a blog from Michael Friedrich\"]]],[10,1],[1,\"p\",[[0,[],0,\"One of his suggestions was podman, so I turned away from the spike I was working on (Configuring Hound) and put all the time in to getting podman working on my laptop\"]]],[1,\"h1\",[[0,[],0,\"The install process\"]]],[10,2],[1,\"p\",[[0,[],0,\"For context, I am using an M1 mac - Which requires some additional configuration.\"]]],[10,3],[1,\"p\",[[0,[],0,\"That's pretty much it.\"]]],[1,\"p\",[[0,[],0,\"Podman will \"],[0,[8],1,\"hijack\"],[0,[],0,\" your docker config file, so stuff like authentication to google cloud artifact registry and custom registries are \"],[0,[9],1,\"just going to work\"]]],[1,\"p\",[[0,[],0,\"Something to note is that if you plan to run containers with local volumes, you will need to change your init command to the below\"]]],[10,4],[1,\"h1\",[[0,[],0,\"I miss things about docker\"]]],[1,\"p\",[[0,[],0,\"This is normal, for the past 10 years you've been typing \"],[0,[10],1,\"docker\"],[0,[],0,\" and \"],[0,[10],1,\"docker-compose\"],[0,[],0,\" \"]]],[1,\"p\",[[0,[],0,\"Firstly we will take care of \"],[0,[10],1,\"docker-compose\"],[0,[],0,\" as podman has it's own compose tool.\"]]],[10,5],[1,\"p\",[[0,[],0,\"It's called... \"],[0,[10],1,\"podman-compose\"]]],[10,6],[1,\"h3\",[[0,[],0,\"Aliasing stuff\"]]],[1,\"p\",[[0,[],0,\"edit your \"],[0,[10],1,\"~/.zshrc\"],[0,[],0,\" or \"],[0,[10],1,\"~/.bashrc\"],[0,[],0,\" file and add the below in\"]]],[10,7],[1,\"p\",[[0,[],0,\"Then update your source \"]]],[10,8],[1,\"p\",[[0,[],0,\"Any time you invoke \"],[0,[10],1,\"docker\"],[0,[],0,\" or \"],[0,[10],1,\"docker-compose\"],[0,[],0,\" it will just be podman sitting back there running for you!\"]]],[1,\"h3\",[[0,[],0,\"Podman Desktop\"]]],[10,9],[10,10],[1,\"h1\",[[0,[],0,\"What is it actually like to live with?\"]]],[1,\"p\",[[0,[],0,\"I have been using podman since the 27th of march, and I have to say I am impressed.\"]]],[1,\"p\",[[0,[],0,\"At work, I am the only person using podman, so support is non existant. This also raises a very important factor of adopting podman\"]]],[1,\"blockquote\",[[0,[],0,\"If my team uses docker, can I change with no affect to my productivity\"]]],[1,\"p\",[[0,[],0,\"The answer: Sort of\"]]],[1,\"h3\",[[0,[],0,\"The good\"]]],[3,\"ul\",[[[0,[],0,\"It works with google artifact registry using my docker \"],[0,[10],1,\"config.json\"],[0,[],0,\" file right off the bat\"]],[[0,[],0,\"Docker pulls default to \"],[0,[10],1,\"docker.io/library\"],[0,[],0,\" so you don't have to change pulling images\"]],[[0,[],0,\"Pulling images from other registries is as you expect, un-changed.\"]]]],[1,\"h3\",[[0,[],0,\"The not so good\"]]],[3,\"ul\",[[[0,[],0,\"Configuration is annoying\"]],[[0,[],0,\"Takes a few extra seconds to spin up a container\"]],[[0,[],0,\"Resources are limited by the VM that runs the containers, so lots of running containers = some are slow\"]],[[0,[],0,\"Skaffold does not work\"]]]],[1,\"p\",[[0,[],0,\"I am going to elaborate on the bad ones, as this is going to be your deciding factor on if you move or not\"]]],[1,\"h4\",[[0,[],0,\"Configuration is annoying\"]]],[1,\"p\",[[0,[],0,\"Because of the VM that podman runs containers in, you have to SSH in to the machine, and make the changes in nano or vi/vim. This gets quite annoying after about 3 minutes. \"]]],[1,\"h4\",[[0,[],0,\"Takes a few extra seconds to spin up a container \"]]],[1,\"p\",[[0,[],0,\"I have not been able to do a side by side comparison (my docker is borked) but running something like the below, just \"],[0,[9,8],2,\"feels\"],[0,[],0,\" slower. \"]]],[10,11],[1,\"h4\",[[0,[],0,\"Resources are limited by the vm\"]]],[1,\"p\",[[0,[],0,\"This is self explanatory. It's actually the same on docker, if you have a look on the docker Desktop app, it's there. The only reason I raise it here is because we're manually setting it, so we see it\"]]],[10,12],[1,\"h4\",[[0,[],0,\"Skaffold does not work\"]]],[1,\"p\",[[0,[],0,\"Skaffold is this super slick tool that allows you to automate so much\"]]],[10,13],[1,\"p\",[[0,[],0,\"The issue I have is that it does not work for building docker images, as it requires either the docker daemon or the docker cli\"]]],[1,\"p\",[[0,[],0,\"There is an open issue about this, but I think it works, providing you follow the most recent comment from my self\"]]],[1,\"p\",[]],[10,14],[1,\"h1\",[[0,[],0,\"Wrapping up\"]]],[3,\"ul\",[[[0,[],0,\"I like podman\"]],[[0,[],0,\"My manager is not sold on it\"]],[[0,[],0,\"Saves the company some \"],[0,[11],1,\"dosh\"]]]],[1,\"p\",[[0,[],0,\"I plan to use podman as much as I can. I can run everything we have at work through podman and it's fine. If I have issues and need to run more complex stuff, I have a local k3s cluster I can use.\"]]]],\"ghostVersion\":\"4.0\"}","lexical":null,"html":"<p>Docker is cool, don't get me wrong. They pretty much brought containerization to the masses. </p><p>So why the breakup letter? What pushed you over the edge</p><ol><li><a href=\"https://boxboat.com/2018/12/07/docker-ce-vs-docker-ee/\">2017: Licensing changes </a></li><li><a href=\"https://www.docker.com/pricing/october-2022-pricing-change-faq/\">Docker hub pricing changes</a></li><li><a href=\"https://www.bleepingcomputer.com/news/security/docker-hub-repositories-hide-over-1-650-malicious-containers/\">Malware hosted on docker</a></li><li><a href=\"https://www.docker.com/pricing/october-2022-pricing-change-faq/#:~:text=On%20October%2027%2C%202022%2C%20Docker,)%2C%20per%20user%20per%20month\">Price changes 2022</a></li><li><a href=\"https://www.infoworld.com/article/3630393/docker-desktop-is-no-longer-free-for-enterprise-users.html\">Docker Desktop pricing</a></li><li><a href=\"https://docs.docker.com/docker-hub/download-rate-limit\">Rate limit</a></li><li><a href=\"https://www.docker.com/blog/no-longer-sunsetting-the-free-team-plan/\">Docker free teams being sunsetted (and then saying sorry for bad comms, and then actually saying sorry and no longer doing it)</a></li></ol><p>Sure, most of these above points are about money, but the docker desktop and docker hub ones annoyed me the most</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">💡</div><div class=\"kg-callout-text\">I will get on to installing podman and how to use it later on in this blog<br>we just need to talk a bit about docker and podman.</div></div><p>Specifically, on mac <a href=\"https://survey.stackoverflow.co/2022/#section-most-popular-technologies-operating-system\">(which has 32% market share on devs)</a> you can only install docker using the desktop CLI. If you then use it for work, you need to pay.</p><p>Of course I am still yet to see Docker Inc take a company to court for not paying for licenses. </p><h2 id=\"so-why-podman-why-switch\">So why podman? Why switch?</h2><p>The real reason I switched was because I broke my docker install so much, that it was easier to switch to a new container runtime <em>thing</em> on my computer, than it was to try fix it.</p><p>Having a search about, I came across a blog from Michael Friedrich</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://dnsmichi.at/2022/03/15/docker-desktop-alternatives-macos-podman-nerdctl-rancher-desktop/?utm_source&#x3D;breadnet-co-uk&amp;utm_medium&#x3D;Docker-its-over-moving-to-podman\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Docker Desktop alternatives on macOS: podman, nerdctl, Rancher Desktop</div><div class=\"kg-bookmark-description\">Docker changed its subscription model including Docker Desktop, thus generatingmore demand for alternatives. In this blog post, we look into podman, nerdctl,and Rancher Desktop as Docker Desktop alternatives on macOS. The Docker Desktop subscription changes[https://www.docker.com/blog/updating-…</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://dnsmichi.at/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">dnsmichi.at</span><span class=\"kg-bookmark-publisher\">Michael Friedrich</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://dnsmichi.at/content/images/2022/03/AC6BA369-E613-464F-AEB7-CCA674DC61BE.jpeg\" alt=\"\"></div></a></figure><p>One of his suggestions was podman, so I turned away from the spike I was working on (Configuring Hound) and put all the time in to getting podman working on my laptop</p><h1 id=\"the-install-process\">The install process</h1><div class=\"kg-card kg-callout-card kg-callout-card-yellow\"><div class=\"kg-callout-emoji\">📬</div><div class=\"kg-callout-text\">I am going to assume that you don't need me to explain what podman is and what docker is, if you are reading this post&nbsp;</div></div><p>For context, I am using an M1 mac - Which requires some additional configuration.</p><pre><code>brew install podman\npodman machine init\npodman machine start</code></pre><p>That's pretty much it.</p><p>Podman will <em>hijack</em> your docker config file, so stuff like authentication to google cloud artifact registry and custom registries are <strong>just going to work</strong></p><p>Something to note is that if you plan to run containers with local volumes, you will need to change your init command to the below</p><pre><code>podman machine init --now --cpus=4 --memory=2046 -v $HOME:$HOME</code></pre><h1 id=\"i-miss-things-about-docker\">I miss things about docker</h1><p>This is normal, for the past 10 years you've been typing <code>docker</code> and <code>docker-compose</code> </p><p>Firstly we will take care of <code>docker-compose</code> as podman has it's own compose tool.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://github.com/containers/podman-compose\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">GitHub - containers/podman-compose: a script to run docker-compose.yml using podman</div><div class=\"kg-bookmark-description\">a script to run docker-compose.yml using podman. Contribute to containers/podman-compose development by creating an account on GitHub.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://github.com/fluidicon.png\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">containers</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://opengraph.githubassets.com/feddadaf0941e1f1c56e0d8857ead38a124850f7d0dd0f6668371a29e3c8c76a/containers/podman-compose\" alt=\"\"></div></a></figure><p>It's called... <code>podman-compose</code></p><pre><code>brew install podman-compose</code></pre><h3 id=\"aliasing-stuff\">Aliasing stuff</h3><p>edit your <code>~/.zshrc</code> or <code>~/.bashrc</code> file and add the below in</p><pre><code>alias \"docker\"=\"podman\"\nalias \"docker-compose\"=\"podman-compose\"\n</code></pre><p>Then update your source </p><pre><code>source ~/.zshrc</code></pre><p>Any time you invoke <code>docker</code> or <code>docker-compose</code> it will just be podman sitting back there running for you!</p><h3 id=\"podman-desktop\">Podman Desktop</h3><pre><code>brew install podman-desktop</code></pre><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2023/04/image.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"1335\" srcset=\"__GHOST_URL__/content/images/size/w600/2023/04/image.png 600w, __GHOST_URL__/content/images/size/w1000/2023/04/image.png 1000w, __GHOST_URL__/content/images/size/w1600/2023/04/image.png 1600w, __GHOST_URL__/content/images/2023/04/image.png 2100w\" sizes=\"(min-width: 720px) 720px\"></figure><h1 id=\"what-is-it-actually-like-to-live-with\">What is it actually like to live with?</h1><p>I have been using podman since the 27th of march, and I have to say I am impressed.</p><p>At work, I am the only person using podman, so support is non existant. This also raises a very important factor of adopting podman</p><blockquote>If my team uses docker, can I change with no affect to my productivity</blockquote><p>The answer: Sort of</p><h3 id=\"the-good\">The good</h3><ul><li>It works with google artifact registry using my docker <code>config.json</code> file right off the bat</li><li>Docker pulls default to <code>docker.io/library</code> so you don't have to change pulling images</li><li>Pulling images from other registries is as you expect, un-changed.</li></ul><h3 id=\"the-not-so-good\">The not so good</h3><ul><li>Configuration is annoying</li><li>Takes a few extra seconds to spin up a container</li><li>Resources are limited by the VM that runs the containers, so lots of running containers = some are slow</li><li>Skaffold does not work</li></ul><p>I am going to elaborate on the bad ones, as this is going to be your deciding factor on if you move or not</p><h4 id=\"configuration-is-annoying\">Configuration is annoying</h4><p>Because of the VM that podman runs containers in, you have to SSH in to the machine, and make the changes in nano or vi/vim. This gets quite annoying after about 3 minutes. </p><h4 id=\"takes-a-few-extra-seconds-to-spin-up-a-container\">Takes a few extra seconds to spin up a container </h4><p>I have not been able to do a side by side comparison (my docker is borked) but running something like the below, just <strong><em>feels</em></strong> slower. </p><pre><code>podman run -it alpine /bin/sh</code></pre><h4 id=\"resources-are-limited-by-the-vm\">Resources are limited by the vm</h4><p>This is self explanatory. It's actually the same on docker, if you have a look on the docker Desktop app, it's there. The only reason I raise it here is because we're manually setting it, so we see it</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2023/04/image-1.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1100\" height=\"628\" srcset=\"__GHOST_URL__/content/images/size/w600/2023/04/image-1.png 600w, __GHOST_URL__/content/images/size/w1000/2023/04/image-1.png 1000w, __GHOST_URL__/content/images/2023/04/image-1.png 1100w\" sizes=\"(min-width: 720px) 720px\"></figure><h4 id=\"skaffold-does-not-work\">Skaffold does not work</h4><p>Skaffold is this super slick tool that allows you to automate so much</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://skaffold.dev\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Skaffold</div><div class=\"kg-bookmark-description\">Easy and Repeatable Container &amp; Kubernetes Development</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://skaffold.dev/favicons/android-192x192.png\" alt=\"\"><span class=\"kg-bookmark-author\">Skaffold</span><span class=\"kg-bookmark-publisher\">Warren Strange, Engineering Director, ForgeRock</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://skaffold.dev/featured-background.jpeg\" alt=\"\"></div></a></figure><p>The issue I have is that it does not work for building docker images, as it requires either the docker daemon or the docker cli</p><p>There is an open issue about this, but I think it works, providing you follow the most recent comment from my self</p><p></p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://github.com/GoogleContainerTools/skaffold/issues/8430\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Feature Request - Support for podman builder · Issue #8430 · GoogleContainerTools/skaffold</div><div class=\"kg-bookmark-description\">It would be great to have podman in the builders list. I have also tried using podman-docker to fake docker commands to use podman in vain (as expected). ❯ skaffold build Generating tags... - azure…</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://github.com/fluidicon.png\" alt=\"\"><span class=\"kg-bookmark-author\">GitHub</span><span class=\"kg-bookmark-publisher\">GoogleContainerTools</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://opengraph.githubassets.com/281d390ecbfd29e95c5012d81a0d1e791974ea590843d0674c4500f7aebde57a/GoogleContainerTools/skaffold/issues/8430\" alt=\"\"></div></a></figure><h1 id=\"wrapping-up\">Wrapping up</h1><ul><li>I like podman</li><li>My manager is not sold on it</li><li>Saves the company some <a href=\"https://www.urbandictionary.com/define.php?term=dosh\">dosh</a></li></ul><p>I plan to use podman as much as I can. I can run everything we have at work through podman and it's fine. If I have issues and need to run more complex stuff, I have a local k3s cluster I can use.</p>","comment_id":"64405506c9cae386d9c53b55","plaintext":"Docker is cool, don't get me wrong. They pretty much brought containerization to the masses.\n\nSo why the breakup letter? What pushed you over the edge\n\n 1. 2017: Licensing changes\n 2. Docker hub pricing changes\n 3. Malware hosted on docker\n 4. Price changes 2022\n 5. Docker Desktop pricing\n 6. Rate limit\n 7. Docker free teams being sunsetted (and then saying sorry for bad comms, and then actually saying sorry and no longer doing it)\n\nSure, most of these above points are about money, but the docker desktop and docker hub ones annoyed me the most\n\n💡I will get on to installing podman and how to use it later on in this blog\nwe just need to talk a bit about docker and podman.\n\nSpecifically, on mac (which has 32% market share on devs) you can only install docker using the desktop CLI. If you then use it for work, you need to pay.\n\nOf course I am still yet to see Docker Inc take a company to court for not paying for licenses.\n\n\nSo why podman? Why switch?\n\nThe real reason I switched was because I broke my docker install so much, that it was easier to switch to a new container runtime thing on my computer, than it was to try fix it.\n\nHaving a search about, I came across a blog from Michael Friedrich\n\nDocker Desktop alternatives on macOS: podman, nerdctl, Rancher DesktopDocker changed its subscription model including Docker Desktop, thus generatingmore demand for alternatives. In this blog post, we look into podman, nerdctl,and Rancher Desktop as Docker Desktop alternatives on macOS. The Docker Desktop subscription changes[https://www.docker.com/blog/updating-…dnsmichi.atMichael Friedrich\n\nOne of his suggestions was podman, so I turned away from the spike I was working on (Configuring Hound) and put all the time in to getting podman working on my laptop\n\n\nThe install process\n\n📬I am going to assume that you don't need me to explain what podman is and what docker is, if you are reading this post \n\nFor context, I am using an M1 mac - Which requires some additional configuration.\n\nbrew install podman\npodman machine init\npodman machine start\n\nThat's pretty much it.\n\nPodman will hijack your docker config file, so stuff like authentication to google cloud artifact registry and custom registries are just going to work\n\nSomething to note is that if you plan to run containers with local volumes, you will need to change your init command to the below\n\npodman machine init --now --cpus=4 --memory=2046 -v $HOME:$HOME\n\n\nI miss things about docker\n\nThis is normal, for the past 10 years you've been typing docker and docker-compose\n\nFirstly we will take care of docker-compose as podman has it's own compose tool.\n\nGitHub - containers/podman-compose: a script to run docker-compose.yml using podmana script to run docker-compose.yml using podman. Contribute to containers/podman-compose development by creating an account on GitHub.GitHubcontainers\n\nIt's called... podman-compose\n\nbrew install podman-compose\n\n\nAliasing stuff\n\nedit your ~/.zshrc or ~/.bashrc file and add the below in\n\nalias \"docker\"=\"podman\"\nalias \"docker-compose\"=\"podman-compose\"\n\n\nThen update your source\n\nsource ~/.zshrc\n\nAny time you invoke docker or docker-compose it will just be podman sitting back there running for you!\n\n\nPodman Desktop\n\nbrew install podman-desktop\n\n\nWhat is it actually like to live with?\n\nI have been using podman since the 27th of march, and I have to say I am impressed.\n\nAt work, I am the only person using podman, so support is non existant. This also raises a very important factor of adopting podman\n\nIf my team uses docker, can I change with no affect to my productivity\n\nThe answer: Sort of\n\n\nThe good\n\n * It works with google artifact registry using my docker config.json file right off the bat\n * Docker pulls default to docker.io/library so you don't have to change pulling images\n * Pulling images from other registries is as you expect, un-changed.\n\n\nThe not so good\n\n * Configuration is annoying\n * Takes a few extra seconds to spin up a container\n * Resources are limited by the VM that runs the containers, so lots of running containers = some are slow\n * Skaffold does not work\n\nI am going to elaborate on the bad ones, as this is going to be your deciding factor on if you move or not\n\nConfiguration is annoying\n\nBecause of the VM that podman runs containers in, you have to SSH in to the machine, and make the changes in nano or vi/vim. This gets quite annoying after about 3 minutes.\n\nTakes a few extra seconds to spin up a container\n\nI have not been able to do a side by side comparison (my docker is borked) but running something like the below, just feels slower.\n\npodman run -it alpine /bin/sh\n\nResources are limited by the vm\n\nThis is self explanatory. It's actually the same on docker, if you have a look on the docker Desktop app, it's there. The only reason I raise it here is because we're manually setting it, so we see it\n\nSkaffold does not work\n\nSkaffold is this super slick tool that allows you to automate so much\n\nSkaffoldEasy and Repeatable Container & Kubernetes DevelopmentSkaffoldWarren Strange, Engineering Director, ForgeRock\n\nThe issue I have is that it does not work for building docker images, as it requires either the docker daemon or the docker cli\n\nThere is an open issue about this, but I think it works, providing you follow the most recent comment from my self\n\n\n\nFeature Request - Support for podman builder · Issue #8430 · GoogleContainerTools/skaffoldIt would be great to have podman in the builders list. I have also tried using podman-docker to fake docker commands to use podman in vain (as expected). ❯ skaffold build Generating tags... - azure…GitHubGoogleContainerTools\n\n\nWrapping up\n\n * I like podman\n * My manager is not sold on it\n * Saves the company some dosh\n\nI plan to use podman as much as I can. I can run everything we have at work through podman and it's fine. If I have issues and need to run more complex stuff, I have a local k3s cluster I can use.","feature_image":"https://images.unsplash.com/photo-1633967920376-33b2d94f091f?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fG90dGVyfGVufDB8fHx8MTY4MTkzNzY5MA&ixlib=rb-4.0.3&q=80&w=2000","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2023-04-19T20:54:30.000Z","updated_at":"2023-08-18T16:22:01.000Z","published_at":"2023-04-19T23:58:13.000Z","custom_excerpt":"Docker, we're breaking up! Tips on moving to podman","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"64b33499c9cae386d9c53c23","uuid":"fd8625d1-a62b-4e16-8bd7-bcc4f79166db","title":"Google Artifact Registry Remote Repositories","slug":"google-artifact-registry-virtual","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"code\",{\"code\":\"LOCATION-docker.pkg.dev/PROJECT-ID/REPOSITORY/IMAGE:TAG\"}],[\"hr\",{}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2023/07/image.png\",\"width\":618,\"height\":1037}],[\"code\",{\"code\":\"resource \\\"google_artifact_registry_repository\\\" \\\"containers\\\" {\\n  location      = var.region\\n  repository_id = \\\"breadnet-cache\\\"\\n  description   = \\\"breadNET Public Image cache\\\"\\n  format        = \\\"DOCKER\\\"\\n  mode          = \\\"REMOTE_REPOSITORY\\\"\\n  project       = var.project\\n  remote_repository_config {\\n    description = \\\"docker hub\\\"\\n    docker_repository {\\n      public_repository = \\\"DOCKER_HUB\\\"\\n    }\\n  }\\n}\\n\"}],[\"code\",{\"code\":\"gcloud auth configure-docker europe-west2-docker.pkg.dev\"}],[\"code\",{\"code\":\"podman pull europe-west2-docker.pkg.dev/breadnet-containers/breadnet-cache/alpine\"}],[\"code\",{\"code\":\"podman pull europe-west2-docker.pkg.dev/breadnet-containers/breadnet-cache/squidfunk/mkdocs-material\"}],[\"code\",{\"code\":\"FROM europe-west2-docker.pkg.dev/breadnet-containers/breadnet-cache/alpine:3.18.2\\n\\nLABEL org.opencontainers.image.title=\\\"Kubectl\\\"\\nLABEL org.opencontainers.image.description=\\\"A Docker image for Kubectl\\\"\\nLABEL org.opencontainers.image.authors=\\\"Bradley Stannard <opensource@breadnet.co.uk>\\\"\\n\\nRUN apk add curl\\n\\nRUN curl -LO \\\"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\\\"\\n\\nRUN rm -rf /var/cache/apk/*\\n\\n\\nRUN install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\\n\\nRUN rm /kubectl\\n\\nENTRYPOINT [\\\"kubectl\\\"]\"}],[\"hr\",{}]],\"markups\":[[\"a\",[\"href\",\"https://cloud.google.com/blog/products/identity-security/take-control-your-supply-chain-artifact-registry\"]],[\"code\"],[\"a\",[\"href\",\"https://cloud.google.com/artifact-registry/docs/transition/transition-from-gcr\"]],[\"a\",[\"href\",\"__GHOST_URL__/docker-its-over-moving-to-podman/\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Google Cloud in April \"],[0,[0],1,\"created a blog post\"],[0,[],0,\" about a new part of the Artifact regsitry called \"],[0,[1],1,\"Remote Repositories\"]]],[1,\"p\",[[0,[],0,\"In todays installment, we will look at what they are, why you'd use one, and how to integrate it with your daily workflow.\"]]],[1,\"h2\",[[0,[],0,\"What is Artifact Registry\"]]],[1,\"p\",[[0,[],0,\"Artifact Registry is the better version of Google Container Registry (gcr.io) which is a place for storing your OCI images. Artifact Registry has more features over \"],[0,[2],1,\"the now deprecated Google Container Registry\"]]],[1,\"p\",[[0,[],0,\"They are, but not limited to:\"]]],[3,\"ul\",[[[0,[],0,\"Storing Helm charts\"]],[[0,[],0,\"Storing APT packages\"]],[[0,[],0,\"Storing go and python packages\"]],[[0,[],0,\"Remote repos (this blog post covers this)\"]],[[0,[],0,\"Maven\"]],[[0,[],0,\"KubeFlow Pipelines\"]],[[0,[],0,\"Global storage or Regional\"]],[[0,[],0,\"Security Scanning of images\"]]]],[1,\"p\",[[0,[],0,\"Unlike the old Google Container Registry, Artifact Registry has a nicer naming schema, making it easier to tell where the images are coming from\"]]],[10,0],[10,1],[1,\"h2\",[[0,[],0,\"What is a remote Repository\"]]],[1,\"p\",[[0,[],0,\"Google defines this as \"],[0,[1],1,\"A repository that acts as a caching proxy for External public artifact repository\"],[0,[],0,\" - which in this case, is currently only Dockehub.\"]]],[1,\"p\",[[0,[],0,\"In human readable terms, this special repository allows you to have cached images in your google account, that are then delivered at line speed (internal traffic) to GKE clusters, cloud run or anything else. You can also use them on your laptop\"]]],[1,\"h2\",[[0,[],0,\"How to create one\"]]],[1,\"p\",[[0,[],0,\"You are able to create a virtual Repository through the UI by navigating to the Artifact Registry and clicking \"],[0,[1],1,\"+ CREATE REPOSITORY\"]]],[1,\"p\",[[0,[],0,\"Select \"],[0,[1],1,\"Remote\"]]],[10,2],[1,\"p\",[[0,[],0,\"Alternativly, if you prefer terraform\"]]],[10,3],[1,\"h2\",[[0,[],0,\"How to actually use it\"]]],[1,\"p\",[[0,[],0,\"Once you have the repository created, you will need to authenticate your local \"],[0,[3],1,\"podman (or docker)\"],[0,[],0,\" to the Artifact Registry.\"]]],[1,\"p\",[[0,[],0,\"Click on the repository, then click \"],[0,[1],1,\"Setup Instructions\"],[0,[],0,\" where a tab will appear with a command similar to the below\"]]],[10,4],[1,\"p\",[[0,[],0,\"Once this is run, you are able to pull images.\"]]],[1,\"h3\",[[0,[],0,\"Official images\"]]],[10,5],[1,\"h3\",[[0,[],0,\"User created images\"]]],[10,6],[1,\"p\",[[0,[],0,\"If we have a docker file like the below, we would simply append \"],[0,[1],1,\"europe-west2-docker.pkg.dev/breadnet-containers/breadnet-cache/\"],[0,[],0,\" to the image name, and it will use the cache\"]]],[10,7],[10,8],[1,\"p\",[[0,[],0,\"As always, if you have any questions please reach out to me!\"]]]],\"ghostVersion\":\"4.0\"}","lexical":null,"html":"<p>Google Cloud in April <a href=\"https://cloud.google.com/blog/products/identity-security/take-control-your-supply-chain-artifact-registry\">created a blog post</a> about a new part of the Artifact regsitry called <code>Remote Repositories</code></p><p>In todays installment, we will look at what they are, why you'd use one, and how to integrate it with your daily workflow.</p><h2 id=\"what-is-artifact-registry\">What is Artifact Registry</h2><p>Artifact Registry is the better version of Google Container Registry (gcr.io) which is a place for storing your OCI images. Artifact Registry has more features over <a href=\"https://cloud.google.com/artifact-registry/docs/transition/transition-from-gcr\">the now deprecated Google Container Registry</a></p><p>They are, but not limited to:</p><ul><li>Storing Helm charts</li><li>Storing APT packages</li><li>Storing go and python packages</li><li>Remote repos (this blog post covers this)</li><li>Maven</li><li>KubeFlow Pipelines</li><li>Global storage or Regional</li><li>Security Scanning of images</li></ul><p>Unlike the old Google Container Registry, Artifact Registry has a nicer naming schema, making it easier to tell where the images are coming from</p><pre><code>LOCATION-docker.pkg.dev/PROJECT-ID/REPOSITORY/IMAGE:TAG</code></pre><hr><h2 id=\"what-is-a-remote-repository\">What is a remote Repository</h2><p>Google defines this as <code>A repository that acts as a caching proxy for External public artifact repository</code> - which in this case, is currently only Dockehub.</p><p>In human readable terms, this special repository allows you to have cached images in your google account, that are then delivered at line speed (internal traffic) to GKE clusters, cloud run or anything else. You can also use them on your laptop</p><h2 id=\"how-to-create-one\">How to create one</h2><p>You are able to create a virtual Repository through the UI by navigating to the Artifact Registry and clicking <code>+ CREATE REPOSITORY</code></p><p>Select <code>Remote</code></p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2023/07/image.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"618\" height=\"1037\" srcset=\"__GHOST_URL__/content/images/size/w600/2023/07/image.png 600w, __GHOST_URL__/content/images/2023/07/image.png 618w\"></figure><p>Alternativly, if you prefer terraform</p><pre><code>resource \"google_artifact_registry_repository\" \"containers\" {\n  location      = var.region\n  repository_id = \"breadnet-cache\"\n  description   = \"breadNET Public Image cache\"\n  format        = \"DOCKER\"\n  mode          = \"REMOTE_REPOSITORY\"\n  project       = var.project\n  remote_repository_config {\n    description = \"docker hub\"\n    docker_repository {\n      public_repository = \"DOCKER_HUB\"\n    }\n  }\n}\n</code></pre><h2 id=\"how-to-actually-use-it\">How to actually use it</h2><p>Once you have the repository created, you will need to authenticate your local <a href=\"__GHOST_URL__/docker-its-over-moving-to-podman/\">podman (or docker)</a> to the Artifact Registry.</p><p>Click on the repository, then click <code>Setup Instructions</code> where a tab will appear with a command similar to the below</p><pre><code>gcloud auth configure-docker europe-west2-docker.pkg.dev</code></pre><p>Once this is run, you are able to pull images.</p><h3 id=\"official-images\">Official images</h3><pre><code>podman pull europe-west2-docker.pkg.dev/breadnet-containers/breadnet-cache/alpine</code></pre><h3 id=\"user-created-images\">User created images</h3><pre><code>podman pull europe-west2-docker.pkg.dev/breadnet-containers/breadnet-cache/squidfunk/mkdocs-material</code></pre><p>If we have a docker file like the below, we would simply append <code>europe-west2-docker.pkg.dev/breadnet-containers/breadnet-cache/</code> to the image name, and it will use the cache</p><pre><code>FROM europe-west2-docker.pkg.dev/breadnet-containers/breadnet-cache/alpine:3.18.2\n\nLABEL org.opencontainers.image.title=\"Kubectl\"\nLABEL org.opencontainers.image.description=\"A Docker image for Kubectl\"\nLABEL org.opencontainers.image.authors=\"Bradley Stannard &lt;opensource@breadnet.co.uk&gt;\"\n\nRUN apk add curl\n\nRUN curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\n\nRUN rm -rf /var/cache/apk/*\n\n\nRUN install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n\nRUN rm /kubectl\n\nENTRYPOINT [\"kubectl\"]</code></pre><hr><p>As always, if you have any questions please reach out to me!</p>","comment_id":"64b33499c9cae386d9c53c23","plaintext":"Google Cloud in April created a blog post about a new part of the Artifact regsitry called Remote Repositories\n\nIn todays installment, we will look at what they are, why you'd use one, and how to integrate it with your daily workflow.\n\n\nWhat is Artifact Registry\n\nArtifact Registry is the better version of Google Container Registry (gcr.io) which is a place for storing your OCI images. Artifact Registry has more features over the now deprecated Google Container Registry\n\nThey are, but not limited to:\n\n * Storing Helm charts\n * Storing APT packages\n * Storing go and python packages\n * Remote repos (this blog post covers this)\n * Maven\n * KubeFlow Pipelines\n * Global storage or Regional\n * Security Scanning of images\n\nUnlike the old Google Container Registry, Artifact Registry has a nicer naming schema, making it easier to tell where the images are coming from\n\nLOCATION-docker.pkg.dev/PROJECT-ID/REPOSITORY/IMAGE:TAG\n\n\nWhat is a remote Repository\n\nGoogle defines this as A repository that acts as a caching proxy for External public artifact repository - which in this case, is currently only Dockehub.\n\nIn human readable terms, this special repository allows you to have cached images in your google account, that are then delivered at line speed (internal traffic) to GKE clusters, cloud run or anything else. You can also use them on your laptop\n\n\nHow to create one\n\nYou are able to create a virtual Repository through the UI by navigating to the Artifact Registry and clicking + CREATE REPOSITORY\n\nSelect Remote\n\nAlternativly, if you prefer terraform\n\nresource \"google_artifact_registry_repository\" \"containers\" {\n  location      = var.region\n  repository_id = \"breadnet-cache\"\n  description   = \"breadNET Public Image cache\"\n  format        = \"DOCKER\"\n  mode          = \"REMOTE_REPOSITORY\"\n  project       = var.project\n  remote_repository_config {\n    description = \"docker hub\"\n    docker_repository {\n      public_repository = \"DOCKER_HUB\"\n    }\n  }\n}\n\n\n\nHow to actually use it\n\nOnce you have the repository created, you will need to authenticate your local podman (or docker) to the Artifact Registry.\n\nClick on the repository, then click Setup Instructions where a tab will appear with a command similar to the below\n\ngcloud auth configure-docker europe-west2-docker.pkg.dev\n\nOnce this is run, you are able to pull images.\n\n\nOfficial images\n\npodman pull europe-west2-docker.pkg.dev/breadnet-containers/breadnet-cache/alpine\n\n\nUser created images\n\npodman pull europe-west2-docker.pkg.dev/breadnet-containers/breadnet-cache/squidfunk/mkdocs-material\n\nIf we have a docker file like the below, we would simply append europe-west2-docker.pkg.dev/breadnet-containers/breadnet-cache/ to the image name, and it will use the cache\n\nFROM europe-west2-docker.pkg.dev/breadnet-containers/breadnet-cache/alpine:3.18.2\n\nLABEL org.opencontainers.image.title=\"Kubectl\"\nLABEL org.opencontainers.image.description=\"A Docker image for Kubectl\"\nLABEL org.opencontainers.image.authors=\"Bradley Stannard <opensource@breadnet.co.uk>\"\n\nRUN apk add curl\n\nRUN curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\n\nRUN rm -rf /var/cache/apk/*\n\n\nRUN install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n\nRUN rm /kubectl\n\nENTRYPOINT [\"kubectl\"]\n\nAs always, if you have any questions please reach out to me!","feature_image":"https://images.unsplash.com/photo-1605745341075-1b7460b99df8?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3wxMTc3M3wwfDF8c2VhcmNofDF8fGRvY2tlcnxlbnwwfHx8fDE2ODk0NjcyNTR8MA&ixlib=rb-4.0.3&q=80&w=2000","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2023-07-16T00:06:49.000Z","updated_at":"2023-07-16T00:28:53.000Z","published_at":"2023-07-16T00:28:53.000Z","custom_excerpt":"What is the Google Artifact Registry remote repo and how do I use it?","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"64b33a2ec9cae386d9c53c96","uuid":"8c3d7ee9-7224-4689-9bba-106456bca847","title":"Dependabot for terraform and terraform modules","slug":"dependabot-for-terraform-and-terraform-modules","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"code\",{\"code\":\"monorepo\\n├── .github\\n│   └── dependabot.yml\\n├── artifact-registry\\n│   └── provider.tf\\n├── dns\\n│   └── provider.tf\\n└── gke\\n    ├── dev\\n    │   └── provider.tf\\n    ├── prod\\n    │   └── provider.tf\\n    └── test\\n        └── provider.tf\\n\"}],[\"code\",{\"code\":\"version: 2\\n\\nupdates:\\n# Terraform - One entry per thing we want to scan as per https://github.com/dependabot/dependabot-core/issues/649\\n  - package-ecosystem: \\\"terraform\\\" # DNS\\n    directory: \\\"/dns\\\"\\n    schedule:\\n      interval: \\\"daily\\\"\\n  - package-ecosystem: \\\"terraform\\\" # GKE Dev\\n    directory: \\\"/gke/dev\\\"\\n    schedule:\\n      interval: \\\"weekly\\\"\\n  - package-ecosystem: \\\"terraform\\\" # GKE Prod\\n    directory: \\\"/gke/prod\\\"\\n    schedule:\\n      interval: \\\"weekly\\\"\\n  - package-ecosystem: \\\"terraform\\\" # GKE Test\\n    directory: \\\"/gke/test\\\"\\n    schedule:\\n      interval: \\\"weekly\\\"\\n  - package-ecosystem: \\\"terraform\\\" # Artifact Registry\\n    directory: \\\"/artifact-registry\\\"\\n    schedule:\\n      interval: \\\"weekly\\\"\"}],[\"markdown\",{\"markdown\":\"| Name | What it does |\\n| --- | --- |\\n| `package-ecosystem` | What package ecosystem to scan from [Supported Packages](https://docs.github.com/en/code-security/dependabot/dependabot-version-updates/about-dependabot-version-updates#supported-repositories-and-ecosystems) |\\n| `directory` | Where this code it should scan lives |\\n| `schedule` | How often to scan, based on [`schedule.interval`](https://docs.github.com/en/code-security/dependabot/dependabot-version-updates/configuration-options-for-the-dependabot.yml-file#scheduleinterval)\\n\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2023/07/image-1.png\",\"width\":1440,\"height\":215}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2023/07/image-2.png\",\"width\":387,\"height\":492}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2023/07/image-3.png\",\"width\":943,\"height\":251}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2023/07/image-4.png\",\"width\":307,\"height\":184}],[\"bookmark\",{\"version\":\"1.0\",\"type\":\"bookmark\",\"url\":\"https://documentation.breadnet.co.uk/kb/dependabot/ignore-terraform-module-version-dependabot/\",\"metadata\":{\"url\":\"https://documentation.breadnet.co.uk/kb/dependabot/ignore-terraform-module-version-dependabot/\",\"title\":\"Ignore terraform module version dependabot - breadNET Documentation\",\"description\":\"breadNET Documentation\",\"author\":null,\"publisher\":\"logo\",\"thumbnail\":\"https://documentation.breadnet.co.uk/assets/images/social/kb/dependabot/ignore-terraform-module-version-dependabot.png\",\"icon\":\"https://documentation.breadnet.co.uk/favicon.ico\"}}]],\"markups\":[[\"a\",[\"href\",\"https://github.blog/2020-06-01-keep-all-your-packages-up-to-date-with-dependabot/\"]],[\"em\"],[\"code\"],[\"a\",[\"href\",\"https://docs.github.com/en/code-security/dependabot/dependabot-version-updates/configuring-dependabot-version-updates#enabling-dependabot-version-updates\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I recently stumbled across \"],[0,[0],1,\"Dependabot\"],[0,[],0,\" and was curious if it can be used to keep Terraform up to date. Turns out you can, but there are some small catches that you should be aware of.\"]]],[1,\"p\",[[0,[],0,\"In todays installment, we look at what it is, why you'd use it, and how to set it up.\"]]],[1,\"h2\",[[0,[],0,\"What is Dependabot \"]]],[1,\"p\",[[0,[],0,\"Dependabot is a tool from GitHub that allows automatic updates and patching of code in repositories. This is especially useful as once you get over about 3 terraform dependencies, you start having the issue where Providers become out dated, and modules fall behind on patches to issues.\"]]],[1,\"p\",[[0,[],0,\"By enabling Dependabot, you basically get a colleage for free who's sole job is finding out dated terraform packages (modules, providers), creating a pull request to update them and managing rebasing.\"]]],[1,\"h2\",[[0,[],0,\"Why would I use it for terraform\"]]],[1,\"p\",[[0,[],0,\"As mentioned, once you get past about 3 providers across a codebase (and in my case, over 30k lines of terraform) keeping things up to date becomes a real hassle. \"]]],[1,\"p\",[[0,[],0,\"Dependabot will search for the releases of Modules and Providers frequently and update them. Even Private modules.\"]]],[1,\"h2\",[[0,[],0,\"What are the limitations\"]]],[1,\"p\",[[0,[],0,\"For some reason, when configuring the terraform dependabot config, you will need to create a new \"],[0,[1],1,\"line item\"],[0,[],0,\" per directory which contains a \"],[0,[2],1,\"provider.tf\"],[0,[],0,\" file, or in the case of modules, the directory.\"]]],[1,\"h2\",[[0,[],0,\"How to set it up\"]]],[1,\"p\",[[0,[],0,\"Lets assume for the time being the below is our file layout\"]]],[10,0],[1,\"p\",[[0,[],0,\"Once you've enabled Dependabot \"],[0,[3],1,\"(follow these instructions)\"],[0,[],0,\" create a file called \"],[0,[2],1,\".github/dependabot.yml\"],[0,[],0,\" and inset the below\"]]],[10,1],[1,\"p\",[[0,[],0,\"What does each part mean?\"]]],[10,2],[1,\"p\",[[0,[],0,\"Once you've created the file, go to your repo and click on \"],[0,[2],1,\"Insights\"]]],[10,3],[1,\"p\",[[0,[],0,\"Then click on \"],[0,[2],1,\"Dependency graph\"]]],[10,4],[1,\"p\",[[0,[],0,\"Then click on Dependabot and you should see a healthy scan\"]]],[10,5],[1,\"h2\",[[0,[],0,\"How to add access to Private Modules\"]]],[1,\"p\",[[0,[],0,\"When creating private modules in a Private Git Repository, you will need to give Dependabot access to these.\"]]],[1,\"p\",[[0,[],0,\"To do so, navigate to your Org's home page, and click on \"],[0,[2],1,\"Settings\"],[0,[],0,\" and then \"],[0,[2],1,\"Code security and analysis\"]]],[1,\"p\",[[0,[],0,\" \"]]],[10,6],[1,\"p\",[[0,[],0,\"Once here, you will need to scroll down to the section that reads \"],[0,[2],1,\"Grant Dependabot access to private repositories\"]]],[1,\"p\",[[0,[],0,\"Here search for the name of all the repos that contain modules. Once added, Dependabot will check if they have releases, and if so, will update them\"]]],[1,\"h2\",[[0,[],0,\"How to ignore certain modules\"]]],[1,\"p\",[[0,[],0,\"This is out of scope for this post, but see the below documentation for an example and walkthrough \"]]],[10,7],[1,\"p\",[]]],\"ghostVersion\":\"4.0\"}","lexical":null,"html":"<p>I recently stumbled across <a href=\"https://github.blog/2020-06-01-keep-all-your-packages-up-to-date-with-dependabot/\">Dependabot</a> and was curious if it can be used to keep Terraform up to date. Turns out you can, but there are some small catches that you should be aware of.</p><p>In todays installment, we look at what it is, why you'd use it, and how to set it up.</p><h2 id=\"what-is-dependabot\">What is Dependabot </h2><p>Dependabot is a tool from GitHub that allows automatic updates and patching of code in repositories. This is especially useful as once you get over about 3 terraform dependencies, you start having the issue where Providers become out dated, and modules fall behind on patches to issues.</p><p>By enabling Dependabot, you basically get a colleage for free who's sole job is finding out dated terraform packages (modules, providers), creating a pull request to update them and managing rebasing.</p><h2 id=\"why-would-i-use-it-for-terraform\">Why would I use it for terraform</h2><p>As mentioned, once you get past about 3 providers across a codebase (and in my case, over 30k lines of terraform) keeping things up to date becomes a real hassle. </p><p>Dependabot will search for the releases of Modules and Providers frequently and update them. Even Private modules.</p><h2 id=\"what-are-the-limitations\">What are the limitations</h2><p>For some reason, when configuring the terraform dependabot config, you will need to create a new <em>line item</em> per directory which contains a <code>provider.tf</code> file, or in the case of modules, the directory.</p><h2 id=\"how-to-set-it-up\">How to set it up</h2><p>Lets assume for the time being the below is our file layout</p><pre><code>monorepo\n├── .github\n│   └── dependabot.yml\n├── artifact-registry\n│   └── provider.tf\n├── dns\n│   └── provider.tf\n└── gke\n    ├── dev\n    │   └── provider.tf\n    ├── prod\n    │   └── provider.tf\n    └── test\n        └── provider.tf\n</code></pre><p>Once you've enabled Dependabot <a href=\"https://docs.github.com/en/code-security/dependabot/dependabot-version-updates/configuring-dependabot-version-updates#enabling-dependabot-version-updates\">(follow these instructions)</a> create a file called <code>.github/dependabot.yml</code> and inset the below</p><pre><code>version: 2\n\nupdates:\n# Terraform - One entry per thing we want to scan as per https://github.com/dependabot/dependabot-core/issues/649\n  - package-ecosystem: \"terraform\" # DNS\n    directory: \"/dns\"\n    schedule:\n      interval: \"daily\"\n  - package-ecosystem: \"terraform\" # GKE Dev\n    directory: \"/gke/dev\"\n    schedule:\n      interval: \"weekly\"\n  - package-ecosystem: \"terraform\" # GKE Prod\n    directory: \"/gke/prod\"\n    schedule:\n      interval: \"weekly\"\n  - package-ecosystem: \"terraform\" # GKE Test\n    directory: \"/gke/test\"\n    schedule:\n      interval: \"weekly\"\n  - package-ecosystem: \"terraform\" # Artifact Registry\n    directory: \"/artifact-registry\"\n    schedule:\n      interval: \"weekly\"</code></pre><p>What does each part mean?</p><!--kg-card-begin: markdown--><table>\n<thead>\n<tr>\n<th>Name</th>\n<th>What it does</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>package-ecosystem</code></td>\n<td>What package ecosystem to scan from <a href=\"https://docs.github.com/en/code-security/dependabot/dependabot-version-updates/about-dependabot-version-updates#supported-repositories-and-ecosystems\">Supported Packages</a></td>\n</tr>\n<tr>\n<td><code>directory</code></td>\n<td>Where this code it should scan lives</td>\n</tr>\n<tr>\n<td><code>schedule</code></td>\n<td>How often to scan, based on <a href=\"https://docs.github.com/en/code-security/dependabot/dependabot-version-updates/configuration-options-for-the-dependabot.yml-file#scheduleinterval\"><code>schedule.interval</code></a></td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: markdown--><p>Once you've created the file, go to your repo and click on <code>Insights</code></p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2023/07/image-1.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1440\" height=\"215\" srcset=\"__GHOST_URL__/content/images/size/w600/2023/07/image-1.png 600w, __GHOST_URL__/content/images/size/w1000/2023/07/image-1.png 1000w, __GHOST_URL__/content/images/2023/07/image-1.png 1440w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Then click on <code>Dependency graph</code></p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2023/07/image-2.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"387\" height=\"492\"></figure><p>Then click on Dependabot and you should see a healthy scan</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2023/07/image-3.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"943\" height=\"251\" srcset=\"__GHOST_URL__/content/images/size/w600/2023/07/image-3.png 600w, __GHOST_URL__/content/images/2023/07/image-3.png 943w\" sizes=\"(min-width: 720px) 720px\"></figure><h2 id=\"how-to-add-access-to-private-modules\">How to add access to Private Modules</h2><p>When creating private modules in a Private Git Repository, you will need to give Dependabot access to these.</p><p>To do so, navigate to your Org's home page, and click on <code>Settings</code> and then <code>Code security and analysis</code></p><p> </p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2023/07/image-4.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"307\" height=\"184\"></figure><p>Once here, you will need to scroll down to the section that reads <code>Grant Dependabot access to private repositories</code></p><p>Here search for the name of all the repos that contain modules. Once added, Dependabot will check if they have releases, and if so, will update them</p><h2 id=\"how-to-ignore-certain-modules\">How to ignore certain modules</h2><p>This is out of scope for this post, but see the below documentation for an example and walkthrough </p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://documentation.breadnet.co.uk/kb/dependabot/ignore-terraform-module-version-dependabot/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Ignore terraform module version dependabot - breadNET Documentation</div><div class=\"kg-bookmark-description\">breadNET Documentation</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://documentation.breadnet.co.uk/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">logo</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://documentation.breadnet.co.uk/assets/images/social/kb/dependabot/ignore-terraform-module-version-dependabot.png\" alt=\"\"></div></a></figure>","comment_id":"64b33a2ec9cae386d9c53c96","plaintext":"I recently stumbled across Dependabot and was curious if it can be used to keep Terraform up to date. Turns out you can, but there are some small catches that you should be aware of.\n\nIn todays installment, we look at what it is, why you'd use it, and how to set it up.\n\n\nWhat is Dependabot\n\nDependabot is a tool from GitHub that allows automatic updates and patching of code in repositories. This is especially useful as once you get over about 3 terraform dependencies, you start having the issue where Providers become out dated, and modules fall behind on patches to issues.\n\nBy enabling Dependabot, you basically get a colleage for free who's sole job is finding out dated terraform packages (modules, providers), creating a pull request to update them and managing rebasing.\n\n\nWhy would I use it for terraform\n\nAs mentioned, once you get past about 3 providers across a codebase (and in my case, over 30k lines of terraform) keeping things up to date becomes a real hassle.\n\nDependabot will search for the releases of Modules and Providers frequently and update them. Even Private modules.\n\n\nWhat are the limitations\n\nFor some reason, when configuring the terraform dependabot config, you will need to create a new line item per directory which contains a provider.tf file, or in the case of modules, the directory.\n\n\nHow to set it up\n\nLets assume for the time being the below is our file layout\n\nmonorepo\n├── .github\n│   └── dependabot.yml\n├── artifact-registry\n│   └── provider.tf\n├── dns\n│   └── provider.tf\n└── gke\n    ├── dev\n    │   └── provider.tf\n    ├── prod\n    │   └── provider.tf\n    └── test\n        └── provider.tf\n\n\nOnce you've enabled Dependabot (follow these instructions) create a file called .github/dependabot.yml and inset the below\n\nversion: 2\n\nupdates:\n# Terraform - One entry per thing we want to scan as per https://github.com/dependabot/dependabot-core/issues/649\n  - package-ecosystem: \"terraform\" # DNS\n    directory: \"/dns\"\n    schedule:\n      interval: \"daily\"\n  - package-ecosystem: \"terraform\" # GKE Dev\n    directory: \"/gke/dev\"\n    schedule:\n      interval: \"weekly\"\n  - package-ecosystem: \"terraform\" # GKE Prod\n    directory: \"/gke/prod\"\n    schedule:\n      interval: \"weekly\"\n  - package-ecosystem: \"terraform\" # GKE Test\n    directory: \"/gke/test\"\n    schedule:\n      interval: \"weekly\"\n  - package-ecosystem: \"terraform\" # Artifact Registry\n    directory: \"/artifact-registry\"\n    schedule:\n      interval: \"weekly\"\n\nWhat does each part mean?\n\n\n\n\nName\nWhat it does\n\n\n\n\npackage-ecosystem\nWhat package ecosystem to scan from Supported Packages\n\n\ndirectory\nWhere this code it should scan lives\n\n\nschedule\nHow often to scan, based on schedule.interval\n\n\n\n\n\nOnce you've created the file, go to your repo and click on Insights\n\nThen click on Dependency graph\n\nThen click on Dependabot and you should see a healthy scan\n\n\nHow to add access to Private Modules\n\nWhen creating private modules in a Private Git Repository, you will need to give Dependabot access to these.\n\nTo do so, navigate to your Org's home page, and click on Settings and then Code security and analysis\n\n\n\nOnce here, you will need to scroll down to the section that reads Grant Dependabot access to private repositories\n\nHere search for the name of all the repos that contain modules. Once added, Dependabot will check if they have releases, and if so, will update them\n\n\nHow to ignore certain modules\n\nThis is out of scope for this post, but see the below documentation for an example and walkthrough\n\nIgnore terraform module version dependabot - breadNET DocumentationbreadNET Documentationlogo","feature_image":"https://images.unsplash.com/photo-1637002722490-5f8ceed9774c?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3wxMTc3M3wwfDF8c2VhcmNofDM1fHxyb2JvdHxlbnwwfHx8fDE2ODk0Njc1MTh8MA&ixlib=rb-4.0.3&q=80&w=2000","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2023-07-16T00:30:38.000Z","updated_at":"2023-07-16T01:00:31.000Z","published_at":"2023-07-16T01:00:31.000Z","custom_excerpt":"Looking at using Dependabot to manage your terraform modules and providers? I've got you covered","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"651d81d5c9cae386d9c53d10","uuid":"4b05b3c7-96a8-4208-83b7-3bbca9cea508","title":"Terraform state chicken and egg problem","slug":"terraform-state-chicken-and-egg-problem","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"hr\",{}],[\"code\",{\"code\":\"REGION=\\\"us-east-1\\\"\\naws s3api create-bucket \\\\\\n\\t--region \\\"${REGION}\\\" \\\\\\n\\t--bucket \\\"companyname-terraformstate-state\\\" \\\\\\n\\naws dynamodb create-table \\\\\\n\\t--region \\\"${REGION}\\\" \\\\\\n\\t--table-name companyname_terraformstate_state \\\\\\n\\t--attribute-definitions AttributeName=LockID,AttributeType=S \\\\\\n\\t--key-schema AttributeName=LockID,KeyType=HASH \\\\\\n\\t--provisioned-throughput ReadCapacityUnits=1,WriteCapacityUnits=1\\n   \"}],[\"code\",{\"code\":\"gsutil mb -c nearline gs://compantname-terraformstate-state\"}],[\"code\",{\"code\":\"resource \\\"google_storage_bucket\\\" \\\"bucket\\\" {\\n  location                    = \\\"europe-west2\\\"\\n  name                        = \\\"companyname-terraformstate-state\\n  project                     = \\\"companyname-terraform-state\\\"\\n  public_access_prevention    = \\\"enforced\\\"\\n  storage_class               = \\\"STANDARD\\\"\\n  uniform_bucket_level_access = true\\n  force_destroy               = false\\n  versioning {\\n    enabled = true\\n  }\\n\\n  lifecycle_rule {\\n    action {\\n      type = \\\"Delete\\\"\\n    }\\n    condition {\\n      num_newer_versions = 100\\n      with_state         = \\\"ARCHIVED\\\"\\n    }\\n  }\\n\\n  lifecycle_rule {\\n    action {\\n      type = \\\"Delete\\\"\\n    }\\n    condition {\\n      days_since_noncurrent_time = 100\\n      with_state                 = \\\"ANY\\\"\\n    }\\n  }\\n\\n}\"}],[\"code\",{\"code\":\"terraform plan\\nterraform apply\\ngit add terraform.tfstate\\ngit add .\\ngit commit -m 'Configuring state bucket for other state'\\ngit push\"}],[\"hr\",{}]],\"markups\":[[\"code\"]],\"sections\":[[1,\"p\",[[0,[],0,\"Since the dawn of time people have been dealing with Chicken and egg problems in terraform.\"]]],[1,\"blockquote\",[[0,[],0,\"I need to create a resource, which is depended on by another resource, but the first resource depends on the last one\"]]],[1,\"p\",[[0,[],0,\" Imagine a circle. \"]]],[1,\"p\",[[0,[],0,\"One of the main issues I've come across when you start a new engagement with a client or a company where they don't use terraform is: How do I create state buckets\"]]],[1,\"p\",[[0,[],0,\"The solution can be as simple or as advanced as you want it to be. Let's explore some solutions:\"]]],[1,\"p\",[[0,[],0,\"I prefer to create a state bucket, then use CI to create more state buckets in terraform. \"]]],[3,\"ul\",[[[0,[],0,\"Command to create the first state bucket\"]],[[0,[],0,\"Terraform in the repo\"]]]],[10,0],[1,\"h2\",[[0,[],0,\"Command to create the first state bucket\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"Most cloud providers allow you to use their CLI, Google has  \"],[0,[0],1,\"gcloud\"],[0,[],0,\" and Amazon has \"],[0,[0],1,\"aws\"],[0,[],0,\" respectively to interact with the cloud provider.\"]]],[1,\"p\",[[0,[],0,\"This solution relies on you running a command. By doing this you get no state to check the bucket against, but the command is set and forget.\"]]],[1,\"p\",[[0,[],0,\"AWS:\"]]],[10,1],[1,\"p\",[[0,[],0,\"GCP\"]]],[10,2],[1,\"p\",[[0,[],0,\"In GCP, state locking is done through the bucket opposed to needing a dynamoDB table. \"]]],[1,\"p\",[[0,[],0,\"You can then point your terraform repo/ directory for managing new state buckets to this bucket. \"]]],[1,\"p\",[]],[1,\"h2\",[[0,[],0,\"Terraform in the repo\"]]],[1,\"p\",[[0,[],0,\"This is the option I chose to go for, as it shows the bucket was created and you've got state for that bucket so you can look and see how the bucket was configured. A nice touch of this is that you can then scan it with things like \"],[0,[0],1,\"tfsec\"],[0,[],0,\" and \"],[0,[0],1,\"checkov\"]]],[1,\"p\",[[0,[],0,\"Because I am a GCP engineer, this will be a GCP example but I am sure chatGPT will be able to convert the below in to your cloud provider of choice \"]]],[1,\"p\",[[0,[],0,\" \"]]],[10,3],[1,\"p\",[[0,[],0,\"You will then run the below commands\"]]],[10,4],[1,\"p\",[[0,[],0,\"You're now free to use this bucket for holding the central state for terraform to provision other state\"]]],[1,\"p\",[]],[10,5],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"If you need help with configuring terraform state, reach out and we can work together on a solution! \"]]]],\"ghostVersion\":\"4.0\"}","lexical":null,"html":"<p>Since the dawn of time people have been dealing with Chicken and egg problems in terraform.</p><blockquote>I need to create a resource, which is depended on by another resource, but the first resource depends on the last one</blockquote><p> Imagine a circle. </p><p>One of the main issues I've come across when you start a new engagement with a client or a company where they don't use terraform is: How do I create state buckets</p><p>The solution can be as simple or as advanced as you want it to be. Let's explore some solutions:</p><p>I prefer to create a state bucket, then use CI to create more state buckets in terraform. </p><ul><li>Command to create the first state bucket</li><li>Terraform in the repo</li></ul><hr><h2 id=\"command-to-create-the-first-state-bucket\">Command to create the first state bucket</h2><p></p><p>Most cloud providers allow you to use their CLI, Google has  <code>gcloud</code> and Amazon has <code>aws</code> respectively to interact with the cloud provider.</p><p>This solution relies on you running a command. By doing this you get no state to check the bucket against, but the command is set and forget.</p><p>AWS:</p><pre><code>REGION=\"us-east-1\"\naws s3api create-bucket \\\n\t--region \"${REGION}\" \\\n\t--bucket \"companyname-terraformstate-state\" \\\n\naws dynamodb create-table \\\n\t--region \"${REGION}\" \\\n\t--table-name companyname_terraformstate_state \\\n\t--attribute-definitions AttributeName=LockID,AttributeType=S \\\n\t--key-schema AttributeName=LockID,KeyType=HASH \\\n\t--provisioned-throughput ReadCapacityUnits=1,WriteCapacityUnits=1\n   </code></pre><p>GCP</p><pre><code>gsutil mb -c nearline gs://compantname-terraformstate-state</code></pre><p>In GCP, state locking is done through the bucket opposed to needing a dynamoDB table. </p><p>You can then point your terraform repo/ directory for managing new state buckets to this bucket. </p><p></p><h2 id=\"terraform-in-the-repo\">Terraform in the repo</h2><p>This is the option I chose to go for, as it shows the bucket was created and you've got state for that bucket so you can look and see how the bucket was configured. A nice touch of this is that you can then scan it with things like <code>tfsec</code> and <code>checkov</code></p><p>Because I am a GCP engineer, this will be a GCP example but I am sure chatGPT will be able to convert the below in to your cloud provider of choice </p><p> </p><pre><code>resource \"google_storage_bucket\" \"bucket\" {\n  location                    = \"europe-west2\"\n  name                        = \"companyname-terraformstate-state\n  project                     = \"companyname-terraform-state\"\n  public_access_prevention    = \"enforced\"\n  storage_class               = \"STANDARD\"\n  uniform_bucket_level_access = true\n  force_destroy               = false\n  versioning {\n    enabled = true\n  }\n\n  lifecycle_rule {\n    action {\n      type = \"Delete\"\n    }\n    condition {\n      num_newer_versions = 100\n      with_state         = \"ARCHIVED\"\n    }\n  }\n\n  lifecycle_rule {\n    action {\n      type = \"Delete\"\n    }\n    condition {\n      days_since_noncurrent_time = 100\n      with_state                 = \"ANY\"\n    }\n  }\n\n}</code></pre><p>You will then run the below commands</p><pre><code>terraform plan\nterraform apply\ngit add terraform.tfstate\ngit add .\ngit commit -m 'Configuring state bucket for other state'\ngit push</code></pre><p>You're now free to use this bucket for holding the central state for terraform to provision other state</p><p></p><hr><p></p><p>If you need help with configuring terraform state, reach out and we can work together on a solution! </p>","comment_id":"651d81d5c9cae386d9c53d10","plaintext":"Since the dawn of time people have been dealing with Chicken and egg problems in terraform.\n\nI need to create a resource, which is depended on by another resource, but the first resource depends on the last one\n\nImagine a circle.\n\nOne of the main issues I've come across when you start a new engagement with a client or a company where they don't use terraform is: How do I create state buckets\n\nThe solution can be as simple or as advanced as you want it to be. Let's explore some solutions:\n\nI prefer to create a state bucket, then use CI to create more state buckets in terraform.\n\n * Command to create the first state bucket\n * Terraform in the repo\n\n\nCommand to create the first state bucket\n\n\n\nMost cloud providers allow you to use their CLI, Google has  gcloud and Amazon has aws respectively to interact with the cloud provider.\n\nThis solution relies on you running a command. By doing this you get no state to check the bucket against, but the command is set and forget.\n\nAWS:\n\nREGION=\"us-east-1\"\naws s3api create-bucket \\\n\t--region \"${REGION}\" \\\n\t--bucket \"companyname-terraformstate-state\" \\\n\naws dynamodb create-table \\\n\t--region \"${REGION}\" \\\n\t--table-name companyname_terraformstate_state \\\n\t--attribute-definitions AttributeName=LockID,AttributeType=S \\\n\t--key-schema AttributeName=LockID,KeyType=HASH \\\n\t--provisioned-throughput ReadCapacityUnits=1,WriteCapacityUnits=1\n   \n\nGCP\n\ngsutil mb -c nearline gs://compantname-terraformstate-state\n\nIn GCP, state locking is done through the bucket opposed to needing a dynamoDB table.\n\nYou can then point your terraform repo/ directory for managing new state buckets to this bucket.\n\n\n\n\nTerraform in the repo\n\nThis is the option I chose to go for, as it shows the bucket was created and you've got state for that bucket so you can look and see how the bucket was configured. A nice touch of this is that you can then scan it with things like tfsec and checkov\n\nBecause I am a GCP engineer, this will be a GCP example but I am sure chatGPT will be able to convert the below in to your cloud provider of choice\n\n\n\nresource \"google_storage_bucket\" \"bucket\" {\n  location                    = \"europe-west2\"\n  name                        = \"companyname-terraformstate-state\n  project                     = \"companyname-terraform-state\"\n  public_access_prevention    = \"enforced\"\n  storage_class               = \"STANDARD\"\n  uniform_bucket_level_access = true\n  force_destroy               = false\n  versioning {\n    enabled = true\n  }\n\n  lifecycle_rule {\n    action {\n      type = \"Delete\"\n    }\n    condition {\n      num_newer_versions = 100\n      with_state         = \"ARCHIVED\"\n    }\n  }\n\n  lifecycle_rule {\n    action {\n      type = \"Delete\"\n    }\n    condition {\n      days_since_noncurrent_time = 100\n      with_state                 = \"ANY\"\n    }\n  }\n\n}\n\nYou will then run the below commands\n\nterraform plan\nterraform apply\ngit add terraform.tfstate\ngit add .\ngit commit -m 'Configuring state bucket for other state'\ngit push\n\nYou're now free to use this bucket for holding the central state for terraform to provision other state\n\n\n\n\n\nIf you need help with configuring terraform state, reach out and we can work together on a solution!","feature_image":"https://images.unsplash.com/photo-1547809890-1bef935add63?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3wxMTc3M3wwfDF8c2VhcmNofDI3fHxlZ2d8ZW58MHx8fHwxNjk2NDA5ODg4fDA&ixlib=rb-4.0.3&q=80&w=2000","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2023-10-04T15:16:37.000Z","updated_at":"2023-10-04T15:49:50.000Z","published_at":"2023-10-04T15:40:00.000Z","custom_excerpt":"Terraform is full of chicken and egg problems, let's solve the state one","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"652c1969c9cae386d9c53d71","uuid":"b2886169-6f92-4cc9-8995-62f2df2e87af","title":"Kubernetes secrets using Google Secret Manager","slug":"kubernets-secrets-using-google-secret-manager","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"markdown\",{\"markdown\":\"| Name | Comments |\\n| --- | --- |\\n| Directly Fetching Secrets | Something we used to do, but the idea of this post is to prevent this |\\n| Kubernetes Secrets CSI Driveer | We tried this and goodness me it was **painfull** |\\n| Using an Init container | Nope. This sounds way too complicated and I dont want to have to play about with files |\\n| External Secrets Operator | Spoiler alert |\\n| B3rg1a$ | Looked too complicated for what we were doing |\\n\"}],[\"markdown\",{\"markdown\":\"| Name | Our Value |\\n| --- | --- |\\n| Cluster Name | `breadnet-cluster` |\\n| Region | `europe-west2`\\n| Zone | `europe-west2-c` |\\n\"}],[\"code\",{\"code\":\"apiVersion: skaffold/v4beta6\\nkind: Config\\nmetadata:\\n  name: external-secrets\\ndeploy:\\n  helm:\\n    releases:\\n      - name: external-secrets\\n        namespace: external-secrets\\n        remoteChart: external-secrets\\n        repo: \\\"https://charts.external-secrets.io\\\"\\n        upgradeOnChange: true\\n\",\"language\":\"yaml\"}],[\"code\",{\"code\":\"➜ kubectl get pods -n external-secrets                         \\nNAME                                                READY   STATUS    RESTARTS   AGE\\nexternal-secrets-7f8fd8d64d-wfkj8                   1/1     Running   0          6d5h\\nexternal-secrets-cert-controller-8499548dd6-ttxtf   1/1     Running   0          6d5h\\nexternal-secrets-webhook-dbc576595-lkxxm            1/1     Running   0          6d5h\\n\"}],[\"bookmark\",{\"version\":\"1.0\",\"type\":\"bookmark\",\"url\":\"https://documentation.breadnet.co.uk/kubernetes/gke/configure-gke-workload-identity/#service-account\",\"metadata\":{\"url\":\"https://documentation.breadnet.co.uk/kubernetes/gke/configure-gke-workload-identity/\",\"title\":\"Configure GKE workload Identity - breadNET Documentation\",\"description\":\"breadNET Documentation\",\"author\":null,\"publisher\":\"logo\",\"thumbnail\":\"https://documentation.breadnet.co.uk/assets/images/social/kubernetes/gke/configure-gke-workload-identity.png\",\"icon\":\"https://documentation.breadnet.co.uk/favicon.ico\"}}],[\"code\",{\"code\":\"apiVersion: v1\\nkind: ServiceAccount\\nmetadata:\\n  name: secret-accessor\\n  namespace: secret-store-namespace\\n  annotations:\\n    iam.gke.io/gcp-service-account: secret-accessor@secret-accessor-demo.gserviceaccount.com\\n\"}],[\"code\",{\"code\":\"apiVersion: external-secrets.io/v1beta1\\nkind: SecretStore\\nmetadata:\\n  name: gcp-store\\n  namespace: secret-store-namespace\\nspec:\\n  provider:\\n    gcpsm:\\n      auth:\\n        workloadIdentity:\\n          clusterLocation: europe-west2-c\\n          clusterName: breadnet-cluster\\n          clusterProjectID: breadnet-gke\\n          serviceAccountRef:\\n            name: secret-accessor\\n      projectID: breadnet-secrets\"}],[\"code\",{\"code\":\"apiVersion: external-secrets.io/v1beta1\\nkind: ExternalSecret\\nmetadata:\\n  name: database-credentials\\nspec:\\n  refreshInterval: 1h             # rate SecretManager pulls GCPSM\\n  secretStoreRef:\\n    kind: SecretStore\\n    name: gcp-store               # name of the SecretStore (or kind specified)\\n  target:\\n    name: database-credentials    # name of the k8s Secret to be created\\n    creationPolicy: Owner\\n  data:\\n  - secretKey: database_username\\n    remoteRef:\\n      key: database_username      # name of the GCPSM secret key\\n  - secretKey: database_password\\n    remoteRef:\\n      key: database_password      # name of the GCPSM secret key\"}],[\"bookmark\",{\"version\":\"1.0\",\"type\":\"bookmark\",\"url\":\"https://external-secrets.io/latest/provider/google-secrets-manager/\",\"metadata\":{\"url\":\"https://external-secrets.io/latest/provider/google-secrets-manager/\",\"title\":\"Google Cloud Secret Manager - External Secrets Operator\",\"description\":null,\"author\":null,\"publisher\":\"External Secrets Operator\",\"thumbnail\":null,\"icon\":\"https://external-secrets.io/latest/assets/images/favicon.png\"}}],[\"hr\",{}]],\"markups\":[[\"a\",[\"href\",\"https://cloud.google.com/secret-manager\"]],[\"strong\"],[\"a\",[\"href\",\"https://issuetracker.google.com/issues/305477780\"]],[\"a\",[\"href\",\"https://internaldeveloperplatform.org/what-is-an-internal-developer-platform/\"]],[\"em\"],[\"a\",[\"href\",\"https://skaffold.dev\"]],[\"a\",[\"href\",\"https://taskfile.dev\"]],[\"a\",[\"href\",\"https://documentation.breadnet.co.uk/kubernetes/gke/configure-gke-workload-identity/\"]],[\"a\",[\"href\",\"https://secrets-store-csi-driver.sigs.k8s.io/topics/sync-as-kubernetes-secret\"]],[\"a\",[\"href\",\"https://github.com/external-secrets/external-secrets/blob/7b8f36b2f007306a106863b12c433edd9c8820ea/pkg/provider/gcp/secretmanager/workload_identity.go#L120-L123\"]],[\"code\"],[\"a\",[\"href\",\"https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#create-a-customresourcedefinition\"]],[\"a\",[\"href\",\"https://cloud.google.com/iam/docs/best-practices-service-accounts\"]],[\"a\",[\"href\",\"documentation.breadnet.co.uk/\"]]],\"sections\":[[1,\"p\",[]],[1,\"p\",[[0,[],0,\"Since my career at the current company cool enough to employ me, I've been almost exclusively working on GKE (Google's Kubernetes offering) \"]]],[1,\"p\",[[0,[],0,\"One (of the many) issues I've been trying to solve in GKE is how to use Google Secrets (\"],[0,[0],1,\"Their secret manager\"],[0,[],0,\") \"],[0,[1],1,\"inside \"],[0,[],0,\"GKE in a simple mean that doest require crazy add ons, and complexity. \"]]],[1,\"p\",[[0,[],0,\"I've opened a Feature Request, but until then this blog post will explain what options you have, and what I did, and then a walkthrough of how to set up what I've done\"]]],[3,\"ul\",[[[0,[2],1,\"[FR] Native support for Google Secret manager in GKE\"]]]],[1,\"h2\",[[0,[],0,\"What is the issue to start with\"]]],[1,\"p\",[[0,[],0,\"We have a lot of applications that run on GKE, these span from simple things like a cron job to backup our state projects, all the way to our in house \"],[0,[3],1,\"multi-layer multi-language IDP\"],[0,[],0,\" - One of the massive issues here is most of these applications need secrets to communicate \"],[0,[4],1,\"onwards\"],[0,[],0,\", be it to Microsoft Auth or to third party APIs. \"]]],[1,\"p\",[[0,[],0,\"How we get these secrets in, is a real pain. We used to manage these by using \"],[0,[5],1,\"Skaffold\"],[0,[],0,\" to pick up local environment variables  at deploy. One issue this has, if the dev doesn't run the pre-requisite \"],[0,[6],1,\"Task command\"],[0,[],0,\" then the secret gets nulled and things break. This happens maybe twice a week. \"]]],[1,\"h2\",[[0,[],0,\"What is the proposed Solution\"]]],[1,\"p\",[[0,[],0,\"Ideally we would allow the application developers to just specify the secret in Google secret manager, and either the file on the pod to mount it to, or the environment variables it should be. \"]]],[1,\"p\",[[0,[],0,\"Below details what I want from a solution\"]]],[3,\"ul\",[[[0,[],0,\"Ability to use Service account of Pod or specific account using \"],[0,[7],1,\"Workload Identity\"]],[[0,[],0,\"Generate a Kubernetes native secret\"]],[[0,[],0,\"Able to mount to pods as either env variables or File\"]]]],[1,\"p\",[[0,[],0,\"I had a dig around on the internet and found a Medium Post which offered the below solutions.\"]]],[10,0],[1,\"p\",[[0,[],0,\"As you can probably guess, we're going to be talking about using External Secrets Operator to get secrets in to our cluster. \"]]],[1,\"p\",[[0,[],0,\"Back in August I had trialed using the CSI driver and it was painful. In order to get secrets to become Kubernetes secrets, \"],[0,[8],1,\"you had to start a pod and mount the secret to it\"],[0,[],0,\". This is a security issue. \"]]],[1,\"h2\",[[0,[],0,\"External Secrets Operator\"]]],[1,\"p\",[[0,[],0,\"I decided to settle on External Secrets Operator (ESO) as it allowed us to sync secrets from GSM on a schedule we define, allows us to specify a service account that has access, and it allows name spacing resources so we can apply RBAC on them. \"]]],[1,\"p\",[[0,[],0,\"Let's just set out the scene of our environment, as some parts are very important to pay attention to. \"]]],[10,1],[1,\"p\",[[0,[],0,\"Something we really need to make sure we pay attention is the Cluster name and Zone. This is because when using Workload Identity the ESO app makes \"],[0,[9],1,\"API calls to the tokens API, these have to match up exactly\"]]],[1,\"p\",[[0,[],0,\"First we need to install the External Secrets Operator, You can do this with Helm, but we use Skaffold for all management operations so below is the skaffold file\"]]],[10,2],[1,\"p\",[[0,[],0,\"Once External Secrets is installed, check all is well. Ideally these should all say \"],[0,[10],1,\"1/1\"],[0,[],0,\" under the ready column\"]]],[10,3],[1,\"p\",[[0,[],0,\"The next step is to create a Google Cloud Service account in the Service project for your application.\"]]],[1,\"p\",[[0,[],0,\"Once done, grant the service account \"],[0,[4,10],1,\"roles/iam.serviceAccountTokenCreator\"],[0,[],1,\" \"],[0,[],0,\"on the project.\"]]],[1,\"p\",[[0,[],0,\"Next thing is to navigate to Secret manager, locate the secret you want the ESO to have access to, then give that service account Secret Viewer on that Secret. \"]]],[1,\"p\",[[0,[],0,\"Custom resource definitions and their \"],[0,[4],1,\"imported\"],[0,[],0,\" resources can either be \"],[0,[11],1,\"Cluster Scoped or Namespaced\"],[0,[],0,\". For our specific configuration, we will be creating a \"],[0,[10],1,\"SecretStore\"],[0,[],0,\" opposed to a \"],[0,[10],1,\"ClusterSecretStore\"],[0,[],0,\" which is the cluster wide one.\"]]],[1,\"p\",[[0,[],0,\"We wont be using \"],[0,[10],1,\"ClusterSecretStore\"],[0,[],0,\" as this means a single service account is used cluster wide, which means this service account has way more permissions than is required \"],[0,[12],1,\"(Not good)\"],[0,[],0,\" \"]]],[1,\"p\",[[0,[],0,\"Before you can move ahead with creating the \"],[0,[10],1,\"SecretStore\"],[0,[],0,\" we need to create a kubernetes service account and annotate the account with the GCP Service account. For this, follow my guide from \"],[0,[10],1,\"Service account\"],[0,[],0,\" to \"],[0,[10],1,\"Kubernetes service account\"],[0,[],0,\" section\"]]],[10,4],[1,\"p\",[[0,[],0,\"We will annotate the service account is like below:\"]]],[10,5],[1,\"p\",[[0,[],0,\"Now create the \"],[0,[10],1,\"SecretStore\"]]],[1,\"p\",[[0,[],0,\"If your secrets are in the same project as the GKE cluster, you can omit the \"],[0,[10],1,\"clusterProjectId\"],[0,[],0,\" field (I think, I cant remember and their documentation is sparse) How ever if you have secrets in a centralized Secret Project, then you should include this field. I have included it below. \"]]],[10,6],[1,\"p\",[[0,[],0,\"Once this is applied, and the service account in the namespace is annotated, you can then access secrets using this store. \"]]],[10,7],[1,\"p\",[[0,[],0,\"This then allows us to sync the Google cloud secret \"],[0,[10],1,\"database_username\"],[0,[],0,\" to the field \"],[0,[10],1,\"database_username\"],[0,[],0,\" in the secret called \"],[0,[10],1,\"database-credentials\"],[0,[],0,\" in the current namespace. \"]]],[1,\"p\",[[0,[],0,\"You're then free to use this secret as you would as a kubernetes native secret.\"]]],[1,\"p\",[[0,[],0,\"The below is the link to the Google Secret managet for ESO, go forth and conquer \"]]],[10,8],[10,9],[1,\"p\",[[0,[],0,\"If you enjoyed this blog post, please consider bookmarking this site! \"]]],[1,\"p\",[[0,[],0,\"You may also be interested in my documentation site which will have more in depth examples etc: \"],[0,[13],1,\"documentation.breadnet.co.uk\"]]],[1,\"p\",[[0,[],0,\"As always you can contact me to discuss this post, or if you need help implementing this as your company please reachout to me and we can discuss ways to get you working cloud native!\"]]],[1,\"p\",[[0,[],0,\"Thanks <3 \"]]],[1,\"p\",[]]],\"ghostVersion\":\"4.0\"}","lexical":null,"html":"<p></p><p>Since my career at the current company cool enough to employ me, I've been almost exclusively working on GKE (Google's Kubernetes offering) </p><p>One (of the many) issues I've been trying to solve in GKE is how to use Google Secrets (<a href=\"https://cloud.google.com/secret-manager\">Their secret manager</a>) <strong>inside </strong>GKE in a simple mean that doest require crazy add ons, and complexity. </p><p>I've opened a Feature Request, but until then this blog post will explain what options you have, and what I did, and then a walkthrough of how to set up what I've done</p><ul><li><a href=\"https://issuetracker.google.com/issues/305477780\">[FR] Native support for Google Secret manager in GKE</a></li></ul><h2 id=\"what-is-the-issue-to-start-with\">What is the issue to start with</h2><p>We have a lot of applications that run on GKE, these span from simple things like a cron job to backup our state projects, all the way to our in house <a href=\"https://internaldeveloperplatform.org/what-is-an-internal-developer-platform/\">multi-layer multi-language IDP</a> - One of the massive issues here is most of these applications need secrets to communicate <em>onwards</em>, be it to Microsoft Auth or to third party APIs. </p><p>How we get these secrets in, is a real pain. We used to manage these by using <a href=\"https://skaffold.dev\">Skaffold</a> to pick up local environment variables  at deploy. One issue this has, if the dev doesn't run the pre-requisite <a href=\"https://taskfile.dev\">Task command</a> then the secret gets nulled and things break. This happens maybe twice a week. </p><h2 id=\"what-is-the-proposed-solution\">What is the proposed Solution</h2><p>Ideally we would allow the application developers to just specify the secret in Google secret manager, and either the file on the pod to mount it to, or the environment variables it should be. </p><p>Below details what I want from a solution</p><ul><li>Ability to use Service account of Pod or specific account using <a href=\"https://documentation.breadnet.co.uk/kubernetes/gke/configure-gke-workload-identity/\">Workload Identity</a></li><li>Generate a Kubernetes native secret</li><li>Able to mount to pods as either env variables or File</li></ul><p>I had a dig around on the internet and found a Medium Post which offered the below solutions.</p><!--kg-card-begin: markdown--><table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Comments</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Directly Fetching Secrets</td>\n<td>Something we used to do, but the idea of this post is to prevent this</td>\n</tr>\n<tr>\n<td>Kubernetes Secrets CSI Driveer</td>\n<td>We tried this and goodness me it was <strong>painfull</strong></td>\n</tr>\n<tr>\n<td>Using an Init container</td>\n<td>Nope. This sounds way too complicated and I dont want to have to play about with files</td>\n</tr>\n<tr>\n<td>External Secrets Operator</td>\n<td>Spoiler alert</td>\n</tr>\n<tr>\n<td>B3rg1a$</td>\n<td>Looked too complicated for what we were doing</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: markdown--><p>As you can probably guess, we're going to be talking about using External Secrets Operator to get secrets in to our cluster. </p><p>Back in August I had trialed using the CSI driver and it was painful. In order to get secrets to become Kubernetes secrets, <a href=\"https://secrets-store-csi-driver.sigs.k8s.io/topics/sync-as-kubernetes-secret\">you had to start a pod and mount the secret to it</a>. This is a security issue. </p><h2 id=\"external-secrets-operator\">External Secrets Operator</h2><p>I decided to settle on External Secrets Operator (ESO) as it allowed us to sync secrets from GSM on a schedule we define, allows us to specify a service account that has access, and it allows name spacing resources so we can apply RBAC on them. </p><p>Let's just set out the scene of our environment, as some parts are very important to pay attention to. </p><!--kg-card-begin: markdown--><table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Our Value</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Cluster Name</td>\n<td><code>breadnet-cluster</code></td>\n</tr>\n<tr>\n<td>Region</td>\n<td><code>europe-west2</code></td>\n</tr>\n<tr>\n<td>Zone</td>\n<td><code>europe-west2-c</code></td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: markdown--><p>Something we really need to make sure we pay attention is the Cluster name and Zone. This is because when using Workload Identity the ESO app makes <a href=\"https://github.com/external-secrets/external-secrets/blob/7b8f36b2f007306a106863b12c433edd9c8820ea/pkg/provider/gcp/secretmanager/workload_identity.go#L120-L123\">API calls to the tokens API, these have to match up exactly</a></p><p>First we need to install the External Secrets Operator, You can do this with Helm, but we use Skaffold for all management operations so below is the skaffold file</p><pre><code class=\"language-yaml\">apiVersion: skaffold/v4beta6\nkind: Config\nmetadata:\n  name: external-secrets\ndeploy:\n  helm:\n    releases:\n      - name: external-secrets\n        namespace: external-secrets\n        remoteChart: external-secrets\n        repo: \"https://charts.external-secrets.io\"\n        upgradeOnChange: true\n</code></pre><p>Once External Secrets is installed, check all is well. Ideally these should all say <code>1/1</code> under the ready column</p><pre><code>➜ kubectl get pods -n external-secrets                         \nNAME                                                READY   STATUS    RESTARTS   AGE\nexternal-secrets-7f8fd8d64d-wfkj8                   1/1     Running   0          6d5h\nexternal-secrets-cert-controller-8499548dd6-ttxtf   1/1     Running   0          6d5h\nexternal-secrets-webhook-dbc576595-lkxxm            1/1     Running   0          6d5h\n</code></pre><p>The next step is to create a Google Cloud Service account in the Service project for your application.</p><p>Once done, grant the service account <em><code>roles/iam.serviceAccountTokenCreator</code> </em>on the project.</p><p>Next thing is to navigate to Secret manager, locate the secret you want the ESO to have access to, then give that service account Secret Viewer on that Secret. </p><p>Custom resource definitions and their <em>imported</em> resources can either be <a href=\"https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#create-a-customresourcedefinition\">Cluster Scoped or Namespaced</a>. For our specific configuration, we will be creating a <code>SecretStore</code> opposed to a <code>ClusterSecretStore</code> which is the cluster wide one.</p><p>We wont be using <code>ClusterSecretStore</code> as this means a single service account is used cluster wide, which means this service account has way more permissions than is required <a href=\"https://cloud.google.com/iam/docs/best-practices-service-accounts\">(Not good)</a> </p><p>Before you can move ahead with creating the <code>SecretStore</code> we need to create a kubernetes service account and annotate the account with the GCP Service account. For this, follow my guide from <code>Service account</code> to <code>Kubernetes service account</code> section</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://documentation.breadnet.co.uk/kubernetes/gke/configure-gke-workload-identity/#service-account\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Configure GKE workload Identity - breadNET Documentation</div><div class=\"kg-bookmark-description\">breadNET Documentation</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://documentation.breadnet.co.uk/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">logo</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://documentation.breadnet.co.uk/assets/images/social/kubernetes/gke/configure-gke-workload-identity.png\" alt=\"\"></div></a></figure><p>We will annotate the service account is like below:</p><pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: secret-accessor\n  namespace: secret-store-namespace\n  annotations:\n    iam.gke.io/gcp-service-account: secret-accessor@secret-accessor-demo.gserviceaccount.com\n</code></pre><p>Now create the <code>SecretStore</code></p><p>If your secrets are in the same project as the GKE cluster, you can omit the <code>clusterProjectId</code> field (I think, I cant remember and their documentation is sparse) How ever if you have secrets in a centralized Secret Project, then you should include this field. I have included it below. </p><pre><code>apiVersion: external-secrets.io/v1beta1\nkind: SecretStore\nmetadata:\n  name: gcp-store\n  namespace: secret-store-namespace\nspec:\n  provider:\n    gcpsm:\n      auth:\n        workloadIdentity:\n          clusterLocation: europe-west2-c\n          clusterName: breadnet-cluster\n          clusterProjectID: breadnet-gke\n          serviceAccountRef:\n            name: secret-accessor\n      projectID: breadnet-secrets</code></pre><p>Once this is applied, and the service account in the namespace is annotated, you can then access secrets using this store. </p><pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: database-credentials\nspec:\n  refreshInterval: 1h             # rate SecretManager pulls GCPSM\n  secretStoreRef:\n    kind: SecretStore\n    name: gcp-store               # name of the SecretStore (or kind specified)\n  target:\n    name: database-credentials    # name of the k8s Secret to be created\n    creationPolicy: Owner\n  data:\n  - secretKey: database_username\n    remoteRef:\n      key: database_username      # name of the GCPSM secret key\n  - secretKey: database_password\n    remoteRef:\n      key: database_password      # name of the GCPSM secret key</code></pre><p>This then allows us to sync the Google cloud secret <code>database_username</code> to the field <code>database_username</code> in the secret called <code>database-credentials</code> in the current namespace. </p><p>You're then free to use this secret as you would as a kubernetes native secret.</p><p>The below is the link to the Google Secret managet for ESO, go forth and conquer </p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://external-secrets.io/latest/provider/google-secrets-manager/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Google Cloud Secret Manager - External Secrets Operator</div><div class=\"kg-bookmark-description\"></div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://external-secrets.io/latest/assets/images/favicon.png\" alt=\"\"><span class=\"kg-bookmark-author\">External Secrets Operator</span></div></div></a></figure><hr><p>If you enjoyed this blog post, please consider bookmarking this site! </p><p>You may also be interested in my documentation site which will have more in depth examples etc: <a href=\"documentation.breadnet.co.uk/\">documentation.breadnet.co.uk</a></p><p>As always you can contact me to discuss this post, or if you need help implementing this as your company please reachout to me and we can discuss ways to get you working cloud native!</p><p>Thanks &lt;3 </p>","comment_id":"652c1969c9cae386d9c53d71","plaintext":"Since my career at the current company cool enough to employ me, I've been almost exclusively working on GKE (Google's Kubernetes offering)\n\nOne (of the many) issues I've been trying to solve in GKE is how to use Google Secrets (Their secret manager) inside GKE in a simple mean that doest require crazy add ons, and complexity.\n\nI've opened a Feature Request, but until then this blog post will explain what options you have, and what I did, and then a walkthrough of how to set up what I've done\n\n * [FR] Native support for Google Secret manager in GKE\n\n\nWhat is the issue to start with\n\nWe have a lot of applications that run on GKE, these span from simple things like a cron job to backup our state projects, all the way to our in house multi-layer multi-language IDP - One of the massive issues here is most of these applications need secrets to communicate onwards, be it to Microsoft Auth or to third party APIs.\n\nHow we get these secrets in, is a real pain. We used to manage these by using Skaffold to pick up local environment variables  at deploy. One issue this has, if the dev doesn't run the pre-requisite Task command then the secret gets nulled and things break. This happens maybe twice a week.\n\n\nWhat is the proposed Solution\n\nIdeally we would allow the application developers to just specify the secret in Google secret manager, and either the file on the pod to mount it to, or the environment variables it should be.\n\nBelow details what I want from a solution\n\n * Ability to use Service account of Pod or specific account using Workload Identity\n * Generate a Kubernetes native secret\n * Able to mount to pods as either env variables or File\n\nI had a dig around on the internet and found a Medium Post which offered the below solutions.\n\n\n\n\nName\nComments\n\n\n\n\nDirectly Fetching Secrets\nSomething we used to do, but the idea of this post is to prevent this\n\n\nKubernetes Secrets CSI Driveer\nWe tried this and goodness me it was painfull\n\n\nUsing an Init container\nNope. This sounds way too complicated and I dont want to have to play about with files\n\n\nExternal Secrets Operator\nSpoiler alert\n\n\nB3rg1a$\nLooked too complicated for what we were doing\n\n\n\n\n\nAs you can probably guess, we're going to be talking about using External Secrets Operator to get secrets in to our cluster.\n\nBack in August I had trialed using the CSI driver and it was painful. In order to get secrets to become Kubernetes secrets, you had to start a pod and mount the secret to it. This is a security issue.\n\n\nExternal Secrets Operator\n\nI decided to settle on External Secrets Operator (ESO) as it allowed us to sync secrets from GSM on a schedule we define, allows us to specify a service account that has access, and it allows name spacing resources so we can apply RBAC on them.\n\nLet's just set out the scene of our environment, as some parts are very important to pay attention to.\n\n\n\n\nName\nOur Value\n\n\n\n\nCluster Name\nbreadnet-cluster\n\n\nRegion\neurope-west2\n\n\nZone\neurope-west2-c\n\n\n\n\n\nSomething we really need to make sure we pay attention is the Cluster name and Zone. This is because when using Workload Identity the ESO app makes API calls to the tokens API, these have to match up exactly\n\nFirst we need to install the External Secrets Operator, You can do this with Helm, but we use Skaffold for all management operations so below is the skaffold file\n\napiVersion: skaffold/v4beta6\nkind: Config\nmetadata:\n  name: external-secrets\ndeploy:\n  helm:\n    releases:\n      - name: external-secrets\n        namespace: external-secrets\n        remoteChart: external-secrets\n        repo: \"https://charts.external-secrets.io\"\n        upgradeOnChange: true\n\n\nOnce External Secrets is installed, check all is well. Ideally these should all say 1/1 under the ready column\n\n➜ kubectl get pods -n external-secrets                         \nNAME                                                READY   STATUS    RESTARTS   AGE\nexternal-secrets-7f8fd8d64d-wfkj8                   1/1     Running   0          6d5h\nexternal-secrets-cert-controller-8499548dd6-ttxtf   1/1     Running   0          6d5h\nexternal-secrets-webhook-dbc576595-lkxxm            1/1     Running   0          6d5h\n\n\nThe next step is to create a Google Cloud Service account in the Service project for your application.\n\nOnce done, grant the service account roles/iam.serviceAccountTokenCreator on the project.\n\nNext thing is to navigate to Secret manager, locate the secret you want the ESO to have access to, then give that service account Secret Viewer on that Secret.\n\nCustom resource definitions and their imported resources can either be Cluster Scoped or Namespaced. For our specific configuration, we will be creating a SecretStore opposed to a ClusterSecretStore which is the cluster wide one.\n\nWe wont be using ClusterSecretStore as this means a single service account is used cluster wide, which means this service account has way more permissions than is required (Not good)\n\nBefore you can move ahead with creating the SecretStore we need to create a kubernetes service account and annotate the account with the GCP Service account. For this, follow my guide from Service account to Kubernetes service account section\n\nConfigure GKE workload Identity - breadNET DocumentationbreadNET Documentationlogo\n\nWe will annotate the service account is like below:\n\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: secret-accessor\n  namespace: secret-store-namespace\n  annotations:\n    iam.gke.io/gcp-service-account: secret-accessor@secret-accessor-demo.gserviceaccount.com\n\n\nNow create the SecretStore\n\nIf your secrets are in the same project as the GKE cluster, you can omit the clusterProjectId field (I think, I cant remember and their documentation is sparse) How ever if you have secrets in a centralized Secret Project, then you should include this field. I have included it below.\n\napiVersion: external-secrets.io/v1beta1\nkind: SecretStore\nmetadata:\n  name: gcp-store\n  namespace: secret-store-namespace\nspec:\n  provider:\n    gcpsm:\n      auth:\n        workloadIdentity:\n          clusterLocation: europe-west2-c\n          clusterName: breadnet-cluster\n          clusterProjectID: breadnet-gke\n          serviceAccountRef:\n            name: secret-accessor\n      projectID: breadnet-secrets\n\nOnce this is applied, and the service account in the namespace is annotated, you can then access secrets using this store.\n\napiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: database-credentials\nspec:\n  refreshInterval: 1h             # rate SecretManager pulls GCPSM\n  secretStoreRef:\n    kind: SecretStore\n    name: gcp-store               # name of the SecretStore (or kind specified)\n  target:\n    name: database-credentials    # name of the k8s Secret to be created\n    creationPolicy: Owner\n  data:\n  - secretKey: database_username\n    remoteRef:\n      key: database_username      # name of the GCPSM secret key\n  - secretKey: database_password\n    remoteRef:\n      key: database_password      # name of the GCPSM secret key\n\nThis then allows us to sync the Google cloud secret database_username to the field database_username in the secret called database-credentials in the current namespace.\n\nYou're then free to use this secret as you would as a kubernetes native secret.\n\nThe below is the link to the Google Secret managet for ESO, go forth and conquer\n\nGoogle Cloud Secret Manager - External Secrets OperatorExternal Secrets Operator\n\nIf you enjoyed this blog post, please consider bookmarking this site!\n\nYou may also be interested in my documentation site which will have more in depth examples etc: documentation.breadnet.co.uk\n\nAs always you can contact me to discuss this post, or if you need help implementing this as your company please reachout to me and we can discuss ways to get you working cloud native!\n\nThanks <3","feature_image":"https://images.unsplash.com/photo-1567908643259-752122573d14?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3wxMTc3M3wwfDF8c2VhcmNofDR8fHdva2luZ2hhbXxlbnwwfHx8fDE2OTc0MDc1Nzd8MA&ixlib=rb-4.0.3&q=80&w=2000","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2023-10-15T16:55:05.000Z","updated_at":"2024-03-14T15:27:21.000Z","published_at":"2023-10-15T22:08:26.000Z","custom_excerpt":"How to get Google Secret manager secrets in to GKE? Look no further. We discuss options and implementing External Secrets Operator ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"655cc600c9cae386d9c53e92","uuid":"9e344032-cb78-4b5e-9337-0cfd2b3f5e47","title":"Kubernetes at home","slug":"kubernetes-at-home","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"hr\",{}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2023/11/cluster.png\",\"width\":1790,\"height\":2360}],[\"code\",{\"code\":\"➜ kubectl get nodes              \\nNAME     STATUS   ROLES                       AGE     VERSION\\nk3s-01   Ready    control-plane,etcd,master   23d     v1.27.6+k3s1\\nk3s-02   Ready    <none>                      23d     v1.27.6+k3s1\\nk3s-03   Ready    <none>                      5d21h   v1.27.7+k3s2\\n\"}],[\"markdown\",{\"markdown\":\"| IP range | Use |\\n| --- | --- |\\n| `172.16.2.0/28` | Node IP address |\\n| `172.16.0.0/25` | Services |\"}],[\"code\",{\"code\":\"services:\\n  http:\\n    - domain: example.breadinfra.net\\n      svc: podinfo\\n      namespace: podinfo\",\"language\":\"yaml\"}],[\"bookmark\",{\"url\":\"https://documentation.breadnet.co.uk/kubernetes/k3s/cloudflare-tunnels-on-k3s/?utm_source=breadnet&utm_medium=blog&utm_campaign=kubernetes%20at%20home\",\"metadata\":{\"url\":\"https://documentation.breadnet.co.uk/kubernetes/k3s/cloudflare-tunnels-on-k3s/\",\"title\":\"Cloudflare Tunnels on k3s - breadNET Documentation\",\"description\":\"breadNET Documentation\",\"author\":null,\"publisher\":\"logo\",\"thumbnail\":\"https://documentation.breadnet.co.uk/assets/images/social/kubernetes/k3s/cloudflare-tunnels-on-k3s.png\",\"icon\":\"https://documentation.breadnet.co.uk/favicon.ico\"}}],[\"code\",{\"code\":\"➜ k get svc    \\nNAME             TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE\\npihole-dhcp      LoadBalancer   10.43.212.91    172.16.0.1    67:32594/UDP                 9d\\npihole-dns-tcp   LoadBalancer   10.43.51.136    172.16.0.1    53:30410/TCP                 9d\\npihole-dns-udp   LoadBalancer   10.43.123.66    172.16.0.1    53:31204/UDP                 9d\\npihole-web       LoadBalancer   10.43.164.153   172.16.0.1    80:31832/TCP,443:30309/TCP   9d\\n\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2023/11/image.png\",\"width\":1311,\"height\":629}],[\"bookmark\",{\"version\":\"1.0\",\"type\":\"bookmark\",\"url\":\"https://documentation.breadnet.co.uk/kubernetes/flux/flux-artifact-registry-google-auth/\",\"metadata\":{\"url\":\"https://documentation.breadnet.co.uk/kubernetes/flux/flux-artifact-registry-google-auth/\",\"title\":\"Authenticate flux with Google Artifact Registry - breadNET Documentation\",\"description\":\"breadNET Documentation\",\"author\":null,\"publisher\":\"logo\",\"thumbnail\":\"https://documentation.breadnet.co.uk/assets/images/social/kubernetes/flux/flux-artifact-registry-google-auth.png\",\"icon\":\"https://documentation.breadnet.co.uk/favicon.ico\"}}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2023/11/image-1.png\",\"width\":459,\"height\":584}],[\"code\",{\"code\":\"root@k3s-01:~# sensors\\ncoretemp-isa-0000\\nAdapter: ISA adapter\\nPackage id 0:  +43.0°C  (high = +84.0°C, crit = +100.0°C)\\nCore 0:        +41.0°C  (high = +84.0°C, crit = +100.0°C)\\nCore 1:        +40.0°C  (high = +84.0°C, crit = +100.0°C)\\n\\nacpitz-acpi-0\\nAdapter: ACPI interface\\ntemp1:        +27.8°C  (crit = +119.0°C)\\ntemp2:        +29.8°C  (crit = +119.0°C)\\n\\npch_skylake-virtual-0\\nAdapter: Virtual device\\ntemp1:        +53.5°C  \\n\\nroot@k3s-01:~# \\n\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2023/11/cluster.jpg\",\"width\":3024,\"height\":4032}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2023/11/cluster-top.jpg\",\"width\":3024,\"height\":4032}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2023/11/switch.jpg\",\"width\":3024,\"height\":4032}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2023/11/image-2.png\",\"width\":1419,\"height\":512}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2023/11/router.jpg\",\"width\":4032,\"height\":3024}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2023/11/simple-network-2.png\",\"width\":3476,\"height\":2868}],[\"hr\",{}]],\"markups\":[[\"em\"],[\"a\",[\"href\",\"https://issuetracker.google.com/issues/305477780\"]],[\"a\",[\"href\",\"https://www.redbrick.sg/blog/shoebox-apartments-is-it-worth-the-investment/#:~:text=Shoebox%20apartments%2C%20also%20known%20as%20%E2%80%98compact%20units%E2%80%99%20are%20typically%20defined%20as%20an%20apartment%20of%20500%20square%20feet%20or%20less%20and%20designed%20for%20a%20single%20occupant\"]],[\"a\",[\"href\",\"https://www.ons.gov.uk/economy/inflationandpriceindices/articles/costoflivinginsights/energy\"]],[\"a\",[\"href\",\"https://en.wikipedia.org/wiki/2021%E2%80%93present_United_Kingdom_cost-of-living_crisis\"]],[\"strong\"],[\"a\",[\"href\",\"https://docs.k3s.io/quick-start\"]],[\"a\",[\"href\",\"https://www.youtube.com/watch?v=UoOcLXfa8EU\"]],[\"code\"],[\"a\",[\"href\",\"https://kubernetes.io/\"]],[\"a\",[\"href\",\"https://gatus.io\"]],[\"a\",[\"href\",\"taskfile.dev/\"]],[\"a\",[\"href\",\"https://pkg.go.dev/github.com/fluxcd/toolkit/cmd/gotk\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I first started learning Kubernetes in 2021, as a Junior Systems Administrator in my bedroom in Cambridge. I'll be honest, I didn't really understand it much. What's a pod? Why cant I just deploy a container? \"]]],[1,\"p\",[[0,[],0,\"Over the years, 2 of them to be exact, I have learning Kubernetes piece by peace, not really following a specific course, so if you're looking for course recommendations, I have none. \"]]],[1,\"p\",[[0,[],0,\"I've spent most of my Career operating and using GKE, a Kubernetes \"],[0,[0],1,\"flavour\"],[0,[],0,\" if you will, from Google. GKE is designed to run on Google cloud, and \"],[0,[1],1,\"sort of\"],[0,[],0,\" integrates with most of the GCP API's \"]]],[1,\"p\",[[0,[],0,\"Enough about my work, you're here to see my Cluster at home.\"]]],[10,0],[1,\"p\",[[0,[],0,\"When I lived at my parents (Oh the good old days) I had a 48U rack with multiple dell servers. Due to the housing crisis affecting the UK, I cant afford to buy a house, let alone rent a place that isn't classified as a \"],[0,[0,2],1,\"shoe box\"],[0,[],1,\" - \"],[0,[],0,\"As such, some design considerations had to be made.\"]]],[1,\"h2\",[[0,[],0,\"Design Considerations\"]]],[3,\"ul\",[[[0,[],0,\"Needs to not be a server chassis\"]],[[0,[],0,\"Needs to be \"],[0,[0],1,\"light\"],[0,[],0,\" on power (\"],[0,[3],1,\"Energy Crisis\"],[0,[],0,\")\"]],[[0,[],0,\"Needs to be somewhat cheap (\"],[0,[4],1,\"Cost of living Crisis\"],[0,[],0,\")\"]],[[0,[],0,\"Needs to be small (read: Shoebox apartment)\"]]]],[1,\"p\",[[0,[],0,\"All in all, I decided the best way to build the cluster was to use a mix of things I owned, and then buy things I did not.\"]]],[3,\"ul\",[[[0,[],0,\"4 Dell 3040 SFF pc's with a minimum of 8gb ram and 128gb SSD (Purchased)\"]],[[0,[],0,\"Ubiquiti EdgeSwitch 8 XP (Owned already)\"]],[[0,[],0,\"Juniper SRX-300 (Purchased)\"]],[[0,[],0,\"Load of ethernet cables (Made them my self)\"]]]],[1,\"p\",[[0,[],0,\"I am currently learning the Juniper CLI with the end goal of replacing my ISP's router with the Juniper and having vlans etc. \"]]],[1,\"h2\",[[0,[],0,\"Kubernetes flavor of choice\"]]],[1,\"p\",[[0,[],0,\"There are lots of distributions of kubernetes you can run, to list a few:\"]]],[3,\"ul\",[[[0,[],0,\"Anthos \"]],[[0,[],0,\"Raw Kubernetes\"]],[[0,[],0,\"Rancher Kubernetes Engine\"]],[[0,[],0,\"K3s\"]],[[0,[],0,\"Microk8s\"]]]],[1,\"p\",[[0,[],0,\"I decided that running full blown vanilla Kubernetes required too many moving parts, so I settled on K3s\"]]],[1,\"blockquote\",[[0,[],0,\"K3s is packaged as a single <70MB binary that reduces the dependencies and steps needed to install, run and auto-update a production Kubernetes cluster\"]]],[1,\"p\",[[0,[],0,\"Due to it being so small, it makes installing the cluster \"],[0,[5],1,\"super simple\"],[0,[],0,\" and not very network intensive\"]]],[1,\"p\",[[0,[],0,\"I wont go in to the install process here, but below are some useful links that I suggest following \"]]],[3,\"ul\",[[[0,[6],1,\"https://docs.k3s.io/quick-start\"]],[[0,[7],1,\"https://www.youtube.com/watch?v=UoOcLXfa8EU\"]]]],[1,\"p\",[[0,[],0,\"You can get away with a single node, but if you plan to run this in \"],[0,[0],1,\"production\"],[0,[],0,\" (ctx: production at home) then I recommend 3 nodes. \"]]],[1,\"p\",[[0,[],0,\"This is the design for the network at a high level. \"]]],[10,1],[1,\"p\",[]],[1,\"h2\",[[0,[],0,\"Design Specifics\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"My cluster is made up of 3 nodes, and a \"],[0,[8],1,\"persistence\"],[0,[],0,\" server.\"]]],[1,\"h3\",[[0,[],0,\"nodes\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"I have one \"],[0,[8],1,\"Control plane\"],[0,[],0,\" node, which is \"],[0,[8],1,\"k3s-01\"],[0,[],0,\", this runs \"],[0,[8],1,\"etcd\"],[0,[],0,\", \"],[0,[8],1,\"control-plane\"],[0,[],0,\" and \"],[0,[8],1,\"master\"]]],[1,\"p\",[[0,[],0,\"Each node runs \"],[0,[8],1,\"Ubuntu 22.04.3 LTS\"],[0,[],0,\", and because I am an idiot and didnt read the listing on Ebay, the nodes are a mix of i3 and i5 processors, but I honestly have not noticed a difference. \"]]],[10,2],[1,\"h3\",[[0,[],0,\"Networking\"]]],[1,\"p\",[[0,[],0,\"I have carved out the below IP ranges for the Cluster\"]]],[1,\"p\",[[0,[],0,\" These IP ranges are used for when the cluster exposes an IP address via either \"],[0,[8],1,\"type: LoadBalancer\"],[0,[],0,\" or \"],[0,[8],1,\"type: Service\"],[0,[],0,\". Something to note is I dont use \"],[0,[8],1,\"type: NodePort\"],[0,[],0,\" as this is just a pain. \"]]],[10,3],[1,\"h2\",[[0,[],0,\"What I'm running\"]]],[1,\"p\",[[0,[],0,\"So with Kubernetes, there's a few ways to manage applications.\"]]],[1,\"p\",[[0,[],0,\"You can use Raw manifest files, and apply them with \"],[0,[8],1,\"kubectl apply -f <name>.yaml\"],[0,[],0,\" or you can use Helm charts, which are packaged \"],[0,[0],1,\"applications\"],[0,[],0,\" and you use the helm CLI to install them.\"]]],[1,\"p\",[[0,[],0,\"There are of course other systems, but I primerly use the above. \"]]],[1,\"p\",[[0,[],0,\"Then there's one level up, which consumes the manifests (be them helm or raw) called GitOps. I've installed a system called Flux which enables you to declare in a girt repo how you want things to look, then Flux will reconcile everything and deploy the changes you made in the Git repo. \"]]],[1,\"h2\",[[0,[],0,\"Things installed\"]]],[1,\"p\",[[0,[],0,\"In the cluster I've got the below installed\"]]],[1,\"h4\",[[0,[],0,\"Cloudflared\"]]],[1,\"p\",[[0,[],0,\"Cloudflared is a system from Cloudflare which allows you to use their tunnels system to send traffic back in to your network without having to port forward and open up your network.\"]]],[1,\"p\",[[0,[],0,\"I've custom built a helm chart that sets up a \"],[0,[8],1,\"ExternalName\"],[0,[],0,\" service in the cloudflared namespace, which then points to an actual service name in the cluster. \"]]],[1,\"p\",[[0,[],0,\"This does seem like an anti-pattern, but it allows you to simply go \"],[0,[8],1,\"kubectl get svc\"],[0,[],0,\" in the \"],[0,[8],1,\"clouflared\"],[0,[],0,\" namespac and see what's being \"],[0,[0],1,\"exposed\"]]],[1,\"p\",[[0,[],0,\"This means the \"],[0,[8],1,\"values.yaml\"],[0,[],0,\" file looks like the below\"]]],[10,4],[1,\"p\",[[0,[],0,\"This is then fronted by a Terraform module pre-configured to create DNS records with the tunnel ID.\"]]],[1,\"p\",[[0,[],0,\"I've put together some documentation on a stripped down version, but if you are interested in the helm chart, please get in contact and I am happy to opensource it \"]]],[10,5],[1,\"p\",[[0,[],0,\"This is managed via Flux\"]]],[1,\"h4\",[[0,[],0,\"PiHole\"]]],[1,\"p\",[[0,[],0,\"Pihole is an adblocking and DNS server that lots of people in the self hosting community are using. I've deployed this in to my cluster via Flux. This gets a Service address of \"],[0,[8],1,\"172.16.0.1\"],[0,[],0,\" which is then configured via the router to be handed out to all clients to use for DNS resolution\"]]],[10,6],[1,\"p\",[[0,[],0,\"This is managed by Flux, so when I get a new Block list, I update it in Git and the container rolls\"]]],[1,\"h3\",[[0,[],0,\"Metallb\"]]],[1,\"p\",[[0,[],0,\"Metallb is a load balancer (I suppose) system that is often deployed to on-premise Kubernetes clusters that acts as a load balancer\"]]],[1,\"blockquote\",[[0,[],0,\"MetalLB is a load-balancer implementation for bare metal \"],[0,[9],1,\"Kubernetes\"],[0,[],0,\" clusters, using standard routing protocols.\"]]],[1,\"p\",[[0,[],0,\"This is used to allow us to assign IP address to Services and service type Load balancers.\"]]],[1,\"p\",[[0,[],0,\"Eventually I plan to configure it to use BGP to get IP address, but I dont know Junos very well or BGP\"]]],[1,\"h3\",[[0,[],0,\"Nginx Ingress Controller\"]]],[1,\"p\",[[0,[],0,\"Nginx ingress controller is a \"],[0,[0],1,\"flavour\"],[0,[],0,\" of NGINX (the popular reverse proxy and web server) designed specifically for Kubernetes. This is used for \"],[0,[8],1,\"kind: Ingress\"],[0,[],0,\" objects where you can configure Host paths, host names and SSL. \"]]],[1,\"p\",[[0,[],0,\"This is managed with Flux\"]]],[1,\"p\",[]],[1,\"h3\",[[0,[],0,\"Snipe IT\"]]],[1,\"p\",[[0,[],0,\"SnipeIT is a system designed to manage IT inventory, more so tracking the physical Assets like their purchase cost, warranty, Invoices, who the device is assigned to, how much it deprecates. \"]]],[1,\"p\",[[0,[],0,\"I've written a custom helm chart for this.\"]]],[1,\"p\",[[0,[],0,\"This is managed with flux\"]]],[1,\"h3\",[[0,[],0,\"Status page\"]]],[1,\"p\",[[0,[],0,\"Status page is using \"],[0,[10],1,\"gatus.io\"],[0,[],0,\" in a self built helm chart to host an internal status page with (currently) 3 resources. Eventually I plan to make this public behind Cloudflare Tunnels, but I need to fix the helm chart.\"]]],[1,\"p\",[[0,[],0,\"One thing is this is pulling an AWS (Hosting the status page on your infrastructure) - so for the time being this will only serve as my status page for in cluster things\"]]],[10,7],[1,\"p\",[[0,[],0,\"This is managed with Flux\"]]],[1,\"h3\",[[0,[],0,\"Grocy\"]]],[1,\"p\",[[0,[],0,\"Grocy is a PHP app designed to help you manage your kitchen inventory. I've run this off and on for quite a while. The main thing that's stopping me from really commiting to it is the Developer's hate towards an external database. Meaning I have to persist the sqlite database on the cluster, and we all know that persistence is Kubernetes gets painful. \"]]],[1,\"p\",[[0,[],0,\"This seaways very nicely on to\"]]],[1,\"h2\",[[0,[],0,\"The Persistence server\"]]],[1,\"p\",[[0,[],0,\"This server is my hacky solution to dealing with running persistent workloads in Kubernetes. \"]]],[1,\"p\",[[0,[],0,\"The server is an Intel i5 with 8gb ram and a 128GB ssd. \"]]],[1,\"p\",[[0,[],0,\"I've installed the below services dirctly on to the node, as opposed to being in podman\"]]],[3,\"ul\",[[[0,[],0,\"MariaDB\"]],[[0,[],0,\"NFS\"]]]],[1,\"p\",[[0,[],0,\"The NFS export is setup that it can be used as persistent Volumes in the cluster for things like Media servers (Coming soon) and Image uploads to applications (Snipe, Grocy etc)\"]]],[1,\"p\",[[0,[],0,\"I will need to look at installing some other things like:\"]]],[3,\"ul\",[[[0,[],0,\"MongoDB\"]],[[0,[],0,\"Postgresql \"]]]],[1,\"p\",[]],[1,\"h3\",[[0,[],0,\"Where are the helm charts stored\"]]],[1,\"p\",[[0,[],0,\"I have a \"],[0,[11],1,\"Taskfile\"],[0,[],0,\" in the monorepo that packages the helm chart as an OCI artifact and pushes it to Google Artifact Registry. \"]]],[1,\"p\",[[0,[],0,\"In the cluster, I have then configured flux to use a custom built credential helper \"]]],[10,8],[1,\"h2\",[]],[1,\"h2\",[[0,[],0,\"What the flux \"]]],[1,\"p\",[[0,[],0,\"I've mentioned that I use Flux to manage almost all the deployments, but what actually is flux.\"]]],[1,\"p\",[[0,[],0,\"Flux is a GitOps toolkit \"],[0,[12],1,\"(GOTK)\"],[0,[],0,\"  from Weave Works, it allows you to declarativly descibe the state of the cluster and then sync that declaration to the cluster. Sure, a simple solution would be a cron job doing \"],[0,[8],0,\"git clone && kubectl apply -f *\"],[0,[0],2,\".yaml\"],[0,[0],1,\" \"],[0,[],0,\"but this allows for logging and structured config.\"]]],[1,\"p\",[[0,[],0,\"This is a screenshot from the repo where I configure the flux system, and the deployments\"]]],[10,9],[1,\"p\",[[0,[8],1,\"flux-system\"],[0,[],0,\" - this contains all the Flux config. It's a brain bender, but flux manages it's self via it's own repo. \"]]],[1,\"p\",[[0,[8],1,\"HelmRelease\"],[0,[],0,\" - Deploys the helm chars from the HelmRepositories I specify. Here you can also pass the values as well as what namespace and version of the chart to use\"]]],[1,\"p\",[[0,[8],1,\"HelmRepository\"],[0,[],0,\" - This is where we tell flux where to get the helm charts we want to use later. Flux supports Cloud storage buckets, OCI (I use this one personally) and HTTPS endpoints, provided there is an \"],[0,[8],1,\"index.yaml\"],[0,[],0,\" file\"]]],[1,\"p\",[[0,[8],1,\"namespaces\"],[0,[],0,\" - Defines the namespaces in the cluster.\"]]],[1,\"p\",[[0,[],0,\"To create a new resource, you just commit a file in the \"],[0,[8],1,\"k3s/*\"],[0,[],0,\" directory and flux will build it. At work I have built a lot more complex solution for this, but at home I don't need something super complicated.\"]]],[1,\"h2\",[[0,[],0,\"Things I would do differently\"]]],[1,\"p\",[[0,[],0,\"So far I would stick with this design. It's quite clean and for me, makes sense. Everything is in Git, it hardly uses much power, and it's HA. What more could you want from a small cluster.\"]]],[1,\"p\",[[0,[],0,\"If I could change something, it would be where it's physically positioned. On top of the water heater is not amazing\"]]],[1,\"h2\",[[0,[],0,\"Talk is cheap, show me the cluster\"]]],[1,\"p\",[[0,[],0,\"Something to note is, this is a small cluster. It runs a few services and nothing more, nothing less. I am not running HPC here.\"]]],[1,\"p\",[[0,[],0,\"If you are worried about the heat the water heater puts off, dont be. That gap between the nodes is enough actually to keep them at a happy enough temperature\"]]],[10,10],[1,\"p\",[[0,[],0,\"If these look a little high to you, something to note is these are \"],[0,[0],1,\"consumer\"],[0,[],0,\" devices, and designed that they can be given to the average joe who will put the computer down the back of their desk with -1 airflow and 200 cubic meters of dust flakes.\"]]],[1,\"p\",[[0,[],0,\"Now I've finished justifying the decision, let me show you the cluster\"]]],[1,\"h3\",[[0,[],0,\"The cluster and it's supporting infra\"]]],[1,\"p\",[]],[10,11],[1,\"p\",[[0,[],0,\"Some more justifying the terrible placement\"]]],[3,\"ul\",[[[0,[],0,\"The router (Black box tied to the copper pipe) techniaclly is water cooled. I know this is really less than ideal, but this is the only place I could put it that I had access to, and kept it cool. You may laugh, but since putting it there, it actually reduces the temprature of the router, really letting me get all 110mbps I paid for. \"]]]],[1,\"h3\",[[0,[],0,\"Cluster from the top\"]]],[10,12],[1,\"h3\",[[0,[],0,\"Switch\"]]],[10,13],[1,\"p\",[[0,[],0,\"The Switch sees a constant load of about 2-5mbps when the cluster is just chilling\"]]],[10,14],[1,\"h3\",[[0,[],0,\"The new router\"]]],[10,15],[1,\"p\",[[0,[],0,\"This is the Juniper SRX-330 I have purchased, I need to learn how to use it. Currently I have PPPoE setup on \"],[0,[8],1,\"ge-0/0/0\"],[0,[],0,\" (White ethernet) and then setting up the servers \"],[0,[0],1,\"lan\"],[0,[],0,\" on \"],[0,[8],1,\"ge-0/0/1\"],[0,[],0,\" (trunk port) to the switch, where the rest of my devices will connect to, and then anything else can go in to the router using an \"],[0,[8],1,\"irb\"],[0,[],0,\" \"]]],[1,\"p\",[[0,[],0,\"This overall leaves the network looking like the below\"]]],[10,16],[10,17],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"I hope you've enjoyed this, if so, or if not, please feel free to get in contact. \"]]],[1,\"p\",[[0,[],0,\"You can contract me to set up a Kubernetes cluster in your house, provided you pay my petrol (I have a Costco membership) and feed me. \"]]]],\"ghostVersion\":\"4.0\"}","lexical":null,"html":"<p>I first started learning Kubernetes in 2021, as a Junior Systems Administrator in my bedroom in Cambridge. I'll be honest, I didn't really understand it much. What's a pod? Why cant I just deploy a container? </p><p>Over the years, 2 of them to be exact, I have learning Kubernetes piece by peace, not really following a specific course, so if you're looking for course recommendations, I have none. </p><p>I've spent most of my Career operating and using GKE, a Kubernetes <em>flavour</em> if you will, from Google. GKE is designed to run on Google cloud, and <a href=\"https://issuetracker.google.com/issues/305477780\">sort of</a> integrates with most of the GCP API's </p><p>Enough about my work, you're here to see my Cluster at home.</p><hr><p>When I lived at my parents (Oh the good old days) I had a 48U rack with multiple dell servers. Due to the housing crisis affecting the UK, I cant afford to buy a house, let alone rent a place that isn't classified as a <em><a href=\"https://www.redbrick.sg/blog/shoebox-apartments-is-it-worth-the-investment/#:~:text=Shoebox%20apartments%2C%20also%20known%20as%20%E2%80%98compact%20units%E2%80%99%20are%20typically%20defined%20as%20an%20apartment%20of%20500%20square%20feet%20or%20less%20and%20designed%20for%20a%20single%20occupant\">shoe box</a> - </em>As such, some design considerations had to be made.</p><h2 id=\"design-considerations\">Design Considerations</h2><ul><li>Needs to not be a server chassis</li><li>Needs to be <em>light</em> on power (<a href=\"https://www.ons.gov.uk/economy/inflationandpriceindices/articles/costoflivinginsights/energy\">Energy Crisis</a>)</li><li>Needs to be somewhat cheap (<a href=\"https://en.wikipedia.org/wiki/2021%E2%80%93present_United_Kingdom_cost-of-living_crisis\">Cost of living Crisis</a>)</li><li>Needs to be small (read: Shoebox apartment)</li></ul><p>All in all, I decided the best way to build the cluster was to use a mix of things I owned, and then buy things I did not.</p><ul><li>4 Dell 3040 SFF pc's with a minimum of 8gb ram and 128gb SSD (Purchased)</li><li>Ubiquiti EdgeSwitch 8 XP (Owned already)</li><li>Juniper SRX-300 (Purchased)</li><li>Load of ethernet cables (Made them my self)</li></ul><p>I am currently learning the Juniper CLI with the end goal of replacing my ISP's router with the Juniper and having vlans etc. </p><h2 id=\"kubernetes-flavor-of-choice\">Kubernetes flavor of choice</h2><p>There are lots of distributions of kubernetes you can run, to list a few:</p><ul><li>Anthos </li><li>Raw Kubernetes</li><li>Rancher Kubernetes Engine</li><li>K3s</li><li>Microk8s</li></ul><p>I decided that running full blown vanilla Kubernetes required too many moving parts, so I settled on K3s</p><blockquote>K3s is packaged as a single &lt;70MB binary that reduces the dependencies and steps needed to install, run and auto-update a production Kubernetes cluster</blockquote><p>Due to it being so small, it makes installing the cluster <strong>super simple</strong> and not very network intensive</p><p>I wont go in to the install process here, but below are some useful links that I suggest following </p><ul><li><a href=\"https://docs.k3s.io/quick-start\">https://docs.k3s.io/quick-start</a></li><li><a href=\"https://www.youtube.com/watch?v=UoOcLXfa8EU\">https://www.youtube.com/watch?v=UoOcLXfa8EU</a></li></ul><p>You can get away with a single node, but if you plan to run this in <em>production</em> (ctx: production at home) then I recommend 3 nodes. </p><p>This is the design for the network at a high level. </p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2023/11/cluster.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1790\" height=\"2360\" srcset=\"__GHOST_URL__/content/images/size/w600/2023/11/cluster.png 600w, __GHOST_URL__/content/images/size/w1000/2023/11/cluster.png 1000w, __GHOST_URL__/content/images/size/w1600/2023/11/cluster.png 1600w, __GHOST_URL__/content/images/2023/11/cluster.png 1790w\" sizes=\"(min-width: 720px) 720px\"></figure><p></p><h2 id=\"design-specifics\">Design Specifics</h2><p></p><p>My cluster is made up of 3 nodes, and a <code>persistence</code> server.</p><h3 id=\"nodes\">nodes</h3><p></p><p>I have one <code>Control plane</code> node, which is <code>k3s-01</code>, this runs <code>etcd</code>, <code>control-plane</code> and <code>master</code></p><p>Each node runs <code>Ubuntu 22.04.3 LTS</code>, and because I am an idiot and didnt read the listing on Ebay, the nodes are a mix of i3 and i5 processors, but I honestly have not noticed a difference. </p><pre><code>➜ kubectl get nodes              \nNAME     STATUS   ROLES                       AGE     VERSION\nk3s-01   Ready    control-plane,etcd,master   23d     v1.27.6+k3s1\nk3s-02   Ready    &lt;none&gt;                      23d     v1.27.6+k3s1\nk3s-03   Ready    &lt;none&gt;                      5d21h   v1.27.7+k3s2\n</code></pre><h3 id=\"networking\">Networking</h3><p>I have carved out the below IP ranges for the Cluster</p><p> These IP ranges are used for when the cluster exposes an IP address via either <code>type: LoadBalancer</code> or <code>type: Service</code>. Something to note is I dont use <code>type: NodePort</code> as this is just a pain. </p><!--kg-card-begin: markdown--><table>\n<thead>\n<tr>\n<th>IP range</th>\n<th>Use</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>172.16.2.0/28</code></td>\n<td>Node IP address</td>\n</tr>\n<tr>\n<td><code>172.16.0.0/25</code></td>\n<td>Services</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: markdown--><h2 id=\"what-im-running\">What I'm running</h2><p>So with Kubernetes, there's a few ways to manage applications.</p><p>You can use Raw manifest files, and apply them with <code>kubectl apply -f &lt;name&gt;.yaml</code> or you can use Helm charts, which are packaged <em>applications</em> and you use the helm CLI to install them.</p><p>There are of course other systems, but I primerly use the above. </p><p>Then there's one level up, which consumes the manifests (be them helm or raw) called GitOps. I've installed a system called Flux which enables you to declare in a girt repo how you want things to look, then Flux will reconcile everything and deploy the changes you made in the Git repo. </p><h2 id=\"things-installed\">Things installed</h2><p>In the cluster I've got the below installed</p><h4 id=\"cloudflared\">Cloudflared</h4><p>Cloudflared is a system from Cloudflare which allows you to use their tunnels system to send traffic back in to your network without having to port forward and open up your network.</p><p>I've custom built a helm chart that sets up a <code>ExternalName</code> service in the cloudflared namespace, which then points to an actual service name in the cluster. </p><p>This does seem like an anti-pattern, but it allows you to simply go <code>kubectl get svc</code> in the <code>clouflared</code> namespac and see what's being <em>exposed</em></p><p>This means the <code>values.yaml</code> file looks like the below</p><pre><code class=\"language-yaml\">services:\n  http:\n    - domain: example.breadinfra.net\n      svc: podinfo\n      namespace: podinfo</code></pre><p>This is then fronted by a Terraform module pre-configured to create DNS records with the tunnel ID.</p><p>I've put together some documentation on a stripped down version, but if you are interested in the helm chart, please get in contact and I am happy to opensource it </p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://documentation.breadnet.co.uk/kubernetes/k3s/cloudflare-tunnels-on-k3s/?utm_source&#x3D;breadnet&amp;utm_medium&#x3D;blog&amp;utm_campaign&#x3D;kubernetes%20at%20home\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Cloudflare Tunnels on k3s - breadNET Documentation</div><div class=\"kg-bookmark-description\">breadNET Documentation</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://documentation.breadnet.co.uk/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">logo</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://documentation.breadnet.co.uk/assets/images/social/kubernetes/k3s/cloudflare-tunnels-on-k3s.png\" alt=\"\"></div></a></figure><p>This is managed via Flux</p><h4 id=\"pihole\">PiHole</h4><p>Pihole is an adblocking and DNS server that lots of people in the self hosting community are using. I've deployed this in to my cluster via Flux. This gets a Service address of <code>172.16.0.1</code> which is then configured via the router to be handed out to all clients to use for DNS resolution</p><pre><code>➜ k get svc    \nNAME             TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE\npihole-dhcp      LoadBalancer   10.43.212.91    172.16.0.1    67:32594/UDP                 9d\npihole-dns-tcp   LoadBalancer   10.43.51.136    172.16.0.1    53:30410/TCP                 9d\npihole-dns-udp   LoadBalancer   10.43.123.66    172.16.0.1    53:31204/UDP                 9d\npihole-web       LoadBalancer   10.43.164.153   172.16.0.1    80:31832/TCP,443:30309/TCP   9d\n</code></pre><p>This is managed by Flux, so when I get a new Block list, I update it in Git and the container rolls</p><h3 id=\"metallb\">Metallb</h3><p>Metallb is a load balancer (I suppose) system that is often deployed to on-premise Kubernetes clusters that acts as a load balancer</p><blockquote>MetalLB is a load-balancer implementation for bare metal <a href=\"https://kubernetes.io/\">Kubernetes</a> clusters, using standard routing protocols.</blockquote><p>This is used to allow us to assign IP address to Services and service type Load balancers.</p><p>Eventually I plan to configure it to use BGP to get IP address, but I dont know Junos very well or BGP</p><h3 id=\"nginx-ingress-controller\">Nginx Ingress Controller</h3><p>Nginx ingress controller is a <em>flavour</em> of NGINX (the popular reverse proxy and web server) designed specifically for Kubernetes. This is used for <code>kind: Ingress</code> objects where you can configure Host paths, host names and SSL. </p><p>This is managed with Flux</p><p></p><h3 id=\"snipe-it\">Snipe IT</h3><p>SnipeIT is a system designed to manage IT inventory, more so tracking the physical Assets like their purchase cost, warranty, Invoices, who the device is assigned to, how much it deprecates. </p><p>I've written a custom helm chart for this.</p><p>This is managed with flux</p><h3 id=\"status-page\">Status page</h3><p>Status page is using <a href=\"https://gatus.io\">gatus.io</a> in a self built helm chart to host an internal status page with (currently) 3 resources. Eventually I plan to make this public behind Cloudflare Tunnels, but I need to fix the helm chart.</p><p>One thing is this is pulling an AWS (Hosting the status page on your infrastructure) - so for the time being this will only serve as my status page for in cluster things</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2023/11/image.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1311\" height=\"629\" srcset=\"__GHOST_URL__/content/images/size/w600/2023/11/image.png 600w, __GHOST_URL__/content/images/size/w1000/2023/11/image.png 1000w, __GHOST_URL__/content/images/2023/11/image.png 1311w\" sizes=\"(min-width: 720px) 720px\"></figure><p>This is managed with Flux</p><h3 id=\"grocy\">Grocy</h3><p>Grocy is a PHP app designed to help you manage your kitchen inventory. I've run this off and on for quite a while. The main thing that's stopping me from really commiting to it is the Developer's hate towards an external database. Meaning I have to persist the sqlite database on the cluster, and we all know that persistence is Kubernetes gets painful. </p><p>This seaways very nicely on to</p><h2 id=\"the-persistence-server\">The Persistence server</h2><p>This server is my hacky solution to dealing with running persistent workloads in Kubernetes. </p><p>The server is an Intel i5 with 8gb ram and a 128GB ssd. </p><p>I've installed the below services dirctly on to the node, as opposed to being in podman</p><ul><li>MariaDB</li><li>NFS</li></ul><p>The NFS export is setup that it can be used as persistent Volumes in the cluster for things like Media servers (Coming soon) and Image uploads to applications (Snipe, Grocy etc)</p><p>I will need to look at installing some other things like:</p><ul><li>MongoDB</li><li>Postgresql </li></ul><p></p><h3 id=\"where-are-the-helm-charts-stored\">Where are the helm charts stored</h3><p>I have a <a href=\"taskfile.dev/\">Taskfile</a> in the monorepo that packages the helm chart as an OCI artifact and pushes it to Google Artifact Registry. </p><p>In the cluster, I have then configured flux to use a custom built credential helper </p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://documentation.breadnet.co.uk/kubernetes/flux/flux-artifact-registry-google-auth/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Authenticate flux with Google Artifact Registry - breadNET Documentation</div><div class=\"kg-bookmark-description\">breadNET Documentation</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://documentation.breadnet.co.uk/favicon.ico\" alt=\"\"><span class=\"kg-bookmark-author\">logo</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://documentation.breadnet.co.uk/assets/images/social/kubernetes/flux/flux-artifact-registry-google-auth.png\" alt=\"\"></div></a></figure><h2></h2><h2 id=\"what-the-flux\">What the flux </h2><p>I've mentioned that I use Flux to manage almost all the deployments, but what actually is flux.</p><p>Flux is a GitOps toolkit <a href=\"https://pkg.go.dev/github.com/fluxcd/toolkit/cmd/gotk\">(GOTK)</a>  from Weave Works, it allows you to declarativly descibe the state of the cluster and then sync that declaration to the cluster. Sure, a simple solution would be a cron job doing <code>git clone &amp;&amp; kubectl apply -f *<em>.yaml</em></code><em> </em>but this allows for logging and structured config.</p><p>This is a screenshot from the repo where I configure the flux system, and the deployments</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2023/11/image-1.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"459\" height=\"584\"></figure><p><code>flux-system</code> - this contains all the Flux config. It's a brain bender, but flux manages it's self via it's own repo. </p><p><code>HelmRelease</code> - Deploys the helm chars from the HelmRepositories I specify. Here you can also pass the values as well as what namespace and version of the chart to use</p><p><code>HelmRepository</code> - This is where we tell flux where to get the helm charts we want to use later. Flux supports Cloud storage buckets, OCI (I use this one personally) and HTTPS endpoints, provided there is an <code>index.yaml</code> file</p><p><code>namespaces</code> - Defines the namespaces in the cluster.</p><p>To create a new resource, you just commit a file in the <code>k3s/*</code> directory and flux will build it. At work I have built a lot more complex solution for this, but at home I don't need something super complicated.</p><h2 id=\"things-i-would-do-differently\">Things I would do differently</h2><p>So far I would stick with this design. It's quite clean and for me, makes sense. Everything is in Git, it hardly uses much power, and it's HA. What more could you want from a small cluster.</p><p>If I could change something, it would be where it's physically positioned. On top of the water heater is not amazing</p><h2 id=\"talk-is-cheap-show-me-the-cluster\">Talk is cheap, show me the cluster</h2><p>Something to note is, this is a small cluster. It runs a few services and nothing more, nothing less. I am not running HPC here.</p><p>If you are worried about the heat the water heater puts off, dont be. That gap between the nodes is enough actually to keep them at a happy enough temperature</p><pre><code>root@k3s-01:~# sensors\ncoretemp-isa-0000\nAdapter: ISA adapter\nPackage id 0:  +43.0°C  (high = +84.0°C, crit = +100.0°C)\nCore 0:        +41.0°C  (high = +84.0°C, crit = +100.0°C)\nCore 1:        +40.0°C  (high = +84.0°C, crit = +100.0°C)\n\nacpitz-acpi-0\nAdapter: ACPI interface\ntemp1:        +27.8°C  (crit = +119.0°C)\ntemp2:        +29.8°C  (crit = +119.0°C)\n\npch_skylake-virtual-0\nAdapter: Virtual device\ntemp1:        +53.5°C  \n\nroot@k3s-01:~# \n</code></pre><p>If these look a little high to you, something to note is these are <em>consumer</em> devices, and designed that they can be given to the average joe who will put the computer down the back of their desk with -1 airflow and 200 cubic meters of dust flakes.</p><p>Now I've finished justifying the decision, let me show you the cluster</p><h3 id=\"the-cluster-and-its-supporting-infra\">The cluster and it's supporting infra</h3><p></p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2023/11/cluster.jpg\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"2667\" srcset=\"__GHOST_URL__/content/images/size/w600/2023/11/cluster.jpg 600w, __GHOST_URL__/content/images/size/w1000/2023/11/cluster.jpg 1000w, __GHOST_URL__/content/images/size/w1600/2023/11/cluster.jpg 1600w, __GHOST_URL__/content/images/size/w2400/2023/11/cluster.jpg 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Some more justifying the terrible placement</p><ul><li>The router (Black box tied to the copper pipe) techniaclly is water cooled. I know this is really less than ideal, but this is the only place I could put it that I had access to, and kept it cool. You may laugh, but since putting it there, it actually reduces the temprature of the router, really letting me get all 110mbps I paid for. </li></ul><h3 id=\"cluster-from-the-top\">Cluster from the top</h3><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2023/11/cluster-top.jpg\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"2667\" srcset=\"__GHOST_URL__/content/images/size/w600/2023/11/cluster-top.jpg 600w, __GHOST_URL__/content/images/size/w1000/2023/11/cluster-top.jpg 1000w, __GHOST_URL__/content/images/size/w1600/2023/11/cluster-top.jpg 1600w, __GHOST_URL__/content/images/size/w2400/2023/11/cluster-top.jpg 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><h3 id=\"switch\">Switch</h3><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2023/11/switch.jpg\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"2667\" srcset=\"__GHOST_URL__/content/images/size/w600/2023/11/switch.jpg 600w, __GHOST_URL__/content/images/size/w1000/2023/11/switch.jpg 1000w, __GHOST_URL__/content/images/size/w1600/2023/11/switch.jpg 1600w, __GHOST_URL__/content/images/size/w2400/2023/11/switch.jpg 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><p>The Switch sees a constant load of about 2-5mbps when the cluster is just chilling</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2023/11/image-2.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1419\" height=\"512\" srcset=\"__GHOST_URL__/content/images/size/w600/2023/11/image-2.png 600w, __GHOST_URL__/content/images/size/w1000/2023/11/image-2.png 1000w, __GHOST_URL__/content/images/2023/11/image-2.png 1419w\" sizes=\"(min-width: 720px) 720px\"></figure><h3 id=\"the-new-router\">The new router</h3><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2023/11/router.jpg\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"1500\" srcset=\"__GHOST_URL__/content/images/size/w600/2023/11/router.jpg 600w, __GHOST_URL__/content/images/size/w1000/2023/11/router.jpg 1000w, __GHOST_URL__/content/images/size/w1600/2023/11/router.jpg 1600w, __GHOST_URL__/content/images/size/w2400/2023/11/router.jpg 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><p>This is the Juniper SRX-330 I have purchased, I need to learn how to use it. Currently I have PPPoE setup on <code>ge-0/0/0</code> (White ethernet) and then setting up the servers <em>lan</em> on <code>ge-0/0/1</code> (trunk port) to the switch, where the rest of my devices will connect to, and then anything else can go in to the router using an <code>irb</code> </p><p>This overall leaves the network looking like the below</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2023/11/simple-network-2.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"1650\" srcset=\"__GHOST_URL__/content/images/size/w600/2023/11/simple-network-2.png 600w, __GHOST_URL__/content/images/size/w1000/2023/11/simple-network-2.png 1000w, __GHOST_URL__/content/images/size/w1600/2023/11/simple-network-2.png 1600w, __GHOST_URL__/content/images/size/w2400/2023/11/simple-network-2.png 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><hr><p></p><p>I hope you've enjoyed this, if so, or if not, please feel free to get in contact. </p><p>You can contract me to set up a Kubernetes cluster in your house, provided you pay my petrol (I have a Costco membership) and feed me. </p>","comment_id":"655cc600c9cae386d9c53e92","plaintext":"I first started learning Kubernetes in 2021, as a Junior Systems Administrator in my bedroom in Cambridge. I'll be honest, I didn't really understand it much. What's a pod? Why cant I just deploy a container?\n\nOver the years, 2 of them to be exact, I have learning Kubernetes piece by peace, not really following a specific course, so if you're looking for course recommendations, I have none.\n\nI've spent most of my Career operating and using GKE, a Kubernetes flavour if you will, from Google. GKE is designed to run on Google cloud, and sort of integrates with most of the GCP API's\n\nEnough about my work, you're here to see my Cluster at home.\n\nWhen I lived at my parents (Oh the good old days) I had a 48U rack with multiple dell servers. Due to the housing crisis affecting the UK, I cant afford to buy a house, let alone rent a place that isn't classified as a shoe box - As such, some design considerations had to be made.\n\n\nDesign Considerations\n\n * Needs to not be a server chassis\n * Needs to be light on power (Energy Crisis)\n * Needs to be somewhat cheap (Cost of living Crisis)\n * Needs to be small (read: Shoebox apartment)\n\nAll in all, I decided the best way to build the cluster was to use a mix of things I owned, and then buy things I did not.\n\n * 4 Dell 3040 SFF pc's with a minimum of 8gb ram and 128gb SSD (Purchased)\n * Ubiquiti EdgeSwitch 8 XP (Owned already)\n * Juniper SRX-300 (Purchased)\n * Load of ethernet cables (Made them my self)\n\nI am currently learning the Juniper CLI with the end goal of replacing my ISP's router with the Juniper and having vlans etc.\n\n\nKubernetes flavor of choice\n\nThere are lots of distributions of kubernetes you can run, to list a few:\n\n * Anthos\n * Raw Kubernetes\n * Rancher Kubernetes Engine\n * K3s\n * Microk8s\n\nI decided that running full blown vanilla Kubernetes required too many moving parts, so I settled on K3s\n\nK3s is packaged as a single <70MB binary that reduces the dependencies and steps needed to install, run and auto-update a production Kubernetes cluster\n\nDue to it being so small, it makes installing the cluster super simple and not very network intensive\n\nI wont go in to the install process here, but below are some useful links that I suggest following\n\n * https://docs.k3s.io/quick-start\n * https://www.youtube.com/watch?v=UoOcLXfa8EU\n\nYou can get away with a single node, but if you plan to run this in production (ctx: production at home) then I recommend 3 nodes.\n\nThis is the design for the network at a high level.\n\n\n\n\nDesign Specifics\n\n\n\nMy cluster is made up of 3 nodes, and a persistence server.\n\n\nnodes\n\n\n\nI have one Control plane node, which is k3s-01, this runs etcd, control-plane and master\n\nEach node runs Ubuntu 22.04.3 LTS, and because I am an idiot and didnt read the listing on Ebay, the nodes are a mix of i3 and i5 processors, but I honestly have not noticed a difference.\n\n➜ kubectl get nodes              \nNAME     STATUS   ROLES                       AGE     VERSION\nk3s-01   Ready    control-plane,etcd,master   23d     v1.27.6+k3s1\nk3s-02   Ready    <none>                      23d     v1.27.6+k3s1\nk3s-03   Ready    <none>                      5d21h   v1.27.7+k3s2\n\n\n\nNetworking\n\nI have carved out the below IP ranges for the Cluster\n\nThese IP ranges are used for when the cluster exposes an IP address via either type: LoadBalancer or type: Service. Something to note is I dont use type: NodePort as this is just a pain.\n\n\n\n\nIP range\nUse\n\n\n\n\n172.16.2.0/28\nNode IP address\n\n\n172.16.0.0/25\nServices\n\n\n\n\n\n\nWhat I'm running\n\nSo with Kubernetes, there's a few ways to manage applications.\n\nYou can use Raw manifest files, and apply them with kubectl apply -f <name>.yaml or you can use Helm charts, which are packaged applications and you use the helm CLI to install them.\n\nThere are of course other systems, but I primerly use the above.\n\nThen there's one level up, which consumes the manifests (be them helm or raw) called GitOps. I've installed a system called Flux which enables you to declare in a girt repo how you want things to look, then Flux will reconcile everything and deploy the changes you made in the Git repo.\n\n\nThings installed\n\nIn the cluster I've got the below installed\n\nCloudflared\n\nCloudflared is a system from Cloudflare which allows you to use their tunnels system to send traffic back in to your network without having to port forward and open up your network.\n\nI've custom built a helm chart that sets up a ExternalName service in the cloudflared namespace, which then points to an actual service name in the cluster.\n\nThis does seem like an anti-pattern, but it allows you to simply go kubectl get svc in the clouflared namespac and see what's being exposed\n\nThis means the values.yaml file looks like the below\n\nservices:\n  http:\n    - domain: example.breadinfra.net\n      svc: podinfo\n      namespace: podinfo\n\nThis is then fronted by a Terraform module pre-configured to create DNS records with the tunnel ID.\n\nI've put together some documentation on a stripped down version, but if you are interested in the helm chart, please get in contact and I am happy to opensource it\n\nCloudflare Tunnels on k3s - breadNET DocumentationbreadNET Documentationlogo\n\nThis is managed via Flux\n\nPiHole\n\nPihole is an adblocking and DNS server that lots of people in the self hosting community are using. I've deployed this in to my cluster via Flux. This gets a Service address of 172.16.0.1 which is then configured via the router to be handed out to all clients to use for DNS resolution\n\n➜ k get svc    \nNAME             TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE\npihole-dhcp      LoadBalancer   10.43.212.91    172.16.0.1    67:32594/UDP                 9d\npihole-dns-tcp   LoadBalancer   10.43.51.136    172.16.0.1    53:30410/TCP                 9d\npihole-dns-udp   LoadBalancer   10.43.123.66    172.16.0.1    53:31204/UDP                 9d\npihole-web       LoadBalancer   10.43.164.153   172.16.0.1    80:31832/TCP,443:30309/TCP   9d\n\n\nThis is managed by Flux, so when I get a new Block list, I update it in Git and the container rolls\n\n\nMetallb\n\nMetallb is a load balancer (I suppose) system that is often deployed to on-premise Kubernetes clusters that acts as a load balancer\n\nMetalLB is a load-balancer implementation for bare metal Kubernetes clusters, using standard routing protocols.\n\nThis is used to allow us to assign IP address to Services and service type Load balancers.\n\nEventually I plan to configure it to use BGP to get IP address, but I dont know Junos very well or BGP\n\n\nNginx Ingress Controller\n\nNginx ingress controller is a flavour of NGINX (the popular reverse proxy and web server) designed specifically for Kubernetes. This is used for kind: Ingress objects where you can configure Host paths, host names and SSL.\n\nThis is managed with Flux\n\n\n\n\nSnipe IT\n\nSnipeIT is a system designed to manage IT inventory, more so tracking the physical Assets like their purchase cost, warranty, Invoices, who the device is assigned to, how much it deprecates.\n\nI've written a custom helm chart for this.\n\nThis is managed with flux\n\n\nStatus page\n\nStatus page is using gatus.io in a self built helm chart to host an internal status page with (currently) 3 resources. Eventually I plan to make this public behind Cloudflare Tunnels, but I need to fix the helm chart.\n\nOne thing is this is pulling an AWS (Hosting the status page on your infrastructure) - so for the time being this will only serve as my status page for in cluster things\n\nThis is managed with Flux\n\n\nGrocy\n\nGrocy is a PHP app designed to help you manage your kitchen inventory. I've run this off and on for quite a while. The main thing that's stopping me from really commiting to it is the Developer's hate towards an external database. Meaning I have to persist the sqlite database on the cluster, and we all know that persistence is Kubernetes gets painful.\n\nThis seaways very nicely on to\n\n\nThe Persistence server\n\nThis server is my hacky solution to dealing with running persistent workloads in Kubernetes.\n\nThe server is an Intel i5 with 8gb ram and a 128GB ssd.\n\nI've installed the below services dirctly on to the node, as opposed to being in podman\n\n * MariaDB\n * NFS\n\nThe NFS export is setup that it can be used as persistent Volumes in the cluster for things like Media servers (Coming soon) and Image uploads to applications (Snipe, Grocy etc)\n\nI will need to look at installing some other things like:\n\n * MongoDB\n * Postgresql\n\n\n\n\nWhere are the helm charts stored\n\nI have a Taskfile in the monorepo that packages the helm chart as an OCI artifact and pushes it to Google Artifact Registry.\n\nIn the cluster, I have then configured flux to use a custom built credential helper\n\nAuthenticate flux with Google Artifact Registry - breadNET DocumentationbreadNET Documentationlogo\n\n\n\n\n\nWhat the flux\n\nI've mentioned that I use Flux to manage almost all the deployments, but what actually is flux.\n\nFlux is a GitOps toolkit (GOTK)  from Weave Works, it allows you to declarativly descibe the state of the cluster and then sync that declaration to the cluster. Sure, a simple solution would be a cron job doing git clone && kubectl apply -f *.yaml but this allows for logging and structured config.\n\nThis is a screenshot from the repo where I configure the flux system, and the deployments\n\nflux-system - this contains all the Flux config. It's a brain bender, but flux manages it's self via it's own repo.\n\nHelmRelease - Deploys the helm chars from the HelmRepositories I specify. Here you can also pass the values as well as what namespace and version of the chart to use\n\nHelmRepository - This is where we tell flux where to get the helm charts we want to use later. Flux supports Cloud storage buckets, OCI (I use this one personally) and HTTPS endpoints, provided there is an index.yaml file\n\nnamespaces - Defines the namespaces in the cluster.\n\nTo create a new resource, you just commit a file in the k3s/* directory and flux will build it. At work I have built a lot more complex solution for this, but at home I don't need something super complicated.\n\n\nThings I would do differently\n\nSo far I would stick with this design. It's quite clean and for me, makes sense. Everything is in Git, it hardly uses much power, and it's HA. What more could you want from a small cluster.\n\nIf I could change something, it would be where it's physically positioned. On top of the water heater is not amazing\n\n\nTalk is cheap, show me the cluster\n\nSomething to note is, this is a small cluster. It runs a few services and nothing more, nothing less. I am not running HPC here.\n\nIf you are worried about the heat the water heater puts off, dont be. That gap between the nodes is enough actually to keep them at a happy enough temperature\n\nroot@k3s-01:~# sensors\ncoretemp-isa-0000\nAdapter: ISA adapter\nPackage id 0:  +43.0°C  (high = +84.0°C, crit = +100.0°C)\nCore 0:        +41.0°C  (high = +84.0°C, crit = +100.0°C)\nCore 1:        +40.0°C  (high = +84.0°C, crit = +100.0°C)\n\nacpitz-acpi-0\nAdapter: ACPI interface\ntemp1:        +27.8°C  (crit = +119.0°C)\ntemp2:        +29.8°C  (crit = +119.0°C)\n\npch_skylake-virtual-0\nAdapter: Virtual device\ntemp1:        +53.5°C  \n\nroot@k3s-01:~# \n\n\nIf these look a little high to you, something to note is these are consumer devices, and designed that they can be given to the average joe who will put the computer down the back of their desk with -1 airflow and 200 cubic meters of dust flakes.\n\nNow I've finished justifying the decision, let me show you the cluster\n\n\nThe cluster and it's supporting infra\n\n\n\nSome more justifying the terrible placement\n\n * The router (Black box tied to the copper pipe) techniaclly is water cooled. I know this is really less than ideal, but this is the only place I could put it that I had access to, and kept it cool. You may laugh, but since putting it there, it actually reduces the temprature of the router, really letting me get all 110mbps I paid for.\n\n\nCluster from the top\n\n\nSwitch\n\nThe Switch sees a constant load of about 2-5mbps when the cluster is just chilling\n\n\nThe new router\n\nThis is the Juniper SRX-330 I have purchased, I need to learn how to use it. Currently I have PPPoE setup on ge-0/0/0 (White ethernet) and then setting up the servers lan on ge-0/0/1 (trunk port) to the switch, where the rest of my devices will connect to, and then anything else can go in to the router using an irb\n\nThis overall leaves the network looking like the below\n\n\n\nI hope you've enjoyed this, if so, or if not, please feel free to get in contact.\n\nYou can contract me to set up a Kubernetes cluster in your house, provided you pay my petrol (I have a Costco membership) and feed me.","feature_image":"__GHOST_URL__/content/images/2023/11/cluster-top-1.jpg","featured":1,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2023-11-21T15:00:16.000Z","updated_at":"2023-11-21T17:49:37.000Z","published_at":"2023-11-21T17:42:44.000Z","custom_excerpt":"Ever wondered what it's like running kubernetes at home? This post tries to answer that","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"658b587ee72f4aacb9d7eb06","uuid":"b87cd1e3-251e-4ebd-82fc-da38ce97bd37","title":"What I'm running at the end of 2023","slug":"what-im-running-at-the-end-of-2023","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"hr\",{}],[\"markdown\",{\"markdown\":\"| Service name | Description | Location |\\n| --- | --- | --- | \\n| Passbolt | Password manager | _cloud_ |\\n| pritunl Zero | Passbolt's security | _cloud_ |\\n| Ghost | This site, a blog | _cloud_ |\\n| Snipe-IT | IT asset manager | On prem (k8s) |\\n| Gatus | Status page with yaml config | k8s |\\n| Pihole | Do you even self host bro? | k8s |\\n| matomo | Self hosted analytics | k8s |\\n| mkdocs site | [Personal Documentation site](https://documentation.breadnet.co.uk/?utm_source=main_blog&utm_medium=blog_post&utm_campaign=self_hosting_2023) for all things I need to document | fly.io |\"}],[\"markdown\",{\"markdown\":\"| Name | Ram | CPU | Storage |\\n| --- | --- | --- | --- |\\n| k3s-01| `8gb` | `Intel(R) Core(TM) i3-6100T CPU @ 3.20GHz` | 500 SSD |\\n| k3s-02 | `8gb` | `Intel(R) Core(TM) i3-6100T CPU @ 3.20GHz` | 120 SSD |\\n| k3s-03 | `8gb` | `Intel(R) Core(TM) i5-6500T CPU @ 2.50GHz` | 120 SSD |\"}],[\"bookmark\",{\"url\":\"__GHOST_URL__/kubernetes-at-home/?utm_source=main_blog&utm_medium=blog_post&utm_campaign=self_hosting_2023\",\"metadata\":{\"url\":\"__GHOST_URL__/kubernetes-at-home/\",\"title\":\"Kubernetes at home\",\"description\":\"Ever wondered what it’s like running kubernetes at home? This post tries to answer that\",\"author\":\"Bradley Stannard\",\"publisher\":\"breadNET\",\"thumbnail\":\"__GHOST_URL__/content/images/2023/11/cluster-top-1.jpg\",\"icon\":\"__GHOST_URL__/content/images/size/w256h256/2020/06/favicon.png\"}}],[\"markdown\",{\"markdown\":\"| Item name | Cost |Cost Type| Running total |\\n| --- | --- | --- | --- |\\n| Digital ocean 3 droplets for 11 months | 171.56 | Operational| 171.56 |\\n| Digital ocean 2 droplets for 1 month | 13.40 | Operational |184.96 |\\n| Domain renewal | 14.79 | Operational |199.75 |\\n| Wasabi Cloud Storage  | 52.86 | Operational | 252.61 |\\n| Google Cloud | 0.12 | Operational |252.73 |\\n| New SFF PC's | 277.98 | Capital Expenditure |530.71 |\\n| New router | 83.00 | Capital Expenditure |613.71 |\\n| Electricity | 97 | Operational | 710.71 | \\n| Office 365 Account| 47.52 | Operational | 758.23|\\n\\n\"}],[\"callout\",{\"calloutEmoji\":\"🤑\",\"calloutText\":\"If you'd like to have £1000, start with £2000 and get a home lab\",\"backgroundColor\":\"grey\"}]],\"markups\":[[\"a\",[\"href\",\"https://peoplemaking.games/@gamesbymanuel/110667316416843436\"]],[\"a\",[\"href\",\"https://docs.2fauth.app\"]],[\"a\",[\"href\",\"mailto:breadmaster69@breadnet.co.uk?subject=Sqlite%20does%20suck%20you're%20right&body=Hi%20Bradley%2C%0D%0A%0D%0AI%20was%20going%20to%20write%20you%20an%20email%20full%20of%20abuse%20about%20how%20sqlite%20is%20actually%20amazing%2C%20but%20then%20I%20realized%20you're%20right.%0D%0A%0D%0AI%20just%20wanted%20to%20let%20you%20know%20how%20right%20you%20were%0D%0A%0D%0ALots%20of%20love.\"]],[\"a\",[\"href\",\"https://github.com/hay-kot/homebox\"]],[\"a\",[\"href\",\"https://www.shelf.nu\"]],[\"code\"],[\"a\",[\"href\",\"__GHOST_URL__/leaving-selfhosted-mail/?utm_source=main_blog&utm_medium=blog_post&utm_campaign=self_hosting_2023\"]],[\"a\",[\"href\",\"https://documentation.breadnet.co.uk/kubernetes/helm/push-chart-to-ar/?utm_source=main_blog&utm_medium=blog_post&utm_campaign=self_hosting_2023\"]],[\"a\",[\"href\",\"https://www.nhsbt.nhs.uk/news/change-to-nhsbt-pricing-of-products-in-201718-and-introduction-of-universal-screening-for-hepatitis-e/#:~:text=(from%20%C2%A3120%20to%20%C2%A3124.46%20per%20unit)\"]],[\"a\",[\"href\",\"https://subway-menu.net/subway-prices-uk#:~:text=%C2%A32.99-,%C2%A35.39,-Italian%20B.M\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I'm going to try and do a yearly blog post detailing what I'm running at the end of each year. \"]]],[1,\"p\",[[0,[],0,\"This year had some pretty interesting additions to the breadNET stack as well as some changes\"]]],[3,\"ul\",[[[0,[],0,\"Kubernetes\"]],[[0,[],0,\"Bringing Matomo back home\"]],[[0,[],0,\"More Zero Trust\"]],[[0,[],0,\"DNS fully migrated to cloudflare\"]],[[0,[],0,\"Shutdown a web server\"]],[[0,[],0,\"Using Azure AD for authentication \"]],[[0,[],0,\"Using GCP for cloud offerings\"]]]],[10,0],[1,\"p\",[[0,[],0,\"I am going to start this post off by managing your expectations, below is what we will cover \"]]],[3,\"ul\",[[[0,[],0,\"What services I'm running\"]],[[0,[],0,\"What services I plan to run\"]],[[0,[],0,\"What's the hardware\"]],[[0,[],0,\"Why am I relying on the cloud more (and what services)\"]],[[0,[],0,\"What this year cost me\"]],[[0,[],0,\"What's the plan for next year\"]]]],[1,\"p\",[]],[1,\"h2\",[[0,[],0,\"What services I'm running\"]]],[1,\"p\",[[0,[],0,\"I've really cut down quite a lot of services from a while ago where I was running like 10 services across multiple VM's on my Dell server.\"]]],[1,\"p\",[[0,[],0,\"Now I've kept it lean with just a few important services. This covers things up in the cloud as well as on prem\"]]],[10,1],[1,\"p\",[]],[1,\"h3\",[[0,[],0,\"What services I plan to run\"]]],[1,\"p\",[[0,[],0,\"Ideally I'd like to get the below up and running in 2024 \"]]],[3,\"ul\",[[[0,[],0,\"Arr Stack: \"],[0,[0],1,\"For downloading Linux ISO's \"]],[[0,[],0,\"Jellyfin: Stream the ISO's to TV's\"]],[[0,[],0,\"Grocy (again): Kitchen inventory system (Much to my partners displeasure)\"]],[[0,[],0,\"Some form of web archive system: So when I link to something in my articles, it will never Dead link\"]],[[0,[],0,\"Home inventory Softare: Ability to know what I own and then claim on insurance (adult stuff I know)\"]],[[0,[],0,\"Book Inventory: So I can see how little I read\"]],[[0,[],0,\"Hound: CodeSearch to search across multiple repos across multiple git hosting companies\"]],[[0,[],0,\"Link Shorter: I would like to build my own (with my coding buddy chatGPT) that is K8's native and uses a custom resource definition I will learn\"]],[[0,[],0,\"Monica: Personal CRM system allowing me to be a better friend\"]],[[0,[1],1,\"2fauth\"],[0,[],0,\": Allows you to manage 2FA codes in a web browser. Only supports sqlite (boo) but does support proxy username headers (woo)\"]]]],[1,\"p\",[]],[1,\"blockquote\",[[0,[],0,\"My beef with sqlite is it's not really a database is it? It's a glorified text file, and it just breaks with nfs3, which means I need to figure out how to get nfsv4 working, and also I can't scale the containers well. If you don't like my take on this, \"],[0,[2],1,\"you can send me an angry email \"]]],[1,\"p\",[]],[1,\"h4\",[[0,[],0,\"Options for the * inventory:\"]]],[3,\"ul\",[[[0,[],0,\"HomeBox: \"],[0,[3],1,\"I dont want to use SQlite\"]],[[0,[4],1,\"Shelf\"],[0,[],0,\": Looks promising, need to see if it has an android app for my scanner, also means I have to self host Supabase\"]]]],[1,\"p\",[]],[1,\"h3\",[[0,[],0,\"Whats the hardware\"]]],[1,\"p\",[[0,[],0,\"For compute I have 3 Dell SFF PC's that run the K3s cluster\"]]],[10,2],[1,\"p\",[[0,[],0,\"If you're wondering why there's a mismatch of CPU, It's because reading is not a strong suit of mine when it comes to Ebay\"]]],[1,\"p\",[[0,[],0,\"For what I call Persistence (Eg: Databases, Storing files etc) I have another Dell SFF PC, with 8GB of ram and an \"],[0,[5],1,\"Intel(R) Core(TM) i5-6500T CPU @ 2.50GHz\"],[0,[],0,\" Processor. This also has around 120GB of storage on an SSD.\"]]],[1,\"h3\",[[0,[],0,\"Why are you relying on the cloud\"]]],[1,\"p\",[[0,[],0,\"For some things I have to rely on the cloud. Let me explain\"]]],[1,\"p\",[[0,[],0,\"At the moment, all my DNS is managed on Cloudflare, who have a very nice simple to use Zero Trust system, which then very nicely integrates with Azure AD (I refuse to call it Entra) - I use Office 365 for my emails as \"],[0,[6],1,\"I migrated off Postfix and Dovecot\"],[0,[],0,\" quite some time ago\"]]],[1,\"p\",[[0,[],0,\"As far as cloud dependency goes, I'm not using too much\"]]],[1,\"p\",[[0,[],0,\"Digital ocean still hosts my Web server (This site), and Passbolt server (Until I migrate this to my home) and GCP is being used for it's Artifact Registry storage as I \"],[0,[7],1,\"store helm charts as OCI objects\"],[0,[],0,\" which is then consumed by flux\"]]],[1,\"p\",[[0,[],0,\"If you're interested about running k3s at home, then:\"]]],[10,3],[1,\"p\",[[0,[],0,\"The end end end (yes I meant to type that 3 times) goal would be to colocate a server somewhere in the UK, and run all my services off that opposed to at home where it's susceptible to power cuts and terrible UK non symmetrical internet. \"]]],[1,\"p\",[]],[1,\"h2\",[[0,[],0,\"What this year cost me\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"This year was a year of investment so the upfront costs (capex) are higher than the Operational Costs (opex)\"]]],[1,\"p\",[[0,[],0,\"All costs are in GBP (Great Brexit Pounds), and table assumes one year cost \"]]],[10,4],[1,\"p\",[[0,[],0,\"All in all, this years home labbing cost me £758.23 ($964, €874.01) which could have bought me \"],[0,[8],1,\"3.45 Litres of blood\"],[0,[],0,\" or \"],[0,[9],1,\"141 ham subways\"],[0,[],0,\" \"]]],[10,5],[1,\"h3\",[[0,[],0,\"What's the plan for next year\"]]],[1,\"p\",[[0,[],0,\"Without sounding like I use AI to write this (I didn't, which is why it reads so badly), 2024 will be a good year for the home lab.\"]]],[1,\"p\",[[0,[],0,\"I plan to migrate another server off of Digital ocean which will save me £67.92 a year (Current price of a droplet at 6 usd +20% tax converted to brexit pounds) \"]]],[1,\"p\",[[0,[],0,\"This £70 will probably get eaten up by my partner in the form of food, if I'm being honest.\"]]],[1,\"p\",[[0,[],0,\"As always a home lab is meant to be fun and a space to play, but my home lab has turned more in to a home production lab, where we still have the gung-ho of a lab, but with more on the line.\"]]],[1,\"p\",[[0,[],0,\"As the lab expands and I move up the country in search of cheaper rent, I plan to custom build a rack for the computers, and get a UPS so they can remain up when I inevitably trip over a power cable, or the power goes out.\"]]],[1,\"p\",[[0,[],0,\"Here's to 2023, and the future\"]]],[1,\"p\",[[0,[],0,\"_bradley \"]]]],\"ghostVersion\":\"4.0\"}","lexical":null,"html":"<p>I'm going to try and do a yearly blog post detailing what I'm running at the end of each year. </p><p>This year had some pretty interesting additions to the breadNET stack as well as some changes</p><ul><li>Kubernetes</li><li>Bringing Matomo back home</li><li>More Zero Trust</li><li>DNS fully migrated to cloudflare</li><li>Shutdown a web server</li><li>Using Azure AD for authentication </li><li>Using GCP for cloud offerings</li></ul><hr><p>I am going to start this post off by managing your expectations, below is what we will cover </p><ul><li>What services I'm running</li><li>What services I plan to run</li><li>What's the hardware</li><li>Why am I relying on the cloud more (and what services)</li><li>What this year cost me</li><li>What's the plan for next year</li></ul><p></p><h2 id=\"what-services-im-running\">What services I'm running</h2><p>I've really cut down quite a lot of services from a while ago where I was running like 10 services across multiple VM's on my Dell server.</p><p>Now I've kept it lean with just a few important services. This covers things up in the cloud as well as on prem</p><!--kg-card-begin: markdown--><table>\n<thead>\n<tr>\n<th>Service name</th>\n<th>Description</th>\n<th>Location</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Passbolt</td>\n<td>Password manager</td>\n<td><em>cloud</em></td>\n</tr>\n<tr>\n<td>pritunl Zero</td>\n<td>Passbolt's security</td>\n<td><em>cloud</em></td>\n</tr>\n<tr>\n<td>Ghost</td>\n<td>This site, a blog</td>\n<td><em>cloud</em></td>\n</tr>\n<tr>\n<td>Snipe-IT</td>\n<td>IT asset manager</td>\n<td>On prem (k8s)</td>\n</tr>\n<tr>\n<td>Gatus</td>\n<td>Status page with yaml config</td>\n<td>k8s</td>\n</tr>\n<tr>\n<td>Pihole</td>\n<td>Do you even self host bro?</td>\n<td>k8s</td>\n</tr>\n<tr>\n<td>matomo</td>\n<td>Self hosted analytics</td>\n<td>k8s</td>\n</tr>\n<tr>\n<td>mkdocs site</td>\n<td><a href=\"https://documentation.breadnet.co.uk/?utm_source=main_blog&amp;utm_medium=blog_post&amp;utm_campaign=self_hosting_2023\">Personal Documentation site</a> for all things I need to document</td>\n<td>fly.io</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: markdown--><p></p><h3 id=\"what-services-i-plan-to-run\">What services I plan to run</h3><p>Ideally I'd like to get the below up and running in 2024 </p><ul><li>Arr Stack: <a href=\"https://peoplemaking.games/@gamesbymanuel/110667316416843436\">For downloading Linux ISO's </a></li><li>Jellyfin: Stream the ISO's to TV's</li><li>Grocy (again): Kitchen inventory system (Much to my partners displeasure)</li><li>Some form of web archive system: So when I link to something in my articles, it will never Dead link</li><li>Home inventory Softare: Ability to know what I own and then claim on insurance (adult stuff I know)</li><li>Book Inventory: So I can see how little I read</li><li>Hound: CodeSearch to search across multiple repos across multiple git hosting companies</li><li>Link Shorter: I would like to build my own (with my coding buddy chatGPT) that is K8's native and uses a custom resource definition I will learn</li><li>Monica: Personal CRM system allowing me to be a better friend</li><li><a href=\"https://docs.2fauth.app\">2fauth</a>: Allows you to manage 2FA codes in a web browser. Only supports sqlite (boo) but does support proxy username headers (woo)</li></ul><p></p><blockquote>My beef with sqlite is it's not really a database is it? It's a glorified text file, and it just breaks with nfs3, which means I need to figure out how to get nfsv4 working, and also I can't scale the containers well. If you don't like my take on this, <a href=\"mailto:breadmaster69@breadnet.co.uk?subject=Sqlite%20does%20suck%20you're%20right&amp;body=Hi%20Bradley%2C%0D%0A%0D%0AI%20was%20going%20to%20write%20you%20an%20email%20full%20of%20abuse%20about%20how%20sqlite%20is%20actually%20amazing%2C%20but%20then%20I%20realized%20you're%20right.%0D%0A%0D%0AI%20just%20wanted%20to%20let%20you%20know%20how%20right%20you%20were%0D%0A%0D%0ALots%20of%20love.\">you can send me an angry email </a></blockquote><p></p><h4 id=\"options-for-the-inventory\">Options for the * inventory:</h4><ul><li>HomeBox: <a href=\"https://github.com/hay-kot/homebox\">I dont want to use SQlite</a></li><li><a href=\"https://www.shelf.nu\">Shelf</a>: Looks promising, need to see if it has an android app for my scanner, also means I have to self host Supabase</li></ul><p></p><h3 id=\"whats-the-hardware\">Whats the hardware</h3><p>For compute I have 3 Dell SFF PC's that run the K3s cluster</p><!--kg-card-begin: markdown--><table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Ram</th>\n<th>CPU</th>\n<th>Storage</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>k3s-01</td>\n<td><code>8gb</code></td>\n<td><code>Intel(R) Core(TM) i3-6100T CPU @ 3.20GHz</code></td>\n<td>500 SSD</td>\n</tr>\n<tr>\n<td>k3s-02</td>\n<td><code>8gb</code></td>\n<td><code>Intel(R) Core(TM) i3-6100T CPU @ 3.20GHz</code></td>\n<td>120 SSD</td>\n</tr>\n<tr>\n<td>k3s-03</td>\n<td><code>8gb</code></td>\n<td><code>Intel(R) Core(TM) i5-6500T CPU @ 2.50GHz</code></td>\n<td>120 SSD</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: markdown--><p>If you're wondering why there's a mismatch of CPU, It's because reading is not a strong suit of mine when it comes to Ebay</p><p>For what I call Persistence (Eg: Databases, Storing files etc) I have another Dell SFF PC, with 8GB of ram and an <code>Intel(R) Core(TM) i5-6500T CPU @ 2.50GHz</code> Processor. This also has around 120GB of storage on an SSD.</p><h3 id=\"why-are-you-relying-on-the-cloud\">Why are you relying on the cloud</h3><p>For some things I have to rely on the cloud. Let me explain</p><p>At the moment, all my DNS is managed on Cloudflare, who have a very nice simple to use Zero Trust system, which then very nicely integrates with Azure AD (I refuse to call it Entra) - I use Office 365 for my emails as <a href=\"__GHOST_URL__/leaving-selfhosted-mail/?utm_source=main_blog&amp;utm_medium=blog_post&amp;utm_campaign=self_hosting_2023\">I migrated off Postfix and Dovecot</a> quite some time ago</p><p>As far as cloud dependency goes, I'm not using too much</p><p>Digital ocean still hosts my Web server (This site), and Passbolt server (Until I migrate this to my home) and GCP is being used for it's Artifact Registry storage as I <a href=\"https://documentation.breadnet.co.uk/kubernetes/helm/push-chart-to-ar/?utm_source=main_blog&amp;utm_medium=blog_post&amp;utm_campaign=self_hosting_2023\">store helm charts as OCI objects</a> which is then consumed by flux</p><p>If you're interested about running k3s at home, then:</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"__GHOST_URL__/kubernetes-at-home/?utm_source&#x3D;main_blog&amp;utm_medium&#x3D;blog_post&amp;utm_campaign&#x3D;self_hosting_2023\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Kubernetes at home</div><div class=\"kg-bookmark-description\">Ever wondered what it’s like running kubernetes at home? This post tries to answer that</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"__GHOST_URL__/content/images/size/w256h256/2020/06/favicon.png\" alt=\"\"><span class=\"kg-bookmark-author\">breadNET</span><span class=\"kg-bookmark-publisher\">Bradley Stannard</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"__GHOST_URL__/content/images/2023/11/cluster-top-1.jpg\" alt=\"\"></div></a></figure><p>The end end end (yes I meant to type that 3 times) goal would be to colocate a server somewhere in the UK, and run all my services off that opposed to at home where it's susceptible to power cuts and terrible UK non symmetrical internet. </p><p></p><h2 id=\"what-this-year-cost-me\">What this year cost me</h2><p></p><p>This year was a year of investment so the upfront costs (capex) are higher than the Operational Costs (opex)</p><p>All costs are in GBP (Great Brexit Pounds), and table assumes one year cost </p><!--kg-card-begin: markdown--><table>\n<thead>\n<tr>\n<th>Item name</th>\n<th>Cost</th>\n<th>Cost Type</th>\n<th>Running total</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Digital ocean 3 droplets for 11 months</td>\n<td>171.56</td>\n<td>Operational</td>\n<td>171.56</td>\n</tr>\n<tr>\n<td>Digital ocean 2 droplets for 1 month</td>\n<td>13.40</td>\n<td>Operational</td>\n<td>184.96</td>\n</tr>\n<tr>\n<td>Domain renewal</td>\n<td>14.79</td>\n<td>Operational</td>\n<td>199.75</td>\n</tr>\n<tr>\n<td>Wasabi Cloud Storage</td>\n<td>52.86</td>\n<td>Operational</td>\n<td>252.61</td>\n</tr>\n<tr>\n<td>Google Cloud</td>\n<td>0.12</td>\n<td>Operational</td>\n<td>252.73</td>\n</tr>\n<tr>\n<td>New SFF PC's</td>\n<td>277.98</td>\n<td>Capital Expenditure</td>\n<td>530.71</td>\n</tr>\n<tr>\n<td>New router</td>\n<td>83.00</td>\n<td>Capital Expenditure</td>\n<td>613.71</td>\n</tr>\n<tr>\n<td>Electricity</td>\n<td>97</td>\n<td>Operational</td>\n<td>710.71</td>\n</tr>\n<tr>\n<td>Office 365 Account</td>\n<td>47.52</td>\n<td>Operational</td>\n<td>758.23</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: markdown--><p>All in all, this years home labbing cost me £758.23 ($964, €874.01) which could have bought me <a href=\"https://www.nhsbt.nhs.uk/news/change-to-nhsbt-pricing-of-products-in-201718-and-introduction-of-universal-screening-for-hepatitis-e/#:~:text=(from%20%C2%A3120%20to%20%C2%A3124.46%20per%20unit)\">3.45 Litres of blood</a> or <a href=\"https://subway-menu.net/subway-prices-uk#:~:text=%C2%A32.99-,%C2%A35.39,-Italian%20B.M\">141 ham subways</a> </p><div class=\"kg-card kg-callout-card kg-callout-card-grey\"><div class=\"kg-callout-emoji\">🤑</div><div class=\"kg-callout-text\">If you'd like to have £1000, start with £2000 and get a home lab</div></div><h3 id=\"whats-the-plan-for-next-year\">What's the plan for next year</h3><p>Without sounding like I use AI to write this (I didn't, which is why it reads so badly), 2024 will be a good year for the home lab.</p><p>I plan to migrate another server off of Digital ocean which will save me £67.92 a year (Current price of a droplet at 6 usd +20% tax converted to brexit pounds) </p><p>This £70 will probably get eaten up by my partner in the form of food, if I'm being honest.</p><p>As always a home lab is meant to be fun and a space to play, but my home lab has turned more in to a home production lab, where we still have the gung-ho of a lab, but with more on the line.</p><p>As the lab expands and I move up the country in search of cheaper rent, I plan to custom build a rack for the computers, and get a UPS so they can remain up when I inevitably trip over a power cable, or the power goes out.</p><p>Here's to 2023, and the future</p><p>_bradley </p>","comment_id":"658b587ee72f4aacb9d7eb06","plaintext":"I'm going to try and do a yearly blog post detailing what I'm running at the end of each year.\n\nThis year had some pretty interesting additions to the breadNET stack as well as some changes\n\n * Kubernetes\n * Bringing Matomo back home\n * More Zero Trust\n * DNS fully migrated to cloudflare\n * Shutdown a web server\n * Using Azure AD for authentication\n * Using GCP for cloud offerings\n\nI am going to start this post off by managing your expectations, below is what we will cover\n\n * What services I'm running\n * What services I plan to run\n * What's the hardware\n * Why am I relying on the cloud more (and what services)\n * What this year cost me\n * What's the plan for next year\n\n\n\n\nWhat services I'm running\n\nI've really cut down quite a lot of services from a while ago where I was running like 10 services across multiple VM's on my Dell server.\n\nNow I've kept it lean with just a few important services. This covers things up in the cloud as well as on prem\n\n\n\n\nService name\nDescription\nLocation\n\n\n\n\nPassbolt\nPassword manager\ncloud\n\n\npritunl Zero\nPassbolt's security\ncloud\n\n\nGhost\nThis site, a blog\ncloud\n\n\nSnipe-IT\nIT asset manager\nOn prem (k8s)\n\n\nGatus\nStatus page with yaml config\nk8s\n\n\nPihole\nDo you even self host bro?\nk8s\n\n\nmatomo\nSelf hosted analytics\nk8s\n\n\nmkdocs site\nPersonal Documentation site for all things I need to document\nfly.io\n\n\n\n\n\n\n\n\nWhat services I plan to run\n\nIdeally I'd like to get the below up and running in 2024\n\n * Arr Stack: For downloading Linux ISO's\n * Jellyfin: Stream the ISO's to TV's\n * Grocy (again): Kitchen inventory system (Much to my partners displeasure)\n * Some form of web archive system: So when I link to something in my articles, it will never Dead link\n * Home inventory Softare: Ability to know what I own and then claim on insurance (adult stuff I know)\n * Book Inventory: So I can see how little I read\n * Hound: CodeSearch to search across multiple repos across multiple git hosting companies\n * Link Shorter: I would like to build my own (with my coding buddy chatGPT) that is K8's native and uses a custom resource definition I will learn\n * Monica: Personal CRM system allowing me to be a better friend\n * 2fauth: Allows you to manage 2FA codes in a web browser. Only supports sqlite (boo) but does support proxy username headers (woo)\n\n\n\nMy beef with sqlite is it's not really a database is it? It's a glorified text file, and it just breaks with nfs3, which means I need to figure out how to get nfsv4 working, and also I can't scale the containers well. If you don't like my take on this, you can send me an angry email\n\n\n\nOptions for the * inventory:\n\n * HomeBox: I dont want to use SQlite\n * Shelf: Looks promising, need to see if it has an android app for my scanner, also means I have to self host Supabase\n\n\n\n\nWhats the hardware\n\nFor compute I have 3 Dell SFF PC's that run the K3s cluster\n\n\n\n\nName\nRam\nCPU\nStorage\n\n\n\n\nk3s-01\n8gb\nIntel(R) Core(TM) i3-6100T CPU @ 3.20GHz\n500 SSD\n\n\nk3s-02\n8gb\nIntel(R) Core(TM) i3-6100T CPU @ 3.20GHz\n120 SSD\n\n\nk3s-03\n8gb\nIntel(R) Core(TM) i5-6500T CPU @ 2.50GHz\n120 SSD\n\n\n\n\n\nIf you're wondering why there's a mismatch of CPU, It's because reading is not a strong suit of mine when it comes to Ebay\n\nFor what I call Persistence (Eg: Databases, Storing files etc) I have another Dell SFF PC, with 8GB of ram and an Intel(R) Core(TM) i5-6500T CPU @ 2.50GHz Processor. This also has around 120GB of storage on an SSD.\n\n\nWhy are you relying on the cloud\n\nFor some things I have to rely on the cloud. Let me explain\n\nAt the moment, all my DNS is managed on Cloudflare, who have a very nice simple to use Zero Trust system, which then very nicely integrates with Azure AD (I refuse to call it Entra) - I use Office 365 for my emails as I migrated off Postfix and Dovecot quite some time ago\n\nAs far as cloud dependency goes, I'm not using too much\n\nDigital ocean still hosts my Web server (This site), and Passbolt server (Until I migrate this to my home) and GCP is being used for it's Artifact Registry storage as I store helm charts as OCI objects which is then consumed by flux\n\nIf you're interested about running k3s at home, then:\n\nKubernetes at homeEver wondered what it’s like running kubernetes at home? This post tries to answer thatbreadNETBradley Stannard\n\nThe end end end (yes I meant to type that 3 times) goal would be to colocate a server somewhere in the UK, and run all my services off that opposed to at home where it's susceptible to power cuts and terrible UK non symmetrical internet.\n\n\n\n\nWhat this year cost me\n\n\n\nThis year was a year of investment so the upfront costs (capex) are higher than the Operational Costs (opex)\n\nAll costs are in GBP (Great Brexit Pounds), and table assumes one year cost\n\n\n\n\nItem name\nCost\nCost Type\nRunning total\n\n\n\n\nDigital ocean 3 droplets for 11 months\n171.56\nOperational\n171.56\n\n\nDigital ocean 2 droplets for 1 month\n13.40\nOperational\n184.96\n\n\nDomain renewal\n14.79\nOperational\n199.75\n\n\nWasabi Cloud Storage\n52.86\nOperational\n252.61\n\n\nGoogle Cloud\n0.12\nOperational\n252.73\n\n\nNew SFF PC's\n277.98\nCapital Expenditure\n530.71\n\n\nNew router\n83.00\nCapital Expenditure\n613.71\n\n\nElectricity\n97\nOperational\n710.71\n\n\nOffice 365 Account\n47.52\nOperational\n758.23\n\n\n\n\n\nAll in all, this years home labbing cost me £758.23 ($964, €874.01) which could have bought me 3.45 Litres of blood or 141 ham subways\n\n🤑If you'd like to have £1000, start with £2000 and get a home lab\n\n\nWhat's the plan for next year\n\nWithout sounding like I use AI to write this (I didn't, which is why it reads so badly), 2024 will be a good year for the home lab.\n\nI plan to migrate another server off of Digital ocean which will save me £67.92 a year (Current price of a droplet at 6 usd +20% tax converted to brexit pounds)\n\nThis £70 will probably get eaten up by my partner in the form of food, if I'm being honest.\n\nAs always a home lab is meant to be fun and a space to play, but my home lab has turned more in to a home production lab, where we still have the gung-ho of a lab, but with more on the line.\n\nAs the lab expands and I move up the country in search of cheaper rent, I plan to custom build a rack for the computers, and get a UPS so they can remain up when I inevitably trip over a power cable, or the power goes out.\n\nHere's to 2023, and the future\n\n_bradley","feature_image":"https://images.unsplash.com/photo-1669659873672-a5f70782726b?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3wxMTc3M3wwfDF8c2VhcmNofDd8fDIwMjN8ZW58MHx8fHwxNzAzNjQyMjE4fDA&ixlib=rb-4.0.3&q=80&w=2000","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2023-12-26T22:49:34.000Z","updated_at":"2023-12-27T02:10:59.000Z","published_at":"2023-12-27T02:10:59.000Z","custom_excerpt":"A quick recap on what I'm running at the end of 2023, and a look to the future of the self hosting lab","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"6706c971e72f4aacb9d7ec7f","uuid":"81dd3133-9417-4bdd-9719-a8661775d17e","title":"Bringing Containerisation to a company near you","slug":"bringing-containerisation-to-a-company-near-you","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"callout\",{\"calloutEmoji\":\"🚨\",\"calloutText\":\"It's important to note that these are my views, and not those of my previous, current and future employers.&nbsp;\",\"backgroundColor\":\"white\"}]],\"markups\":[[\"em\"]],\"sections\":[[10,0],[1,\"p\",[[0,[],0,\"I joined $retailCompany as a budding young engineer who had just cut their teeth fully on Kubernetes and Google cloud. I had my certification and I was prepared to use it.\"]]],[1,\"p\",[[0,[],0,\"After around a year of dilly dallying, it was finally time to deploy \"],[0,[0],1,\"A kubernetes\"],[0,[],0,\" to our cloud provider of choice, GCP.\"]]],[1,\"p\",[]]],\"ghostVersion\":\"4.0\"}","lexical":null,"html":"<div class=\"kg-card kg-callout-card kg-callout-card-white\"><div class=\"kg-callout-emoji\">🚨</div><div class=\"kg-callout-text\">It's important to note that these are my views, and not those of my previous, current and future employers.&nbsp;</div></div><p>I joined $retailCompany as a budding young engineer who had just cut their teeth fully on Kubernetes and Google cloud. I had my certification and I was prepared to use it.</p><p>After around a year of dilly dallying, it was finally time to deploy <em>A kubernetes</em> to our cloud provider of choice, GCP.</p>","comment_id":"6706c971e72f4aacb9d7ec7f","plaintext":"🚨It's important to note that these are my views, and not those of my previous, current and future employers. \n\nI joined $retailCompany as a budding young engineer who had just cut their teeth fully on Kubernetes and Google cloud. I had my certification and I was prepared to use it.\n\nAfter around a year of dilly dallying, it was finally time to deploy A kubernetes to our cloud provider of choice, GCP.","feature_image":"https://images.unsplash.com/photo-1667372459510-55b5e2087cd0?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3wxMTc3M3wwfDF8c2VhcmNofDZ8fGt1YmVybmV0ZXN8ZW58MHx8fHwxNzI4NDk4MzU4fDA&ixlib=rb-4.0.3&q=80&w=2000","featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2024-10-09T18:20:33.000Z","updated_at":"2025-01-02T14:14:00.000Z","published_at":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"67769f2ee72f4aacb9d7ecbc","uuid":"d22f17c5-fd1d-4103-8e4b-d6e51b1ccca1","title":"2024 Year in review","slug":"2024-year-in-review","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"image\",{\"src\":\"__GHOST_URL__/content/images/2025/01/F9F8C433-641C-41B7-A04B-DB5A9CBBAD8A_1_102.jpeg\",\"width\":1536,\"height\":2048}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2025/01/03951064-8924-438D-8268-ED1BD99ED590_1_201_a.jpeg\",\"width\":2896,\"height\":2884}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2025/01/image.png\",\"width\":2760,\"height\":1292}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2025/01/image-1.png\",\"width\":550,\"height\":548,\"cardWidth\":\"\"}],[\"markdown\",{\"markdown\":\"| Service | Plan |\\n| --- | --- |\\n| Auth Services | Cloudflare Zero |\\n| Ghost Blog | Kubernetes Ghost blog or Static site |\\n| Passbolt | On prem passbolt |\"}],[\"hr\",{}]],\"markups\":[[\"em\"],[\"a\",[\"href\",\"__GHOST_URL__/what-im-running-at-the-end-of-2023/\"]],[\"a\",[\"href\",\"https://en.wikipedia.org/wiki/Barry_Island\"]],[\"a\",[\"href\",\"https://www.google.com/maps/dir/Berkshire/Barry+Island,+Barry/@51.6366402,-2.8929707,188791m/data=!3m2!1e3!4b1!4m16!4m15!1m5!1m1!1s0x48769bd8c5af65a3:0xf3d97f73063f8d6d!2m2!1d-1.1853677!2d51.4669939!1m5!1m1!1s0x486e05e4fc7a9865:0xc61cb81f44727466!2m2!1d-3.2688889!2d51.3930556!2m1!1b1!3e0?entry=ttu&g_ep=EgoyMDI0MTIxMS4wIKXMDSoJLDEwMjExMjMzSAFQAw%3D%3D\"]],[\"a\",[\"href\",\"https://m.do.co/c/77be3c3aa96c\"]],[\"a\",[\"href\",\"https://documentation.breadnet.co.uk/kb/networking/ee-update-dns/\"]],[\"a\",[\"href\",\"https://documentation.breadnet.co.uk/kb/networking/show-broadband-password-ee/\"]],[\"a\",[\"href\",\"https://documentation.breadnet.co.uk\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"As 2024 has already come to an end, I thought I should probably write a post as I've not made one in over a year... I thought no better way to round it out with what I've been up to\"]]],[1,\"h2\",[[0,[],0,\"Let's start with my personal life.\"]]],[1,\"p\",[[0,[],0,\"I've moved house to a nice apartment along the river Kennet, with amazing views of the Town. It's a recently built 2 bed apartment so I now have an office, opposed to my desk being \"],[0,[0],1,\"literally\"],[0,[],0,\" in the middle of the Dining Room/Kitchen/Living room/ Hallway... Fun of a one bed apartment. \"]]],[1,\"p\",[[0,[],0,\"I've changed \"],[0,[0],1,\"careers\"],[0,[],0,\". I have spent several years of my career as a DevOps/ Cloud engineer and after a few events at work, I thought perhaps I am better suited to security, as it works quite well with my ADHD. That is to say, pushing boundaries, trying to break things and then writing detailed documents on how this happened.\"]]],[1,\"p\",[[0,[],0,\"Since writing the \"],[0,[1],1,\"last blog post in 2023\"],[0,[],0,\" I've been learning to ride a motorbike too, I started on a Yamaha YZF-R 125, and then in around June I purchased a BMW G310GS, and on my birthday I took it down to Cornwall for a day trip. I've also been on a day trip to \"],[0,[2],1,\"Barry Island\"],[0,[],0,\" which is where Gavin and Stacey was filmed, where I had a great chat with some local bikers who found it hilarious I rode from \"],[0,[3],1,\"Berkshire to Barry\"],[0,[],0,\" on the backroads and then went home.\"]]],[1,\"p\",[[0,[],0,\"I am hoping to fly out to the united states of 'murica to see a friend and, hopefully, build out a datacentre in a shed. \"]]],[1,\"h2\",[[0,[],0,\"Now on to the technology \"]]],[1,\"p\",[[0,[],0,\"This year I had the simple plan to move a few things from the cloud, back to on premise.\"]]],[1,\"p\",[[0,[],0,\"I used to run the below applications on a few \"],[0,[4],1,\"Digital Ocean Droplets\"],[0,[],0,\", but I've since moved them back\"]]],[3,\"ul\",[[[0,[],0,\"Monica\"]],[[0,[],0,\"Mealie\"]],[[0,[],0,\"Passbolt\"]],[[0,[],0,\"Grocy\"]],[[0,[],0,\"Snipe IT\"]],[[0,[],0,\"Matomo\"]],[[0,[],0,\"Cloudflared\"]],[[0,[],0,\"Gatus\"]]]],[1,\"p\",[[0,[],0,\"I've moved them to my home cluster, which I have since moved from K3S to Talos, which I will be writing a blog post about later\"]]],[10,0],[1,\"p\",[[0,[],0,\"The above is my 3 node k3s cluster running on ubuntu, plus an additional node running mariadb and NFS exports for Persistent volumes. \"]]],[1,\"p\",[[0,[],0,\"Plan is to add 3 more nodes to my \"],[0,[0],1,\"lab\"],[0,[],0,\" and run a specific OS for Storage, as well as a node with an SSD for Databases. You would be surprised to know, it's more Dell 3040 SFF's \"]]],[10,1],[1,\"p\",[[0,[],0,\"The reason I went for the Dell 3040's is, they're cheap, decent CPU's, can take around 16gb of max ram, and the largest SSD you can possibly find. This means you can really get some chooch out of them.\"]]],[1,\"p\",[[0,[],0,\"The cluster should eventually be 3 nodes, and one control plane node with scheduling disabled. This means the control plane node can just crack on with working out what's going on in the cluster, and not wasting CPU and Ram on random apps I'm running.\"]]],[1,\"p\",[[0,[],0,\"Back in my old apartment, I was running the ISP's default router, which provided great reads like \"],[0,[5],1,\"\\\"Update DNS on EE Router\\\"\"],[0,[],0,\" and \"],[0,[6],1,\"\\\"Show Broadband password on EE router\\\"\"],[0,[],0,\". For my birthday I decided to \"],[0,[0],1,\"splash the cash, \"],[0,[],0,\"all £400ish and got a Unifi router, POE Switch, Unifi Access point and a Switch Lite\"]]],[10,2],[1,\"p\",[[0,[],0,\"I am so thankful to be off EE internet, and on to \"],[0,[0],1,\"literally\"],[0,[],0,\" anything else, in this case, Hyperoptic. I'm on, I think, a 150/150 (symmetrical) package which is quite decent as most ISP's in the UK offer pretty decent download speed, then abysmal upload speed. Take EE, offering 150/30. If someone can explain to me, how this is fair then I am all ears\"]]],[10,3],[1,\"p\",[[0,[],0,\"Now I am on a decent internet package, I am able to host a lot more public services on my connection, as well as actually backup things like photos, documents and files to S3 and it wont take all night.\"]]],[1,\"h3\",[[0,[],0,\"Plan to move more on-prem\"]]],[1,\"p\",[[0,[],0,\"Currently there are a few services I still have on \"],[0,[4],1,\"Digital ocean\"],[0,[],0,\" that I plan to migrate\"]]],[10,4],[1,\"p\",[[0,[],0,\"Something I have on fly.io is my \"],[0,[7],1,\"Documentation Site\"],[0,[],0,\", which with my plan for my new cluster is to move my Documentation site to my cluster. The idea is that I will run a OCI registry, as well as GitHub actions runner on the cluster to run all my internal CI jobs\"]]],[10,5],[1,\"p\",[[0,[],0,\"I'm going to close this post out as I am rambling too much, and leaking too much fun about my new cluster.\"]]],[1,\"p\",[[0,[],0,\"I plan to try and post more this year on the blog, as most of my material goes to my \"],[0,[7],1,\"Documentation\"],[0,[],0,\" site\"]]]],\"ghostVersion\":\"4.0\"}","lexical":null,"html":"<p>As 2024 has already come to an end, I thought I should probably write a post as I've not made one in over a year... I thought no better way to round it out with what I've been up to</p><h2 id=\"lets-start-with-my-personal-life\">Let's start with my personal life.</h2><p>I've moved house to a nice apartment along the river Kennet, with amazing views of the Town. It's a recently built 2 bed apartment so I now have an office, opposed to my desk being <em>literally</em> in the middle of the Dining Room/Kitchen/Living room/ Hallway... Fun of a one bed apartment. </p><p>I've changed <em>careers</em>. I have spent several years of my career as a DevOps/ Cloud engineer and after a few events at work, I thought perhaps I am better suited to security, as it works quite well with my ADHD. That is to say, pushing boundaries, trying to break things and then writing detailed documents on how this happened.</p><p>Since writing the <a href=\"__GHOST_URL__/what-im-running-at-the-end-of-2023/\">last blog post in 2023</a> I've been learning to ride a motorbike too, I started on a Yamaha YZF-R 125, and then in around June I purchased a BMW G310GS, and on my birthday I took it down to Cornwall for a day trip. I've also been on a day trip to <a href=\"https://en.wikipedia.org/wiki/Barry_Island\">Barry Island</a> which is where Gavin and Stacey was filmed, where I had a great chat with some local bikers who found it hilarious I rode from <a href=\"https://www.google.com/maps/dir/Berkshire/Barry+Island,+Barry/@51.6366402,-2.8929707,188791m/data=!3m2!1e3!4b1!4m16!4m15!1m5!1m1!1s0x48769bd8c5af65a3:0xf3d97f73063f8d6d!2m2!1d-1.1853677!2d51.4669939!1m5!1m1!1s0x486e05e4fc7a9865:0xc61cb81f44727466!2m2!1d-3.2688889!2d51.3930556!2m1!1b1!3e0?entry=ttu&amp;g_ep=EgoyMDI0MTIxMS4wIKXMDSoJLDEwMjExMjMzSAFQAw%3D%3D\">Berkshire to Barry</a> on the backroads and then went home.</p><p>I am hoping to fly out to the united states of 'murica to see a friend and, hopefully, build out a datacentre in a shed. </p><h2 id=\"now-on-to-the-technology\">Now on to the technology </h2><p>This year I had the simple plan to move a few things from the cloud, back to on premise.</p><p>I used to run the below applications on a few <a href=\"https://m.do.co/c/77be3c3aa96c\">Digital Ocean Droplets</a>, but I've since moved them back</p><ul><li>Monica</li><li>Mealie</li><li>Passbolt</li><li>Grocy</li><li>Snipe IT</li><li>Matomo</li><li>Cloudflared</li><li>Gatus</li></ul><p>I've moved them to my home cluster, which I have since moved from K3S to Talos, which I will be writing a blog post about later</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2025/01/F9F8C433-641C-41B7-A04B-DB5A9CBBAD8A_1_102.jpeg\" class=\"kg-image\" alt loading=\"lazy\" width=\"1536\" height=\"2048\" srcset=\"__GHOST_URL__/content/images/size/w600/2025/01/F9F8C433-641C-41B7-A04B-DB5A9CBBAD8A_1_102.jpeg 600w, __GHOST_URL__/content/images/size/w1000/2025/01/F9F8C433-641C-41B7-A04B-DB5A9CBBAD8A_1_102.jpeg 1000w, __GHOST_URL__/content/images/2025/01/F9F8C433-641C-41B7-A04B-DB5A9CBBAD8A_1_102.jpeg 1536w\" sizes=\"(min-width: 720px) 720px\"></figure><p>The above is my 3 node k3s cluster running on ubuntu, plus an additional node running mariadb and NFS exports for Persistent volumes. </p><p>Plan is to add 3 more nodes to my <em>lab</em> and run a specific OS for Storage, as well as a node with an SSD for Databases. You would be surprised to know, it's more Dell 3040 SFF's </p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2025/01/03951064-8924-438D-8268-ED1BD99ED590_1_201_a.jpeg\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"1992\" srcset=\"__GHOST_URL__/content/images/size/w600/2025/01/03951064-8924-438D-8268-ED1BD99ED590_1_201_a.jpeg 600w, __GHOST_URL__/content/images/size/w1000/2025/01/03951064-8924-438D-8268-ED1BD99ED590_1_201_a.jpeg 1000w, __GHOST_URL__/content/images/size/w1600/2025/01/03951064-8924-438D-8268-ED1BD99ED590_1_201_a.jpeg 1600w, __GHOST_URL__/content/images/size/w2400/2025/01/03951064-8924-438D-8268-ED1BD99ED590_1_201_a.jpeg 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><p>The reason I went for the Dell 3040's is, they're cheap, decent CPU's, can take around 16gb of max ram, and the largest SSD you can possibly find. This means you can really get some chooch out of them.</p><p>The cluster should eventually be 3 nodes, and one control plane node with scheduling disabled. This means the control plane node can just crack on with working out what's going on in the cluster, and not wasting CPU and Ram on random apps I'm running.</p><p>Back in my old apartment, I was running the ISP's default router, which provided great reads like <a href=\"https://documentation.breadnet.co.uk/kb/networking/ee-update-dns/\">\"Update DNS on EE Router\"</a> and <a href=\"https://documentation.breadnet.co.uk/kb/networking/show-broadband-password-ee/\">\"Show Broadband password on EE router\"</a>. For my birthday I decided to <em>splash the cash, </em>all £400ish and got a Unifi router, POE Switch, Unifi Access point and a Switch Lite</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2025/01/image.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"936\" srcset=\"__GHOST_URL__/content/images/size/w600/2025/01/image.png 600w, __GHOST_URL__/content/images/size/w1000/2025/01/image.png 1000w, __GHOST_URL__/content/images/size/w1600/2025/01/image.png 1600w, __GHOST_URL__/content/images/size/w2400/2025/01/image.png 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><p>I am so thankful to be off EE internet, and on to <em>literally</em> anything else, in this case, Hyperoptic. I'm on, I think, a 150/150 (symmetrical) package which is quite decent as most ISP's in the UK offer pretty decent download speed, then abysmal upload speed. Take EE, offering 150/30. If someone can explain to me, how this is fair then I am all ears</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2025/01/image-1.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"550\" height=\"548\"></figure><p>Now I am on a decent internet package, I am able to host a lot more public services on my connection, as well as actually backup things like photos, documents and files to S3 and it wont take all night.</p><h3 id=\"plan-to-move-more-on-prem\">Plan to move more on-prem</h3><p>Currently there are a few services I still have on <a href=\"https://m.do.co/c/77be3c3aa96c\">Digital ocean</a> that I plan to migrate</p><!--kg-card-begin: markdown--><table>\n<thead>\n<tr>\n<th>Service</th>\n<th>Plan</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Auth Services</td>\n<td>Cloudflare Zero</td>\n</tr>\n<tr>\n<td>Ghost Blog</td>\n<td>Kubernetes Ghost blog or Static site</td>\n</tr>\n<tr>\n<td>Passbolt</td>\n<td>On prem passbolt</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: markdown--><p>Something I have on fly.io is my <a href=\"https://documentation.breadnet.co.uk\">Documentation Site</a>, which with my plan for my new cluster is to move my Documentation site to my cluster. The idea is that I will run a OCI registry, as well as GitHub actions runner on the cluster to run all my internal CI jobs</p><hr><p>I'm going to close this post out as I am rambling too much, and leaking too much fun about my new cluster.</p><p>I plan to try and post more this year on the blog, as most of my material goes to my <a href=\"https://documentation.breadnet.co.uk\">Documentation</a> site</p>","comment_id":"67769f2ee72f4aacb9d7ecbc","plaintext":"As 2024 has already come to an end, I thought I should probably write a post as I've not made one in over a year... I thought no better way to round it out with what I've been up to\n\n\nLet's start with my personal life.\n\nI've moved house to a nice apartment along the river Kennet, with amazing views of the Town. It's a recently built 2 bed apartment so I now have an office, opposed to my desk being literally in the middle of the Dining Room/Kitchen/Living room/ Hallway... Fun of a one bed apartment.\n\nI've changed careers. I have spent several years of my career as a DevOps/ Cloud engineer and after a few events at work, I thought perhaps I am better suited to security, as it works quite well with my ADHD. That is to say, pushing boundaries, trying to break things and then writing detailed documents on how this happened.\n\nSince writing the last blog post in 2023 I've been learning to ride a motorbike too, I started on a Yamaha YZF-R 125, and then in around June I purchased a BMW G310GS, and on my birthday I took it down to Cornwall for a day trip. I've also been on a day trip to Barry Island which is where Gavin and Stacey was filmed, where I had a great chat with some local bikers who found it hilarious I rode from Berkshire to Barry on the backroads and then went home.\n\nI am hoping to fly out to the united states of 'murica to see a friend and, hopefully, build out a datacentre in a shed.\n\n\nNow on to the technology\n\nThis year I had the simple plan to move a few things from the cloud, back to on premise.\n\nI used to run the below applications on a few Digital Ocean Droplets, but I've since moved them back\n\n * Monica\n * Mealie\n * Passbolt\n * Grocy\n * Snipe IT\n * Matomo\n * Cloudflared\n * Gatus\n\nI've moved them to my home cluster, which I have since moved from K3S to Talos, which I will be writing a blog post about later\n\nThe above is my 3 node k3s cluster running on ubuntu, plus an additional node running mariadb and NFS exports for Persistent volumes.\n\nPlan is to add 3 more nodes to my lab and run a specific OS for Storage, as well as a node with an SSD for Databases. You would be surprised to know, it's more Dell 3040 SFF's\n\nThe reason I went for the Dell 3040's is, they're cheap, decent CPU's, can take around 16gb of max ram, and the largest SSD you can possibly find. This means you can really get some chooch out of them.\n\nThe cluster should eventually be 3 nodes, and one control plane node with scheduling disabled. This means the control plane node can just crack on with working out what's going on in the cluster, and not wasting CPU and Ram on random apps I'm running.\n\nBack in my old apartment, I was running the ISP's default router, which provided great reads like \"Update DNS on EE Router\" and \"Show Broadband password on EE router\". For my birthday I decided to splash the cash, all £400ish and got a Unifi router, POE Switch, Unifi Access point and a Switch Lite\n\nI am so thankful to be off EE internet, and on to literally anything else, in this case, Hyperoptic. I'm on, I think, a 150/150 (symmetrical) package which is quite decent as most ISP's in the UK offer pretty decent download speed, then abysmal upload speed. Take EE, offering 150/30. If someone can explain to me, how this is fair then I am all ears\n\nNow I am on a decent internet package, I am able to host a lot more public services on my connection, as well as actually backup things like photos, documents and files to S3 and it wont take all night.\n\n\nPlan to move more on-prem\n\nCurrently there are a few services I still have on Digital ocean that I plan to migrate\n\n\n\n\nService\nPlan\n\n\n\n\nAuth Services\nCloudflare Zero\n\n\nGhost Blog\nKubernetes Ghost blog or Static site\n\n\nPassbolt\nOn prem passbolt\n\n\n\n\n\nSomething I have on fly.io is my Documentation Site, which with my plan for my new cluster is to move my Documentation site to my cluster. The idea is that I will run a OCI registry, as well as GitHub actions runner on the cluster to run all my internal CI jobs\n\nI'm going to close this post out as I am rambling too much, and leaking too much fun about my new cluster.\n\nI plan to try and post more this year on the blog, as most of my material goes to my Documentation site","feature_image":"https://images.unsplash.com/1/work-station-straight-on-view.jpg?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3wxMTc3M3wwfDF8c2VhcmNofDMyfHxjb21wdXRlcnxlbnwwfHx8fDE3MzU4NTAxNzh8MA&ixlib=rb-4.0.3&q=80&w=2000","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2025-01-02T14:14:06.000Z","updated_at":"2025-01-02T20:37:50.000Z","published_at":"2024-12-31T20:35:00.000Z","custom_excerpt":"2024, great fun. Broke kubernetes many times, changed careers and big plans for 2025","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"68cb40b7b1baac19a80760b6","uuid":"abd45bad-b025-413a-a39b-9e9f0664db9f","title":"Homelab V2","slug":"homelab-v2","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"image\",{\"triggerBrowse\":true,\"src\":\"__GHOST_URL__/content/images/2020/06/EmbeddedImage--7-.jpg\",\"caption\":\"The old server rack\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2025/09/IMG_3620-1.jpeg\",\"width\":4032,\"height\":3024,\"caption\":\"All my servers, switches and routers in the back of my old car on their way to Jack\"}],[\"image\",{\"triggerBrowse\":true,\"src\":\"__GHOST_URL__/content/images/size/w2000/2023/11/cluster-top-1.jpg\",\"caption\":\"Old K3s cluster in Wokingham\"}],[\"bookmark\",{\"version\":\"1.0\",\"type\":\"bookmark\",\"url\":\"__GHOST_URL__/kubernetes-at-home/\",\"metadata\":{\"url\":\"__GHOST_URL__/kubernetes-at-home/\",\"title\":\"Kubernetes at home\",\"description\":\"Ever wondered what it’s like running kubernetes at home? This post tries to answer that\",\"author\":\"Bradley Stannard\",\"publisher\":\"breadNET\",\"thumbnail\":\"__GHOST_URL__/content/images/2023/11/cluster-top-1.jpg\",\"icon\":\"__GHOST_URL__/content/images/size/w256h256/2020/06/favicon.png\"}}],[\"markdown\",{\"markdown\":\"| Use | Count |\\n| --- | --- |\\n| Kubernetes | 4 |\\n| Database | 1|\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2025/09/image-1.png\",\"width\":2994,\"height\":1856}],[\"markdown\",{\"markdown\":\"> Todo: Write blog post on Talos cluster!\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2025/09/image-2.png\",\"width\":1070,\"height\":164}]],\"markups\":[[\"a\",[\"href\",\"__GHOST_URL__/kubernetes-at-home/\"]],[\"em\"],[\"a\",[\"href\",\"https://documentation.breadnet.co.uk/outage/2023-11-26-04/\"]],[\"code\"],[\"a\",[\"href\",\"https://github.com/hound-search/hound\"]],[\"a\",[\"href\",\"https://vikunja.io\"]],[\"a\",[\"href\",\"https://docs.karakeep.app\"]],[\"a\",[\"href\",\"https://github.com/CorentinTh/it-tools\"]],[\"a\",[\"href\",\"https://wallosapp.com\"]],[\"a\",[\"href\",\"https://lubelogger.com\"]],[\"a\",[\"href\",\"https://donetick.com\"]],[\"a\",[\"href\",\"https://freshrss.org/index.html\"]],[\"strong\"]],\"sections\":[[1,\"p\",[[0,[],0,\"It's like the original \"],[0,[0],1,\"Home lab\"],[0,[],0,\" but this time, not quite as bad.\"]]],[1,\"p\",[[0,[],0,\"Home labs are very personal, what works for me wont work for you. What you like, I may dislike. Homelab V2.\"]]],[1,\"p\",[[0,[],0,\"The whole idea behind why Homelab V2 has come about, is because one of the services I host, Mealie, crashed and burnt so badly that we were having to go back to using cook books and digging through the backup JSON to find recipes.\"]]],[1,\"p\",[[0,[],0,\"The new lab is designed to score high on the SOAP scale (significant other acceptance parameters) - that is to say, my partner finds it simple to consume services and treats it like it's a Saas product.\"]]],[1,\"p\",[[0,[],0,\"It's designed to be immutable, git native and cheap as possible to run, with as little maintenance required. \"]]],[1,\"h2\",[[0,[],0,\"Brief overview of the homelab's I've had\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"It started in my parents house where I used to have a 48U rack\"]]],[10,0],[1,\"p\",[[0,[],0,\"I then migrated most of my things to SaaS offerings because I had to sell the servers, which was a sad day. Luckily they were able to go to an old friend of mine from School who would put them to good use\"]]],[10,1],[1,\"p\",[[0,[],0,\"I moved around a lot between moving out of my parents house and then when I got my own apartment, which meant I couldn't have anything permanent setup.\"]]],[1,\"p\",[[0,[],0,\"Finally when I landed in Berkshire, I had means to actually setup something \"],[0,[1],1,\"somewhat\"],[0,[],0,\" reliable. I present, k3s at home\"]]],[10,2],[1,\"p\",[[0,[],0,\"This cluster was great, it only really \"],[0,[2],1,\"had one outage\"],[0,[],0,\", and for the most part was pretty decent.\"]]],[10,3],[1,\"p\",[[0,[],0,\"However, there were massive reliability issues when I moved to my new address. Namely, the IP address all changed for some reason, and the certificates also expired. I decided it was time to do something about it\"]]],[1,\"p\",[]],[1,\"h1\",[[0,[],0,\"Homelab v2\"]]],[1,\"p\",[[0,[],0,\"Homelab V2 is, for all intensive purposes, RAID but with computers. \"]]],[1,\"p\",[[0,[],0,\"The lab is built off of a stack of Dell 3040 SFF PC's with 8gb ram each, and 128GB SSD's. \"]]],[1,\"p\",[[0,[],0,\"Currently there are 6 of these, with 5 running, and one as a spare.\"]]],[10,4],[1,\"p\",[[0,[],0,\"As you can deduct from the table above, I am still running Kubernetes. But this is where it changes. I am now running Talos, a full OS rendition of Kubernetes designed to be lightweight and it's all configured from a file. You can get cool tools like \"],[0,[3],1,\"talhelper\"],[0,[],0,\" that help you template out config and generate node config off a single central file.\"]]],[10,5],[1,\"p\",[[0,[],0,\"Unlike K3s which is installed on Ubuntu as a binary, Talos is the operating system and also the Kubernetes. \"]]],[1,\"p\",[[0,[],0,\"I wont go too much in to the Kubernetes setup details here, as I plan to write another blog post about this where I touch one some more interesting details\"]]],[10,6],[10,7],[1,\"p\",[[0,[],0,\"I've got 3 nodes in this cluster. You're probably thinking \\\"hang on, there's 4 nodes in the table above, but only 3 here?\\\" and you're right.\"]]],[1,\"p\",[[0,[],0,\"The 4th node is a single node cluster called \"],[0,[3],1,\"utils\"],[0,[],0,\" that runs some \"],[0,[1],1,\"utility\"],[0,[],0,\" apps. (I touch on these apps later)\"]]],[1,\"p\",[[0,[],0,\"The last node is just an Ubuntu host running \"],[0,[3],1,\"mariadb\"],[0,[],0,\" as of current, however some apps require Postgres so I need to get that installed.\"]]],[1,\"h2\",[[0,[],0,\"What apps are you running\"]]],[1,\"p\",[[0,[],0,\"Currently I am running\"]]],[3,\"ul\",[[[0,[],0,\"Mealie\"]],[[0,[],0,\"Gatus\"]],[[0,[],0,\"Matrix server (blog post soon...)\"]],[[0,[],0,\"Pocket-id (does all my OIDC auth)\"]],[[0,[],0,\"Hound (\"],[0,[4],1,\"Git repo search\"],[0,[],0,\")\"]],[[0,[],0,\"Atlantis (Terraform PR automation)\"]],[[0,[],0,\"Postfix Mail forwarder (Allows me to setup internal apps to send mail without having to deal with per application authentication. It's authenticated in one place)\"]]]],[1,\"h2\",[[0,[],0,\"What apps are planned\"]]],[3,\"ul\",[[[0,[],0,\"Jellyfin\"]],[[0,[1],1,\"Linux ISO\"],[0,[],0,\" Downloading software\"]],[[0,[],0,\"Pinchflat\"]],[[0,[],0,\"ErsatxTV\"]],[[0,[],0,\"Airtrail\"]],[[0,[],0,\"Kanboard.org OR \"],[0,[5],1,\"Vikunja\"]],[[0,[],0,\"Karakeep (\"],[0,[6],1,\"Previously Hoarder\"],[0,[],0,\")\"]],[[0,[],0,\"Paperless-ngx\"]],[[0,[7],1,\"IT Tools\"]],[[0,[],0,\"changedetection.io\"]],[[0,[],0,\"Snipe-IT (Used to host this, just need to revive it from the dead)\"]],[[0,[8],1,\"Wallos\"]],[[0,[],0,\"Wazuh\"]],[[0,[],0,\"Umami\"]],[[0,[],0,\"Matomo (Need to revive from the dead...)\"]],[[0,[],0,\"Terraform registry Cace (Using NGINX) to speed up local Atlantis\"]],[[0,[],0,\"Lube Logger (\"],[0,[9],1,\"It's for cars...\"],[0,[],0,\")\"]],[[0,[10],1,\"Donetick\"]],[[0,[11],1,\"FreshRSS\"]]]],[1,\"h2\",[[0,[],0,\"Where are you storing everything\"]]],[1,\"p\",[[0,[],0,\"I was able to pick up a Synology nas on Ebay for around £800 (Yes, that went down \"],[0,[12],1,\"very\"],[0,[],0,\" well with the Partner) and serving iSCSI LUN's over to the Talos nodes\"]]],[1,\"h2\",[[0,[],0,\"Closing notes\"]]],[1,\"p\",[[0,[],0,\"This is a real great starting point for the lab. Everything is fresh and new, and I am doing it right the first time. Keep an eye out on the site for new posts\"]]],[1,\"p\",[]]],\"ghostVersion\":\"4.0\"}","lexical":null,"html":"<p>It's like the original <a href=\"__GHOST_URL__/kubernetes-at-home/\">Home lab</a> but this time, not quite as bad.</p><p>Home labs are very personal, what works for me wont work for you. What you like, I may dislike. Homelab V2.</p><p>The whole idea behind why Homelab V2 has come about, is because one of the services I host, Mealie, crashed and burnt so badly that we were having to go back to using cook books and digging through the backup JSON to find recipes.</p><p>The new lab is designed to score high on the SOAP scale (significant other acceptance parameters) - that is to say, my partner finds it simple to consume services and treats it like it's a Saas product.</p><p>It's designed to be immutable, git native and cheap as possible to run, with as little maintenance required. </p><h2 id=\"brief-overview-of-the-homelabs-ive-had\">Brief overview of the homelab's I've had</h2><p></p><p>It started in my parents house where I used to have a 48U rack</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2020/06/EmbeddedImage--7-.jpg\" class=\"kg-image\" alt loading=\"lazy\"><figcaption>The old server rack</figcaption></figure><p>I then migrated most of my things to SaaS offerings because I had to sell the servers, which was a sad day. Luckily they were able to go to an old friend of mine from School who would put them to good use</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2025/09/IMG_3620-1.jpeg\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"1500\" srcset=\"__GHOST_URL__/content/images/size/w600/2025/09/IMG_3620-1.jpeg 600w, __GHOST_URL__/content/images/size/w1000/2025/09/IMG_3620-1.jpeg 1000w, __GHOST_URL__/content/images/size/w1600/2025/09/IMG_3620-1.jpeg 1600w, __GHOST_URL__/content/images/size/w2400/2025/09/IMG_3620-1.jpeg 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption>All my servers, switches and routers in the back of my old car on their way to Jack</figcaption></figure><p>I moved around a lot between moving out of my parents house and then when I got my own apartment, which meant I couldn't have anything permanent setup.</p><p>Finally when I landed in Berkshire, I had means to actually setup something <em>somewhat</em> reliable. I present, k3s at home</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/size/w2000/2023/11/cluster-top-1.jpg\" class=\"kg-image\" alt loading=\"lazy\"><figcaption>Old K3s cluster in Wokingham</figcaption></figure><p>This cluster was great, it only really <a href=\"https://documentation.breadnet.co.uk/outage/2023-11-26-04/\">had one outage</a>, and for the most part was pretty decent.</p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"__GHOST_URL__/kubernetes-at-home/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Kubernetes at home</div><div class=\"kg-bookmark-description\">Ever wondered what it’s like running kubernetes at home? This post tries to answer that</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"__GHOST_URL__/content/images/size/w256h256/2020/06/favicon.png\" alt=\"\"><span class=\"kg-bookmark-author\">breadNET</span><span class=\"kg-bookmark-publisher\">Bradley Stannard</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"__GHOST_URL__/content/images/2023/11/cluster-top-1.jpg\" alt=\"\"></div></a></figure><p>However, there were massive reliability issues when I moved to my new address. Namely, the IP address all changed for some reason, and the certificates also expired. I decided it was time to do something about it</p><p></p><h1 id=\"homelab-v2\">Homelab v2</h1><p>Homelab V2 is, for all intensive purposes, RAID but with computers. </p><p>The lab is built off of a stack of Dell 3040 SFF PC's with 8gb ram each, and 128GB SSD's. </p><p>Currently there are 6 of these, with 5 running, and one as a spare.</p><!--kg-card-begin: markdown--><table>\n<thead>\n<tr>\n<th>Use</th>\n<th>Count</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Kubernetes</td>\n<td>4</td>\n</tr>\n<tr>\n<td>Database</td>\n<td>1</td>\n</tr>\n</tbody>\n</table>\n<!--kg-card-end: markdown--><p>As you can deduct from the table above, I am still running Kubernetes. But this is where it changes. I am now running Talos, a full OS rendition of Kubernetes designed to be lightweight and it's all configured from a file. You can get cool tools like <code>talhelper</code> that help you template out config and generate node config off a single central file.</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2025/09/image-1.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"1240\" srcset=\"__GHOST_URL__/content/images/size/w600/2025/09/image-1.png 600w, __GHOST_URL__/content/images/size/w1000/2025/09/image-1.png 1000w, __GHOST_URL__/content/images/size/w1600/2025/09/image-1.png 1600w, __GHOST_URL__/content/images/size/w2400/2025/09/image-1.png 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Unlike K3s which is installed on Ubuntu as a binary, Talos is the operating system and also the Kubernetes. </p><p>I wont go too much in to the Kubernetes setup details here, as I plan to write another blog post about this where I touch one some more interesting details</p><!--kg-card-begin: markdown--><blockquote>\n<p>Todo: Write blog post on Talos cluster!</p>\n</blockquote>\n<!--kg-card-end: markdown--><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2025/09/image-2.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1070\" height=\"164\" srcset=\"__GHOST_URL__/content/images/size/w600/2025/09/image-2.png 600w, __GHOST_URL__/content/images/size/w1000/2025/09/image-2.png 1000w, __GHOST_URL__/content/images/2025/09/image-2.png 1070w\" sizes=\"(min-width: 720px) 720px\"></figure><p>I've got 3 nodes in this cluster. You're probably thinking \"hang on, there's 4 nodes in the table above, but only 3 here?\" and you're right.</p><p>The 4th node is a single node cluster called <code>utils</code> that runs some <em>utility</em> apps. (I touch on these apps later)</p><p>The last node is just an Ubuntu host running <code>mariadb</code> as of current, however some apps require Postgres so I need to get that installed.</p><h2 id=\"what-apps-are-you-running\">What apps are you running</h2><p>Currently I am running</p><ul><li>Mealie</li><li>Gatus</li><li>Matrix server (blog post soon...)</li><li>Pocket-id (does all my OIDC auth)</li><li>Hound (<a href=\"https://github.com/hound-search/hound\">Git repo search</a>)</li><li>Atlantis (Terraform PR automation)</li><li>Postfix Mail forwarder (Allows me to setup internal apps to send mail without having to deal with per application authentication. It's authenticated in one place)</li></ul><h2 id=\"what-apps-are-planned\">What apps are planned</h2><ul><li>Jellyfin</li><li><em>Linux ISO</em> Downloading software</li><li>Pinchflat</li><li>ErsatxTV</li><li>Airtrail</li><li>Kanboard.org OR <a href=\"https://vikunja.io\">Vikunja</a></li><li>Karakeep (<a href=\"https://docs.karakeep.app\">Previously Hoarder</a>)</li><li>Paperless-ngx</li><li><a href=\"https://github.com/CorentinTh/it-tools\">IT Tools</a></li><li>changedetection.io</li><li>Snipe-IT (Used to host this, just need to revive it from the dead)</li><li><a href=\"https://wallosapp.com\">Wallos</a></li><li>Wazuh</li><li>Umami</li><li>Matomo (Need to revive from the dead...)</li><li>Terraform registry Cace (Using NGINX) to speed up local Atlantis</li><li>Lube Logger (<a href=\"https://lubelogger.com\">It's for cars...</a>)</li><li><a href=\"https://donetick.com\">Donetick</a></li><li><a href=\"https://freshrss.org/index.html\">FreshRSS</a></li></ul><h2 id=\"where-are-you-storing-everything\">Where are you storing everything</h2><p>I was able to pick up a Synology nas on Ebay for around £800 (Yes, that went down <strong>very</strong> well with the Partner) and serving iSCSI LUN's over to the Talos nodes</p><h2 id=\"closing-notes\">Closing notes</h2><p>This is a real great starting point for the lab. Everything is fresh and new, and I am doing it right the first time. Keep an eye out on the site for new posts</p>","comment_id":"68cb40b7b1baac19a80760b6","plaintext":"It's like the original Home lab but this time, not quite as bad.\n\nHome labs are very personal, what works for me wont work for you. What you like, I may dislike. Homelab V2.\n\nThe whole idea behind why Homelab V2 has come about, is because one of the services I host, Mealie, crashed and burnt so badly that we were having to go back to using cook books and digging through the backup JSON to find recipes.\n\nThe new lab is designed to score high on the SOAP scale (significant other acceptance parameters) - that is to say, my partner finds it simple to consume services and treats it like it's a Saas product.\n\nIt's designed to be immutable, git native and cheap as possible to run, with as little maintenance required.\n\n\nBrief overview of the homelab's I've had\n\n\n\nIt started in my parents house where I used to have a 48U rack\n\nI then migrated most of my things to SaaS offerings because I had to sell the servers, which was a sad day. Luckily they were able to go to an old friend of mine from School who would put them to good use\n\nI moved around a lot between moving out of my parents house and then when I got my own apartment, which meant I couldn't have anything permanent setup.\n\nFinally when I landed in Berkshire, I had means to actually setup something somewhat reliable. I present, k3s at home\n\nThis cluster was great, it only really had one outage, and for the most part was pretty decent.\n\nKubernetes at homeEver wondered what it’s like running kubernetes at home? This post tries to answer thatbreadNETBradley Stannard\n\nHowever, there were massive reliability issues when I moved to my new address. Namely, the IP address all changed for some reason, and the certificates also expired. I decided it was time to do something about it\n\n\n\n\nHomelab v2\n\nHomelab V2 is, for all intensive purposes, RAID but with computers.\n\nThe lab is built off of a stack of Dell 3040 SFF PC's with 8gb ram each, and 128GB SSD's.\n\nCurrently there are 6 of these, with 5 running, and one as a spare.\n\n\n\n\nUse\nCount\n\n\n\n\nKubernetes\n4\n\n\nDatabase\n1\n\n\n\n\n\nAs you can deduct from the table above, I am still running Kubernetes. But this is where it changes. I am now running Talos, a full OS rendition of Kubernetes designed to be lightweight and it's all configured from a file. You can get cool tools like talhelper that help you template out config and generate node config off a single central file.\n\nUnlike K3s which is installed on Ubuntu as a binary, Talos is the operating system and also the Kubernetes.\n\nI wont go too much in to the Kubernetes setup details here, as I plan to write another blog post about this where I touch one some more interesting details\n\n\n\n\nTodo: Write blog post on Talos cluster!\n\n\n\nI've got 3 nodes in this cluster. You're probably thinking \"hang on, there's 4 nodes in the table above, but only 3 here?\" and you're right.\n\nThe 4th node is a single node cluster called utils that runs some utility apps. (I touch on these apps later)\n\nThe last node is just an Ubuntu host running mariadb as of current, however some apps require Postgres so I need to get that installed.\n\n\nWhat apps are you running\n\nCurrently I am running\n\n * Mealie\n * Gatus\n * Matrix server (blog post soon...)\n * Pocket-id (does all my OIDC auth)\n * Hound (Git repo search)\n * Atlantis (Terraform PR automation)\n * Postfix Mail forwarder (Allows me to setup internal apps to send mail without having to deal with per application authentication. It's authenticated in one place)\n\n\nWhat apps are planned\n\n * Jellyfin\n * Linux ISO Downloading software\n * Pinchflat\n * ErsatxTV\n * Airtrail\n * Kanboard.org OR Vikunja\n * Karakeep (Previously Hoarder)\n * Paperless-ngx\n * IT Tools\n * changedetection.io\n * Snipe-IT (Used to host this, just need to revive it from the dead)\n * Wallos\n * Wazuh\n * Umami\n * Matomo (Need to revive from the dead...)\n * Terraform registry Cace (Using NGINX) to speed up local Atlantis\n * Lube Logger (It's for cars...)\n * Donetick\n * FreshRSS\n\n\nWhere are you storing everything\n\nI was able to pick up a Synology nas on Ebay for around £800 (Yes, that went down very well with the Partner) and serving iSCSI LUN's over to the Talos nodes\n\n\nClosing notes\n\nThis is a real great starting point for the lab. Everything is fresh and new, and I am doing it right the first time. Keep an eye out on the site for new posts","feature_image":"__GHOST_URL__/content/images/2025/09/IMG_2012.JPG","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"all","created_at":"2025-09-17T23:13:59.000Z","updated_at":"2025-10-15T09:15:34.000Z","published_at":"2025-09-18T00:41:33.000Z","custom_excerpt":"breadNET Lab is getting built back better than ever, based on Talos. Some new apps are lined up to get deployed!","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null}],"posts_authors":[{"id":"6425f3bc192c0c150413d5b8","post_id":"6425f3bc192c0c150413d58b","author_id":"1","sort_order":0},{"id":"6425f3bc192c0c150413d5bb","post_id":"6425f3bc192c0c150413d58c","author_id":"1","sort_order":0},{"id":"6425f3bd192c0c150413d5bf","post_id":"6425f3bc192c0c150413d58d","author_id":"1","sort_order":0},{"id":"6425f3bd192c0c150413d5c2","post_id":"6425f3bc192c0c150413d58e","author_id":"1","sort_order":0},{"id":"6425f3bd192c0c150413d5c5","post_id":"6425f3bc192c0c150413d58f","author_id":"1","sort_order":0},{"id":"6425f3bd192c0c150413d5ca","post_id":"6425f3bc192c0c150413d590","author_id":"1","sort_order":0},{"id":"6425f3bd192c0c150413d5cd","post_id":"6425f3bc192c0c150413d591","author_id":"1","sort_order":0},{"id":"6425f3bd192c0c150413d5cf","post_id":"6425f3bc192c0c150413d592","author_id":"1","sort_order":0},{"id":"6425f3bd192c0c150413d5d3","post_id":"6425f3bc192c0c150413d593","author_id":"1","sort_order":0},{"id":"6425f3bd192c0c150413d5d9","post_id":"6425f3bc192c0c150413d594","author_id":"1","sort_order":0},{"id":"6425f3bd192c0c150413d5dc","post_id":"6425f3bc192c0c150413d595","author_id":"1","sort_order":0},{"id":"6425f3bd192c0c150413d5de","post_id":"6425f3bc192c0c150413d596","author_id":"1","sort_order":0},{"id":"6425f3be192c0c150413d5e6","post_id":"6425f3bc192c0c150413d597","author_id":"1","sort_order":0},{"id":"6425f3be192c0c150413d5ec","post_id":"6425f3bc192c0c150413d598","author_id":"1","sort_order":0},{"id":"6425f3be192c0c150413d5ef","post_id":"6425f3bc192c0c150413d599","author_id":"1","sort_order":0},{"id":"6425f3be192c0c150413d5f1","post_id":"6425f3bc192c0c150413d59a","author_id":"1","sort_order":0},{"id":"6425f3be192c0c150413d5f5","post_id":"6425f3bc192c0c150413d59b","author_id":"1","sort_order":0},{"id":"6425f3be192c0c150413d5fd","post_id":"6425f3bc192c0c150413d59c","author_id":"1","sort_order":0},{"id":"6425f3be192c0c150413d603","post_id":"6425f3bc192c0c150413d59d","author_id":"1","sort_order":0},{"id":"6425f3bf192c0c150413d60c","post_id":"6425f3bc192c0c150413d59e","author_id":"1","sort_order":0},{"id":"6425f3bf192c0c150413d617","post_id":"6425f3bc192c0c150413d59f","author_id":"1","sort_order":0},{"id":"6425f3bf192c0c150413d620","post_id":"6425f3bc192c0c150413d5a0","author_id":"1","sort_order":0},{"id":"6425f3bf192c0c150413d622","post_id":"6425f3bc192c0c150413d5a1","author_id":"1","sort_order":0},{"id":"6425f3bf192c0c150413d629","post_id":"6425f3bc192c0c150413d5a2","author_id":"1","sort_order":0},{"id":"6425f3bf192c0c150413d633","post_id":"6425f3bc192c0c150413d5a3","author_id":"1","sort_order":0},{"id":"6425f3bf192c0c150413d63a","post_id":"6425f3bc192c0c150413d5a4","author_id":"1","sort_order":0},{"id":"6425f3bf192c0c150413d63f","post_id":"6425f3bc192c0c150413d5a5","author_id":"1","sort_order":0},{"id":"6425f3bf192c0c150413d651","post_id":"6425f3bc192c0c150413d5a6","author_id":"1","sort_order":0},{"id":"6425f3bf192c0c150413d659","post_id":"6425f3bc192c0c150413d5a7","author_id":"1","sort_order":0},{"id":"6425f3bf192c0c150413d660","post_id":"6425f3bc192c0c150413d5a8","author_id":"1","sort_order":0},{"id":"6425f3c0192c0c150413d667","post_id":"6425f3bc192c0c150413d5a9","author_id":"1","sort_order":0},{"id":"6425f3c0192c0c150413d66c","post_id":"6425f3bc192c0c150413d5aa","author_id":"1","sort_order":0},{"id":"6425f3c0192c0c150413d671","post_id":"6425f3bc192c0c150413d5ab","author_id":"1","sort_order":0},{"id":"6425f3c0192c0c150413d677","post_id":"6425f3bc192c0c150413d5ac","author_id":"1","sort_order":0},{"id":"6425f3c0192c0c150413d67b","post_id":"6425f3bc192c0c150413d5ad","author_id":"1","sort_order":0},{"id":"6425f3c0192c0c150413d67f","post_id":"6425f3bc192c0c150413d5ae","author_id":"1","sort_order":0},{"id":"6425f3c0192c0c150413d686","post_id":"6425f3bc192c0c150413d5af","author_id":"1","sort_order":0},{"id":"6425f3c0192c0c150413d689","post_id":"6425f3bc192c0c150413d5b0","author_id":"1","sort_order":0},{"id":"6425f3c0192c0c150413d691","post_id":"6425f3bc192c0c150413d5b1","author_id":"1","sort_order":0},{"id":"6425f3c1192c0c150413d696","post_id":"6425f3bc192c0c150413d5b2","author_id":"1","sort_order":0},{"id":"6425f3c1192c0c150413d69b","post_id":"6425f3bc192c0c150413d5b3","author_id":"1","sort_order":0},{"id":"6425f3c1192c0c150413d6a0","post_id":"6425f3bc192c0c150413d5b4","author_id":"1","sort_order":0},{"id":"6425f3c1192c0c150413d6a6","post_id":"6425f3bc192c0c150413d5b5","author_id":"1","sort_order":0},{"id":"6427175b5da07d2d3382d212","post_id":"6427175a5da07d2d3382d211","author_id":"1","sort_order":0},{"id":"64405506c9cae386d9c53b56","post_id":"64405506c9cae386d9c53b55","author_id":"1","sort_order":0},{"id":"64b33499c9cae386d9c53c24","post_id":"64b33499c9cae386d9c53c23","author_id":"1","sort_order":0},{"id":"64b33a2ec9cae386d9c53c97","post_id":"64b33a2ec9cae386d9c53c96","author_id":"1","sort_order":0},{"id":"651d81d5c9cae386d9c53d11","post_id":"651d81d5c9cae386d9c53d10","author_id":"1","sort_order":0},{"id":"652c1969c9cae386d9c53d72","post_id":"652c1969c9cae386d9c53d71","author_id":"1","sort_order":0},{"id":"655cc600c9cae386d9c53e93","post_id":"655cc600c9cae386d9c53e92","author_id":"1","sort_order":0},{"id":"658b587ee72f4aacb9d7eb07","post_id":"658b587ee72f4aacb9d7eb06","author_id":"1","sort_order":0},{"id":"6706c971e72f4aacb9d7ec80","post_id":"6706c971e72f4aacb9d7ec7f","author_id":"1","sort_order":0},{"id":"67769f2ee72f4aacb9d7ecbd","post_id":"67769f2ee72f4aacb9d7ecbc","author_id":"1","sort_order":0},{"id":"68cb40b7b1baac19a80760b7","post_id":"68cb40b7b1baac19a80760b6","author_id":"1","sort_order":0}],"posts_meta":[{"id":"6425f3bd192c0c150413d5d4","post_id":"6425f3bc192c0c150413d593","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"How to homelab","meta_description":"In this, we will look in to building your homelab, where to get parts and what to install as well as what you can do","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null,"email_only":0},{"id":"6425f3be192c0c150413d5e7","post_id":"6425f3bc192c0c150413d597","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"breadNET Cloud Migration","meta_description":"How did I move all my servers to the cloud? Ansible, automation and CI/CD","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null,"email_only":0},{"id":"6425f3be192c0c150413d5f6","post_id":"6425f3bc192c0c150413d59b","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Cloud security and how it affects you","meta_description":"Thinking of moving to the cloud? You should read this to ensure you're as secure as possible","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null,"email_only":0},{"id":"6425f3be192c0c150413d5fe","post_id":"6425f3bc192c0c150413d59c","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Terraform for OVH and Openstack","meta_description":"Having issues working with Terraform with either OVH and Openstack? Well, I think I solved your issues. ","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null,"email_only":0},{"id":"6425f3bf192c0c150413d62a","post_id":"6425f3bc192c0c150413d5a2","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"DNS using terraform","meta_description":"Need to get codefresh to deploy your DNS records using terraform cloud for back end state and git for source control? Well me too!","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null,"email_only":0},{"id":"6425f3bf192c0c150413d640","post_id":"6425f3bc192c0c150413d5a5","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Zero trust and ways forward  |  breadNET","meta_description":"Are you looking at implimenting zerotrust in to your company? Or just trying to get a better understanding on what it is? I deep dive in to my setup.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null,"email_only":0},{"id":"6425f3c0192c0c150413d67c","post_id":"6425f3bc192c0c150413d5ad","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Migrating from BookStack to Mkdocs","meta_description":"Read how and why I moved away from bookstack to mkdocs","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null,"email_only":0},{"id":"642717775da07d2d3382d217","post_id":"6427175a5da07d2d3382d211","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":"Photo by <a href=\"https://unsplash.com/@joser0337?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit\">José Ramos</a> / <a href=\"https://unsplash.com/?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit\">Unsplash</a>","email_only":0},{"id":"6440551ac9cae386d9c53b5b","post_id":"64405506c9cae386d9c53b55","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null,"email_only":0},{"id":"64b33974c9cae386d9c53c88","post_id":"64b33499c9cae386d9c53c23","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null,"email_only":0},{"id":"64b33a79c9cae386d9c53ca0","post_id":"64b33a2ec9cae386d9c53c96","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null,"email_only":0},{"id":"651d820fc9cae386d9c53d16","post_id":"651d81d5c9cae386d9c53d10","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null,"email_only":0},{"id":"652c625ac9cae386d9c53e7b","post_id":"652c1969c9cae386d9c53d71","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null,"email_only":0},{"id":"655cebd8c9cae386d9c54015","post_id":"655cc600c9cae386d9c53e92","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":"My Kubernetes Cluster","email_only":0},{"id":"658b8452e72f4aacb9d7ec3d","post_id":"658b587ee72f4aacb9d7eb06","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null,"email_only":0},{"id":"6706ca6ae72f4aacb9d7ec89","post_id":"6706c971e72f4aacb9d7ec7f","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null,"email_only":0},{"id":"6776f87ce72f4aacb9d7ed86","post_id":"67769f2ee72f4aacb9d7ecbc","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null,"email_only":0},{"id":"68cb427eb1baac19a80760be","post_id":"68cb40b7b1baac19a80760b6","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":"The new home lab, yes it's in the washing room again.","email_only":0}],"posts_products":[{"id":"642606c8192c0c150413d6d5","post_id":"6425f3bc192c0c150413d5b5","product_id":"6425f076192c0c150413d32d","sort_order":0},{"id":"642606c8192c0c150413d6d6","post_id":"6425f3bc192c0c150413d5b5","product_id":"6425f076192c0c150413d32e","sort_order":1},{"id":"642717775da07d2d3382d215","post_id":"6427175a5da07d2d3382d211","product_id":"6425f076192c0c150413d32d","sort_order":0},{"id":"642717775da07d2d3382d216","post_id":"6427175a5da07d2d3382d211","product_id":"6425f076192c0c150413d32e","sort_order":1},{"id":"6434a3c1c9cae386d9c53b49","post_id":"6425f3bc192c0c150413d5b4","product_id":"6425f076192c0c150413d32d","sort_order":0},{"id":"6434a3c1c9cae386d9c53b4a","post_id":"6425f3bc192c0c150413d5b4","product_id":"6425f076192c0c150413d32e","sort_order":1},{"id":"644054dac9cae386d9c53b50","post_id":"6425f3bc192c0c150413d5b3","product_id":"6425f076192c0c150413d32d","sort_order":0},{"id":"644054dac9cae386d9c53b51","post_id":"6425f3bc192c0c150413d5b3","product_id":"6425f076192c0c150413d32e","sort_order":1},{"id":"6440551ac9cae386d9c53b59","post_id":"64405506c9cae386d9c53b55","product_id":"6425f076192c0c150413d32d","sort_order":0},{"id":"6440551ac9cae386d9c53b5a","post_id":"64405506c9cae386d9c53b55","product_id":"6425f076192c0c150413d32e","sort_order":1},{"id":"64a06df1c9cae386d9c53c16","post_id":"6425f3bc192c0c150413d5a1","product_id":"6425f076192c0c150413d32d","sort_order":0},{"id":"64a06df1c9cae386d9c53c17","post_id":"6425f3bc192c0c150413d5a1","product_id":"6425f076192c0c150413d32e","sort_order":1},{"id":"64a06ecec9cae386d9c53c1b","post_id":"6425f3bc192c0c150413d5b0","product_id":"6425f076192c0c150413d32d","sort_order":0},{"id":"64a06ecec9cae386d9c53c1c","post_id":"6425f3bc192c0c150413d5b0","product_id":"6425f076192c0c150413d32e","sort_order":1},{"id":"64b334c2c9cae386d9c53c27","post_id":"64b33499c9cae386d9c53c23","product_id":"6425f076192c0c150413d32d","sort_order":0},{"id":"64b334c2c9cae386d9c53c28","post_id":"64b33499c9cae386d9c53c23","product_id":"6425f076192c0c150413d32e","sort_order":1},{"id":"64b33a45c9cae386d9c53c9a","post_id":"64b33a2ec9cae386d9c53c96","product_id":"6425f076192c0c150413d32d","sort_order":0},{"id":"64b33a45c9cae386d9c53c9b","post_id":"64b33a2ec9cae386d9c53c96","product_id":"6425f076192c0c150413d32e","sort_order":1},{"id":"651d820fc9cae386d9c53d14","post_id":"651d81d5c9cae386d9c53d10","product_id":"6425f076192c0c150413d32d","sort_order":0},{"id":"651d820fc9cae386d9c53d15","post_id":"651d81d5c9cae386d9c53d10","product_id":"6425f076192c0c150413d32e","sort_order":1},{"id":"652c1b30c9cae386d9c53d75","post_id":"652c1969c9cae386d9c53d71","product_id":"6425f076192c0c150413d32d","sort_order":0},{"id":"652c1b30c9cae386d9c53d76","post_id":"652c1969c9cae386d9c53d71","product_id":"6425f076192c0c150413d32e","sort_order":1},{"id":"655cc701c9cae386d9c53e96","post_id":"655cc600c9cae386d9c53e92","product_id":"6425f076192c0c150413d32d","sort_order":0},{"id":"655cc701c9cae386d9c53e97","post_id":"655cc600c9cae386d9c53e92","product_id":"6425f076192c0c150413d32e","sort_order":1},{"id":"6560b0a8c9cae386d9c5403c","post_id":"6425f3bc192c0c150413d590","product_id":"6425f076192c0c150413d32d","sort_order":0},{"id":"6560b0a8c9cae386d9c5403d","post_id":"6425f3bc192c0c150413d590","product_id":"6425f076192c0c150413d32e","sort_order":1},{"id":"6560b11cc9cae386d9c54041","post_id":"6425f3bc192c0c150413d58c","product_id":"6425f076192c0c150413d32d","sort_order":0},{"id":"6560b11cc9cae386d9c54042","post_id":"6425f3bc192c0c150413d58c","product_id":"6425f076192c0c150413d32e","sort_order":1},{"id":"6560b35bc9cae386d9c54046","post_id":"6425f3bc192c0c150413d5ad","product_id":"6425f076192c0c150413d32d","sort_order":0},{"id":"6560b35bc9cae386d9c54047","post_id":"6425f3bc192c0c150413d5ad","product_id":"6425f076192c0c150413d32e","sort_order":1},{"id":"658b5897e72f4aacb9d7eb0a","post_id":"658b587ee72f4aacb9d7eb06","product_id":"6425f076192c0c150413d32d","sort_order":0},{"id":"658b5897e72f4aacb9d7eb0b","post_id":"658b587ee72f4aacb9d7eb06","product_id":"6425f076192c0c150413d32e","sort_order":1},{"id":"6706c976e72f4aacb9d7ec83","post_id":"6706c971e72f4aacb9d7ec7f","product_id":"6425f076192c0c150413d32d","sort_order":0},{"id":"6706c976e72f4aacb9d7ec84","post_id":"6706c971e72f4aacb9d7ec7f","product_id":"6425f076192c0c150413d32e","sort_order":1},{"id":"67769f40e72f4aacb9d7ecc0","post_id":"67769f2ee72f4aacb9d7ecbc","product_id":"6425f076192c0c150413d32d","sort_order":0},{"id":"67769f40e72f4aacb9d7ecc1","post_id":"67769f2ee72f4aacb9d7ecbc","product_id":"6425f076192c0c150413d32e","sort_order":1},{"id":"67ed2e4ee72f4aacb9d7ed9f","post_id":"6425f3bc192c0c150413d58e","product_id":"6425f076192c0c150413d32d","sort_order":0},{"id":"67ed2e4ee72f4aacb9d7eda0","post_id":"6425f3bc192c0c150413d58e","product_id":"6425f076192c0c150413d32e","sort_order":1},{"id":"68cb40bdb1baac19a80760ba","post_id":"68cb40b7b1baac19a80760b6","product_id":"6425f076192c0c150413d32d","sort_order":0},{"id":"68cb40bdb1baac19a80760bb","post_id":"68cb40b7b1baac19a80760b6","product_id":"6425f076192c0c150413d32e","sort_order":1}],"posts_tags":[{"id":"6425f3bc192c0c150413d5b6","post_id":"6425f3bc192c0c150413d58b","tag_id":"6425f3bb192c0c150413d550","sort_order":0},{"id":"6425f3bc192c0c150413d5b7","post_id":"6425f3bc192c0c150413d58b","tag_id":"6425f3bb192c0c150413d588","sort_order":1},{"id":"6425f3bc192c0c150413d5b9","post_id":"6425f3bc192c0c150413d58c","tag_id":"6425f3bb192c0c150413d551","sort_order":0},{"id":"6425f3bc192c0c150413d5ba","post_id":"6425f3bc192c0c150413d58c","tag_id":"6425f3bb192c0c150413d588","sort_order":1},{"id":"6425f3bd192c0c150413d5bc","post_id":"6425f3bc192c0c150413d58d","tag_id":"6425f3bb192c0c150413d550","sort_order":0},{"id":"6425f3bd192c0c150413d5bd","post_id":"6425f3bc192c0c150413d58d","tag_id":"6425f3bb192c0c150413d552","sort_order":1},{"id":"6425f3bd192c0c150413d5be","post_id":"6425f3bc192c0c150413d58d","tag_id":"6425f3bb192c0c150413d588","sort_order":2},{"id":"6425f3bd192c0c150413d5c0","post_id":"6425f3bc192c0c150413d58e","tag_id":"6425f3bb192c0c150413d551","sort_order":0},{"id":"6425f3bd192c0c150413d5c1","post_id":"6425f3bc192c0c150413d58e","tag_id":"6425f3bb192c0c150413d588","sort_order":1},{"id":"6425f3bd192c0c150413d5c3","post_id":"6425f3bc192c0c150413d58f","tag_id":"6425f3bb192c0c150413d554","sort_order":0},{"id":"6425f3bd192c0c150413d5c4","post_id":"6425f3bc192c0c150413d58f","tag_id":"6425f3bb192c0c150413d588","sort_order":1},{"id":"6425f3bd192c0c150413d5c6","post_id":"6425f3bc192c0c150413d590","tag_id":"6425f3bb192c0c150413d550","sort_order":0},{"id":"6425f3bd192c0c150413d5c7","post_id":"6425f3bc192c0c150413d590","tag_id":"6425f3bb192c0c150413d552","sort_order":1},{"id":"6425f3bd192c0c150413d5c8","post_id":"6425f3bc192c0c150413d590","tag_id":"6425f3bb192c0c150413d553","sort_order":2},{"id":"6425f3bd192c0c150413d5c9","post_id":"6425f3bc192c0c150413d590","tag_id":"6425f3bb192c0c150413d588","sort_order":3},{"id":"6425f3bd192c0c150413d5cb","post_id":"6425f3bc192c0c150413d591","tag_id":"6425f3bb192c0c150413d554","sort_order":0},{"id":"6425f3bd192c0c150413d5cc","post_id":"6425f3bc192c0c150413d591","tag_id":"6425f3bb192c0c150413d588","sort_order":1},{"id":"6425f3bd192c0c150413d5ce","post_id":"6425f3bc192c0c150413d592","tag_id":"6425f3bb192c0c150413d588","sort_order":0},{"id":"6425f3bd192c0c150413d5d0","post_id":"6425f3bc192c0c150413d593","tag_id":"6425f3bb192c0c150413d555","sort_order":0},{"id":"6425f3bd192c0c150413d5d1","post_id":"6425f3bc192c0c150413d593","tag_id":"6425f3bb192c0c150413d554","sort_order":1},{"id":"6425f3bd192c0c150413d5d2","post_id":"6425f3bc192c0c150413d593","tag_id":"6425f3bb192c0c150413d588","sort_order":2},{"id":"6425f3bd192c0c150413d5d5","post_id":"6425f3bc192c0c150413d594","tag_id":"6425f3bb192c0c150413d554","sort_order":0},{"id":"6425f3bd192c0c150413d5d6","post_id":"6425f3bc192c0c150413d594","tag_id":"6425f3bb192c0c150413d556","sort_order":1},{"id":"6425f3bd192c0c150413d5d7","post_id":"6425f3bc192c0c150413d594","tag_id":"6425f3bb192c0c150413d557","sort_order":2},{"id":"6425f3bd192c0c150413d5d8","post_id":"6425f3bc192c0c150413d594","tag_id":"6425f3bb192c0c150413d588","sort_order":3},{"id":"6425f3bd192c0c150413d5da","post_id":"6425f3bc192c0c150413d595","tag_id":"6425f3bb192c0c150413d554","sort_order":0},{"id":"6425f3bd192c0c150413d5db","post_id":"6425f3bc192c0c150413d595","tag_id":"6425f3bb192c0c150413d588","sort_order":1},{"id":"6425f3bd192c0c150413d5dd","post_id":"6425f3bc192c0c150413d596","tag_id":"6425f3bb192c0c150413d588","sort_order":0},{"id":"6425f3be192c0c150413d5df","post_id":"6425f3bc192c0c150413d597","tag_id":"6425f3bb192c0c150413d558","sort_order":0},{"id":"6425f3be192c0c150413d5e0","post_id":"6425f3bc192c0c150413d597","tag_id":"6425f3bb192c0c150413d559","sort_order":1},{"id":"6425f3be192c0c150413d5e1","post_id":"6425f3bc192c0c150413d597","tag_id":"6425f3bb192c0c150413d55a","sort_order":2},{"id":"6425f3be192c0c150413d5e2","post_id":"6425f3bc192c0c150413d597","tag_id":"6425f3bb192c0c150413d55f","sort_order":3},{"id":"6425f3be192c0c150413d5e3","post_id":"6425f3bc192c0c150413d597","tag_id":"6425f3bb192c0c150413d55c","sort_order":4},{"id":"6425f3be192c0c150413d5e4","post_id":"6425f3bc192c0c150413d597","tag_id":"6425f3bb192c0c150413d560","sort_order":5},{"id":"6425f3be192c0c150413d5e5","post_id":"6425f3bc192c0c150413d597","tag_id":"6425f3bb192c0c150413d588","sort_order":6},{"id":"6425f3be192c0c150413d5e8","post_id":"6425f3bc192c0c150413d598","tag_id":"6425f3bb192c0c150413d55a","sort_order":0},{"id":"6425f3be192c0c150413d5e9","post_id":"6425f3bc192c0c150413d598","tag_id":"6425f3bb192c0c150413d558","sort_order":1},{"id":"6425f3be192c0c150413d5ea","post_id":"6425f3bc192c0c150413d598","tag_id":"6425f3bb192c0c150413d554","sort_order":2},{"id":"6425f3be192c0c150413d5eb","post_id":"6425f3bc192c0c150413d598","tag_id":"6425f3bb192c0c150413d588","sort_order":3},{"id":"6425f3be192c0c150413d5ed","post_id":"6425f3bc192c0c150413d599","tag_id":"6425f3bb192c0c150413d557","sort_order":0},{"id":"6425f3be192c0c150413d5ee","post_id":"6425f3bc192c0c150413d599","tag_id":"6425f3bb192c0c150413d588","sort_order":1},{"id":"6425f3be192c0c150413d5f0","post_id":"6425f3bc192c0c150413d59a","tag_id":"6425f3bb192c0c150413d588","sort_order":0},{"id":"6425f3be192c0c150413d5f2","post_id":"6425f3bc192c0c150413d59b","tag_id":"6425f3bb192c0c150413d558","sort_order":0},{"id":"6425f3be192c0c150413d5f3","post_id":"6425f3bc192c0c150413d59b","tag_id":"6425f3bb192c0c150413d55b","sort_order":1},{"id":"6425f3be192c0c150413d5f4","post_id":"6425f3bc192c0c150413d59b","tag_id":"6425f3bb192c0c150413d588","sort_order":2},{"id":"6425f3be192c0c150413d5f7","post_id":"6425f3bc192c0c150413d59c","tag_id":"6425f3bb192c0c150413d55c","sort_order":0},{"id":"6425f3be192c0c150413d5f8","post_id":"6425f3bc192c0c150413d59c","tag_id":"6425f3bb192c0c150413d55a","sort_order":1},{"id":"6425f3be192c0c150413d5f9","post_id":"6425f3bc192c0c150413d59c","tag_id":"6425f3bb192c0c150413d55d","sort_order":2},{"id":"6425f3be192c0c150413d5fa","post_id":"6425f3bc192c0c150413d59c","tag_id":"6425f3bb192c0c150413d558","sort_order":3},{"id":"6425f3be192c0c150413d5fb","post_id":"6425f3bc192c0c150413d59c","tag_id":"6425f3bb192c0c150413d554","sort_order":4},{"id":"6425f3be192c0c150413d5fc","post_id":"6425f3bc192c0c150413d59c","tag_id":"6425f3bb192c0c150413d588","sort_order":5},{"id":"6425f3be192c0c150413d5ff","post_id":"6425f3bc192c0c150413d59d","tag_id":"6425f3bb192c0c150413d55e","sort_order":0},{"id":"6425f3be192c0c150413d600","post_id":"6425f3bc192c0c150413d59d","tag_id":"6425f3bb192c0c150413d554","sort_order":1},{"id":"6425f3be192c0c150413d601","post_id":"6425f3bc192c0c150413d59d","tag_id":"6425f3bb192c0c150413d55b","sort_order":2},{"id":"6425f3be192c0c150413d602","post_id":"6425f3bc192c0c150413d59d","tag_id":"6425f3bb192c0c150413d588","sort_order":3},{"id":"6425f3bf192c0c150413d604","post_id":"6425f3bc192c0c150413d59e","tag_id":"6425f3bb192c0c150413d55a","sort_order":0},{"id":"6425f3bf192c0c150413d605","post_id":"6425f3bc192c0c150413d59e","tag_id":"6425f3bb192c0c150413d55c","sort_order":1},{"id":"6425f3bf192c0c150413d606","post_id":"6425f3bc192c0c150413d59e","tag_id":"6425f3bb192c0c150413d559","sort_order":2},{"id":"6425f3bf192c0c150413d607","post_id":"6425f3bc192c0c150413d59e","tag_id":"6425f3bb192c0c150413d558","sort_order":3},{"id":"6425f3bf192c0c150413d608","post_id":"6425f3bc192c0c150413d59e","tag_id":"6425f3bb192c0c150413d560","sort_order":4},{"id":"6425f3bf192c0c150413d609","post_id":"6425f3bc192c0c150413d59e","tag_id":"6425f3bb192c0c150413d55d","sort_order":5},{"id":"6425f3bf192c0c150413d60a","post_id":"6425f3bc192c0c150413d59e","tag_id":"6425f3bb192c0c150413d555","sort_order":6},{"id":"6425f3bf192c0c150413d60b","post_id":"6425f3bc192c0c150413d59e","tag_id":"6425f3bb192c0c150413d588","sort_order":7},{"id":"6425f3bf192c0c150413d60d","post_id":"6425f3bc192c0c150413d59f","tag_id":"6425f3bb192c0c150413d561","sort_order":0},{"id":"6425f3bf192c0c150413d60e","post_id":"6425f3bc192c0c150413d59f","tag_id":"6425f3bb192c0c150413d55f","sort_order":1},{"id":"6425f3bf192c0c150413d60f","post_id":"6425f3bc192c0c150413d59f","tag_id":"6425f3bb192c0c150413d559","sort_order":2},{"id":"6425f3bf192c0c150413d610","post_id":"6425f3bc192c0c150413d59f","tag_id":"6425f3bb192c0c150413d55a","sort_order":3},{"id":"6425f3bf192c0c150413d611","post_id":"6425f3bc192c0c150413d59f","tag_id":"6425f3bb192c0c150413d558","sort_order":4},{"id":"6425f3bf192c0c150413d612","post_id":"6425f3bc192c0c150413d59f","tag_id":"6425f3bb192c0c150413d55d","sort_order":5},{"id":"6425f3bf192c0c150413d613","post_id":"6425f3bc192c0c150413d59f","tag_id":"6425f3bb192c0c150413d555","sort_order":6},{"id":"6425f3bf192c0c150413d614","post_id":"6425f3bc192c0c150413d59f","tag_id":"6425f3bb192c0c150413d560","sort_order":7},{"id":"6425f3bf192c0c150413d615","post_id":"6425f3bc192c0c150413d59f","tag_id":"6425f3bb192c0c150413d551","sort_order":8},{"id":"6425f3bf192c0c150413d616","post_id":"6425f3bc192c0c150413d59f","tag_id":"6425f3bb192c0c150413d588","sort_order":9},{"id":"6425f3bf192c0c150413d618","post_id":"6425f3bc192c0c150413d5a0","tag_id":"6425f3bb192c0c150413d558","sort_order":0},{"id":"6425f3bf192c0c150413d619","post_id":"6425f3bc192c0c150413d5a0","tag_id":"6425f3bb192c0c150413d55b","sort_order":1},{"id":"6425f3bf192c0c150413d61a","post_id":"6425f3bc192c0c150413d5a0","tag_id":"6425f3bb192c0c150413d551","sort_order":2},{"id":"6425f3bf192c0c150413d61b","post_id":"6425f3bc192c0c150413d5a0","tag_id":"6425f3bb192c0c150413d559","sort_order":3},{"id":"6425f3bf192c0c150413d61c","post_id":"6425f3bc192c0c150413d5a0","tag_id":"6425f3bb192c0c150413d55c","sort_order":4},{"id":"6425f3bf192c0c150413d61d","post_id":"6425f3bc192c0c150413d5a0","tag_id":"6425f3bb192c0c150413d55f","sort_order":5},{"id":"6425f3bf192c0c150413d61e","post_id":"6425f3bc192c0c150413d5a0","tag_id":"6425f3bb192c0c150413d55a","sort_order":6},{"id":"6425f3bf192c0c150413d61f","post_id":"6425f3bc192c0c150413d5a0","tag_id":"6425f3bb192c0c150413d588","sort_order":7},{"id":"6425f3bf192c0c150413d621","post_id":"6425f3bc192c0c150413d5a1","tag_id":"6425f3bb192c0c150413d588","sort_order":0},{"id":"6425f3bf192c0c150413d623","post_id":"6425f3bc192c0c150413d5a2","tag_id":"6425f3bb192c0c150413d55f","sort_order":0},{"id":"6425f3bf192c0c150413d624","post_id":"6425f3bc192c0c150413d5a2","tag_id":"6425f3bb192c0c150413d55c","sort_order":1},{"id":"6425f3bf192c0c150413d625","post_id":"6425f3bc192c0c150413d5a2","tag_id":"6425f3bb192c0c150413d562","sort_order":2},{"id":"6425f3bf192c0c150413d626","post_id":"6425f3bc192c0c150413d5a2","tag_id":"6425f3bb192c0c150413d563","sort_order":3},{"id":"6425f3bf192c0c150413d627","post_id":"6425f3bc192c0c150413d5a2","tag_id":"6425f3bb192c0c150413d558","sort_order":4},{"id":"6425f3bf192c0c150413d628","post_id":"6425f3bc192c0c150413d5a2","tag_id":"6425f3bb192c0c150413d588","sort_order":5},{"id":"6425f3bf192c0c150413d62b","post_id":"6425f3bc192c0c150413d5a3","tag_id":"6425f3bb192c0c150413d564","sort_order":0},{"id":"6425f3bf192c0c150413d62c","post_id":"6425f3bc192c0c150413d5a3","tag_id":"6425f3bb192c0c150413d558","sort_order":1},{"id":"6425f3bf192c0c150413d62d","post_id":"6425f3bc192c0c150413d5a3","tag_id":"6425f3bb192c0c150413d565","sort_order":2},{"id":"6425f3bf192c0c150413d62e","post_id":"6425f3bc192c0c150413d5a3","tag_id":"6425f3bb192c0c150413d566","sort_order":3},{"id":"6425f3bf192c0c150413d62f","post_id":"6425f3bc192c0c150413d5a3","tag_id":"6425f3bb192c0c150413d567","sort_order":4},{"id":"6425f3bf192c0c150413d630","post_id":"6425f3bc192c0c150413d5a3","tag_id":"6425f3bb192c0c150413d568","sort_order":5},{"id":"6425f3bf192c0c150413d631","post_id":"6425f3bc192c0c150413d5a3","tag_id":"6425f3bb192c0c150413d557","sort_order":6},{"id":"6425f3bf192c0c150413d632","post_id":"6425f3bc192c0c150413d5a3","tag_id":"6425f3bb192c0c150413d588","sort_order":7},{"id":"6425f3bf192c0c150413d634","post_id":"6425f3bc192c0c150413d5a4","tag_id":"6425f3bb192c0c150413d569","sort_order":0},{"id":"6425f3bf192c0c150413d635","post_id":"6425f3bc192c0c150413d5a4","tag_id":"6425f3bb192c0c150413d558","sort_order":1},{"id":"6425f3bf192c0c150413d636","post_id":"6425f3bc192c0c150413d5a4","tag_id":"6425f3bb192c0c150413d562","sort_order":2},{"id":"6425f3bf192c0c150413d637","post_id":"6425f3bc192c0c150413d5a4","tag_id":"6425f3bb192c0c150413d551","sort_order":3},{"id":"6425f3bf192c0c150413d638","post_id":"6425f3bc192c0c150413d5a4","tag_id":"6425f3bb192c0c150413d55b","sort_order":4},{"id":"6425f3bf192c0c150413d639","post_id":"6425f3bc192c0c150413d5a4","tag_id":"6425f3bb192c0c150413d588","sort_order":5},{"id":"6425f3bf192c0c150413d63b","post_id":"6425f3bc192c0c150413d5a5","tag_id":"6425f3bb192c0c150413d55b","sort_order":0},{"id":"6425f3bf192c0c150413d63c","post_id":"6425f3bc192c0c150413d5a5","tag_id":"6425f3bb192c0c150413d56a","sort_order":1},{"id":"6425f3bf192c0c150413d63d","post_id":"6425f3bc192c0c150413d5a5","tag_id":"6425f3bb192c0c150413d557","sort_order":2},{"id":"6425f3bf192c0c150413d63e","post_id":"6425f3bc192c0c150413d5a5","tag_id":"6425f3bb192c0c150413d588","sort_order":3},{"id":"6425f3bf192c0c150413d641","post_id":"6425f3bc192c0c150413d5a6","tag_id":"6425f3bb192c0c150413d55f","sort_order":0},{"id":"6425f3bf192c0c150413d642","post_id":"6425f3bc192c0c150413d5a6","tag_id":"6425f3bb192c0c150413d56b","sort_order":1},{"id":"6425f3bf192c0c150413d643","post_id":"6425f3bc192c0c150413d5a6","tag_id":"6425f3bb192c0c150413d55c","sort_order":2},{"id":"6425f3bf192c0c150413d644","post_id":"6425f3bc192c0c150413d5a6","tag_id":"6425f3bb192c0c150413d56c","sort_order":3},{"id":"6425f3bf192c0c150413d645","post_id":"6425f3bc192c0c150413d5a6","tag_id":"6425f3bb192c0c150413d56d","sort_order":4},{"id":"6425f3bf192c0c150413d646","post_id":"6425f3bc192c0c150413d5a6","tag_id":"6425f3bb192c0c150413d56e","sort_order":5},{"id":"6425f3bf192c0c150413d647","post_id":"6425f3bc192c0c150413d5a6","tag_id":"6425f3bb192c0c150413d55a","sort_order":6},{"id":"6425f3bf192c0c150413d648","post_id":"6425f3bc192c0c150413d5a6","tag_id":"6425f3bb192c0c150413d568","sort_order":7},{"id":"6425f3bf192c0c150413d649","post_id":"6425f3bc192c0c150413d5a6","tag_id":"6425f3bb192c0c150413d563","sort_order":8},{"id":"6425f3bf192c0c150413d64a","post_id":"6425f3bc192c0c150413d5a6","tag_id":"6425f3bb192c0c150413d550","sort_order":9},{"id":"6425f3bf192c0c150413d64b","post_id":"6425f3bc192c0c150413d5a6","tag_id":"6425f3bb192c0c150413d552","sort_order":10},{"id":"6425f3bf192c0c150413d64c","post_id":"6425f3bc192c0c150413d5a6","tag_id":"6425f3bb192c0c150413d55d","sort_order":11},{"id":"6425f3bf192c0c150413d64d","post_id":"6425f3bc192c0c150413d5a6","tag_id":"6425f3bb192c0c150413d54e","sort_order":12},{"id":"6425f3bf192c0c150413d64e","post_id":"6425f3bc192c0c150413d5a6","tag_id":"6425f3bb192c0c150413d554","sort_order":13},{"id":"6425f3bf192c0c150413d64f","post_id":"6425f3bc192c0c150413d5a6","tag_id":"6425f3bb192c0c150413d557","sort_order":14},{"id":"6425f3bf192c0c150413d650","post_id":"6425f3bc192c0c150413d5a6","tag_id":"6425f3bb192c0c150413d588","sort_order":15},{"id":"6425f3bf192c0c150413d652","post_id":"6425f3bc192c0c150413d5a7","tag_id":"6425f3bb192c0c150413d558","sort_order":0},{"id":"6425f3bf192c0c150413d653","post_id":"6425f3bc192c0c150413d5a7","tag_id":"6425f3bb192c0c150413d55c","sort_order":1},{"id":"6425f3bf192c0c150413d654","post_id":"6425f3bc192c0c150413d5a7","tag_id":"6425f3bb192c0c150413d560","sort_order":2},{"id":"6425f3bf192c0c150413d655","post_id":"6425f3bc192c0c150413d5a7","tag_id":"6425f3bb192c0c150413d56f","sort_order":3},{"id":"6425f3bf192c0c150413d656","post_id":"6425f3bc192c0c150413d5a7","tag_id":"6425f3bb192c0c150413d570","sort_order":4},{"id":"6425f3bf192c0c150413d657","post_id":"6425f3bc192c0c150413d5a7","tag_id":"6425f3bb192c0c150413d55a","sort_order":5},{"id":"6425f3bf192c0c150413d658","post_id":"6425f3bc192c0c150413d5a7","tag_id":"6425f3bb192c0c150413d588","sort_order":6},{"id":"6425f3bf192c0c150413d65a","post_id":"6425f3bc192c0c150413d5a8","tag_id":"6425f3bb192c0c150413d571","sort_order":0},{"id":"6425f3bf192c0c150413d65b","post_id":"6425f3bc192c0c150413d5a8","tag_id":"6425f3bb192c0c150413d54f","sort_order":1},{"id":"6425f3bf192c0c150413d65c","post_id":"6425f3bc192c0c150413d5a8","tag_id":"6425f3bb192c0c150413d561","sort_order":2},{"id":"6425f3bf192c0c150413d65d","post_id":"6425f3bc192c0c150413d5a8","tag_id":"6425f3bb192c0c150413d572","sort_order":3},{"id":"6425f3bf192c0c150413d65e","post_id":"6425f3bc192c0c150413d5a8","tag_id":"6425f3bb192c0c150413d56c","sort_order":4},{"id":"6425f3bf192c0c150413d65f","post_id":"6425f3bc192c0c150413d5a8","tag_id":"6425f3bb192c0c150413d588","sort_order":5},{"id":"6425f3c0192c0c150413d661","post_id":"6425f3bc192c0c150413d5a9","tag_id":"6425f3bb192c0c150413d55c","sort_order":0},{"id":"6425f3c0192c0c150413d662","post_id":"6425f3bc192c0c150413d5a9","tag_id":"6425f3bb192c0c150413d573","sort_order":1},{"id":"6425f3c0192c0c150413d663","post_id":"6425f3bc192c0c150413d5a9","tag_id":"6425f3bb192c0c150413d574","sort_order":2},{"id":"6425f3c0192c0c150413d664","post_id":"6425f3bc192c0c150413d5a9","tag_id":"6425f3bb192c0c150413d575","sort_order":3},{"id":"6425f3c0192c0c150413d665","post_id":"6425f3bc192c0c150413d5a9","tag_id":"6425f3bb192c0c150413d576","sort_order":4},{"id":"6425f3c0192c0c150413d666","post_id":"6425f3bc192c0c150413d5a9","tag_id":"6425f3bb192c0c150413d588","sort_order":5},{"id":"6425f3c0192c0c150413d668","post_id":"6425f3bc192c0c150413d5aa","tag_id":"6425f3bb192c0c150413d577","sort_order":0},{"id":"6425f3c0192c0c150413d669","post_id":"6425f3bc192c0c150413d5aa","tag_id":"6425f3bb192c0c150413d578","sort_order":1},{"id":"6425f3c0192c0c150413d66a","post_id":"6425f3bc192c0c150413d5aa","tag_id":"6425f3bb192c0c150413d55a","sort_order":2},{"id":"6425f3c0192c0c150413d66b","post_id":"6425f3bc192c0c150413d5aa","tag_id":"6425f3bb192c0c150413d588","sort_order":3},{"id":"6425f3c0192c0c150413d66d","post_id":"6425f3bc192c0c150413d5ab","tag_id":"6425f3bb192c0c150413d579","sort_order":0},{"id":"6425f3c0192c0c150413d66e","post_id":"6425f3bc192c0c150413d5ab","tag_id":"6425f3bb192c0c150413d57a","sort_order":1},{"id":"6425f3c0192c0c150413d66f","post_id":"6425f3bc192c0c150413d5ab","tag_id":"6425f3bb192c0c150413d57b","sort_order":2},{"id":"6425f3c0192c0c150413d670","post_id":"6425f3bc192c0c150413d5ab","tag_id":"6425f3bb192c0c150413d588","sort_order":3},{"id":"6425f3c0192c0c150413d672","post_id":"6425f3bc192c0c150413d5ac","tag_id":"6425f3bb192c0c150413d57c","sort_order":0},{"id":"6425f3c0192c0c150413d673","post_id":"6425f3bc192c0c150413d5ac","tag_id":"6425f3bb192c0c150413d57d","sort_order":1},{"id":"6425f3c0192c0c150413d674","post_id":"6425f3bc192c0c150413d5ac","tag_id":"6425f3bb192c0c150413d57e","sort_order":2},{"id":"6425f3c0192c0c150413d675","post_id":"6425f3bc192c0c150413d5ac","tag_id":"6425f3bb192c0c150413d56c","sort_order":3},{"id":"6425f3c0192c0c150413d676","post_id":"6425f3bc192c0c150413d5ac","tag_id":"6425f3bb192c0c150413d588","sort_order":4},{"id":"6425f3c0192c0c150413d678","post_id":"6425f3bc192c0c150413d5ad","tag_id":"6425f3bb192c0c150413d57f","sort_order":0},{"id":"6425f3c0192c0c150413d679","post_id":"6425f3bc192c0c150413d5ad","tag_id":"6425f3bb192c0c150413d580","sort_order":1},{"id":"6425f3c0192c0c150413d67a","post_id":"6425f3bc192c0c150413d5ad","tag_id":"6425f3bb192c0c150413d588","sort_order":2},{"id":"6425f3c0192c0c150413d67d","post_id":"6425f3bc192c0c150413d5ae","tag_id":"6425f3bb192c0c150413d57a","sort_order":0},{"id":"6425f3c0192c0c150413d67e","post_id":"6425f3bc192c0c150413d5ae","tag_id":"6425f3bb192c0c150413d588","sort_order":1},{"id":"6425f3c0192c0c150413d680","post_id":"6425f3bc192c0c150413d5af","tag_id":"6425f3bb192c0c150413d552","sort_order":0},{"id":"6425f3c0192c0c150413d681","post_id":"6425f3bc192c0c150413d5af","tag_id":"6425f3bb192c0c150413d55f","sort_order":1},{"id":"6425f3c0192c0c150413d682","post_id":"6425f3bc192c0c150413d5af","tag_id":"6425f3bb192c0c150413d56b","sort_order":2},{"id":"6425f3c0192c0c150413d683","post_id":"6425f3bc192c0c150413d5af","tag_id":"6425f3bb192c0c150413d580","sort_order":3},{"id":"6425f3c0192c0c150413d684","post_id":"6425f3bc192c0c150413d5af","tag_id":"6425f3bb192c0c150413d56d","sort_order":4},{"id":"6425f3c0192c0c150413d685","post_id":"6425f3bc192c0c150413d5af","tag_id":"6425f3bb192c0c150413d588","sort_order":5},{"id":"6425f3c0192c0c150413d687","post_id":"6425f3bc192c0c150413d5b0","tag_id":"6425f3bb192c0c150413d581","sort_order":0},{"id":"6425f3c0192c0c150413d688","post_id":"6425f3bc192c0c150413d5b0","tag_id":"6425f3bb192c0c150413d588","sort_order":1},{"id":"6425f3c0192c0c150413d68a","post_id":"6425f3bc192c0c150413d5b1","tag_id":"6425f3bb192c0c150413d582","sort_order":0},{"id":"6425f3c0192c0c150413d68b","post_id":"6425f3bc192c0c150413d5b1","tag_id":"6425f3bb192c0c150413d583","sort_order":1},{"id":"6425f3c0192c0c150413d68c","post_id":"6425f3bc192c0c150413d5b1","tag_id":"6425f3bb192c0c150413d552","sort_order":2},{"id":"6425f3c0192c0c150413d68d","post_id":"6425f3bc192c0c150413d5b1","tag_id":"6425f3bb192c0c150413d56b","sort_order":3},{"id":"6425f3c0192c0c150413d68e","post_id":"6425f3bc192c0c150413d5b1","tag_id":"6425f3bb192c0c150413d55f","sort_order":4},{"id":"6425f3c0192c0c150413d68f","post_id":"6425f3bc192c0c150413d5b1","tag_id":"6425f3bb192c0c150413d56c","sort_order":5},{"id":"6425f3c0192c0c150413d690","post_id":"6425f3bc192c0c150413d5b1","tag_id":"6425f3bb192c0c150413d588","sort_order":6},{"id":"6425f3c1192c0c150413d692","post_id":"6425f3bc192c0c150413d5b2","tag_id":"6425f3bb192c0c150413d56b","sort_order":0},{"id":"6425f3c1192c0c150413d693","post_id":"6425f3bc192c0c150413d5b2","tag_id":"6425f3bb192c0c150413d55f","sort_order":1},{"id":"6425f3c1192c0c150413d694","post_id":"6425f3bc192c0c150413d5b2","tag_id":"6425f3bb192c0c150413d55c","sort_order":2},{"id":"6425f3c1192c0c150413d695","post_id":"6425f3bc192c0c150413d5b2","tag_id":"6425f3bb192c0c150413d588","sort_order":3},{"id":"6425f3c1192c0c150413d697","post_id":"6425f3bc192c0c150413d5b3","tag_id":"6425f3bb192c0c150413d584","sort_order":0},{"id":"6425f3c1192c0c150413d698","post_id":"6425f3bc192c0c150413d5b3","tag_id":"6425f3bb192c0c150413d55c","sort_order":1},{"id":"6425f3c1192c0c150413d699","post_id":"6425f3bc192c0c150413d5b3","tag_id":"6425f3bb192c0c150413d55f","sort_order":2},{"id":"6425f3c1192c0c150413d69a","post_id":"6425f3bc192c0c150413d5b3","tag_id":"6425f3bb192c0c150413d588","sort_order":3},{"id":"6425f3c1192c0c150413d69c","post_id":"6425f3bc192c0c150413d5b4","tag_id":"6425f3bb192c0c150413d56c","sort_order":0},{"id":"6425f3c1192c0c150413d69d","post_id":"6425f3bc192c0c150413d5b4","tag_id":"6425f3bb192c0c150413d552","sort_order":1},{"id":"6425f3c1192c0c150413d69e","post_id":"6425f3bc192c0c150413d5b4","tag_id":"6425f3bb192c0c150413d585","sort_order":2},{"id":"6425f3c1192c0c150413d69f","post_id":"6425f3bc192c0c150413d5b4","tag_id":"6425f3bb192c0c150413d588","sort_order":3},{"id":"6425f3c1192c0c150413d6a1","post_id":"6425f3bc192c0c150413d5b5","tag_id":"6425f3bb192c0c150413d57a","sort_order":0},{"id":"6425f3c1192c0c150413d6a2","post_id":"6425f3bc192c0c150413d5b5","tag_id":"6425f3bb192c0c150413d586","sort_order":1},{"id":"6425f3c1192c0c150413d6a3","post_id":"6425f3bc192c0c150413d5b5","tag_id":"6425f3bb192c0c150413d587","sort_order":2},{"id":"6425f3c1192c0c150413d6a4","post_id":"6425f3bc192c0c150413d5b5","tag_id":"6425f3bb192c0c150413d56c","sort_order":3},{"id":"6425f3c1192c0c150413d6a5","post_id":"6425f3bc192c0c150413d5b5","tag_id":"6425f3bb192c0c150413d588","sort_order":4},{"id":"642724e75da07d2d3382d2c9","post_id":"6427175a5da07d2d3382d211","tag_id":"6425f3bb192c0c150413d558","sort_order":0},{"id":"642724e75da07d2d3382d2ca","post_id":"6427175a5da07d2d3382d211","tag_id":"6425f3bb192c0c150413d563","sort_order":1},{"id":"642724e75da07d2d3382d2cb","post_id":"6427175a5da07d2d3382d211","tag_id":"6425f3bb192c0c150413d55c","sort_order":2},{"id":"642724e75da07d2d3382d2cc","post_id":"6427175a5da07d2d3382d211","tag_id":"6425f3bb192c0c150413d584","sort_order":3},{"id":"64408071c9cae386d9c53bf7","post_id":"64405506c9cae386d9c53b55","tag_id":"6425f3bb192c0c150413d552","sort_order":0},{"id":"64408071c9cae386d9c53bf8","post_id":"64405506c9cae386d9c53b55","tag_id":"64408071c9cae386d9c53bf6","sort_order":1},{"id":"64b33974c9cae386d9c53c85","post_id":"64b33499c9cae386d9c53c23","tag_id":"6425f3bb192c0c150413d56c","sort_order":0},{"id":"64b33974c9cae386d9c53c86","post_id":"64b33499c9cae386d9c53c23","tag_id":"6425f3bb192c0c150413d552","sort_order":1},{"id":"64b33974c9cae386d9c53c87","post_id":"64b33499c9cae386d9c53c23","tag_id":"64408071c9cae386d9c53bf6","sort_order":2},{"id":"64b3412fc9cae386d9c53cfb","post_id":"64b33a2ec9cae386d9c53c96","tag_id":"64b3412fc9cae386d9c53cfa","sort_order":0},{"id":"64b3412fc9cae386d9c53cfc","post_id":"64b33a2ec9cae386d9c53c96","tag_id":"6425f3bb192c0c150413d584","sort_order":1},{"id":"64b3412fc9cae386d9c53cfd","post_id":"64b33a2ec9cae386d9c53c96","tag_id":"6425f3bb192c0c150413d55c","sort_order":2},{"id":"651d898ac9cae386d9c53d6a","post_id":"651d81d5c9cae386d9c53d10","tag_id":"6425f3bb192c0c150413d55c","sort_order":0},{"id":"651d898ac9cae386d9c53d6b","post_id":"651d81d5c9cae386d9c53d10","tag_id":"6425f3bb192c0c150413d56c","sort_order":1},{"id":"651d898ac9cae386d9c53d6c","post_id":"651d81d5c9cae386d9c53d10","tag_id":"6425f3bb192c0c150413d564","sort_order":2},{"id":"652c62afc9cae386d9c53e82","post_id":"652c1969c9cae386d9c53d71","tag_id":"652c62afc9cae386d9c53e7f","sort_order":0},{"id":"652c62afc9cae386d9c53e83","post_id":"652c1969c9cae386d9c53d71","tag_id":"6425f3bb192c0c150413d56c","sort_order":1},{"id":"652c62afc9cae386d9c53e84","post_id":"652c1969c9cae386d9c53d71","tag_id":"652c62afc9cae386d9c53e80","sort_order":2},{"id":"652c62afc9cae386d9c53e85","post_id":"652c1969c9cae386d9c53d71","tag_id":"6425f3bb192c0c150413d587","sort_order":3},{"id":"652c62afc9cae386d9c53e86","post_id":"652c1969c9cae386d9c53d71","tag_id":"6425f3bb192c0c150413d57e","sort_order":4},{"id":"652c62afc9cae386d9c53e87","post_id":"652c1969c9cae386d9c53d71","tag_id":"652c62afc9cae386d9c53e81","sort_order":5},{"id":"655cec0dc9cae386d9c5402b","post_id":"655cc600c9cae386d9c53e92","tag_id":"6425f3bb192c0c150413d587","sort_order":0},{"id":"655cec0dc9cae386d9c5402c","post_id":"655cc600c9cae386d9c53e92","tag_id":"6425f3bb192c0c150413d586","sort_order":1},{"id":"655cec0dc9cae386d9c5402d","post_id":"655cc600c9cae386d9c53e92","tag_id":"6425f3bb192c0c150413d555","sort_order":2},{"id":"658b83ece72f4aacb9d7ec3b","post_id":"658b587ee72f4aacb9d7eb06","tag_id":"6425f3bb192c0c150413d581","sort_order":0},{"id":"68cb5535b1baac19a807618b","post_id":"68cb40b7b1baac19a80760b6","tag_id":"68cb5535b1baac19a807618a","sort_order":0}],"products":[{"id":"6425f076192c0c150413d32d","name":"Free","slug":"free","active":1,"welcome_page_url":null,"visibility":"public","trial_days":0,"description":null,"type":"free","currency":null,"monthly_price":null,"yearly_price":null,"created_at":"2023-03-30T20:26:30.000Z","updated_at":"2023-03-30T20:26:30.000Z","monthly_price_id":null,"yearly_price_id":null},{"id":"6425f076192c0c150413d32e","name":"bradley@breadnet.co.uk","slug":"default-product","active":1,"welcome_page_url":null,"visibility":"public","trial_days":0,"description":null,"type":"paid","currency":"USD","monthly_price":500,"yearly_price":5000,"created_at":"2023-03-30T20:26:30.000Z","updated_at":"2023-03-30T20:31:10.000Z","monthly_price_id":null,"yearly_price_id":null}],"products_benefits":[],"roles":[{"id":"6425f076192c0c150413d322","name":"Administrator","description":"Administrators","created_at":"2023-03-30T20:26:30.000Z","updated_at":"2023-03-30T20:26:30.000Z"},{"id":"6425f076192c0c150413d323","name":"Editor","description":"Editors","created_at":"2023-03-30T20:26:30.000Z","updated_at":"2023-03-30T20:26:30.000Z"},{"id":"6425f076192c0c150413d324","name":"Author","description":"Authors","created_at":"2023-03-30T20:26:30.000Z","updated_at":"2023-03-30T20:26:30.000Z"},{"id":"6425f076192c0c150413d325","name":"Contributor","description":"Contributors","created_at":"2023-03-30T20:26:30.000Z","updated_at":"2023-03-30T20:26:30.000Z"},{"id":"6425f076192c0c150413d326","name":"Owner","description":"Blog Owner","created_at":"2023-03-30T20:26:30.000Z","updated_at":"2023-03-30T20:26:30.000Z"},{"id":"6425f076192c0c150413d327","name":"Admin Integration","description":"External Apps","created_at":"2023-03-30T20:26:30.000Z","updated_at":"2023-03-30T20:26:30.000Z"},{"id":"6425f076192c0c150413d328","name":"Ghost Explore Integration","description":"Internal Integration for the Ghost Explore directory","created_at":"2023-03-30T20:26:30.000Z","updated_at":"2023-03-30T20:26:30.000Z"},{"id":"6425f076192c0c150413d329","name":"Self-Serve Migration Integration","description":"Internal Integration for the Self-Serve migration tool","created_at":"2023-03-30T20:26:30.000Z","updated_at":"2023-03-30T20:26:30.000Z"},{"id":"6425f076192c0c150413d32a","name":"DB Backup Integration","description":"Internal DB Backup Client","created_at":"2023-03-30T20:26:30.000Z","updated_at":"2023-03-30T20:26:30.000Z"},{"id":"6425f076192c0c150413d32b","name":"Scheduler Integration","description":"Internal Scheduler Client","created_at":"2023-03-30T20:26:30.000Z","updated_at":"2023-03-30T20:26:30.000Z"}],"roles_users":[{"id":"6425f076192c0c150413d32c","role_id":"6425f076192c0c150413d326","user_id":"1"},{"id":"6425f3bb192c0c150413d545","role_id":"6425f076192c0c150413d324","user_id":"6425f3ba192c0c150413d544"}],"settings":[{"id":"6425f07b192c0c150413d4d6","group":"core","key":"last_mentions_report_email_timestamp","value":null,"type":"number","flags":null,"created_at":"2023-03-30T20:26:35.000Z","updated_at":"2023-03-30T20:26:35.000Z"},{"id":"6425f07b192c0c150413d4d7","group":"core","key":"db_hash","value":"0ebd0354-0d23-4420-a491-34383a416263","type":"string","flags":null,"created_at":"2023-03-30T20:26:35.000Z","updated_at":"2023-03-30T20:26:35.000Z"},{"id":"6425f07b192c0c150413d4d8","group":"core","key":"routes_hash","value":"3d180d52c663d173a6be791ef411ed01","type":"string","flags":null,"created_at":"2023-03-30T20:26:35.000Z","updated_at":"2023-03-30T20:26:39.000Z"},{"id":"6425f07b192c0c150413d4d9","group":"core","key":"next_update_check","value":"1761149416","type":"number","flags":null,"created_at":"2023-03-30T20:26:35.000Z","updated_at":"2025-10-21T16:00:17.000Z"},{"id":"6425f07b192c0c150413d4da","group":"core","key":"notifications","value":"[{\"dismissible\":true,\"location\":\"bottom\",\"status\":\"alert\",\"id\":\"fdd813d0-50f9-11ef-a473-a3f8947146bd\",\"createdAtVersion\":\"5.40.2\",\"custom\":false,\"createdAt\":\"2024-08-02T17:06:50.000Z\",\"type\":\"info\",\"top\":false,\"message\":\"Ghost <a href=\\\"https://github.com/TryGhost/Ghost/releases\\\">5.89.0</a> has been released, <a href=\\\"https://ghost.org/update/?v=5.40.2\\\">click here</a> to upgrade.\",\"seen\":false,\"addedAt\":\"2024-08-06T20:17:59.465Z\"},{\"dismissible\":true,\"location\":\"bottom\",\"status\":\"alert\",\"id\":\"6dc38fb8-484d-49a7-a331-aa8acdf4eed4\",\"createdAtVersion\":\"5.40.2\",\"custom\":true,\"createdAt\":\"2025-08-27T13:58:18.000Z\",\"type\":\"info\",\"top\":true,\"message\":\"<strong>Ghost 6.0 is now available</strong> - You are using an old version of Ghost, which means you don't have access to the latest features. <a href=\\\"https://ghost.org/changelog/6/\\\" target=\\\"_blank\\\" rel=\\\"noopener\\\">Read more!</a>\",\"seen\":true,\"addedAt\":\"2025-09-14T21:08:35.319Z\",\"seenBy\":[\"1\"]}]","type":"array","flags":null,"created_at":"2023-03-30T20:26:35.000Z","updated_at":"2025-09-17T07:37:02.000Z"},{"id":"6425f07b192c0c150413d4db","group":"core","key":"version_notifications","value":"[]","type":"array","flags":null,"created_at":"2023-03-30T20:26:35.000Z","updated_at":"2023-03-30T20:26:35.000Z"},{"id":"6425f07b192c0c150413d4dc","group":"core","key":"admin_session_secret","value":"d8c2587fdc6e20035f70ab160858d90c2729fd293903f3eba2fedf1e66a84c0c","type":"string","flags":null,"created_at":"2023-03-30T20:26:35.000Z","updated_at":"2023-03-30T20:26:35.000Z"},{"id":"6425f07b192c0c150413d4dd","group":"core","key":"theme_session_secret","value":"831428aeb07aae8220cb7200e29dd361be42c7bc4e7bee71f54f7592ad303aca","type":"string","flags":null,"created_at":"2023-03-30T20:26:35.000Z","updated_at":"2023-03-30T20:26:35.000Z"},{"id":"6425f07b192c0c150413d4de","group":"core","key":"ghost_public_key","value":"-----BEGIN RSA PUBLIC KEY-----\nMIGJAoGBAMSfUOv0giMyVfQ/2/W6Vtnnr0PPk8fTgk0BEGBYsKS0299MqoGxDpt4bHiZXGVg\nmIPhzP1ew4AtZf1vQfsJfwU8j6do6B9Euh7palxcWgGv4PqSVQGqZHTEPR3POx21JjVXNa2f\neIq4YEK9X4mn9EJvel/IkLeD+rLT/zncJN2fAgMBAAE=\n-----END RSA PUBLIC KEY-----\n","type":"string","flags":null,"created_at":"2023-03-30T20:26:35.000Z","updated_at":"2023-03-30T20:26:35.000Z"},{"id":"6425f07b192c0c150413d4df","group":"core","key":"ghost_private_key","value":"-----BEGIN RSA PRIVATE KEY-----\nMIICXAIBAAKBgQDEn1Dr9IIjMlX0P9v1ulbZ569Dz5PH04JNARBgWLCktNvfTKqBsQ6beGx4\nmVxlYJiD4cz9XsOALWX9b0H7CX8FPI+naOgfRLoe6WpcXFoBr+D6klUBqmR0xD0dzzsdtSY1\nVzWtn3iKuGBCvV+Jp/RCb3pfyJC3g/qy0/853CTdnwIDAQABAoGAZaPNmwu8MiGVGTvhorb+\nqon/qdgmFogZtWC1bHisZ5guiNIC2iQC3w9zUyxD/QvgC1ywMNHJ0fql3EqlGvYfuFVskC6D\nIxs1qz8NfexB6m/GIafS4vpNyNhHTwfjhzvYjRwdXbCL/EoqhCYIOZIh4JYcQfH8lKct69gP\nlndB5jECQQD+2mDOuhhmfxzlxHOtdkn+eq9abQhu3GTTGD0NV12e9pGMvYV7J47AAWDWM0hi\n2b490t6SYgOSd6ik+MPtpHpTAkEAxYHZW2Pkg3SlRm1VWRBPmLsVFXG88+POEE5WmZQzrQ+y\n3mKAU8I4VKE2FB3DuR2UMuMgFAXgselsSFvn6GxeBQJBAL6YqW/R42+8PutjqbOf1KTaMgxp\noLXwZAMvjHFO1niNJLth/dLl+a+FGFv5zHgJKmroKobFgLBohxqh5T30FXsCQBeyT5jGcGlR\nOm8ruPy5l0M01v++qLP+Vf/g1kUQ4e5WnthNqyXhfI6y1OEu/Qedw+UjGPsuBVpzLhm0ZSue\nrN0CQBhlV4pomJII1dIaMomMtAueJEH+EJnvnRJWEHCVxXi7nEqGVpsjXByiRzZkRxA1tFHU\n0txavcB4paguXM81Lcs=\n-----END RSA PRIVATE KEY-----\n","type":"string","flags":null,"created_at":"2023-03-30T20:26:35.000Z","updated_at":"2023-03-30T20:26:35.000Z"},{"id":"6425f07b192c0c150413d4e0","group":"core","key":"members_public_key","value":"-----BEGIN RSA PUBLIC KEY-----\nMIGJAoGBAJ0/HwSVuWEtqzksibIoLdgiu0Fggsm4VCmdu6Wp67yL8brF5IqtYZwVSCNh4xNj\nUDcT4uWyL1CN/ZAChKFLsEzm0RUBQYGuCegRqDMgPcOGVJzXD17VYQDZSL+nnt+3PM1y/kXk\nSe+MnyevgCNVaLYncdR0YQIH+vcpgIwbqJZrAgMBAAE=\n-----END RSA PUBLIC KEY-----\n","type":"string","flags":null,"created_at":"2023-03-30T20:26:35.000Z","updated_at":"2023-03-30T20:26:35.000Z"},{"id":"6425f07b192c0c150413d4e1","group":"core","key":"members_private_key","value":"-----BEGIN RSA PRIVATE KEY-----\nMIICXQIBAAKBgQCdPx8ElblhLas5LImyKC3YIrtBYILJuFQpnbulqeu8i/G6xeSKrWGcFUgj\nYeMTY1A3E+Llsi9Qjf2QAoShS7BM5tEVAUGBrgnoEagzID3DhlSc1w9e1WEA2Ui/p57ftzzN\ncv5F5EnvjJ8nr4AjVWi2J3HUdGECB/r3KYCMG6iWawIDAQABAoGAL4E9Rq5Q9enNG5W396mp\nx+0eMy8L9BMK4hOz9+VwwWJp4eodFEQzA+tuArp01oufVTRdzNTa9HQhEVuhY/UwymFAOq/o\n6r406CRPDazjRQH0gqNB6xn/Y5JxON0eOlsr+oFBjXYj6iuiiHxySAbxNJXpFYx7kVRvRm+g\n6tXpakkCQQDXAeM8aJU2fX5y0AU8V4dCSU/8BEQzUE0f3n4FDugGbNnWZjljgCguP4kaJ3WK\nzPLZAAr+deMGkqHG/VdGLf1PAkEAuzoJDP0X3wgIAnD4/p0DtRIvKJcKSmtzMW74y9VlLSEh\nRviZs7oxtdkyvCb1BQXfRbvlPV/Q+SaCRBIkFDLmJQJBAKRqaR6UyyX0KXIa6SYPuPtTnLqX\neJ/U1GV+etfbYksnih2N5mzUX+Wd6HX7nsMZnsuaYM3d0FICZ3wH12lvWbkCQH+kaVc5lXSw\nfQVtD9RGJ6ji5NO/ZXynFLm29lfA/GFwHMfN5LvrTMi1cyljRdhO0JEyMG9PS6EQ5+6Qj5hV\nuyECQQDI4UXP/Ex7O0Izru/xV/DVbVVG/zG8iracDahPODAZdpCki+c3Fg5lZYLhd0m2mb1j\ndUtVi7B0VUkrrrTmjNxa\n-----END RSA PRIVATE KEY-----\n","type":"string","flags":null,"created_at":"2023-03-30T20:26:35.000Z","updated_at":"2023-03-30T20:26:35.000Z"},{"id":"6425f07b192c0c150413d4e2","group":"core","key":"members_email_auth_secret","value":"81842d873885bbc60d90f26e159a3f7e2a30aec169a0581d81f660831f5a994e3ab9250b20a4b603ef0cc04660b9948937252b0f41d1d5eeaf5135e5e432a7ce","type":"string","flags":null,"created_at":"2023-03-30T20:26:35.000Z","updated_at":"2023-03-30T20:26:35.000Z"},{"id":"6425f07b192c0c150413d4e5","group":"site","key":"title","value":"breadNET","type":"string","flags":"PUBLIC","created_at":"2020-06-05T16:44:14.000Z","updated_at":"2020-06-05T16:45:22.000Z"},{"id":"6425f07b192c0c150413d4e6","group":"site","key":"description","value":"DevOps Engineer, tinkerer and home labber","type":"string","flags":"PUBLIC","created_at":"2020-06-05T16:44:14.000Z","updated_at":"2022-11-24T18:06:35.000Z"},{"id":"6425f07b192c0c150413d4e7","group":"site","key":"logo","value":null,"type":"string","flags":"PUBLIC","created_at":"2020-06-05T16:44:14.000Z","updated_at":"2020-06-05T17:28:12.000Z"},{"id":"6425f07b192c0c150413d4e8","group":"site","key":"cover_image","value":"__GHOST_URL__/content/images/2020/07/IMG_8577.JPG","type":"string","flags":null,"created_at":"2020-06-05T16:44:14.000Z","updated_at":"2020-07-02T14:23:23.000Z"},{"id":"6425f07b192c0c150413d4e9","group":"site","key":"icon","value":"__GHOST_URL__/content/images/2020/06/favicon.png","type":"string","flags":null,"created_at":"2020-06-05T16:44:14.000Z","updated_at":"2020-06-05T17:25:56.000Z"},{"id":"6425f07b192c0c150413d4ea","group":"site","key":"accent_color","value":"#15171A","type":"string","flags":"PUBLIC","created_at":"2020-08-12T21:10:12.000Z","updated_at":"2020-08-12T22:10:51.000Z"},{"id":"6425f07b192c0c150413d4eb","group":"site","key":"locale","value":"en","type":"string","flags":null,"created_at":"2020-08-12T21:10:12.000Z","updated_at":"2020-08-12T21:10:12.000Z"},{"id":"6425f07b192c0c150413d4ec","group":"site","key":"timezone","value":"Etc/UTC","type":"string","flags":null,"created_at":"2020-08-12T21:10:12.000Z","updated_at":"2020-08-12T21:10:12.000Z"},{"id":"6425f07b192c0c150413d4ed","group":"site","key":"codeinjection_head","value":"<!-- Cloudflare Web Analytics -->\n<script defer src='https://static.cloudflareinsights.com/beacon.min.js' data-cf-beacon='{\"token\": \"8c19c40feca54d92bca95441e2b70e4c\"}'></script>\n<!-- End Cloudflare Web Analytics -->\n\n<script defer src=\"https://umami.breadnet.co.uk/script.js\" data-website-id=\"e16eaae8-50f1-43eb-bda5-cb35881498a0\"></script>\n","type":"string","flags":null,"created_at":"2020-08-12T21:10:12.000Z","updated_at":"2025-10-19T23:06:24.000Z"},{"id":"6425f07b192c0c150413d4ee","group":"site","key":"codeinjection_foot","value":null,"type":"string","flags":null,"created_at":"2020-08-12T21:10:12.000Z","updated_at":"2020-08-12T21:10:12.000Z"},{"id":"6425f07b192c0c150413d4ef","group":"site","key":"facebook","value":null,"type":"string","flags":null,"created_at":"2020-06-05T16:44:14.000Z","updated_at":"2020-08-18T23:13:07.000Z"},{"id":"6425f07b192c0c150413d4f0","group":"site","key":"twitter","value":null,"type":"string","flags":null,"created_at":"2020-06-05T16:44:14.000Z","updated_at":"2020-08-18T23:13:07.000Z"},{"id":"6425f07b192c0c150413d4f1","group":"site","key":"navigation","value":"[{\"label\":\"Home\",\"url\":\"https://breadnet.co.uk\"},{\"label\":\"My Setup\",\"url\":\"/what-it-takes-to-run-breadnet/\"},{\"label\":\"The story\",\"url\":\"/the-story/\"}]","type":"array","flags":null,"created_at":"2020-06-05T16:44:14.000Z","updated_at":"2025-04-02T12:33:07.000Z"},{"id":"6425f07b192c0c150413d4f2","group":"site","key":"secondary_navigation","value":"[{\"label\":\"Linkedin\",\"url\":\"https://www.linkedin.com/in/bradley-stannard/\"},{\"label\":\"Documentation\",\"url\":\"https://documentation.breadnet.co.uk/?mtm_campaign=breadnet&mtm_kwd=navbar\"},{\"label\":\"Contact\",\"url\":\"mailto:webmaster@breadnet.co.uk\"},{\"label\":\"Github\",\"url\":\"https://github.com/userbradley\"}]","type":"array","flags":null,"created_at":"2020-06-05T16:44:14.000Z","updated_at":"2025-04-02T12:33:07.000Z"},{"id":"6425f07b192c0c150413d4f3","group":"site","key":"meta_title","value":null,"type":"string","flags":null,"created_at":"2020-06-05T16:44:14.000Z","updated_at":"2020-06-05T16:44:14.000Z"},{"id":"6425f07b192c0c150413d4f4","group":"site","key":"meta_description","value":"Homelab, Linux and cloud. Find something interesting ","type":"string","flags":null,"created_at":"2020-06-05T16:44:14.000Z","updated_at":"2021-12-14T01:04:22.000Z"},{"id":"6425f07b192c0c150413d4f5","group":"site","key":"og_image","value":null,"type":"string","flags":null,"created_at":"2020-06-05T16:44:14.000Z","updated_at":"2020-06-05T16:44:14.000Z"},{"id":"6425f07b192c0c150413d4f6","group":"site","key":"og_title","value":null,"type":"string","flags":null,"created_at":"2020-06-05T16:44:14.000Z","updated_at":"2020-06-05T16:44:14.000Z"},{"id":"6425f07b192c0c150413d4f7","group":"site","key":"og_description","value":null,"type":"string","flags":null,"created_at":"2020-06-05T16:44:14.000Z","updated_at":"2020-06-05T16:44:14.000Z"},{"id":"6425f07b192c0c150413d4f8","group":"site","key":"twitter_image","value":null,"type":"string","flags":null,"created_at":"2020-06-05T16:44:14.000Z","updated_at":"2020-06-05T16:44:14.000Z"},{"id":"6425f07b192c0c150413d4f9","group":"site","key":"twitter_title","value":null,"type":"string","flags":null,"created_at":"2020-06-05T16:44:14.000Z","updated_at":"2020-06-05T16:44:14.000Z"},{"id":"6425f07b192c0c150413d4fa","group":"site","key":"twitter_description","value":null,"type":"string","flags":null,"created_at":"2020-06-05T16:44:14.000Z","updated_at":"2020-06-05T16:44:14.000Z"},{"id":"6425f07b192c0c150413d4fb","group":"theme","key":"active_theme","value":"casper3","type":"string","flags":"RO","created_at":"2023-03-30T20:26:35.000Z","updated_at":"2023-03-30T20:39:55.000Z"},{"id":"6425f07b192c0c150413d4fc","group":"private","key":"is_private","value":"false","type":"boolean","flags":null,"created_at":"2023-03-30T20:26:35.000Z","updated_at":"2023-03-30T20:26:35.000Z"},{"id":"6425f07b192c0c150413d4fd","group":"private","key":"password","value":"","type":"string","flags":null,"created_at":"2023-03-30T20:26:35.000Z","updated_at":"2023-03-30T20:26:35.000Z"},{"id":"6425f07b192c0c150413d4fe","group":"private","key":"public_hash","value":"c2ea0b6b2be591bef36f4acf1d0416","type":"string","flags":null,"created_at":"2020-06-05T16:44:14.000Z","updated_at":"2020-06-05T16:44:14.000Z"},{"id":"6425f07b192c0c150413d4ff","group":"members","key":"default_content_visibility","value":"public","type":"string","flags":null,"created_at":"2020-06-05T16:44:14.000Z","updated_at":"2020-06-05T16:44:14.000Z"},{"id":"6425f07b192c0c150413d500","group":"members","key":"default_content_visibility_tiers","value":"[]","type":"array","flags":null,"created_at":"2023-03-30T20:26:35.000Z","updated_at":"2023-03-30T20:26:35.000Z"},{"id":"6425f07b192c0c150413d501","group":"members","key":"members_signup_access","value":"all","type":"string","flags":null,"created_at":"2020-08-12T22:06:11.000Z","updated_at":"2020-08-12T22:06:11.000Z"},{"id":"6425f07b192c0c150413d502","group":"members","key":"members_support_address","value":"noreply","type":"string","flags":"PUBLIC,RO","created_at":"2023-03-30T20:26:35.000Z","updated_at":"2023-03-30T20:26:35.000Z"},{"id":"6425f07b192c0c150413d505","group":"members","key":"stripe_plans","value":"[{\"name\":\"Monthly\",\"currency\":\"usd\",\"interval\":\"month\",\"amount\":0},{\"name\":\"Yearly\",\"currency\":\"usd\",\"interval\":\"year\",\"amount\":0}]","type":"array","flags":null,"created_at":"2020-08-12T22:06:11.000Z","updated_at":"2020-08-12T22:06:11.000Z"},{"id":"6425f07b192c0c150413d508","group":"members","key":"stripe_connect_livemode","value":null,"type":"boolean","flags":null,"created_at":"2020-08-12T22:06:11.000Z","updated_at":"2020-08-12T22:06:11.000Z"},{"id":"6425f07b192c0c150413d509","group":"members","key":"stripe_connect_display_name","value":null,"type":"string","flags":null,"created_at":"2020-08-12T22:06:11.000Z","updated_at":"2020-08-12T22:06:11.000Z"},{"id":"6425f07b192c0c150413d50b","group":"members","key":"members_monthly_price_id","value":null,"type":"string","flags":null,"created_at":"2023-03-30T20:26:35.000Z","updated_at":"2023-03-30T20:26:35.000Z"},{"id":"6425f07b192c0c150413d50c","group":"members","key":"members_yearly_price_id","value":null,"type":"string","flags":null,"created_at":"2023-03-30T20:26:35.000Z","updated_at":"2023-03-30T20:26:35.000Z"},{"id":"6425f07b192c0c150413d50d","group":"members","key":"members_track_sources","value":"true","type":"boolean","flags":null,"created_at":"2023-03-30T20:26:35.000Z","updated_at":"2023-03-30T20:26:35.000Z"},{"id":"6425f07b192c0c150413d50e","group":"portal","key":"portal_name","value":"true","type":"boolean","flags":null,"created_at":"2020-08-12T21:10:12.000Z","updated_at":"2020-08-12T21:10:12.000Z"},{"id":"6425f07b192c0c150413d50f","group":"portal","key":"portal_button","value":"false","type":"boolean","flags":null,"created_at":"2020-08-12T21:10:12.000Z","updated_at":"2020-08-12T21:10:12.000Z"},{"id":"6425f07b192c0c150413d510","group":"portal","key":"portal_plans","value":"[\"free\",\"monthly\",\"yearly\"]","type":"array","flags":null,"created_at":"2020-08-12T21:10:12.000Z","updated_at":"2020-08-12T22:10:51.000Z"},{"id":"6425f07b192c0c150413d511","group":"portal","key":"portal_products","value":"[]","type":"array","flags":null,"created_at":"2023-03-30T20:26:35.000Z","updated_at":"2023-03-30T20:26:35.000Z"},{"id":"6425f07b192c0c150413d512","group":"portal","key":"portal_button_style","value":"icon-and-text","type":"string","flags":null,"created_at":"2020-08-12T21:10:12.000Z","updated_at":"2020-08-12T21:10:12.000Z"},{"id":"6425f07b192c0c150413d513","group":"portal","key":"portal_button_icon","value":null,"type":"string","flags":null,"created_at":"2020-08-12T21:10:12.000Z","updated_at":"2020-08-12T21:10:12.000Z"},{"id":"6425f07b192c0c150413d514","group":"portal","key":"portal_button_signup_text","value":"Subscribe","type":"string","flags":null,"created_at":"2020-08-12T21:10:12.000Z","updated_at":"2020-08-12T21:10:12.000Z"},{"id":"6425f07b192c0c150413d515","group":"email","key":"mailgun_domain","value":"noreply.breadnet.co.uk","type":"string","flags":null,"created_at":"2020-08-12T22:06:11.000Z","updated_at":"2021-12-10T20:21:30.000Z"},{"id":"6425f07b192c0c150413d516","group":"email","key":"mailgun_api_key","value":"b204a7e0d6f125a2c0177a3fa10159e7-7005f37e-a453b108","type":"string","flags":null,"created_at":"2020-08-12T22:06:11.000Z","updated_at":"2021-12-10T20:21:30.000Z"},{"id":"6425f07b192c0c150413d517","group":"email","key":"mailgun_base_url","value":"https://api.eu.mailgun.net/v3","type":"string","flags":null,"created_at":"2020-08-12T22:06:11.000Z","updated_at":"2020-08-12T22:06:11.000Z"},{"id":"6425f07b192c0c150413d518","group":"email","key":"email_track_opens","value":"true","type":"boolean","flags":null,"created_at":"2023-03-30T20:26:35.000Z","updated_at":"2023-03-30T20:26:35.000Z"},{"id":"6425f07b192c0c150413d519","group":"email","key":"email_track_clicks","value":"true","type":"boolean","flags":null,"created_at":"2023-03-30T20:26:35.000Z","updated_at":"2023-03-30T20:26:35.000Z"},{"id":"6425f07b192c0c150413d51b","group":"amp","key":"amp","value":"true","type":"boolean","flags":null,"created_at":"2020-06-05T16:44:14.000Z","updated_at":"2020-08-16T01:36:23.000Z"},{"id":"6425f07b192c0c150413d51c","group":"amp","key":"amp_gtag_id","value":null,"type":"string","flags":null,"created_at":"2020-08-12T21:10:12.000Z","updated_at":"2020-08-12T21:10:12.000Z"},{"id":"6425f07b192c0c150413d51d","group":"firstpromoter","key":"firstpromoter","value":"false","type":"boolean","flags":null,"created_at":"2023-03-30T20:26:35.000Z","updated_at":"2023-03-30T20:26:35.000Z"},{"id":"6425f07b192c0c150413d51e","group":"firstpromoter","key":"firstpromoter_id","value":null,"type":"string","flags":null,"created_at":"2023-03-30T20:26:35.000Z","updated_at":"2023-03-30T20:26:35.000Z"},{"id":"6425f07b192c0c150413d51f","group":"labs","key":"labs","value":"{}","type":"object","flags":null,"created_at":"2020-06-05T16:44:14.000Z","updated_at":"2021-12-10T19:46:42.000Z"},{"id":"6425f07b192c0c150413d520","group":"slack","key":"slack_url","value":"","type":"string","flags":null,"created_at":"2023-03-30T20:26:35.000Z","updated_at":"2023-03-30T20:26:35.000Z"},{"id":"6425f07b192c0c150413d521","group":"slack","key":"slack_username","value":"Ghost","type":"string","flags":null,"created_at":"2020-06-05T16:44:14.000Z","updated_at":"2021-12-14T01:36:08.000Z"},{"id":"6425f07b192c0c150413d522","group":"unsplash","key":"unsplash","value":"true","type":"boolean","flags":null,"created_at":"2020-06-05T16:44:14.000Z","updated_at":"2020-06-05T16:47:08.000Z"},{"id":"6425f07b192c0c150413d523","group":"views","key":"shared_views","value":"[]","type":"array","flags":null,"created_at":"2020-08-12T21:10:12.000Z","updated_at":"2020-08-12T21:10:12.000Z"},{"id":"6425f07b192c0c150413d524","group":"editor","key":"editor_default_email_recipients","value":"visibility","type":"string","flags":null,"created_at":"2023-03-30T20:26:35.000Z","updated_at":"2023-03-30T20:26:35.000Z"},{"id":"6425f07b192c0c150413d525","group":"editor","key":"editor_default_email_recipients_filter","value":"all","type":"string","flags":null,"created_at":"2023-03-30T20:26:35.000Z","updated_at":"2023-03-30T20:26:35.000Z"},{"id":"6425f07b192c0c150413d526","group":"comments","key":"comments_enabled","value":"off","type":"string","flags":null,"created_at":"2023-03-30T20:26:35.000Z","updated_at":"2023-03-30T20:26:35.000Z"},{"id":"6425f07b192c0c150413d527","group":"analytics","key":"outbound_link_tagging","value":"true","type":"boolean","flags":null,"created_at":"2023-03-30T20:26:35.000Z","updated_at":"2023-03-30T20:26:35.000Z"}],"snippets":[{"id":"655cedaac9cae386d9c54036","name":"Button","mobiledoc":"{\"version\":\"0.3.2\",\"atoms\":[],\"cards\":[[\"button\",{\"alignment\":\"center\",\"buttonText\":\"Skip to the photos\",\"buttonUrl\":\"#talk-is-cheap-show-me-the-cluster\"}]],\"markups\":[],\"sections\":[[10,0]]}","created_at":"2023-11-21T17:49:30.000Z","updated_at":"2023-11-21T17:49:30.000Z"}],"stripe_prices":[],"stripe_products":[],"tags":[{"id":"6425f076192c0c150413d330","name":"News","slug":"news","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2023-03-30T20:26:30.000Z","updated_at":"2023-03-30T20:26:30.000Z"},{"id":"6425f3bb192c0c150413d54e","name":"Getting Started","slug":"getting-started","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2020-06-05T16:44:12.000Z","updated_at":"2020-06-05T16:44:12.000Z"},{"id":"6425f3bb192c0c150413d54f","name":"Who Am I","slug":"whoa-am-i","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2020-06-05T16:55:41.000Z","updated_at":"2020-06-05T17:39:58.000Z"},{"id":"6425f3bb192c0c150413d550","name":"cool stuff","slug":"cool-stuff","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2020-06-05T17:20:44.000Z","updated_at":"2020-06-05T17:20:44.000Z"},{"id":"6425f3bb192c0c150413d551","name":"servers","slug":"servers","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2020-06-05T17:36:06.000Z","updated_at":"2020-06-05T17:36:06.000Z"},{"id":"6425f3bb192c0c150413d552","name":"docker","slug":"docker","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2020-06-06T15:06:48.000Z","updated_at":"2020-06-06T15:06:48.000Z"},{"id":"6425f3bb192c0c150413d553","name":"media","slug":"media","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2020-06-06T21:03:43.000Z","updated_at":"2020-06-06T21:03:43.000Z"},{"id":"6425f3bb192c0c150413d554","name":"How To","slug":"how-to","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2020-06-09T17:56:41.000Z","updated_at":"2020-06-09T17:56:41.000Z"},{"id":"6425f3bb192c0c150413d555","name":"homelab","slug":"homelab","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2020-06-12T16:30:37.000Z","updated_at":"2020-06-12T16:30:37.000Z"},{"id":"6425f3bb192c0c150413d556","name":"zerotier","slug":"zerotier","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2020-06-12T18:07:38.000Z","updated_at":"2020-06-12T18:07:38.000Z"},{"id":"6425f3bb192c0c150413d557","name":"ssh","slug":"ssh","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2020-06-12T18:07:38.000Z","updated_at":"2020-06-12T18:07:38.000Z"},{"id":"6425f3bb192c0c150413d558","name":"cloud","slug":"cloud","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2020-07-22T11:51:48.000Z","updated_at":"2020-07-22T11:51:48.000Z"},{"id":"6425f3bb192c0c150413d559","name":"ansible","slug":"ansible","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2020-07-22T11:51:48.000Z","updated_at":"2020-07-22T11:51:48.000Z"},{"id":"6425f3bb192c0c150413d55a","name":"automation","slug":"automation","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2020-07-22T11:51:48.000Z","updated_at":"2020-07-22T11:51:48.000Z"},{"id":"6425f3bb192c0c150413d55b","name":"security","slug":"security","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2020-09-26T13:27:07.000Z","updated_at":"2020-09-26T13:27:07.000Z"},{"id":"6425f3bb192c0c150413d55c","name":"terraform","slug":"terraform","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2020-12-26T16:37:20.000Z","updated_at":"2020-12-26T16:37:20.000Z"},{"id":"6425f3bb192c0c150413d55d","name":"git","slug":"git","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2020-12-26T16:37:20.000Z","updated_at":"2020-12-26T16:37:20.000Z"},{"id":"6425f3bb192c0c150413d55e","name":"Authentication","slug":"authentication","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2021-01-09T17:26:37.000Z","updated_at":"2021-01-09T17:26:37.000Z"},{"id":"6425f3bb192c0c150413d55f","name":"cicd","slug":"cicd","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2021-04-01T12:55:05.000Z","updated_at":"2021-04-01T12:55:05.000Z"},{"id":"6425f3bb192c0c150413d560","name":"ovh","slug":"ovh","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2021-04-01T12:55:05.000Z","updated_at":"2021-04-01T12:55:05.000Z"},{"id":"6425f3bb192c0c150413d561","name":"about","slug":"about","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2021-04-10T03:06:59.000Z","updated_at":"2021-04-10T03:06:59.000Z"},{"id":"6425f3bb192c0c150413d562","name":"dns","slug":"dns","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2021-05-24T19:56:37.000Z","updated_at":"2021-05-24T19:56:37.000Z"},{"id":"6425f3bb192c0c150413d563","name":"cloudflare","slug":"cloudflare","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2021-05-24T19:56:37.000Z","updated_at":"2021-05-24T19:56:37.000Z"},{"id":"6425f3bb192c0c150413d564","name":"aws","slug":"aws","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2021-06-29T11:16:47.000Z","updated_at":"2021-06-29T11:16:47.000Z"},{"id":"6425f3bb192c0c150413d565","name":"storage","slug":"storage","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2021-06-29T11:16:47.000Z","updated_at":"2021-06-29T11:16:47.000Z"},{"id":"6425f3bb192c0c150413d566","name":"amazon","slug":"amazon","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2021-06-29T11:16:47.000Z","updated_at":"2021-06-29T11:16:47.000Z"},{"id":"6425f3bb192c0c150413d567","name":"ec2","slug":"ec2","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2021-06-29T11:16:47.000Z","updated_at":"2021-06-29T11:16:47.000Z"},{"id":"6425f3bb192c0c150413d568","name":"cli","slug":"cli","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2021-06-29T11:16:47.000Z","updated_at":"2021-06-29T11:16:47.000Z"},{"id":"6425f3bb192c0c150413d569","name":"mail","slug":"mail","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2021-08-15T01:19:04.000Z","updated_at":"2021-08-15T01:19:04.000Z"},{"id":"6425f3bb192c0c150413d56a","name":"proxy","slug":"proxy","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2021-12-10T18:46:47.000Z","updated_at":"2021-12-10T18:46:47.000Z"},{"id":"6425f3bb192c0c150413d56b","name":"codefresh","slug":"codefresh","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2021-12-14T01:01:04.000Z","updated_at":"2021-12-14T01:01:04.000Z"},{"id":"6425f3bb192c0c150413d56c","name":"google cloud","slug":"google-cloud","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2021-12-14T01:01:04.000Z","updated_at":"2021-12-14T01:01:04.000Z"},{"id":"6425f3bb192c0c150413d56d","name":"mkdocs","slug":"mkdocs","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2021-12-14T01:01:04.000Z","updated_at":"2021-12-14T01:01:04.000Z"},{"id":"6425f3bb192c0c150413d56e","name":"markdown","slug":"markdown","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2021-12-14T01:01:04.000Z","updated_at":"2021-12-14T01:01:04.000Z"},{"id":"6425f3bb192c0c150413d56f","name":"openstack","slug":"openstack","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2022-03-02T23:23:03.000Z","updated_at":"2022-03-02T23:23:03.000Z"},{"id":"6425f3bb192c0c150413d570","name":"cloud-init","slug":"cloud-init","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2022-03-02T23:23:03.000Z","updated_at":"2022-03-02T23:23:03.000Z"},{"id":"6425f3bb192c0c150413d571","name":"careers","slug":"careers","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2022-03-02T23:56:59.000Z","updated_at":"2022-03-02T23:56:59.000Z"},{"id":"6425f3bb192c0c150413d572","name":"devops","slug":"devops","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2022-03-02T23:56:59.000Z","updated_at":"2022-03-02T23:56:59.000Z"},{"id":"6425f3bb192c0c150413d573","name":"iac","slug":"iac","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2022-03-03T20:15:09.000Z","updated_at":"2022-03-03T20:15:09.000Z"},{"id":"6425f3bb192c0c150413d574","name":"wasabi","slug":"wasabi","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2022-03-03T20:15:09.000Z","updated_at":"2022-03-03T20:15:09.000Z"},{"id":"6425f3bb192c0c150413d575","name":"s3","slug":"s3","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2022-03-03T20:15:09.000Z","updated_at":"2022-03-03T20:15:09.000Z"},{"id":"6425f3bb192c0c150413d576","name":"iam","slug":"iam","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2022-03-03T20:15:09.000Z","updated_at":"2022-03-03T20:15:09.000Z"},{"id":"6425f3bb192c0c150413d577","name":"python","slug":"python","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2022-04-14T17:32:23.000Z","updated_at":"2022-04-14T17:32:23.000Z"},{"id":"6425f3bb192c0c150413d578","name":"opsec","slug":"opsec","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2022-04-14T17:32:23.000Z","updated_at":"2022-04-14T17:32:23.000Z"},{"id":"6425f3bb192c0c150413d579","name":"grocy","slug":"grocy","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2022-04-27T15:57:40.000Z","updated_at":"2022-04-27T15:57:40.000Z"},{"id":"6425f3bb192c0c150413d57a","name":"selfhosted","slug":"selfhosted","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2022-04-27T15:57:40.000Z","updated_at":"2022-04-27T15:57:40.000Z"},{"id":"6425f3bb192c0c150413d57b","name":"barcodes","slug":"barcodes","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2022-04-27T15:57:40.000Z","updated_at":"2022-04-27T15:57:40.000Z"},{"id":"6425f3bb192c0c150413d57c","name":"reverse engineering","slug":"reverse-engineering","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2022-08-24T22:03:08.000Z","updated_at":"2022-08-24T22:03:08.000Z"},{"id":"6425f3bb192c0c150413d57d","name":"BeReal","slug":"bereal","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2022-08-24T22:03:08.000Z","updated_at":"2022-08-24T22:03:08.000Z"},{"id":"6425f3bb192c0c150413d57e","name":"API","slug":"api","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2022-08-24T22:03:08.000Z","updated_at":"2022-08-24T22:03:08.000Z"},{"id":"6425f3bb192c0c150413d57f","name":"documentation","slug":"documentation","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2022-09-30T00:14:42.000Z","updated_at":"2022-09-30T00:14:42.000Z"},{"id":"6425f3bb192c0c150413d580","name":"docs as code","slug":"docs-as-code","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2022-09-30T00:14:42.000Z","updated_at":"2022-09-30T00:14:42.000Z"},{"id":"6425f3bb192c0c150413d581","name":"year in review","slug":"year-in-review","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2022-11-24T18:16:34.000Z","updated_at":"2022-11-24T18:16:34.000Z"},{"id":"6425f3bb192c0c150413d582","name":"free","slug":"free","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2022-12-29T23:12:57.000Z","updated_at":"2022-12-29T23:12:57.000Z"},{"id":"6425f3bb192c0c150413d583","name":"business","slug":"business","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2022-12-29T23:12:57.000Z","updated_at":"2022-12-29T23:12:57.000Z"},{"id":"6425f3bb192c0c150413d584","name":"GitHub actions","slug":"github-actions","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2023-03-05T22:17:41.000Z","updated_at":"2023-03-05T22:17:41.000Z"},{"id":"6425f3bb192c0c150413d585","name":"nginx","slug":"nginx","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2023-03-05T22:32:00.000Z","updated_at":"2023-03-05T22:32:00.000Z"},{"id":"6425f3bb192c0c150413d586","name":"k3s","slug":"k3s","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2023-03-15T00:01:09.000Z","updated_at":"2023-03-15T00:01:09.000Z"},{"id":"6425f3bb192c0c150413d587","name":"kubernetes","slug":"kubernetes","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2023-03-15T00:01:09.000Z","updated_at":"2023-03-15T00:01:09.000Z"},{"id":"6425f3bb192c0c150413d588","name":"#Import 2023-03-30 20:40","slug":"hash-import-2023-03-30-20-40","description":null,"feature_image":null,"parent_id":null,"visibility":"internal","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2023-03-30T20:40:28.000Z","updated_at":"2023-03-30T20:40:28.000Z"},{"id":"64408071c9cae386d9c53bf6","name":"podman","slug":"podman","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2023-04-19T23:59:45.000Z","updated_at":"2023-04-19T23:59:45.000Z"},{"id":"64b3412fc9cae386d9c53cfa","name":"dependabot","slug":"dependabot","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2023-07-16T01:00:31.000Z","updated_at":"2023-07-16T01:00:31.000Z"},{"id":"652c62afc9cae386d9c53e7f","name":"gke","slug":"gke","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2023-10-15T22:07:43.000Z","updated_at":"2023-10-15T22:07:43.000Z"},{"id":"652c62afc9cae386d9c53e80","name":"google secret manager","slug":"google-secret-manager","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2023-10-15T22:07:43.000Z","updated_at":"2023-10-15T22:07:43.000Z"},{"id":"652c62afc9cae386d9c53e81","name":"gcp","slug":"gcp","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2023-10-15T22:07:43.000Z","updated_at":"2023-10-15T22:07:43.000Z"},{"id":"68cb5535b1baac19a807618a","name":"homelab-v2","slug":"homelab-v2","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2025-09-18T00:41:25.000Z","updated_at":"2025-09-18T00:41:25.000Z"}],"users":[{"id":"1","name":"Bradley Stannard","slug":"bradley","password":"$2a$10$H5mVyC7Alcz3wcJZsTmvp.BX7a44r1gojqfxraPl2ilVs5OF5HUkW","email":"bradley@breadnet.co.uk","profile_image":"__GHOST_URL__/content/images/2023/03/bradley.jpg","cover_image":"__GHOST_URL__/content/images/2023/03/bradley-banner.JPG","bio":"UK based cloud engineer writing about his cloud journey ","website":null,"location":"Reading, UK","facebook":null,"twitter":null,"accessibility":"{\"nightShift\":true,\"whatsNew\":{\"lastSeenDate\":\"2025-08-04T08:10:56.000+00:00\"}}","status":"active","locale":null,"visibility":"public","meta_title":null,"meta_description":null,"tour":null,"last_seen":"2025-10-21T16:00:15.000Z","comment_notifications":1,"free_member_signup_notification":1,"paid_subscription_started_notification":1,"paid_subscription_canceled_notification":0,"mention_notifications":1,"milestone_notifications":1,"created_at":"2023-03-30T20:26:30.000Z","updated_at":"2025-10-21T16:00:15.000Z"},{"id":"6425f3ba192c0c150413d544","name":"thatonesysadmin","slug":"thatonesysadmin","password":"$2a$10$QBiHP7OOgO1oaQE.T1tQYOnir.6oEMalxiTBS.9ISdlkd4lt2MBIe","email":"breadnet@thatonesysadmin.com","profile_image":null,"cover_image":null,"bio":"Programmer by trade, Security expert by passion. ","website":"http://thatonesysadmin.com","location":"The USA","facebook":null,"twitter":null,"accessibility":null,"status":"locked","locale":null,"visibility":"public","meta_title":null,"meta_description":null,"tour":null,"last_seen":"2020-08-09T17:51:28.000Z","comment_notifications":1,"free_member_signup_notification":1,"paid_subscription_started_notification":1,"paid_subscription_canceled_notification":0,"mention_notifications":1,"milestone_notifications":1,"created_at":"2020-07-19T01:38:28.000Z","updated_at":"2020-08-10T21:52:56.000Z"}]}}]}
